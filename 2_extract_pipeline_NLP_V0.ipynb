{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <mark> <b> > 2.0 </b> Pipeline de Extracao de dados de documentos - NLP </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2_extract_pipeline_NLP_V0.ipynb</b>    |     Atual notebook com as funçoes para processamento de documentos com soluçao NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules e config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import platform\n",
    "import subprocess\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from urllib import response\n",
    "\n",
    "from outlook_msg import Message\n",
    "import extract_msg\n",
    "import zipfile\n",
    "from pyunpack import Archive\n",
    "import py7zr\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import PyPDF2\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "import locale\n",
    "import time, copy\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import cv2\n",
    "import fitz  # Módulo PyMuPDF\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import Image, ImageDraw\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar\n",
    "import matplotlib.pyplot as plt\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Token\n",
    "from spacy.language import Language\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "# Modulos da solucao\n",
    "# import modules.extrai_pdf_pesquisavel as Extc\n",
    "import modules.cronometro as cron\n",
    "import modules.nova_extracao_pdf_pesquisavel as novaextra \n",
    "import modules.trata_model as tmod\n",
    "import modules.trata_pdf as tpdf\n",
    "import modules.utils as utl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = nlp.remove_pipe('ner')\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt_BR.utf8'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. XXX Path para planilha de processamento de batches\n",
    "conf_export_plan_path = 'processamentos/processamento_batches/df_conf_export_batch.xlsx'\n",
    "\n",
    "\n",
    "\n",
    "# 2. XXX  Tratando nome de carga do df_processamento\n",
    "map_analise_path = \"processamentos/mapeamento_analise\"\n",
    "\n",
    "# 3. XXX  prefixo de nome do arquivo de exportaçao\n",
    "df_root_pipe_file = \"df_root_\"\n",
    "\n",
    "\n",
    "\n",
    "# 6. IMPORTANTE - MUDOU - Path para gestao de imagens resized\n",
    "image_resized_path = \"processamentos/temp/images/processadas\"\n",
    "\n",
    "\n",
    "#### Config - E-mail\n",
    "# 1. Caminho do arquivo uma mensagem especifica\n",
    "msg_dir_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails'\n",
    "\n",
    "# 2. Path para arquivos atachados compactados\n",
    "msg_attachment_zip = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/13_attachments'\n",
    "\n",
    "\n",
    "#### Config - messages\n",
    "# 3. Caminho do arquivo uma mensagem especifica\n",
    "msg_outros_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/12_messages'\n",
    "\n",
    "# 4. Path para arquivos recebidos manualmente\n",
    "arquivos_recebidos_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/14_documentos_recebidos'\n",
    "\n",
    "\n",
    "####Config Processamento Pipeline\n",
    "\n",
    "# 5. Path para documentos para extracao\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 7. path para arquivos json\n",
    "json_path = \"pipeline_extracao_documentos/5_documentos_processados/jsons\"\n",
    "\n",
    "# 7. Path para DFs e CSVs exportados\n",
    "export_path = \"pipeline_extracao_documentos/6_geral_administacao/exports\"\n",
    "\n",
    "# 8. Path para lixeira\n",
    "root_garbage_path = \"pipeline_extracao_documentos/0_lixeira\"\n",
    "\n",
    "\n",
    "#### paths de objetos para criacao/gestao (dicionarios/datasets)\n",
    "cnae_dict_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets/CNAE_X_ITEM_SERVICO_PREFEITURAS.xlsx\"\n",
    "\n",
    "\n",
    "# 12. poppler path\n",
    "poppler_path = \"/home/dani-boy/miniconda3/envs/tables-detr/bin\"\n",
    "\n",
    "# 13. path para config Tesseract\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "#Modelo atual\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "# definindo localizadcao para pt_BR\n",
    "locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")\n",
    "\n",
    "# logging.basicConfig(\n",
    "#     filename='config/log_ocorrencias.log',\n",
    "#     level=logging.INFO, \n",
    "#     format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "#     datefmt='%d/%m/%Y %H:%M:%S'\n",
    "# )\n",
    "\n",
    "# logging.info(\"kernel reiniciado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcoes originais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>A.</b> Funcoes de Imagem </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX IMPORTANTE - ESTA E A FUNCAO PARA SER UTILIZADA: POIS CONVERTE PARA CINZA E RESIZE: (4134, 5846)\n",
    "def convert_resize_gray(original_file_name, file_path, image_resized_path):\n",
    "\n",
    "    name_image = utl.conv_filename_no_ext(original_file_name)\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(name_image)}.jpg')\n",
    "    pages = convert_from_path(file_path, 500, poppler_path=poppler_path)\n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((4134, 5846))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "    imagem_gray = resized_pages[0].convert('L')\n",
    "    imagem_gray.save(image_resized_name, 'JPEG')\n",
    "\n",
    "    return  imagem_gray, image_resized_name\n",
    "\n",
    "# XXX Pequenos mas poderosos\n",
    "def extract_text_PIL(image, coordinates):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    image_croped = image.crop((x0, y0, x1, y1))\n",
    "    texto_extraido = pytesseract.image_to_string(image_croped, lang='por', config='--psm 6')\n",
    "    return texto_extraido \n",
    "\n",
    "\n",
    "# 5. XXX Ajusta textoYYY\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>B.</b> Funcoes de Frames </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funçao importante para buscar coordenadas do frame em funçao do contexto\n",
    "def get_coordinates_filter_by_context(pdf_pesquisavel_map, model_map, context_mapping, tipo):\n",
    "    \n",
    "    row_frame = utl.filtrar_df(frames_nf_v4_df, model=model_map, context_mapping=context_mapping, type=tipo)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [((row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p']) if pdf_pesquisavel_map else (row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1']))]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates\n",
    "\n",
    "\n",
    "\n",
    "# XXXpara buscar melhor as coordendas dos FRAMES\n",
    "def get_coordinates_filter(pdf_pesquisavel_map, model, tipo, label, section):\n",
    "    \n",
    "    row_frame = utl.filtrar_df(frames_nf_v4_df, model=model, type=tipo, label=label, section_json=section)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [((row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p']) if pdf_pesquisavel_map else (row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1']))]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>C.</b> Funcoes de Processamento e Extracao </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.A Dados iniciais - PDF PESQUISAVEL\t\n",
    "def pesquisa_prefeitura_pdf_pesquisavel(idx, row, row_info, map_directory, original_file_name, file_path, debug):    \n",
    "    \n",
    "    \n",
    "   # Carregar o arquivo PDF\n",
    "    pdf_document = fitz.open(file_path)\n",
    "\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 4\n",
    "    x1 = 600\n",
    "    y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    \n",
    "    if debug:\n",
    "        print(f'\\ndentro da funçao: pesquisa_prefeitura_pdf_pesquisavel: doc.:{original_file_name} | diretorio: {map_directory}  text: \\n\\n{text}\\n\\n')\n",
    "    \n",
    "    if text:\n",
    "       page_number = 0\n",
    "       #print(page_number)\n",
    "    else:\n",
    "       page_number = 1\n",
    "       #print(page_number)\n",
    "    \n",
    "    pdf_document.close()\n",
    "   \n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 0. INFOMACOES INICIAIS - RASTER PDF\n",
    "def processar_dados_iniciais(idx, row, row_info, section, map_directory, original_file_name, file_path, debug):\n",
    "    \n",
    "    # lista_texto_extraido = []\n",
    "\n",
    "    nf_dados_doc = {}\n",
    "    nf_dados_doc['secao'] = section\n",
    "    pdf_pesquisavel = None\n",
    "    extracted_txt = pesquisa_prefeitura_pdf_pesquisavel(idx, row, row_info, map_directory, original_file_name, file_path, debug)\n",
    "    if debug:\n",
    "        print(f'\\n1. funcao: processar_dados_iniciais: doc.:{original_file_name} | diretorio: {map_directory} apos funcao: pesquisa_prefeitura_pdf_pesquisavel: extracted_txt:\\n{extracted_txt}\\n\\n')\n",
    "    \n",
    "    if extracted_txt:\n",
    "        pdf_pesquisavel = True\n",
    "        print(f'extracted_txt: {extracted_txt} - portanto pdf_pesquisavel: {pdf_pesquisavel} ')\n",
    "        \n",
    "    else:\n",
    "        pdf_pesquisavel = False\n",
    "        print(f'extracted_txt: {extracted_txt} - portanto pdf_pesquisavel: {pdf_pesquisavel} ') \n",
    "       \n",
    "       \n",
    "        # WTF\n",
    "        x0 = 220\n",
    "        y0 = 0\n",
    "        x1= 3858\n",
    "        y1 = 1572\n",
    "        \n",
    "        # usando novo processo que gera o arquivo \"on the fly\" imagem_gray (converte PDF para imagem de tamanho grande (4134, 5846) - torna-a cinza e a salva)\n",
    "        imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "        extracted_txt = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "        #print(f'extracted_txt: {extracted_txt}')\n",
    "        if debug:\n",
    "            print(f'\\n2. funcao: processar_dados_iniciaisdoc.:{original_file_name} | diretorio: {map_directory}  apos : extract_text_PIL: extracted_txt:\\n{extracted_txt}\\n\\n')\n",
    "    \n",
    "    nf_dados_doc['file_name'] = original_file_name    \n",
    "    nf_dados_doc['pdf_pesquisavel'] = pdf_pesquisavel \n",
    "    value = {}   \n",
    "    texto_tratado = texto_extraido(extracted_txt)\n",
    "    value = define_dados_iniciais(idx, row, row_info, texto_tratado, debug)\n",
    "    if debug:\n",
    "        print(f'\\n3. funcao: processar_dados_iniciais doc.:{original_file_name} | diretorio: {map_directory} | apos funcao: define_dados_iniciais() value \\n{value}\\n\\n')\n",
    "    if value:\n",
    "        nf_dados_doc.update(value)\n",
    "   \n",
    "\n",
    "\n",
    "    return nf_dados_doc\n",
    "\n",
    "\n",
    "# 1.B CABECALHO XXX Funcoes de extracao -cabecalho Raster\n",
    "def processar_cabecalho_R_PDF(idx, row, row_info, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "    \n",
    "    data_box_valores = {}\n",
    "    data_box_conferencia = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    \n",
    "    batch_name_row_info = row_info.get('batch')\n",
    "    #status_documento_row_info = row_info.get('status_documento')\n",
    "    information_row_info = row_info.get('informations')\n",
    "    action_item_row_info = row_info.get('action_item')\n",
    "    \n",
    "    # Busco a imagem np do documento\n",
    "    image_np_row_info = row_info.get('image_np')\n",
    "    \n",
    "    data_box_valores['action_item'] = action_item_row_info\n",
    "    data_box_valores['informations'] = information_row_info\n",
    "    data_box_valores['processo'] = context_mapping\n",
    "    data_box_valores['conf_cod'] = 0\n",
    "\n",
    "\n",
    "                     \n",
    "    \n",
    "    # busco coordenadas para o contexto\n",
    "    if mapping_method == \"frame_&_sframe_field\":\n",
    "        tipo_4_coordinates = \"frame\"\n",
    "        tipo_4_filter = \"sframe_field\"\n",
    "    \n",
    "    #print(f'\\n2. Dentro func: section: {section} mapping_method: {mapping_method} | context_mapping: {context_mapping} | model_map: {model_map} | original_file_name: {original_file_name}\\n')\n",
    "   \n",
    "    # 2. usando a funcao de extracao de coordenadas por contexto    \n",
    "    coordinates = get_coordinates_filter_by_context(pdf_pesquisavel_map, model_map, context_mapping, tipo_4_coordinates)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    #print(f'x0: {x0} | y0: {y0} | x1: {x1} | y1: {y1}')\n",
    "    x0 = int(x0)\n",
    "    y0 = int(y0)\n",
    "    x1 = int(x1)\n",
    "    y1 = int(y1) \n",
    "    # 3. Cropo a imagem - novo modelo\n",
    "    cropped_image_np = image_np_row_info[y0:y1, x0:x1] # ajustar nos demais\n",
    "    data_box_conferencia[f'box_{context_mapping}'] = cropped_image_np\n",
    "    data_box_conferencia[f'coordinates_{context_mapping}'] = coordinates\n",
    "    # 4. Converto para PIL\n",
    "    cropped_image_pil = Image.fromarray(cropped_image_np)\n",
    "    # 6. Executo OCR\n",
    "    texto_extraido = pytesseract.image_to_string(cropped_image_pil, lang='por')\n",
    "    # 7. Trato o texto extraido = text_splited\n",
    "    text_splited = texto_extraido_cabecalho(texto_extraido)\n",
    "    if debug:\n",
    "        print()\n",
    "        plt.imshow(cropped_image_np)\n",
    "        plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "        plt.show()\n",
    "        print(f'\\ncoordinates {coordinates} - \\ntexto_extraido:\\n{text_splited}\\n')\n",
    "        \n",
    "    # 8. Efetuo o filtro para a iteracao\n",
    "    filtered_frame_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['context_mapping'] == context_mapping) & (frames_nf_v4_df['type'] == tipo_4_filter)]\n",
    "    \n",
    "    # 9. iter sobre o filtro\n",
    "    for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "        try:\n",
    "            section = row_frame['section_json']\n",
    "            label = row_frame['label']\n",
    "            reference = row_frame['reference']\n",
    "            string_pesquisa = row_frame['marcador_inicio']  \n",
    "            keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_box_valores[label] = texto\n",
    "            if debug:\n",
    "               print(f'\\nidx: {index_frame:> 3} | label: {label} |  string_pesquisa:{string_pesquisa} | dentro do try do raster PDF cabecalho - texto: \\n{texto}\\n\\n')\n",
    "        except Exception as e:\n",
    "            msg = (f\"{e}\")\n",
    "            data_box_conferencia[label] = msg\n",
    "    \n",
    "\n",
    "    # Verificações após o loop\n",
    "    for key, value in data_box_valores.items():\n",
    "        if key == 'numero_nota_fiscal' and value is None:\n",
    "            action_item_row_info = 'BREAK_PROCESS'\n",
    "            information_row_info = 'Número da Nota não encontrado'\n",
    "            #logging.error(f\" {batch_name} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")  # Ou registre o erro de outra forma que preferir\n",
    "        \n",
    "        elif key == 'codigo_verificacao' and value != None:\n",
    "            codigo_verificacao_nf = value\n",
    "            tam_codigo_verificacao = len(codigo_verificacao_nf)\n",
    "            data_box_valores['conf_cod'] = tam_codigo_verificacao\n",
    "            \n",
    "        \n",
    "        elif key != 'numero_nota_fiscal' and value is None:\n",
    "            logging.error(f\" {batch_name_row_info} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")  # Ou registre o erro de outra forma que preferir\n",
    "\n",
    "            \n",
    "      # if value is None:\n",
    "        #     logging.error(f\" {batch_name} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")  # Ou registre o erro de outra forma que preferir\n",
    "\n",
    "    data_box_valores['action_item'] = action_item_row_info\n",
    "    data_box_valores['informations'] = information_row_info\n",
    "\n",
    "    \n",
    "    return data_box_valores\n",
    "\n",
    "# 1.A CABECALHO - PDF PESQUISAVEL  \n",
    "def extrai_cabecalho_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "    \n",
    "    nf_data_cabecalho = {}\n",
    "    lista_erros = []\n",
    "    label = \"1_frame_dados_nf\"\n",
    "    \n",
    "    batch_name_row_info = row_info.get('batch')\n",
    "    information_row_info = row_info.get('informations')\n",
    "    action_item_row_info = row_info.get('action_item')\n",
    "    \n",
    "    nf_data_cabecalho['secao'] = section\n",
    "    nf_data_cabecalho['action_item'] = action_item_row_info\n",
    "    nf_data_cabecalho['informations'] = information_row_info\n",
    "    nf_data_cabecalho['processo'] = 'mapeamento regex - PDF pesquisavel'\n",
    "    \n",
    "    if debug:\n",
    "        print(f'\\n\\n2. dentro da funçao extrai_cabecalho_PDF: batch_name: {batch_name_row_info}\\n\\n')\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    tipo = \"frame\"\n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    \n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    if debug:\n",
    "        print(f'\\n3. x0: {x0}, y0: {y0}, x1: {x1}, y1: {y1} f_0: {f_0} f_1: {f_1} | text: \\n{text} \\n\\n')\n",
    "\n",
    "    try:\n",
    "        numero_nota_match = re.search(r'Número da Nota:\\s+(\\d+)', text)\n",
    "        if numero_nota_match:\n",
    "            numero_nf = numero_nota_match.group(1)\n",
    "            nf_data_cabecalho['numero_nota_fiscal'] = numero_nf\n",
    "            #nf_data_cabecalho['informations'] = 'documento com numero de nota fiscal'\n",
    "            if debug:\n",
    "                print(f'\\nnr_nro_nf: {nr_nro_nf} - doc: {original_file_name}\\n')\n",
    "        else:\n",
    "            msg = (f\"Número da Nota não encontrado\")\n",
    "            nf_data_cabecalho['numero_nota_fiscal'] = None\n",
    "            information_row_info = 'Número da Nota não encontrado'\n",
    "            nf_data_cabecalho['informations'] = information_row_info\n",
    "            action_item_row_info = 'BREAK_PROCESS'\n",
    "            nf_data_cabecalho['action_item'] = action_item_row_info\n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} | numero NF nao encontrado {e}\")\n",
    "        nf_data_cabecalho['numero_nota_fiscal'] = None\n",
    "        information_row_info = 'Número da Nota não encontrado'\n",
    "        nf_data_cabecalho['informations'] = information_row_info\n",
    "        action_item_row_info = 'BREAK_PROCESS'\n",
    "        nf_data_cabecalho['action_item'] = action_item_row_info\n",
    "\n",
    "    # Extrair Competência\n",
    "    competencia_match = re.search(r'Competência:\\s+(.+)', text)\n",
    "    if competencia_match:\n",
    "        nf_data_cabecalho['competencia'] = competencia_match.group(1)\n",
    "\n",
    "    # Extrair Data e Hora de Emissão\n",
    "    data_emissao_match = re.search(r'Data e Hora da Emissão:\\s+(.+)', text)\n",
    "    if data_emissao_match:\n",
    "        nf_data_cabecalho['dt_hr_emissao'] = data_emissao_match.group(1)\n",
    "        \n",
    "    # Extrair codigo Verificacao\n",
    "    codigo_verificacao_match = re.search(r'Código Verificação:\\s+(.+)', text)\n",
    "    if codigo_verificacao_match:\n",
    "        codigo_verificacao_nf = codigo_verificacao_match.group(1)\n",
    "        nf_data_cabecalho['codigo_verificacao'] =  codigo_verificacao_nf\n",
    "        tam_codigo_verificacao = len(codigo_verificacao_nf)\n",
    "        nf_data_cabecalho['conf_cod'] = tam_codigo_verificacao\n",
    "        \n",
    "    \n",
    "    \n",
    "    pdf_document.close()\n",
    "    \n",
    "    return nf_data_cabecalho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b>1.X</b> Funcoes NLP </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ent_new(text, patterns):\n",
    "    #nlp = spacy.blank(\"pt\")\n",
    "    #ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    ruler.add_patterns(patterns)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tokens = []\n",
    "    ents = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        span = doc.char_span(ent.start_char, ent.end_char, label=ent.label_)\n",
    "        ents.append(span)\n",
    "        \n",
    "    for token in doc:\n",
    "        start = token.idx\n",
    "        end = start + len(token)\n",
    "        tokens.append((token.text, start, end))\n",
    "        \n",
    "    return doc, tokens, ents\n",
    "\n",
    "\n",
    "\n",
    "# chunk.text, chunk.start, chunk.end, chunk.root.head.lemma_, chunk.root.dep_, chunk.doc\n",
    "def load_json(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ocrmypdf(input_file, output_file):\n",
    "    command = [\n",
    "        'ocrmypdf',\n",
    "        '--language', 'por',\n",
    "        '--deskew',\n",
    "        input_file,\n",
    "        output_file\n",
    "    ]\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"OCRmyPDF completed successfully. Output saved to {output_file}.\")\n",
    "    else:\n",
    "        print(f\"OCRmyPDF failed with error: {result.stderr.decode('utf-8')}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# Função para definir o atributo \"is_cnpj\"\n",
    "@Language.component(\"set_cnpj_attribute\")\n",
    "def set_cnpj_attribute(doc):\n",
    "    for i, token in enumerate(doc):\n",
    "        if i < len(doc) - 1:\n",
    "            next_token = doc[i + 1]\n",
    "            if token.shape_ == \"dd.ddd.ddd/\" and next_token.shape_ == \"dddd-dd\":\n",
    "                token._.is_cnpj = True\n",
    "                next_token._.is_cnpj = True\n",
    "            else:\n",
    "                token._.is_cnpj = False\n",
    "    return doc        \n",
    "\n",
    "\n",
    "# Registro do atributo 'is_cnpj'\n",
    "Token.set_extension('is_cnpj', force=True, default=False)\n",
    "\n",
    "\n",
    "# Função para aplicar o matcher\n",
    "@Language.component(\"apply_cnpj_matcher\")\n",
    "def apply_cnpj_matcher(doc):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        for token in span:\n",
    "            token._.is_cnpj = True\n",
    "    return doc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.A Extracao de texto de todo o documento - PDF PESQUISAVEL\t\n",
    "def extrai_texto_PDF_P(idx, row, row_info, section, map_directory, original_file_name, file_path, debug):    \n",
    "    \n",
    "   # Carregar o arquivo PDF\n",
    "    pdf_document = fitz.open(file_path)\n",
    "\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text_P = page.get_text(\"text\")\n",
    "    \n",
    "    pdf_document.close()\n",
    "    \n",
    "    texto_PDF_P = text_P.replace('\\n', ' ') \n",
    "    if debug:\n",
    "        print(f'\\nFUNC extrai_texto_PDF_P: doc.:{original_file_name} | diretorio: {map_directory}  texto_PDF_P: \\n\\n{texto_PDF_P}\\n\\n')\n",
    "\n",
    "    return texto_PDF_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"set_cnpj_attribute\") # Adicione esta etapa se você quiser definir o atributo manualmente\n",
    "\n",
    "nlp.add_pipe(\"apply_cnpj_matcher\")  # Adicione esta etapa para aplicar o matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matcher Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Matcher Patterns\n",
    "#======================================== 1. CABECALHO\n",
    "# 1. Número da Nota:\n",
    "numero_nota_pattern = [\n",
    "    {\"LOWER\": \"número\"},\n",
    "    {\"LOWER\": \"da\"},\n",
    "    {\"LOWER\": \"nota\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_DIGIT\": True}\n",
    "]\n",
    "matcher.add(\"numero_nota_fiscal\", [numero_nota_pattern])\n",
    "\n",
    "\n",
    "# 2. Competência:\n",
    "competencia_pattern = [\n",
    "    {\"LOWER\": \"competência\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"?\"},\n",
    "    {\"ORTH\": {\"REGEX\": \"^[A-Z][a-z]+/[0-9]{4}$\"}}   \n",
    "]    \n",
    "matcher.add(\"competencia\", [competencia_pattern])\n",
    "\n",
    "# 3. Data e Hora de Emissão:\n",
    "data_hora_emissao_pattern = [\n",
    "    {\"LOWER\": \"data\"},\n",
    "    {\"LOWER\": \"e\"},\n",
    "    {\"LOWER\": \"hora\"},\n",
    "    {\"LOWER\": \"da\"},\n",
    "    {\"LOWER\": \"emissão\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"SHAPE\": \"dd/dd/dddd\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"SHAPE\": \"dd:dd:dd\"}\n",
    "]\n",
    "matcher.add(\"dt_hr_emissao\", [data_hora_emissao_pattern])\n",
    "\n",
    "# 4. Código de Verificação:\n",
    "codigo_verificacao_pattern = [\n",
    "    {\"LOWER\": \"código\"},\n",
    "    {\"LOWER\": \"verificação\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_ASCII\": True, \"LENGTH\": 9}\n",
    "]\n",
    "matcher.add(\"codigo_verificacao\", [codigo_verificacao_pattern])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#========================================  5. VALOR TOTAL\n",
    "valor_total_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"total\"},\n",
    "    {\"LOWER\": \"da\", \"OP\": \"?\"},\n",
    "    {\"LOWER\": \"nota\", \"OP\": \"?\"},\n",
    "    {\"TEXT\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_TOTAL\", [valor_total_pattern])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#======================================== 7. VALORES E IMPOSTOS\n",
    "# 1. VALOR_SERVICOS\n",
    "valor_servicos_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"serviços\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "\n",
    "matcher.add(\"VALOR_SERVICOS\", [valor_servicos_pattern])\n",
    "\n",
    "\n",
    "# 2. VALOR DEDUÇÃO:\n",
    "valor_deducao_pattern = [\n",
    "    {\"LOWER\": \"dedução\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "\n",
    "matcher.add(\"VALOR_DEDUCAO\", [valor_deducao_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 3. DESC. INCOND: RASTER_PDF\n",
    "valor_incondR_pattern = [\n",
    "    {\"LOWER\": \"base\"},\n",
    "    {\"LOWER\": \"de\"},\n",
    "    {\"IS_SPACE\": True},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"ORTH\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}    \n",
    "]\n",
    "matcher.add(\"VALOR_INCONDP\", [valor_incondR_pattern])\n",
    "\n",
    "\n",
    "# 3.A DESC. INCOND: - PDF_Pesquisavel   #DESC. INCOND:\n",
    "valor_incond_patternP = [\n",
    "    {\"LOWER\": \"desc\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},\n",
    "    {\"LOWER\": \"incond\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"ORTH\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}    \n",
    "]\n",
    "matcher.add(\"VALOR_INCONDR\", [valor_incond_patternP])\n",
    "\n",
    "\n",
    "\n",
    "# 4. BASE DE CÁLCULO:  RASTER_PDF\n",
    "valor_calculoR_pattern = [\n",
    "    {\"LOWER\": \"cálculo\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_CALCULOR\", [valor_calculoR_pattern])\n",
    "\n",
    "\n",
    "# 4.A BASE DE CÁLCULO:  PDF_P\n",
    "valor_calculoP_pattern = [\n",
    "    {\"LOWER\": \"base\"},\n",
    "    {\"LOWER\": \"de\"},\n",
    "    {\"LOWER\": \"cálculo\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_CALCULOP\", [valor_calculoP_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 5. Alíquota d,dd\n",
    "valor_aliquota_pattern = [\n",
    "    {\"LOWER\": \"alíquota\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"d,dd\", \"OP\": \"?\"},\n",
    "    {\"ORTH\": \"%\"}\n",
    "\n",
    "]\n",
    "matcher.add(\"VALOR_ALIQUOTA\", [valor_aliquota_pattern])\n",
    "\n",
    "# 5.1 Alíquota d\n",
    "valor_aliquota2_pattern = [\n",
    "    {\"LOWER\": \"alíquota\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"d\", \"OP\": \"?\"},\n",
    "    {\"ORTH\": \"%\"}\n",
    "\n",
    "]\n",
    "matcher.add(\"VALOR_ALIQUOTA2\", [valor_aliquota2_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 6. VALOR ISS:\n",
    "valor_iss_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"iss\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_ISS\", [valor_iss_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 7. VALOR ISS RETIDO:\n",
    "valor_issretido_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"iss\"},\n",
    "    {\"LOWER\": \"retido\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_ISSRETIDO\", [valor_issretido_pattern])\n",
    "\n",
    "\n",
    "# 8. DESC. COND:\n",
    "valor_desccond_pattern = [\n",
    "    {\"LOWER\": \"desc\"},\n",
    "    {\"ORTH\": \".\"},\n",
    "    {\"LOWER\": \"cond\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_DESCCOND\", [valor_desccond_pattern])\n",
    "\n",
    "\n",
    "# 9. VALOR PIS:\n",
    "valor_pis_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"pis\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_PIS\", [valor_pis_pattern])\n",
    "\n",
    "\n",
    "# 10. VALOR COFINS:\n",
    "valor_cofins_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"cofins\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_COFINS\", [valor_cofins_pattern])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 11. VALOR IR:\n",
    "valor_ir_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"ir\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_IR\", [valor_ir_pattern])\n",
    "\n",
    "\n",
    "# 12. VALOR INSS:\n",
    "valor_inss_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"inss\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_INSS\", [valor_inss_pattern])\n",
    "\n",
    "\n",
    "# 13. VALOR CSLL:\n",
    "valor_csll_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"csll\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_CSLL\", [valor_csll_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 14. OUTRAS RETENÇÕES:\n",
    "valor_outrasreten_pattern = [\n",
    "    {\"LOWER\": \"outras\"},\n",
    "    {\"LOWER\": \"retenções\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_OUTRAS\", [valor_outrasreten_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 15. VALOR LÍQUIDO:\n",
    "valor_liquido_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"líquido\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_LIQUIDO\", [valor_liquido_pattern])\n",
    "\n",
    "\n",
    "\n",
    "#======================================== 9. OUTRAS INFORMAÇOES / CRITICAS\n",
    "# 1. EXIGIBILIDADE ISS\n",
    "exigibilidade_iss_pattern = [\n",
    "    {\"LOWER\": \"exigibilidade\"},\n",
    "    {\"LOWER\": \"iss\"},\n",
    "    {\"LOWER\": {\"IN\": [\"exigivel\", \"não exigivel\"]}}\n",
    "]\n",
    "matcher.add(\"EXIGIBILIDADE_ISS\", [exigibilidade_iss_pattern])\n",
    "\n",
    "\n",
    "# 2. REGIME TRIBUTAÇÃO\n",
    "padrao_regime_tributacao = [\n",
    "    {\"LOWER\": \"regime\"},\n",
    "    {\"LOWER\": \"tributação\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"+\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"*\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": \"simples\", \"OP\": \"?\"},\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"REGIME_TRIBUTACAO\", [padrao_regime_tributacao])\n",
    "\n",
    "# 3. SIMPLES NACIONAL = NAO\n",
    "simples_nacional_nao_pattern = [\n",
    "    {\"LOWER\": \"simples\"},\n",
    "    {\"LOWER\": \"nacional\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": \"não\"}\n",
    "]\n",
    "matcher.add(\"SIMPLES_NACIONAL_NAO\", [simples_nacional_nao_pattern])\n",
    "\n",
    "# 3.1 SIMPLES NACIONAL = SIM\n",
    "simples_nacional_pattern = [\n",
    "    {\"LOWER\": \"simples\"},\n",
    "    {\"LOWER\": \"nacional\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": \"sim\", \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"ORTH\": \"(\", \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"ORTH\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"ORTH\": \"%\", \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"ORTH\": \")\", \"OP\": \"?\"}\n",
    "]\n",
    "matcher.add(\"SIMPLES_NACIONAL_SIM\", [simples_nacional_pattern])\n",
    "\n",
    "\n",
    "# 4. ISSQN RETIDO\n",
    "issqn_retido_pattern = [\n",
    "    {\"LOWER\": \"issqn\"},\n",
    "    {\"LOWER\": \"retido\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": {\"IN\": [\"sim\", \"não\"]}}\n",
    "]\n",
    "matcher.add(\"ISSQN_RETIDO\", [issqn_retido_pattern])\n",
    "\n",
    "\n",
    "# 5. LOCAL. PRESTAÇÃO SERVIÇO\n",
    "local_prestacao_servico_pattern = [\n",
    "    {\"LOWER\": \"local\"},\n",
    "    {\"ORTH\": \".\"},\n",
    "    {\"LOWER\": \"prestação\"},\n",
    "    {\"LOWER\": \"serviço\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"+\"},  # para lidar com múltiplos espaços\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"+\"},  # para a cidade\n",
    "    {\"ORTH\": \"-\", \"OP\": \"?\"},\n",
    "    {\"IS_UPPER\": True, \"LENGTH\": 2, \"OP\": \"?\"}  # para a sigla do estado\n",
    "]\n",
    "matcher.add(\"LOCAL_PRESTACAO_SERVICO\", [local_prestacao_servico_pattern])\n",
    "\n",
    "# 6. LOCAL INCIDÊNCIA\n",
    "local_incidencia_pattern = [\n",
    "    {\"LOWER\": \"local\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},\n",
    "    {\"LOWER\": \"incidência\"},\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"+\"},  # Nome da cidade\n",
    "    {\"ORTH\": \"-\", \"OP\": \"?\"},  # Hífen opcional\n",
    "    {\"SHAPE\": \"XX\", \"OP\": \"?\"}  # Sigla do estado\n",
    "]\n",
    "matcher.add(\"LOCAL_INCIDENCIA\", [local_incidencia_pattern])\n",
    "\n",
    "\n",
    "# observacao_pattern = [\n",
    "#     {\"LOWER\": \"observação\"},\n",
    "#     {\"ORTH\": \":\"},\n",
    "#     {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "#     {\"LOWER\": \"-\", \"OP\": \"?\"},\n",
    "#     {\"IS_PRINT\": True, \"OP\": \"+\"}\n",
    "# ]\n",
    "\n",
    "# matcher.add(\"OBSERVACAO\", [observacao_pattern])# 6. Alíquota\n",
    "valor_aliquota_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"iss\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"+\"},\n",
    "    {\"ORTH\": \"\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"},\n",
    "    {\"ORTH\": \"%\"}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "colors = {\n",
    "            \"secretaria\": \"linear-gradient(90deg, #2ADB5E, #1FA346)\", # Verde Degrade\n",
    "            \"tipo_documento\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\", #Azul medio degrade\n",
    "            \"nome_prefeitura\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\", # Roxo claro para lilaz - degrade bem bacana\n",
    "            \"nome_section\": \"linear-gradient(90deg, #FFA9FB, #BF7FBC)\", #  lilaz - Degrade\n",
    "            \"nome_section\": \"#FFEA7F\", # Laranja claro\n",
    "            \"SAFRA\": \"#CCA10C\", # Terracota\n",
    "            \"SAFRA\": \"#AB9BFC\", # Roxo claro \n",
    "            \"INTENT\": \"#7AECEC\", # Azul bem claro\n",
    "            \"NOME\": \"#EE8AF8\" # Rosa medio\n",
    "        }          \n",
    "\n",
    "patternsPrefeitura = [\n",
    "                        {\"label\": \"nome_prefeitura\", \"pattern\": [{\"LOWER\": \"prefeitura\"}, {\"LOWER\": \"municipal\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"mesquita\"}], \"id\": \"PM_MESQUITA\"},\n",
    "                        {\"label\": \"nome_prefeitura\", \"pattern\": [{\"LOWER\": \"prefeitura\"}, {\"LOWER\": \"municipal\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"mage\"}], \"id\": \"PM_MAGE\"},\n",
    "                        {\"label\": \"nome_prefeitura\", \"pattern\": [{\"LOWER\": \"prefeitura\"}, {\"LOWER\": \"municipal\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"sao\"}, {\"LOWER\": \"pedro\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"aldeia\"}], \"id\": \"PM_SPA\"}\n",
    "\n",
    "                        ]\n",
    "\n",
    "\n",
    "patternsSection = [\n",
    "                        {\"label\": \"nome_section\", \"pattern\": [{\"LOWER\": \"prestador\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"serviços\"}], \"id\": \"2. PRESTADOR DE SERVIÇO\"},\n",
    "                        {\"label\": \"nome_section\", \"pattern\": [{\"LOWER\": \"tomador\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"serviços\"}], \"id\": \"3. TOMADOR DE SERVIÇO\"},\n",
    "                        {\"label\": \"nome_section\", \"pattern\": [{\"LOWER\": \"discriminação\"}, {\"LOWER\": \"dos\"}, {\"LOWER\": \"serviços\"}], \"id\": \"4. DESCRIMINACAO DOS SERVIÇOS\"},\n",
    "                        {\"label\": \"nome_section\", \"pattern\": [{\"LOWER\": \"dados\"}, {\"LOWER\": \"complementares\"}], \"id\": \"8. DADOS COMPLEMENTARES\"},\n",
    "\n",
    "                        ]\n",
    "\n",
    "\n",
    "patternsSecretarias = [{\"label\": \"secretaria\", \"pattern\": [{\"LOWER\": \"secretaria\"}, {\"LOWER\": \"municipal\"}, {\"LOWER\": \"da\"}, {\"LOWER\": \"fazenda\"},], \"id\": \"SECRETARIA\"}] \n",
    "\n",
    "\n",
    "patternsTipoDocumento = [\n",
    "                        {\"label\": \"tipo_documento\", \"pattern\": [{\"LOWER\": \"nota\"}, {\"LOWER\": \"fiscal\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"serviços\"}, {\"LOWER\": \"eletrônica\"}, {\"LOWER\": \"-\"}, {\"LOWER\": \"nfs-e\"}], \"id\": \"NFS-e\"}\n",
    "                        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "patternsTemp = [\n",
    "                    {\"label\":\"TOTAL\", \"pattern\": [{\"LOWER\": \"quantidade\", \"OP\":\"?\"}, {\"LOWER\": \"total\", \"OP\":\"*\"}], \"id\": \"qtde-total\"},\n",
    "                    {\"label\":\"ENTREGUE\", \"pattern\": [{\"LOWER\": \"quantidade\", \"OP\":\"?\"},{\"LOWER\": \"entregue\",\"OP\":\"*\"}],\"id\": \"qtde-entregue\"},\n",
    "                    {\"label\":\"ENTREGUE\", \"pattern\": [{\"LOWER\": \"quantidade\", \"OP\":\"?\"},{\"LOWER\": \"entreguei\",\"OP\":\"*\"}],\"id\": \"qtde-entregue\"},\n",
    "                    {\"label\":\"ENTREGUE\", \"pattern\": [{\"LOWER\": \"foram\", \"OP\":\"?\"},{\"LOWER\": \"entregues\",\"OP\":\"*\"}],\"id\": \"qtde-entregue\"},\n",
    "                    {\"label\":\"SALDO\", \"pattern\": [{\"LOWER\": \"saldo\",\"OP\":\"*\"}], \"id\": \"qtde-saldo\"}]\n",
    "\n",
    "\n",
    "patternsOthers = [{\"label\": \"PERSON\", \"pattern\": \"Daniel\", \"id\": \"pessoa-daniel\"}] \n",
    " \n",
    "patternsCult = [\n",
    "    {\n",
    "        \"label\":\"CULTURA\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"soja\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"milho\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"sorgo\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"trigo\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"milheto\", \"OP\":\"?\"},  \n",
    "            \n",
    "        ],    \n",
    "    \"id\": \"cultura\"}]\n",
    "\n",
    "patternsQuant = [{\"label\":\"TOTAL\", \"pattern\": [{\"LOWER\": \"quantidade\", \"OP\":\"?\"}, {\"LOWER\": \"total\", \"OP\":\"*\"}], \"id\": \"qtde-total\"},\n",
    "                 {\"label\":\"ENTREGUE\", \"pattern\": [{\"LOWER\": \"quantidade\", \"OP\":\"?\"},{\"LOWER\": \"entregue\",\"OP\":\"*\"}],\"id\": \"qtde-entregue\"},\n",
    "                 {\"label\":\"ENTREGUE\", \"pattern\": [{\"LOWER\": \"quantidade\", \"OP\":\"?\"},{\"LOWER\": \"entreguei\",\"OP\":\"*\"}],\"id\": \"qtde-entregue\"},\n",
    "                 {\"label\":\"ENTREGUE\", \"pattern\": [{\"LOWER\": \"foram\", \"OP\":\"?\"},{\"LOWER\": \"entregues\",\"OP\":\"*\"}],\"id\": \"qtde-entregue\"},\n",
    "                 {\"label\":\"SALDO\", \"pattern\": [{\"LOWER\": \"saldo\",\"OP\":\"*\"}], \"id\": \"qtde-saldo\"}]\n",
    "\n",
    "\n",
    "patternsSafra = [{\"label\":\"SAFRA\", \"pattern\": [{\"LOWER\": \"safra\", \"OP\":\"?\"},{\"LOWER\": \"safras\", \"OP\":\"?\"}], \"id\": \"safra\"}]\n",
    "\n",
    "\n",
    "patternsNroSafra = [{\"label\":\"NR_SAF\", \"pattern\": [{\"SHAPE\": \"dd/dd\", \"OP\":\"*\"}], \"id\": \"nro_safra\"},\n",
    "                    {\"label\":\"NR_SAF\", \"pattern\": [{\"lower\": \"próxima\", \"OP\":\"*\"}], \"id\": \"nro_safra\"},\n",
    "                    {\"label\":\"NR_SAF\", \"pattern\": [{\"lower\": \"passada\", \"OP\":\"*\"}], \"id\": \"nro_safra\"}]   \n",
    "\n",
    "patternsCliente = [{\"label\": \"CLIENTE\", \"pattern\": [{\"LOWER\": \"berdinazzi\"}], \"id\": \"cli-berdinazzi\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"LOWER\": \"lopito\"}], \"id\": \"cli-lopito\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"LOWER\": \"bungue\"}], \"id\": \"cli-bungue\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"bunge\", \"bongue\", \"bumgue\"]}}}], \"id\": \"cli-bungue\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"berdinazi\"]}}}], \"id\": \"cli-berdinazzi\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"matarazzo\"]}}}], \"id\": \"cli-matarazzo\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"mezenga\"]}}}], \"id\": \"cli-mezenga\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"LOWER\": \"rei\"}, {\"LOWER\": \"do\"}, {\"LOWER\": \"gado\"}], \"id\": \"cli-reidogado\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"rei-do-gado\"]}}}], \"id\": \"cli-reidogado\"},   \n",
    "                   ]\n",
    "\n",
    "\n",
    "patternsContrato = [{\"label\": \"CONTRATO\", \"pattern\": [{\"LOWER\": \"contrato\", \"OP\":\"?\"}], \"id\": \"contrato\"},\n",
    "                    {\"label\": \"CONTRATO\", \"pattern\": [{\"LOWER\": \"contratos\", \"OP\":\"?\"}], \"id\": \"contrato\"}]\n",
    "\n",
    "patternsNroContrato = [{\"label\": \"NR_CONT\", \"pattern\": [{\"SHAPE\": \"dddX\", \"OP\":\"*\"}], \"id\": \"nro_contrato\"},\n",
    "                       {\"label\": \"NR_CONT\", \"pattern\": [{\"POS\": \"NUM\", \"SHAPE\": \"ddd\", \"OP\": \"*\"},\n",
    "                                                            {\"POS\": \"PROPN\", \"SHAPE\": \"X\", \"OP\": \"*\"}], \"id\": \"nro_contrato\"}]\n",
    "\n",
    "patternsIntent = [{\"label\": \"INTENT\", \"pattern\": [{\"IS_TITLE\": True, \"OP\":\"*\"}], \"id\": \"user-intent\"},\n",
    "                  {\"label\": \"INTENT\", \"pattern\": [{\"POS\": \"ADJ\", \"OP\":\"*\"}, {\"POS\": \"VERB\", \"OP\":\"*\"}], \"id\": \"user-intent\"},\n",
    "                  {\"label\": \"INTENT\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"preciso\", \"gostaria\", \"informar\"]}}}], \"id\": \"user-intent\"}]\n",
    "\n",
    "\n",
    "patterns = patternsPrefeitura + patternsSection + patternsSecretarias + patternsTipoDocumento \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valor = 'NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e'\n",
    "valor.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 - Processo de Extracao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>2.x</b> Templates e Dics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames_nf_v4_df: 2.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def define_dados_iniciais(idx, row, row_info, texto_tratado, debug):\n",
    "    \n",
    "    dados_iniciais_nf = {}\n",
    "    #status_documento_row_info = row_info.get('status_documento')\n",
    "    action_item_row_info = row_info.get('action_item')\n",
    "    information_row_info = row_info.get('informations')\n",
    "    \n",
    "    dados_iniciais_nf['action_item'] = action_item_row_info\n",
    "    dados_iniciais_nf['informations'] = information_row_info\n",
    "    \n",
    "    print(f'\\nDentro da func define_dados_iniciais:  -action_item_row_info: {action_item_row_info}')\n",
    "   \n",
    "\n",
    "\n",
    "    prefeitura_encontrada = None\n",
    "    de_para_encontrado = None\n",
    "\n",
    "    # 7. ZZZ Dicionário para mapear Prefeitura com sua sigla\n",
    "    de_para_prefeitura = {\n",
    "        \"PREFEITURA DA CIDADE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA DA CIDADE DE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA MUNICIPAL DE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\": \"PM_SPA\",\n",
    "        \"MUNICIPAL DE SAO PEDRO DA ALDEIA\": \"PM_SPA\",\n",
    "        \"PREFEITURA MUNICIPAL DE SAO PEDRO DA\\nALDEIA\": \"PM_SPA\",\n",
    "        \"PREFEITURA MUNICIPAL DE SAO PEDRO DA\": \"PM_SPA\",\n",
    "        \"PREFEITURA MUNICIPAL DE MESQUITA\": \"PM_MESQUITA\",\n",
    "        \"PREFEITURA MUNICIPAL DE DE MESQUITA\": \"PM_MESQUITA\",\n",
    "        # ... adicione \n",
    "    }\n",
    "    \n",
    "\n",
    "    templates = {\n",
    "        (\"PM_MAGE\", None): \"MAGE\",\n",
    "        (\"PM_SPA\", None): \"SPA\",\n",
    "        (\"PM_MESQUITA\", None): \"MESQUITA\",\n",
    "        (\"Pague agora com o seu Pix\", None): \"NAO_PROCESSAR\",\n",
    "        # ... adicione outras combinações aqui\n",
    "    }\n",
    "\n",
    "    cnpj_encontrado = None\n",
    "    # Verifique cada linha do texto\n",
    "    for linha in texto_tratado:\n",
    "        for pref in de_para_prefeitura.keys():\n",
    "            if pref in linha:\n",
    "                #print(linha)\n",
    "                prefeitura_encontrada = pref\n",
    "                dados_iniciais_nf['prefeitura'] = prefeitura_encontrada\n",
    "                if debug:\n",
    "                    print(f'\\n4.funcao: define_dados_iniciais(texto_tratado) - dentro do loop for de pesquisa prefeitura - prefeitura_encontrada: \\n{prefeitura_encontrada}\\n\\n')\n",
    "    # Saímos do loop, agora vamos verificar qual template usar\n",
    "    if prefeitura_encontrada:\n",
    "        de_para_pm = de_para_prefeitura.get(prefeitura_encontrada)\n",
    "        dados_iniciais_nf['de_para_pm'] = de_para_pm\n",
    "        if debug:\n",
    "            print(f'\\n5.funcao: define_dados_iniciais(texto_tratado) - if prefeitura_encontrada - de_para_pm \\n{de_para_pm}\\n\\n')\n",
    "        if not de_para_pm:\n",
    "            de_para_pm = de_para_prefeitura.get(prefeitura_encontrada, \"NAO_PROCESSAR\")\n",
    "            dados_iniciais_nf['de_para_pm'] = de_para_pm\n",
    "            #print(de_para_pm)\n",
    "    else:\n",
    "        de_para_pm = \"NAO_PROCESSAR\"\n",
    "        action_item_row_info = 'BREAK_PROCESS'\n",
    "        information_row_info = 'Nao identificado dados iniciais para o documento'\n",
    "        \n",
    "     \n",
    "        \n",
    "    # Verifique cada linha do texto\n",
    "    for linha in texto_tratado:\n",
    "        for de_para, cnpj in templates.keys():\n",
    "            if cnpj and cnpj in linha:\n",
    "                cnpj_encontrado = cnpj\n",
    "                dados_iniciais_nf['cnpj_encontrado'] = cnpj_encontrado\n",
    "                \n",
    "                \n",
    "    # Saímos do loop, agora vamos verificar qual template usar\n",
    "    if de_para_pm:\n",
    "        template_usar = templates.get((de_para_pm, cnpj_encontrado))\n",
    "        logging.info(f'usara template {template_usar} para: {cnpj_encontrado}')\n",
    "        # print(template_usar)\n",
    "        dados_iniciais_nf['model'] = template_usar\n",
    "        if not template_usar:\n",
    "            template_usar = templates.get((de_para_pm, None), \"TEMPLATE_NAO_ENCONTRADO\")\n",
    "            dados_iniciais_nf['model'] = 'NAO_ENC.' \n",
    "            action_item_row_info = 'BREAK_PROCESS'\n",
    "            information_row_info = 'model nao encontrado'\n",
    "    else:\n",
    "        template_usar = \"TEMPLATE_NAO_ENCONTRADO\"\n",
    "        dados_iniciais_nf['model'] = 'NAO_ENC.'\n",
    "        action_item_row_info = 'BREAK_PROCESS'\n",
    "        information_row_info = 'model nao encontrado'\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    #Confirmando se template existe em frames    \n",
    "    try:        \n",
    "        f_type = 'frame'\n",
    "        #template_usar = 'SAO_PEDRO_SUPERMIX'\n",
    "        result = filtrar_df(frames_nf_v4_df, type=f_type, de_para_pm=de_para_pm, model=template_usar)\n",
    "        model = result['model'].values[0]\n",
    "        if model:\n",
    "            template_oficial = model\n",
    "            if model == template_usar:\n",
    "                dados_iniciais_nf['model'] = template_oficial\n",
    "            else:    \n",
    "                template_usar = \"necessario cadastrar\"\n",
    "                dados_iniciais_nf['model'] = \"CADASTRAR\"\n",
    "                \n",
    "            dados_iniciais_nf['model'] = template_usar\n",
    "        else:\n",
    "            template_usar = \"necessario cadastrar\"\n",
    "            dados_iniciais_nf['model'] = \"CADASTRAR\"\n",
    "\n",
    "                \n",
    "    except Exception as e:\n",
    "       error_msg = (f\"Erro busca do template: {e}\") \n",
    "    \n",
    "    dados_iniciais_nf['action_item'] = action_item_row_info \n",
    "    dados_iniciais_nf['informations'] = information_row_info         \n",
    "        \n",
    "    return dados_iniciais_nf  \n",
    "\n",
    "\n",
    "\n",
    "nf_model_path = \"config/modelos/frames_nf_v11.xlsx\"\n",
    "\n",
    "#Le a planilha e cria do DF\n",
    "frames_nf_v4_df = pd.read_excel(nf_model_path)\n",
    "\n",
    "\n",
    "# Cria dicionários para armazenar diferentes tipos de elementos do modelo\n",
    "document_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'document'].iloc[0]\n",
    "boundaries_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'boundaries']\n",
    "sections_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'section']\n",
    "frames_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'frame']\n",
    "sframe_fields_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'sframe_field']\n",
    "field_boxes_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'field_box']\n",
    "\n",
    "ver = tmod.get_template_version(frames_nf_v4_df, 'MAGE')\n",
    "\n",
    "frames_nf_v4_df.head(5)\n",
    "\n",
    "print(f'frames_nf_v4_df: {ver}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Buscar proximo Batch caso nao esteja rodando email\n",
    "batch_name = utl.busca_proximo_batch(conf_export_plan_path)\n",
    "\n",
    "# 2. XXX Definiçao do path para salvar o arquivo\n",
    "file_path_root_pipe = os.path.join(map_analise_path, df_root_pipe_file + batch_name + \".xlsx\")\n",
    "\n",
    "\n",
    "#3. XXX Ler a planilha e cria df_documento_recebido\n",
    "df_root_pipe = pd.read_excel(file_path_root_pipe)\n",
    "\n",
    "\n",
    "#4. XXX  Ajustar o indice\n",
    "df_root_pipe.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "\n",
    "df_root_pipe.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>2.x</b> ExecuÇao do Pipeline de Extracao </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_cabecalho_PDF_P(idx, row, row_info, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "    \n",
    "    data_box_valores = {}\n",
    "    data_box_conferencia = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    \n",
    "    batch_name_row_info = row_info.get('batch')\n",
    "    #status_documento_row_info = row_info.get('status_documento')\n",
    "    information_row_info = row_info.get('informations')\n",
    "    action_item_row_info = row_info.get('action_item')\n",
    "    \n",
    "    # Busco a imagem np do documento\n",
    "    image_np_row_info = row_info.get('image_np')\n",
    "    \n",
    "    data_box_valores['action_item'] = action_item_row_info\n",
    "    data_box_valores['informations'] = information_row_info\n",
    "    data_box_valores['processo'] = context_mapping\n",
    "    data_box_valores['conf_cod'] = 0\n",
    "\n",
    "\n",
    "    # 8. Efetuo o filtro para a iteracao\n",
    "    filtered_frame_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['context_mapping'] == context_mapping) & (frames_nf_v4_df['type'] == tipo_4_filter)]\n",
    "    \n",
    "    # 9. iter sobre o filtro\n",
    "    for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "        try:\n",
    "            section = row_frame['section_json']\n",
    "            label = row_frame['label']\n",
    "            reference = row_frame['reference']\n",
    "            string_pesquisa = row_frame['marcador_inicio']  \n",
    "            keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_box_valores[label] = texto\n",
    "            if debug:\n",
    "               print(f'\\nidx: {index_frame:> 3} | label: {label} |  string_pesquisa:{string_pesquisa} | dentro do try do raster PDF cabecalho - texto: \\n{texto}\\n\\n')\n",
    "        except Exception as e:\n",
    "            msg = (f\"{e}\")\n",
    "            data_box_conferencia[label] = msg\n",
    "    \n",
    "\n",
    "    # Verificações após o loop\n",
    "    for key, value in data_box_valores.items():\n",
    "        if key == 'numero_nota_fiscal' and value is None:\n",
    "            action_item_row_info = 'BREAK_PROCESS'\n",
    "            information_row_info = 'Número da Nota não encontrado'\n",
    "            #logging.error(f\" {batch_name} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")  # Ou registre o erro de outra forma que preferir\n",
    "        \n",
    "        elif key == 'codigo_verificacao' and value != None:\n",
    "            codigo_verificacao_nf = value\n",
    "            tam_codigo_verificacao = len(codigo_verificacao_nf)\n",
    "            data_box_valores['conf_cod'] = tam_codigo_verificacao\n",
    "            \n",
    "        \n",
    "        elif key != 'numero_nota_fiscal' and value is None:\n",
    "            logging.error(f\" {batch_name_row_info} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")  # Ou registre o erro de outra forma que preferir\n",
    "\n",
    "            \n",
    "      # if value is None:\n",
    "        #     logging.error(f\" {batch_name} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")  # Ou registre o erro de outra forma que preferir\n",
    "\n",
    "    data_box_valores['action_item'] = action_item_row_info\n",
    "    data_box_valores['informations'] = information_row_info\n",
    "\n",
    "    \n",
    "    return data_box_valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 9. iterar sobre o filtro\n",
    "for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "    # ... (seu código anterior)\n",
    "\n",
    "    raw_value = next((doc[start:end].text for match_id, start, end in matches if nlp.vocab.strings[match_id] == label), None)\n",
    "    \n",
    "    # Encontrar o marcador mais semelhante no valor bruto\n",
    "    most_similar_reference = max([reference], key=lambda x: similar(x, raw_value))\n",
    "    \n",
    "    # Substituir o marcador mais semelhante\n",
    "    final_value = raw_value.replace(most_similar_reference, \"\").strip()\n",
    "    \n",
    "    print(f'\\nidx: {index_frame:> 3} | label: {label} |  reference: {most_similar_reference} - raw_value: {raw_value} | final_value: {final_value} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_23/MESQUITA_PDF_31282023_2258/159871/2023 -4.pdf'\n",
    "original_file_name = os.path.basename(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = \"1. CABECALHO\"\n",
    "mapping_method = \"frame_&_sframe_field\" # significa que as coordenadas estao em frames e os valores dos campos nos sframe_fields\n",
    "context_mapping = \"data_cabecalho\"\n",
    "def_replace = True \n",
    "model_map = 'MESQUITA'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_cabecalho_PDF_P(idx, row, row_info, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "    data_box_valores = {}\n",
    "    if mapping_method == \"frame_&_sframe_field\":\n",
    "        tipo_4_coordinates = \"frame\"\n",
    "        tipo_4_filter = \"sframe_field\"\n",
    "\n",
    "    # 8. Efetuo o filtro para a iteracao\n",
    "    filtered_frame_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['context_mapping'] == context_mapping) & (frames_nf_v4_df['type'] == tipo_4_filter)]\n",
    "\n",
    "    # 9. iter sobre o filtro\n",
    "    for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "        section = row_frame['section_json']\n",
    "        label = row_frame['label']\n",
    "        reference = row_frame['reference']\n",
    "        string_pesquisa = row_frame['marcador_inicio']  \n",
    "        \n",
    "        raw_value = next((doc[start:end].text for match_id, start, end in matches if nlp.vocab.strings[match_id] == label), None)\n",
    "        \n",
    "        most_similar_reference = max([reference], key=lambda x: similar(x, raw_value))\n",
    "        ##print(f'\\nmost_similar_reference: {most_similar_reference}\\n')\n",
    "        final_value = raw_value.split(\":\", 1)[-1].strip()\n",
    "        data_box_valores[label] = final_value\n",
    "\n",
    "    return data_box_valores        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numero_nota_fiscal': '20234',\n",
       " 'competencia': 'Julho/2023',\n",
       " 'dt_hr_emissao': '27/07/2023 15:11:00',\n",
       " 'codigo_verificacao': '4FDA9FBAE'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_box_valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Número da Nota: 20234'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numero_nota_fiscal_value = next((doc[start:end].text for match_id, start, end in matches if nlp.vocab.strings[match_id] == 'numero_nota_fiscal'), None)\n",
    "numero_nota_fiscal_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta e o principio da melhor funcao do mundo\n",
    "def extracao_pipeline(qualquer_df, fase, atividade, status, debug=False, prestador=True, tomador=True, servicos=True, total=True, cnae=True, valores_impostos=True, complementares=True, outras_informacoes=True, observacoes=True):\n",
    "    \n",
    "    doc_info = {}\n",
    "    resumo = {}\n",
    "    row_teste_info = []\n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    func_fase = fase\n",
    "    func_atividade = atividade\n",
    "    func_status = status\n",
    "    lista_dicts = []\n",
    "    conf_processo = {}\n",
    "    lista_conferencia = []\n",
    "   \n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        dados_iniciais = {}\n",
    "        row_info = row.to_dict()\n",
    "        message_erro = []\n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        map_document_unique_id = idx\n",
    "        map_seq = row['seq']\n",
    "        map_batch_name = row['batch']\n",
    "        map_fase_processo = row['fase_processo']\n",
    "        map_nome_atividade = row['nome_atividade']\n",
    "        map_status_documento = row['status_documento']\n",
    "        map_original_file_name = row['original_file_name']\n",
    "        map_directory = row['directory']\n",
    "        map_one_page = row['one_page']\n",
    "        map_palavra_chave = row['palavra_chave']\n",
    "        map_document_tag = row['document_tag']\n",
    "        map_action_item = row['action_item']\n",
    "        map_level = row['level']\n",
    "        file_path = row['file_path']\n",
    "        row_info['document_unique_id'] = map_document_unique_id\n",
    "    \n",
    "        # XXX Nivel 1 - Definindo que documentos serao tratados    \n",
    "        if map_status_documento == 'PREPROCESS_EXTRACT':\n",
    "            \n",
    "            action_item_row_info = 'CONTINUE_PROCESS'\n",
    "            row_info['action_item'] = action_item_row_info\n",
    "            information_row_info = 'iniciado processamento'\n",
    "            row_info['informations'] = information_row_info\n",
    "            # 0. DADOS GERAIS DOCUMENTO\n",
    "            section = \"0. DADOS INICIAIS\"\n",
    "            try:\n",
    "                valores = {}\n",
    "                # 1. XXX Extracao de todos os dados do documento PDF Pesquisavel\n",
    "                texto_PDF = extrai_texto_PDF_P(idx, row, row_info, section, map_directory, map_original_file_name, file_path, debug)\n",
    "                if texto_PDF:\n",
    "                    pdf_pesquisavel_map = True\n",
    "                else:\n",
    "                    ppdf_pesquisavel_mapl = False\n",
    "                \n",
    "                # 2. XXX IMPORTANTE - Efetuo a busca de entidades e efetuo a tokenizaÇao do documento\n",
    "                doc, tokens, ents = show_ent_new(texto_PDF, patterns=patterns)\n",
    "                \n",
    "                prefeitura_map = [ent.orth_ for ent in doc.ents if ent.label_ == \"nome_prefeitura\"][0]\n",
    "                de_para_pm = [ent.id_ for ent in doc.ents if ent.label_ == \"nome_prefeitura\"][0]\n",
    "                secretaria_map = [ent.orth_ for ent in doc.ents if ent.label_ == \"secretaria\"][0]\n",
    "                tipo_documento_map = [ent.orth_ for ent in doc.ents if ent.label_ == \"tipo_documento\"][0]\n",
    "                \n",
    "                #print(f'\\nNivel 1 - map_seq: {map_seq} | pref.: {prefeitura_map} de_para_pm: {de_para_pm} | documento: {map_original_file_name}')\n",
    "                f_type = 'document'\n",
    "                result = utl.filtrar_df(frames_nf_v4_df, type=f_type, de_para_pm=de_para_pm)\n",
    "                #print(result)\n",
    "                model = result['model'].values[0]\n",
    "                #print(f'\\nNivel 1 - map_seq: {map_seq} | pref.: {prefeitura_map} de_para_pm: {de_para_pm} | model: {model} | documento: {map_original_file_name}')\n",
    "            except Exception as e:\n",
    "                msg = (f'Erro ao processar_dados_iniciais: {e}')\n",
    "            finally:\n",
    "                row_info['pdf_pesquisavel'] = pdf_pesquisavel_map\n",
    "                row_info['texto_PDF'] = texto_PDF\n",
    "                row_info['doc_PDF'] = doc\n",
    "                row_info['model'] = model\n",
    "                row_info['tipo_nota_fiscal'] = tipo_documento_map\n",
    "                row_info['secretaria'] = secretaria_map\n",
    "                row_info['prefeitura'] = prefeitura_map\n",
    "                information_row_info = \"Este e apenas um comeco - mas bem comeco mesmo\"\n",
    "                action_item_row_info = 'CONTINUE_PROCESS'\n",
    "   \n",
    "            # XXX Nivel 2 - Definindo que os documentos legiveis serao tratados\n",
    "            if action_item_row_info == 'CONTINUE_PROCESS':\n",
    "                \n",
    "                # if not pdf_pesquisavel_map:\n",
    "                #     # NOVO PROCESSO DE TRATAMENTO DE IMAGEM - Convertendo a imagem para numpy array\n",
    "                #     if debug:\n",
    "                #         print(\"irei gerar a imagem_np\")\n",
    "                #     imagem_gray, image_resized_name = convert_resize_gray(map_original_file_name, file_path, image_resized_path)\n",
    "                #     imagem_gray_rgb = imagem_gray.convert(\"RGB\")\n",
    "                #     imagem_gray_np = np.array(imagem_gray_rgb)\n",
    "                #     row_info['image_np'] = imagem_gray_np\n",
    "                \n",
    "                # 1. CABECALHO\n",
    "                # try:\n",
    "                section = \"1. CABECALHO\"\n",
    "                valores = {}\n",
    "                mapping_method = \"frame_&_sframe_field\" # significa que as coordenadas estao em frames e os valores dos campos nos sframe_fields\n",
    "                context_mapping = \"data_cabecalho\"\n",
    "                def_replace = True \n",
    "                \n",
    "                if pdf_pesquisavel_map:\n",
    "                     valores = processar_cabecalho_PDF_P(idx, row, row_info, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, map_original_file_name, file_path, debug)\n",
    "                     row_info.update(valores) \n",
    "                # else:\n",
    "                #     valores = processar_cabecalho_R_PDF(idx, row, row_info, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, map_original_file_name, file_path, debug)   \n",
    "                #     row_info.update(valores)\n",
    "         \n",
    "                # #status_documento_row_info = row_info.get('status_documento')\n",
    "                # action_item_row_info = row_info.get('action_item')\n",
    "                # information_row_info = row_info.get('informations')   \n",
    "                information_row_info = \"Este e apenas um comeco\"\n",
    "                action_item_row_info = 'CONTINUE_PROCESS'\n",
    "                \n",
    "                # XXX Nivel 3 - Definindo que os documentos legiveis serao tratados realmente\n",
    "                if action_item_row_info == 'BREAK_PROCESS':\n",
    "                    #msg = (f'Processo inicial: {map_batch_name} | {map_original_file_name} | diretorio: {map_directory} - information_row_info: {information_row_info}')\n",
    "                    if debug:\n",
    "                        print(f'\\nINFELIZMENTE - seq: {map_seq} doc: {map_original_file_name} dir: {map_directory} - NAO SERA PROCESSADO  | inf: {information_row_info} \\n\\n')\n",
    "               \n",
    "                    #row_info['informations'] = msg\n",
    "                    # logging.error(msg)\n",
    "                    lista_dicts.append(row_info)\n",
    "                    continue \n",
    "                \n",
    "                    \n",
    "                elif action_item_row_info == 'CONTINUE_PROCESS':\n",
    "                    if debug:\n",
    "                        print(f'\\nEBA, BORA CONTINUAR - seq: {map_seq} - proxima section: | PDF Pesquisavel: {pdf_pesquisavel_map} doc: {map_original_file_name} dir: {map_directory} | action_item: {action_item_row_info} | inf: {information_row_info} \\n\\n')\n",
    "                        print()\n",
    "                        print(valores)\n",
    "                    \n",
    "                    information_row_info = 'Cabecalho processado'\n",
    "                    row_info['informations'] = information_row_info\n",
    "                    \n",
    "                    \n",
    "                    # guarda_texto_doc = {}\n",
    "                    # guarda_texto_doc, linhas = cria_guarda_doc_ref_R_PDF(idx, row, de_para_map, model_map, map_original_file_name, file_path, image_resized_path, debug)\n",
    "            \n",
    "                    # 2. PRESTADOR DE SERVIÇO\n",
    "                    if prestador == True:\n",
    "                        section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        valores = {}\n",
    "                        erros_prestador = {}\n",
    "                        data_tomador = {}\n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valores = extrai_prestador_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                        else:\n",
    "                            valores = extrai_prestador_R_PDF(idx, row, row_info, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                        \n",
    "                        if not isinstance(valores, dict):\n",
    "                            msg_erro = (f\"\\nErro na linha {idx}: 'valores' não é um dicionário. Tipo: {type(valores)}, Valor: {valores}\")\n",
    "                        else:\n",
    "                            row_info.update(valores)\n",
    "                            \n",
    "                        # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                        # if debug:\n",
    "                        #     print(msg)\n",
    "                        # logging.info(msg)\n",
    "                    \n",
    "                    # 3. TOMADOR DE SERVIÇO\n",
    "                    if tomador == True:\n",
    "                        section = \"3. TOMADOR DE SERVIÇO\"\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        \n",
    "                        valores = {}\n",
    "                        erros = []\n",
    "                        data_tomador = {}\n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        \n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valores = extrai_tomador_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                        else:   \n",
    "                            valores = extrai_tomador_R_PDF(idx, row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                            \n",
    "                        if not isinstance(valores, dict):\n",
    "                            print(f\"\\nErro na linha {idx}: 'valores' não é um dicionário. Tipo: {type(valores)}, Valor: {valores}\")\n",
    "                        else:\n",
    "                            row_info.update(valores)\n",
    "                        \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)\n",
    "                    \n",
    "                    # 4. DESCRIMINACAO DOS SERVIÇOS\n",
    "                    if servicos == True:\n",
    "                        if debug:\n",
    "                            print(f'processando servicos para: {map_original_file_name}')\n",
    "                        section = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "                        valores = {}\n",
    "                        nf_data_servico = {} \n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        \n",
    "                        if pdf_pesquisavel_map:\n",
    "                            nf_data_servico = processar_servicos_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, map_original_file_name, file_path, debug)\n",
    "                        else:\n",
    "                            label = \"discriminacao_servicos\"\n",
    "                            tipo = \"field_box\"\n",
    "                            def_replace = True\n",
    "                            \n",
    "                            # ItSs  working\n",
    "                            texto_extraido = extracao_documento_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_map, model_map, def_replace, map_original_file_name, debug)\n",
    "                            row_info[label] = texto_extraido\n",
    "                            \n",
    "                        msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                        if debug:\n",
    "                            print(msg)\n",
    "                        logging.info(msg)     \n",
    "\n",
    "\n",
    "                        try:\n",
    "                            texto_extraido = nf_data_servico['discriminacao_servicos'] \n",
    "                            row_info['discriminacao_servicos'] = texto_extraido \n",
    "                        except Exception as e:\n",
    "                            msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "                            discrimanacao_servico = \"Descricao nao encontrada\"\n",
    "                            row_info['discriminacao_servicos'] = texto_extraido\n",
    "\n",
    "                    \n",
    "                    # 5. VALOR TOTAL\n",
    "                    if total == True:\n",
    "                        section = \"5. VALOR TOTAL\"\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        #valores = {}\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valor_total_documento = processar_valor_total_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, model_map, map_original_file_name, file_path, debug)\n",
    "                            if valor_total_documento:\n",
    "                                if debug:\n",
    "                                    print(f'\\nvalor_total_documento: {valor_total_documento} | doc: {map_original_file_name}\\n')\n",
    "                                row_info['valor_total_nota'] = valor_total_documento\n",
    "                        else:\n",
    "                            label = \"valor_total_nota\"\n",
    "                            tipo = \"field_box\"\n",
    "                            def_replace = True\n",
    "                            texto_extraido = extracao_documento_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_map, model_map, def_replace, map_original_file_name, debug)\n",
    "                            if texto_extraido: \n",
    "                                valor_total_match = re.search(r'R\\$ ([\\d,.]+)', texto_extraido)\n",
    "                                if valor_total_match:\n",
    "                                    valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "                                    try:\n",
    "                                        # valores['secao'] = section\n",
    "                                        valor_total_documento = float(valor_total_sem_formatacao)\n",
    "                                    except Exception as e:\n",
    "                                        # valores['secao'] = section\n",
    "                                        valor_total_documento = 0.0\n",
    "                                        msg = (f'Processo inicial: {batch_name} | {map_original_file_name:>25} | diretorio: {map_directory} | {e}')\n",
    "                                        #logging.error(f\" {batch_name} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")\n",
    "                        \n",
    "                                    if valor_total_documento:\n",
    "                                        if debug:\n",
    "                                            print(f'\\nvalor_total_documento: {valor_total_documento} | doc: {map_original_file_name}\\n')\n",
    "                                        row_info['valor_total_nota'] = valor_total_documento\n",
    "         \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)\n",
    "                    \n",
    "                    # 6. CNAE e Item da Lista de Serviços \n",
    "                    if cnae == True:\n",
    "                        section = \"6. CNAE e Item da Lista de Serviços\"\n",
    "                        data_box_valores = {}\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        f_0_cnae = 0.95\n",
    "                        f_1_cnae = 1.15\n",
    "                        f_0_it = 0.95     #0.95\n",
    "                        f_1_it = 1.15    # 1\n",
    "                        \n",
    "                        mapping_method = \"frame_&_sframe_field\" # significa que as coordenadas estao em frames e os valores dos campos nos sframe_fields\n",
    "                        context_mapping = \"data_cnae\"\n",
    "                        def_replace = True\n",
    "                        \n",
    "                        if pdf_pesquisavel_map:\n",
    "                            data_box_valores = extracao_documento_CNAE_ITEM_PDF_P(idx, row, row_info, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, def_replace, map_original_file_name, file_path, debug)\n",
    "                        else:\n",
    "                            data_box_valores = extracao_documento_CNAE_ITEM_R_PDF(idx, row, row_info, guarda_texto_doc, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, def_replace, map_original_file_name, file_path, debug)\n",
    "                            \n",
    "                        if data_box_valores:\n",
    "                            row_info.update(data_box_valores)    \n",
    "\n",
    "                    \n",
    "                    # 7. VALORES E IMPOSTOS\n",
    "                    if valores_impostos == True:\n",
    "                        section = \"7. VALORES E IMPOSTOS\"\n",
    "                        # if debug:\n",
    "                        print(f'processando {section} para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                        valores = {}\n",
    "                        nf_data_valores = {}\n",
    "                        lista_impostos = []\n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valores = extrai_valores_impostos_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                            row_info.update(valores)\n",
    "                        \n",
    "                        else:\n",
    "                            tipo = \"field_box\"\n",
    "                            father_value = \"5_frame_valores_impostos\"\n",
    "                            valores = extracao_impostos_R_PDF(section, tipo, father_value, de_para_map, model_map, map_original_file_name, file_path)\n",
    "                            #print(valores)\n",
    "                            row_info.update(valores)\n",
    "                        \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg) \n",
    "                    \n",
    "                    # 8. DADOS COMPLEMENTARES\n",
    "                    if complementares == True:\n",
    "                        section = '8. DADOS COMPLEMENTARES'\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        nf_data_dados_complementares = {}\n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            nf_data_valores = extrai_dados_complementares_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                        else:\n",
    "                            label = \"dados_complementares\"\n",
    "                            tipo = \"field_box\"\n",
    "                            def_replace = False\n",
    "                            # ItSs  working\n",
    "                            texto_extraido = extracao_complementar_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_map, model_map, def_replace, map_original_file_name, debug)\n",
    "                            row_info[label] = texto_extraido  \n",
    "\n",
    "                    \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)     \n",
    "                    \n",
    "                    # 9. OUTRAS INFORMAÇOES / CRITICAS\n",
    "                    if outras_informacoes == True:\n",
    "                        section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        tipo = \"field_box\"\n",
    "                        father_value = \"5_frame_inf_criticas\"\n",
    "                        valores = {} \n",
    "                        nf_data_outras_informacoes = {}\n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valores = extrai_outras_informacoes_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "                            if valores:\n",
    "                                row_info.update(valores)\n",
    "                        else:\n",
    "                            section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "                            tipo = \"field_box\"\n",
    "                            father_value = \"5_frame_inf_criticas\"\n",
    "                            valores = extracao_inforacoes_criticas_R_PDF(section, tipo, father_value, de_para_map, model_map, map_original_file_name, file_path)\n",
    "                            if valores:\n",
    "                                row_info.update(valores)\n",
    "                            \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)          \n",
    "                            \n",
    "                    \n",
    "                    # 10. OBSERVACOES\n",
    "                    if observacoes == True:  \n",
    "                        section = \"10. OBSERVACOES\"\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')  \n",
    "                        data_observacao = {}\n",
    "                        valores = {}\n",
    "                        f_0 = 0.9\n",
    "                        f_1 = 1.1\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valores = extrai_outras_informacoes_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "                            if valores:\n",
    "                                row_info.update(valores)\n",
    "                        else:\n",
    "                            section = '10. OBSERVACOES'\n",
    "                            tipo = \"field_box\"\n",
    "                            father_value = \"6_section_inf_complementares_criticas\" \n",
    "                            \n",
    "                            label = \"observacao\"\n",
    "                            tipo = \"field_box\"\n",
    "                            def_replace = True\n",
    "                            valores = extracao_observacoees_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_map, model_map, def_replace, map_original_file_name, debug)\n",
    "                            if valores:\n",
    "                                row_info.update(valores)\n",
    "                    \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)           \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                lista_dicts.append(row_info)\n",
    "                \n",
    "                \n",
    "            elif action_item_row_info == 'BREAK_PROCESS':\n",
    "                \n",
    "                msg = (f'Documento sem qualidade para pesquisa inicial: {map_batch_name} | {map_original_file_name} | diretorio: {map_directory}')\n",
    "                row_info['informations'] = msg  \n",
    "                \n",
    "            \n",
    "                lista_dicts.append(row_info)\n",
    "                continue\n",
    "                         \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        elif map_status_documento == 'NO_PROCESS':\n",
    "            msg = (f'Documento nao sera tratado neste escopo: {map_batch_name} | {map_original_file_name} | diretorio: {map_directory}')\n",
    "            row_info['action_item'] = \"NO_PROCESS\"    \n",
    "            row_info['informations'] = msg \n",
    "            lista_dicts.append(row_info)\n",
    "            continue\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        #lista_dicts.append(row_info)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    #logging.info(f'processamento finalizado para: {batch_name}') \n",
    "    \n",
    "    print(f'processamento de {i} documentos')\n",
    "    \n",
    "    novo_df = pd.DataFrame(lista_dicts)\n",
    "    \n",
    "    #return lista_dicts\n",
    "    return novo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processamento de 14 documentos\n"
     ]
    }
   ],
   "source": [
    "# analisar_pdf_pesquisavel\n",
    "fase = 'analise'\n",
    "atividade = 'PREPROCESS' \n",
    "status = 'PREPROCESS_EXTRACT'\n",
    "raw_document_list = []\n",
    "dados_prest = {}\n",
    "\n",
    "lista_dicts = []\n",
    "#logging.info(f'Execuçao do pipeline para {batch_name} | df_root_pipe: {file_path_root_pipe} fase: {fase} atividade: {atividade} status: {status}  template: {ver}')\n",
    "\n",
    "# 1. Processar somente dados iniciais e cabeçalho\n",
    "df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=False, cnae=False, valores_impostos=False, complementares=False, outras_informacoes=False, observacoes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>date_time</th>\n",
       "      <th>batch</th>\n",
       "      <th>fase_processo</th>\n",
       "      <th>nome_atividade</th>\n",
       "      <th>status_documento</th>\n",
       "      <th>acao_executada</th>\n",
       "      <th>original_file_name</th>\n",
       "      <th>directory</th>\n",
       "      <th>one_page</th>\n",
       "      <th>...</th>\n",
       "      <th>texto_PDF</th>\n",
       "      <th>doc_PDF</th>\n",
       "      <th>model</th>\n",
       "      <th>tipo_nota_fiscal</th>\n",
       "      <th>secretaria</th>\n",
       "      <th>prefeitura</th>\n",
       "      <th>numero_nota_fiscal</th>\n",
       "      <th>competencia</th>\n",
       "      <th>dt_hr_emissao</th>\n",
       "      <th>codigo_verificacao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>24/09/2023 09:23:12</td>\n",
       "      <td>Batch_23</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>2023 -4.pdf</td>\n",
       "      <td>159871</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>27/07/2023, 15:12 Nota Fiscal de Serviços Elet...</td>\n",
       "      <td>(27/07/2023, ,, 15:12, Nota, Fiscal, de, Servi...</td>\n",
       "      <td>MESQUITA</td>\n",
       "      <td>NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e</td>\n",
       "      <td>SECRETARIA MUNICIPAL DA FAZENDA</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MESQUITA</td>\n",
       "      <td>20234</td>\n",
       "      <td>Julho/2023</td>\n",
       "      <td>27/07/2023 15:11:00</td>\n",
       "      <td>4FDA9FBAE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>24/09/2023 09:23:12</td>\n",
       "      <td>Batch_23</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>2023 -3.pdf</td>\n",
       "      <td>159871</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>27/07/2023, 15:05 Nota Fiscal de Serviços Elet...</td>\n",
       "      <td>(27/07/2023, ,, 15:05, Nota, Fiscal, de, Servi...</td>\n",
       "      <td>MESQUITA</td>\n",
       "      <td>NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e</td>\n",
       "      <td>SECRETARIA MUNICIPAL DA FAZENDA</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MESQUITA</td>\n",
       "      <td>20234</td>\n",
       "      <td>Julho/2023</td>\n",
       "      <td>27/07/2023 15:11:00</td>\n",
       "      <td>4FDA9FBAE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq            date_time     batch fase_processo nome_atividade  \\\n",
       "0    6  24/09/2023 09:23:12  Batch_23       analise   scan_analise   \n",
       "1    8  24/09/2023 09:23:12  Batch_23       analise   scan_analise   \n",
       "\n",
       "     status_documento acao_executada original_file_name directory  one_page  \\\n",
       "0  PREPROCESS_EXTRACT        Analise        2023 -4.pdf    159871      True   \n",
       "1  PREPROCESS_EXTRACT        Analise        2023 -3.pdf    159871      True   \n",
       "\n",
       "   ...                                          texto_PDF  \\\n",
       "0  ...  27/07/2023, 15:12 Nota Fiscal de Serviços Elet...   \n",
       "1  ...  27/07/2023, 15:05 Nota Fiscal de Serviços Elet...   \n",
       "\n",
       "                                             doc_PDF     model  \\\n",
       "0  (27/07/2023, ,, 15:12, Nota, Fiscal, de, Servi...  MESQUITA   \n",
       "1  (27/07/2023, ,, 15:05, Nota, Fiscal, de, Servi...  MESQUITA   \n",
       "\n",
       "                             tipo_nota_fiscal  \\\n",
       "0  NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e   \n",
       "1  NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e   \n",
       "\n",
       "                        secretaria                        prefeitura  \\\n",
       "0  SECRETARIA MUNICIPAL DA FAZENDA  PREFEITURA MUNICIPAL DE MESQUITA   \n",
       "1  SECRETARIA MUNICIPAL DA FAZENDA  PREFEITURA MUNICIPAL DE MESQUITA   \n",
       "\n",
       "  numero_nota_fiscal competencia        dt_hr_emissao codigo_verificacao  \n",
       "0              20234  Julho/2023  27/07/2023 15:11:00          4FDA9FBAE  \n",
       "1              20234  Julho/2023  27/07/2023 15:11:00          4FDA9FBAE  \n",
       "\n",
       "[2 rows x 31 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"27/07/2023, 15:12 Nota Fiscal de Serviços Eletrônica (NFSe) https://nfe.mesquita.rj.gov.br 1/1 PREFEITURA MUNICIPAL DE MESQUITA SECRETARIA MUNICIPAL DA FAZENDA NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e Número da Nota: 20234 Competência: Julho/2023 Data e Hora da Emissão: 27/07/2023 15:11:00 Código Verificação: 4FDA9FBAE PRESTADOR DE SERVIÇOS CPF/CNPJ:  50.921.369/0001-05 Inscrição Municipal:  952538 Telefone:  2297268232.. Inscrição Estadual:   Nome/Razão Social: MEDSORIA CLINICA DE AVALIACAO MEDICA E PSICOLOGICA DO TRAFEGO DE MESQUITA LTDA Nome de Fantasia: Endereço: RUA PROCOPIO ,631 LOJA A ,SANTO ELIAS - Mesquita-RJ E-mail: LARA_VSORIA@HOTMAIL.COM TOMADOR DE SERVIÇOS CPF/CNPJ:  06.047.087/0033-16    |     INSC:MUNICIPAL: RG:   Telefone: Inscrição Estadual:   Nome/Razão Social: REDE D'OR SAO LUIZ S.A. Endereço:  OLINDA ELLIS N° 93 BAIRRO: CAMPO GRANDE CIDADE: RIO DE JANEIRO - RJ CEP: 23045160 E-mail: Não Informado DISCRIMINAÇÃO DOS SERVIÇOS Ref a Plantões de Janeiro, 36h no Setor de Radiologia - Médica: Lara Veiga Soria Catuladeira. VALOR TOTAL DA NOTA: R$ 4.280,16 CNAE - 8630502 - ATIVIDADE MÉDICA AMBULATORIAL COM RECURSOS PARA REALIZAÇÃO DE EXAMES COMPLEMENTARES Item da Lista de Serviços - 4.03 - HOSPITAIS, CLÍNICAS, LABORATÓRIOS, SANATÓRIOS, MANICÔMIOS, CASAS DE SAÚDE, PRONTOS-SOCORROS, AMBULATÓRIOS E CONGÊNERES.   VALOR SERVIÇOS: R$ 4.280,16 VALOR DEDUÇÃO: R$ 0,00 DESC. INCOND: R$ 0,00 BASE DE CÁLCULO: R$ 4.280,16 ALÍQUOTA: 2,01% VALOR ISS: R$ 86,03 VALOR ISS RETIDO: R$ 0,00 DESC. COND: R$ 0,00 ____________________________________________________________________ VALOR PIS: R$ 0,00 VALOR COFINS: R$ 0,00 VALOR IR: R$ 0,00 VALOR INSS: R$ 0,00 VALOR CSLL: R$ 0,00 OUTRAS RETENÇÕES: R$ 0,00 VALOR LÍQUIDO: R$ 4.280,16 DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRITICAS EXIGIBILIDADE ISS Exigivel REGIME TRIBUTAÇÃO Sociedade Limitada SIMPLES NACIONAL Sim ( 2,01% ) ISSQN RETIDO Não LOCAL. PRESTAÇÃO SERVIÇO Mesquita - RJ LOCAL INCIDÊNCIA Mesquita - RJ Observação: LEI DA TRANSPARÊNCIA FISCAL NR. 12.741, DE 8 DE DEZEMBRO DE 2012. - PRESTADOR OPTANTE DO SIMPLES NACIONAL (ALÍQUOTA: 2,01 %) ESTA NFS-E FOI EMITIDA EM SUBSTITUIÇÃO À NFS-E 20231 Valor Aproximado dos Tributos Federais R$ 575,68 (Alíq 13,45), Tributos Estaduais R$ 0,00 (Alíq 0,00 IBPT) e Municipal de R$ 95,45 (Alíq IBPT 2,23 IBPT) \""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_PDF_Pesquisavel = lista_dicts[0]['content_PDF_P']\n",
    "texto_PDF_Pesquisavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">27/07/2023, 15:12 Nota Fiscal de Serviços Eletrônica (NFSe) https://nfe.mesquita.rj.gov.br 1/1 \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    PREFEITURA MUNICIPAL DE MESQUITA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">nome_prefeitura</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #2ADB5E, #1FA346); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SECRETARIA MUNICIPAL DA FAZENDA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">secretaria</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #09D6FF, #08A0D1); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">tipo_documento</span>\n",
       "</mark>\n",
       " Número da Nota: 20234 Competência: Julho/2023 Data e Hora da Emissão: 27/07/2023 15:11:00 Código Verificação: 4FDA9FBAE \n",
       "<mark class=\"entity\" style=\"background: #FFEA7F; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    PRESTADOR DE SERVIÇOS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">nome_section</span>\n",
       "</mark>\n",
       " CPF/CNPJ:  50.921.369/0001-05 Inscrição Municipal:  952538 Telefone:  2297268232.. Inscrição Estadual:   Nome/Razão Social: MEDSORIA CLINICA DE AVALIACAO MEDICA E PSICOLOGICA DO TRAFEGO DE MESQUITA LTDA Nome de Fantasia: Endereço: RUA PROCOPIO ,631 LOJA A ,SANTO ELIAS - Mesquita-RJ E-mail: LARA_VSORIA@HOTMAIL.COM \n",
       "<mark class=\"entity\" style=\"background: #FFEA7F; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    TOMADOR DE SERVIÇOS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">nome_section</span>\n",
       "</mark>\n",
       " CPF/CNPJ:  06.047.087/0033-16    |     INSC:MUNICIPAL: RG:   Telefone: Inscrição Estadual:   Nome/Razão Social: REDE D'OR SAO LUIZ S.A. Endereço:  OLINDA ELLIS N° 93 BAIRRO: CAMPO GRANDE CIDADE: RIO DE JANEIRO - RJ CEP: 23045160 E-mail: Não Informado \n",
       "<mark class=\"entity\" style=\"background: #FFEA7F; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    DISCRIMINAÇÃO DOS SERVIÇOS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">nome_section</span>\n",
       "</mark>\n",
       " Ref a Plantões de Janeiro, 36h no Setor de Radiologia - Médica: Lara Veiga Soria Catuladeira. VALOR TOTAL DA NOTA: R$ 4.280,16 CNAE - 8630502 - ATIVIDADE MÉDICA AMBULATORIAL COM RECURSOS PARA REALIZAÇÃO DE EXAMES COMPLEMENTARES Item da Lista de Serviços - 4.03 - HOSPITAIS, CLÍNICAS, LABORATÓRIOS, SANATÓRIOS, MANICÔMIOS, CASAS DE SAÚDE, PRONTOS-SOCORROS, AMBULATÓRIOS E CONGÊNERES.   VALOR SERVIÇOS: R$ 4.280,16 VALOR DEDUÇÃO: R$ 0,00 DESC. INCOND: R$ 0,00 BASE DE CÁLCULO: R$ 4.280,16 ALÍQUOTA: 2,01% VALOR ISS: R$ 86,03 VALOR ISS RETIDO: R$ 0,00 DESC. COND: R$ 0,00 ____________________________________________________________________ VALOR PIS: R$ 0,00 VALOR COFINS: R$ 0,00 VALOR IR: R$ 0,00 VALOR INSS: R$ 0,00 VALOR CSLL: R$ 0,00 OUTRAS RETENÇÕES: R$ 0,00 VALOR LÍQUIDO: R$ 4.280,16 \n",
       "<mark class=\"entity\" style=\"background: #FFEA7F; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    DADOS COMPLEMENTARES\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">nome_section</span>\n",
       "</mark>\n",
       " OUTRAS INFORMAÇÕES / CRITICAS EXIGIBILIDADE ISS Exigivel REGIME TRIBUTAÇÃO Sociedade Limitada SIMPLES NACIONAL Sim ( 2,01% ) ISSQN RETIDO Não LOCAL. PRESTAÇÃO SERVIÇO Mesquita - RJ LOCAL INCIDÊNCIA Mesquita - RJ Observação: LEI DA TRANSPARÊNCIA FISCAL NR. 12.741, DE 8 DE DEZEMBRO DE 2012. - PRESTADOR OPTANTE DO SIMPLES NACIONAL (ALÍQUOTA: 2,01 %) ESTA NFS-E FOI EMITIDA EM SUBSTITUIÇÃO À NFS-E 20231 Valor Aproximado dos Tributos Federais R$ 575,68 (Alíq 13,45), Tributos Estaduais R$ 0,00 (Alíq 0,00 IBPT) e Municipal de R$ 95,45 (Alíq IBPT 2,23 IBPT) </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc, tokens, ents = show_ent_new(texto_PDF_Pesquisavel, patterns=patterns)\n",
    "\n",
    "displacy.render(doc, style=\"ent\", options={\"colors\": colors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stt:  28 | end: 33 | string_id:             numero_nota_fiscal   span.text:                        Número da Nota: 20234 \n",
      "\n",
      "stt:  33 | end: 36 | string_id:                    competencia   span.text:                      Competência: Julho/2023 \n",
      "\n",
      "stt:  36 | end: 44 | string_id:                  dt_hr_emissao   span.text:  Data e Hora da Emissão: 27/07/2023 15:11:00 \n",
      "\n",
      "stt:  44 | end: 48 | string_id:             codigo_verificacao   span.text:                Código Verificação: 4FDA9FBAE \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(texto_PDF_Pesquisavel)\n",
    "\n",
    "# Executar o Matcher no Doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Exibir os resultados\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Obter a string de identificação\n",
    "    span = doc[start:end]  # Obter o trecho correspondente\n",
    "    if string_id == 'numero_nota_fiscal':\n",
    "        print(f'\\nstt: {start:>3} | end:{end:>3} | string_id: {string_id:>30}   span.text:{span.text:>45} ')\n",
    "    elif string_id == 'competencia':    \n",
    "        print(f'\\nstt: {start:>3} | end:{end:>3} | string_id: {string_id:>30}   span.text:{span.text:>45} ')\n",
    "    elif string_id == 'dt_hr_emissao':\n",
    "        print(f'\\nstt: {start:>3} | end:{end:>3} | string_id: {string_id:>30}   span.text:{span.text:>45} ')\n",
    "    elif string_id == 'codigo_verificacao':\n",
    "        print(f'\\nstt: {start:>3} | end:{end:>3} | string_id: {string_id:>30}   span.text:{span.text:>45} ')    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27/07/2023, 15:12 Nota Fiscal de Serviços Eletrônica (NFSe) https://nfe.mesquita.rj.gov.br 1/1 PREFEITURA MUNICIPAL DE MESQUITA SECRETARIA MUNICIPAL DA FAZENDA NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e Número da Nota: 20234 Competência: Julho/2023 Data e Hora da Emissão: 27/07/2023 15:11:00 Código Verificação: 4FDA9FBAE PRESTADOR DE SERVIÇOS CPF/CNPJ:  50.921.369/0001-05 Inscrição Municipal:  952538 Telefone:  2297268232.. Inscrição Estadual:   Nome/Razão Social: MEDSORIA CLINICA DE AVALIACAO MEDICA E PSICOLOGICA DO TRAFEGO DE MESQUITA LTDA Nome de Fantasia: Endereço: RUA PROCOPIO ,631 LOJA A ,SANTO ELIAS - Mesquita-RJ E-mail: LARA_VSORIA@HOTMAIL.COM TOMADOR DE SERVIÇOS CPF/CNPJ:  06.047.087/0033-16    |     INSC:MUNICIPAL: RG:   Telefone: Inscrição Estadual:   Nome/Razão Social: REDE D'OR SAO LUIZ S.A. Endereço:  OLINDA ELLIS N° 93 BAIRRO: CAMPO GRANDE CIDADE: RIO DE JANEIRO - RJ CEP: 23045160 E-mail: Não Informado DISCRIMINAÇÃO DOS SERVIÇOS Ref a Plantões de Janeiro, 36h no Setor de Radiologia - Médica: Lara Veiga Soria Catuladeira. VALOR TOTAL DA NOTA: R$ 4.280,16 CNAE - 8630502 - ATIVIDADE MÉDICA AMBULATORIAL COM RECURSOS PARA REALIZAÇÃO DE EXAMES COMPLEMENTARES Item da Lista de Serviços - 4.03 - HOSPITAIS, CLÍNICAS, LABORATÓRIOS, SANATÓRIOS, MANICÔMIOS, CASAS DE SAÚDE, PRONTOS-SOCORROS, AMBULATÓRIOS E CONGÊNERES.   VALOR SERVIÇOS: R$ 4.280,16 VALOR DEDUÇÃO: R$ 0,00 DESC. INCOND: R$ 0,00 BASE DE CÁLCULO: R$ 4.280,16 ALÍQUOTA: 2,01% VALOR ISS: R$ 86,03 VALOR ISS RETIDO: R$ 0,00 DESC. COND: R$ 0,00 ____________________________________________________________________ VALOR PIS: R$ 0,00 VALOR COFINS: R$ 0,00 VALOR IR: R$ 0,00 VALOR INSS: R$ 0,00 VALOR CSLL: R$ 0,00 OUTRAS RETENÇÕES: R$ 0,00 VALOR LÍQUIDO: R$ 4.280,16 DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRITICAS EXIGIBILIDADE ISS Exigivel REGIME TRIBUTAÇÃO Sociedade Limitada SIMPLES NACIONAL Sim ( 2,01% ) ISSQN RETIDO Não LOCAL. PRESTAÇÃO SERVIÇO Mesquita - RJ LOCAL INCIDÊNCIA Mesquita - RJ Observação: LEI DA TRANSPARÊNCIA FISCAL NR. 12.741, DE 8 DE DEZEMBRO DE 2012. - PRESTADOR OPTANTE DO SIMPLES NACIONAL (ALÍQUOTA: 2,01 %) ESTA NFS-E FOI EMITIDA EM SUBSTITUIÇÃO À NFS-E 20231 Valor Aproximado dos Tributos Federais R$ 575,68 (Alíq 13,45), Tributos Estaduais R$ 0,00 (Alíq 0,00 IBPT) e Municipal de R$ 95,45 (Alíq IBPT 2,23 IBPT) "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valor = [ent.orth_ for ent in doc.ents if ent.label_ == \"tipo_documento\"][0]\n",
    "valor = [ent.orth_ for ent in doc.ents if ent.label_ == \"secretaria\"][0]\n",
    "valor\n",
    "valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SECRETARIA MUNICIPAL DA FAZENDA'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valor = [ent.orth_ for ent in doc.ents if ent.label_ == \"secretaria\"][0]\n",
    "valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SECRETARIA'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valor = [ent.id_ for ent in doc.ents if ent.label_ == \"secretaria\"][0]\n",
    "valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFEITURA MUNICIPAL DE MESQUITA'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefeitura_encontrada = [ent.orth_ for ent in doc.ents if ent.label_ == \"nome_prefeitura\"][0]\n",
    "prefeitura_encontrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   13 |                   PREFEITURA MUNICIPAL DE MESQUITA |           nome_prefeitura |                          PM_MESQUIA  |     17   ||       95 |    127\n",
      "   17 |                    SECRETARIA MUNICIPAL DA FAZENDA |                secretaria |                          SECRETARIA  |     21   ||      128 |    159\n",
      "   21 |         NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e |            tipo_documento |                               NFS-e  |     28   ||      160 |    202\n",
      "   48 |                              PRESTADOR DE SERVIÇOS |              nome_section |             2. PRESTADOR DE SERVIÇO  |     51   ||      323 |    344\n",
      "  109 |                                TOMADOR DE SERVIÇOS |              nome_section |               3. TOMADOR DE SERVIÇO  |    112   ||      660 |    679\n",
      "  171 |                         DISCRIMINAÇÃO DOS SERVIÇOS |              nome_section |       4. DESCRIMINACAO DOS SERVIÇOS  |    174   ||      931 |    957\n",
      "  389 |                               DADOS COMPLEMENTARES |              nome_section |             8. DADOS COMPLEMENTARES  |    391   ||     1748 |   1768\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.start:>5} | {ent.text:>50} | {ent.label_:>25} | {ent.id_:>35}  |   {ent.end:>4}   ||   {ent.start_char:>6} | {ent.end_char:>6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texto_amostra = \"NFS-e Número da Nota: 20234 Competência: Julho/2023\"\n",
    "texto_amostra = \"NFS-e Número da Nota: 20233 Competência:\"\n",
    "\n",
    "doc = nlp(texto_amostra)\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Exibir os resultados\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Obter a string de identificação\n",
    "    span = doc[start:end]  # Obter o trecho correspondente\n",
    "    print(f\"{string_id}: {span.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisys\n",
    "syntatic = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"T_texto\",\"T_shape\", \"T_is_alpha\", \"T_is_digit\", \"T_is_title\", \"T_is_punct\", \"T_is_sent_start\", \"T_is_right_punct\", \"T_is_stop\", \"T_is_quote\", \"T_is_currency\", \"T_morph\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    syntatic.loc[i,\"id\"] = token.i\n",
    "    syntatic.loc[i,\"T_texto\"] = token.text\n",
    "    syntatic.loc[i,\"T_shape\"] = token.shape_\n",
    "    syntatic.loc[i,\"T_is_alpha\"] = token.is_alpha\n",
    "    syntatic.loc[i,\"T_is_digit\"] = token.is_digit\n",
    "    syntatic.loc[i,\"T_is_title\"] = token.is_title\n",
    "    syntatic.loc[i,\"T_is_punct\"] = token.is_punct\n",
    "    syntatic.loc[i,\"T_is_sent_start\"] = token.is_sent_start\n",
    "    syntatic.loc[i,\"T_is_right_punct\"] = token.is_right_punct\n",
    "    syntatic.loc[i,\"T_is_stop\"] = token.is_stop\n",
    "    syntatic.loc[i,\"T_is_quote\"] = token.is_quote\n",
    "    syntatic.loc[i,\"T_is_currency\"] = token.is_currency\n",
    "    syntatic.loc[i,\"T_morph\"] = token.morph\n",
    "    i = i+1\n",
    "\n",
    "syntatic.head(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Texto</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Tag_explainned</th>\n",
       "      <th>token_POS</th>\n",
       "      <th>POS_explainned</th>\n",
       "      <th>dep</th>\n",
       "      <th>T. Head</th>\n",
       "      <th>dep explained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>27/07/2023</td>\n",
       "      <td>27/07/2023</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>27/07/2023</td>\n",
       "      <td>(Gender=Masc, Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>punct</td>\n",
       "      <td>27/07/2023</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>15:12</td>\n",
       "      <td>15:12</td>\n",
       "      <td>NUM</td>\n",
       "      <td>numeral</td>\n",
       "      <td>NUM</td>\n",
       "      <td>numeral</td>\n",
       "      <td>punct</td>\n",
       "      <td>27/07/2023</td>\n",
       "      <td>(NumType=Card)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Nota</td>\n",
       "      <td>Nota</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Nota</td>\n",
       "      <td>(Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Fiscal</td>\n",
       "      <td>Fiscal</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>flat:name</td>\n",
       "      <td>Nota</td>\n",
       "      <td>(Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>489</td>\n",
       "      <td>Alíq</td>\n",
       "      <td>Alíq</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>appos</td>\n",
       "      <td>Tributos</td>\n",
       "      <td>(Gender=Masc, Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>490</td>\n",
       "      <td>IBPT</td>\n",
       "      <td>IBPT</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>flat:name</td>\n",
       "      <td>Alíq</td>\n",
       "      <td>(Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>491</td>\n",
       "      <td>2,23</td>\n",
       "      <td>2,23</td>\n",
       "      <td>NUM</td>\n",
       "      <td>numeral</td>\n",
       "      <td>NUM</td>\n",
       "      <td>numeral</td>\n",
       "      <td>nummod</td>\n",
       "      <td>IBPT</td>\n",
       "      <td>(NumType=Card)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>492</td>\n",
       "      <td>IBPT</td>\n",
       "      <td>IBPT</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>appos</td>\n",
       "      <td>Alíq</td>\n",
       "      <td>(Gender=Masc, Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>493</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>punct</td>\n",
       "      <td>Alíq</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>494 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id       Texto       Lemma    Tag Tag_explainned token_POS  \\\n",
       "0      0  27/07/2023  27/07/2023   NOUN           noun      NOUN   \n",
       "1      1           ,           ,  PUNCT    punctuation     PUNCT   \n",
       "2      2       15:12       15:12    NUM        numeral       NUM   \n",
       "3      3        Nota        Nota  PROPN    proper noun     PROPN   \n",
       "4      4      Fiscal      Fiscal  PROPN    proper noun     PROPN   \n",
       "..   ...         ...         ...    ...            ...       ...   \n",
       "489  489        Alíq        Alíq  PROPN    proper noun     PROPN   \n",
       "490  490        IBPT        IBPT  PROPN    proper noun     PROPN   \n",
       "491  491        2,23        2,23    NUM        numeral       NUM   \n",
       "492  492        IBPT        IBPT  PROPN    proper noun     PROPN   \n",
       "493  493           )           )  PUNCT    punctuation     PUNCT   \n",
       "\n",
       "    POS_explainned        dep     T. Head               dep explained  \n",
       "0             noun       ROOT  27/07/2023  (Gender=Masc, Number=Sing)  \n",
       "1      punctuation      punct  27/07/2023                          ()  \n",
       "2          numeral      punct  27/07/2023              (NumType=Card)  \n",
       "3      proper noun       ROOT        Nota               (Number=Sing)  \n",
       "4      proper noun  flat:name        Nota               (Number=Sing)  \n",
       "..             ...        ...         ...                         ...  \n",
       "489    proper noun      appos    Tributos  (Gender=Masc, Number=Sing)  \n",
       "490    proper noun  flat:name        Alíq               (Number=Sing)  \n",
       "491        numeral     nummod        IBPT              (NumType=Card)  \n",
       "492    proper noun      appos        Alíq  (Gender=Masc, Number=Sing)  \n",
       "493    punctuation      punct        Alíq                          ()  \n",
       "\n",
       "[494 rows x 10 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ocrmypdf(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o subset para analise\n",
    "df_conf = df[['seq', 'batch', 'original_file_name', 'directory','status_documento', 'model', 'secao', 'prefeitura', 'de_para_pm', 'model', 'action_item', 'pdf_pesquisavel', 'processo', 'numero_nota_fiscal', 'competencia', 'dt_hr_emissao', 'codigo_verificacao','conf_cod' ]]\n",
    "df_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando DF para analises\n",
    "df.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "ordem_status = ['PREPROCESS_EXTRACT', 'NO_PROCESS', 'root_analise']\n",
    "ordem_action_item = ['CONTINUE_PROCESS', 'BREAK_PROCESS', 'NO_PROCESS']\n",
    "\n",
    "\n",
    "df['status_documento'] = pd.Categorical(df['status_documento'], categories=ordem_status, ordered=True)\n",
    "df['action_item'] = pd.Categorical(df['action_item'], categories=ordem_action_item, ordered=True)\n",
    "\n",
    "df.sort_values(by=['status_documento', 'action_item', 'seq'], ascending=[True, True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX 1.Processar todas as secoes do documento\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=True, tomador=True, servicos=True, total=True, cnae=True, valores_impostos=True, complementares=True, outras_informacoes=True, observacoes=True)\n",
    "\n",
    "\n",
    "\n",
    "# 5. Processar valor Total\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=True, cnae=False, valores_impostos=False, complementares=False, outras_informacoes=False, observacoes=False)\n",
    "\n",
    "# 6. Processar CNAE\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=False, cnae=True, valores_impostos=False, complementares=False, outras_informacoes=False, observacoes=False)\n",
    "\n",
    "# 7. Processar Impostos\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=False, cnae=False, valores_impostos=, complementares=False, outras_informacoes=False, observacoes=False)\n",
    "\n",
    "# 8. complementar e observaçoes\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=False, cnae=False, valores_impostos=False, complementares=True, outras_informacoes=True, observacoes=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tables-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
