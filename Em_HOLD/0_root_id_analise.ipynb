{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <mark> Root Unique_ID - Onde Tudo Começa </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Atual notebook com as funçoes para processamento de PDF Pesquisavel e Raster e a criaçao dos Dataframes de forma dependente e unica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import platform\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from urllib import response\n",
    "\n",
    "from outlook_msg import Message\n",
    "import extract_msg\n",
    "import zipfile\n",
    "from pyunpack import Archive\n",
    "import py7zr\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "from fuzzywuzzy import fuzz\n",
    "import PyPDF2\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "import locale\n",
    "import time, copy\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import cv2\n",
    "import fitz  # Módulo PyMuPDF\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import Image, ImageDraw\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar\n",
    "from pdf2image import convert_from_path\n",
    "import matplotlib.pyplot as plt\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "# Modulos da solucao\n",
    "import modules.extrai_pdf_pesquisavel as Extc\n",
    "import modules.cronometro as cron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt_BR.utf8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Config - E-mail\n",
    "# 1. Caminho do arquivo uma mensagem especifica\n",
    "msg_dir_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails'\n",
    "\n",
    "# 2. Path para arquivos atachados compactados\n",
    "msg_attachment_zip = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/13_attachments'\n",
    "\n",
    "\n",
    "#### Config - messages\n",
    "# 3. Caminho do arquivo uma mensagem especifica\n",
    "msg_outros_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/12_messages'\n",
    "\n",
    "# 4. Path para arquivos recebidos manualmente\n",
    "arquivos_recebidos_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/14_documentos_recebidos'\n",
    "\n",
    "\n",
    "####Config Processamento Pipeline\n",
    "\n",
    "# 5. Path para documentos para extracao\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "# 6. Path para gestao de imagens resized\n",
    "image_resized_path = \"pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas\"\n",
    "\n",
    "\n",
    "# 7. Path para DFs e CSVs exportados\n",
    "export_path = \"pipeline_extracao_documentos/6_geral_administacao/exports\"\n",
    "\n",
    "# 8. Path para lixeira\n",
    "root_garbage_path = \"pipeline_extracao_documentos/0_lixeira\"\n",
    "\n",
    "\n",
    "\n",
    "# 12. poppler path\n",
    "poppler_path = \"/home/dani-boy/miniconda3/envs/tables-detr/bin\"\n",
    "\n",
    "# 13. path para config Tesseract\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "#Modelo atual\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "# definindo localizadcao para pt_BR\n",
    "locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. POC Unique_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1 - Funcoes para e-mail e extracao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch</th>\n",
       "      <th>Data</th>\n",
       "      <th>File</th>\n",
       "      <th>Type</th>\n",
       "      <th>Level</th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Parent_Unique_ID</th>\n",
       "      <th>Hash</th>\n",
       "      <th>File_Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Batch, Data, File, Type, Level, Unique_ID, Parent_Unique_ID, Hash, File_Path]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Geraçao do hash do arquivo\n",
    "def generate_file_hash(file_path):\n",
    "    # Abre o arquivo em modo de leitura de bytes\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # Lê o conteúdo do arquivo\n",
    "        file_data = f.read()\n",
    "        # Utiliza o algoritmo SHA-256 para gerar o hash\n",
    "        file_hash = hashlib.sha256(file_data).hexdigest()\n",
    "    return file_hash\n",
    "\n",
    "# 2. Geraçao do Unique_id do arquivo\n",
    "def generate_unique_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# 3. XXX Busca proximo Batch\n",
    "def busca_proximo_batch():\n",
    "    # Abre o arquivo Excel e lê a coluna 'batch'\n",
    "    df = pd.read_excel(\"pipeline_extracao_documentos/6_geral_administacao/exports/df_documento_recebido.xlsx\", usecols=[\"batch\"])\n",
    "    # Pega o último valor da coluna 'batch'\n",
    "    last_value = df.iloc[-1, 0]\n",
    "    \n",
    "    # Extraí o número do último batch e adiciona 1 para o próximo\n",
    "    last_number = int(last_value.split(\"_\")[1])\n",
    "    next_number = last_number + 1\n",
    "    \n",
    "    # Forma o nome do próximo batch\n",
    "    next_batch = f\"Batch_{next_number}\"\n",
    "    \n",
    "    return next_batch    \n",
    "\n",
    "# 4. Função para verificar e criar a pasta se não existir\n",
    "def check_and_create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        \n",
    "# 5. converte nome do arquivo\n",
    "def conv_filename(title):\n",
    "    \n",
    "    # Divide o título em nome e extensão\n",
    "    name, extension = title.rsplit('.', 1) if '.' in title else (title, \"\")\n",
    "\n",
    "    # Remove acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substiti espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remove quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converte para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    # Adiciona a extensão de volta, se houver\n",
    "    if extension:\n",
    "        filename += '.' + extension.lower()\n",
    "\n",
    "    return filename\n",
    "\n",
    "# 6. converte nome do arquivo retirando extensao\n",
    "def conv_filename_no_ext(title):\n",
    "    \n",
    "    # Divida o título em nome e extensão (mas ignore a extensão)\n",
    "    name = title.rsplit('.', 1)[0] if '.' in title else title\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    return filename         \n",
    "\n",
    "# 7. funçao que MOVE documentos e gera add_log_transaction_entry para df_log_transctions\n",
    "def move_doc_processed_file(batch_name, src_path, tgt_path):\n",
    "    \n",
    "    function = \"move_doc_processed_file\"\n",
    "    source_path = src_path\n",
    "    file = os.path.basename(source_path)\n",
    "    sub_dir = os.path.join(tgt_path, batch_name)\n",
    "    destination_path = os.path.join(sub_dir, file)\n",
    "    document_action = \"move_processed_file\"\n",
    "    transaction_detail = (f'document {file} moved by: {function}')\n",
    "    df_move = pd.DataFrame()\n",
    "    try:\n",
    "        document_unique_id = get_document_id_by_file(batch_name, file)\n",
    "        check_and_create_folder(destination_path)\n",
    "        shutil.move(source_path, destination_path)\n",
    "        sucess = True\n",
    "        move_log = add_log_transaction_entry(document_unique_id, batch_name, file, document_action, src_path, tgt_path, transaction_detail, sucess)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao mover documento: {e}\")\n",
    "        sucess = False\n",
    "    \n",
    "    return move_log    \n",
    "\n",
    "# 8. Função para adicionar um novo registro em df_source\n",
    "def add_source_entry(batch_name, file_path, file, type, level, parent_unique_id):\n",
    "    #unique_id = generate_unique_id(type)\n",
    "    unique_id = generate_unique_id()\n",
    "    time_now = cron.timenow_pt_BR()   \n",
    "    file_hash = generate_file_hash(file_path) \n",
    "    if level == 1:\n",
    "        parent_unique_id = unique_id\n",
    "    data = {\n",
    "        'Batch': batch_name,\n",
    "        'Data': time_now,\n",
    "        'File': file,\n",
    "        'Type': type,\n",
    "        'Level': level,\n",
    "        'Unique_ID': unique_id,\n",
    "        'Parent_Unique_ID': parent_unique_id,\n",
    "        'Hash': file_hash,\n",
    "        'File_Path': file_path\n",
    "    }\n",
    "    return data\n",
    "\n",
    "# 9. Add nova linha para atualizar df_log_transctions\n",
    "def add_log_transaction_entry(document_unique_id,batch_name, file, document_action, src_path, tgt_path, transaction_detail, sucess=True):\n",
    "\n",
    "    data_log = {\n",
    "        'Dt_Time': cron.timenow_pt_BR(),\n",
    "        'Batch': batch_name,\n",
    "        'File' : file,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Action': document_action,\n",
    "        'Sorce': src_path,\n",
    "        'Target': tgt_path,\n",
    "        'Transction_Detail': transaction_detail,\n",
    "        'Sucess': sucess,    \n",
    "    }\n",
    "    \n",
    "        \n",
    "    return data_log\n",
    "\n",
    "\n",
    "# 10. Consulta multiparametros\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# 12. Busca filhos - simples\n",
    "def get_children(batch, file_path):\n",
    "    \n",
    "    file = os.path.basename(file_path)\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    # Buscando todos dos filhos de um documento\n",
    "    return filtrar_df(df_id_relations, Batch=batch, Parent_Unique_ID=document_unique_id)\n",
    "\n",
    "\n",
    "# 13. Busca pai -simples\n",
    "def get_father(batch, file_path):\n",
    "    \n",
    "    file = os.path.basename(file_path)\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    # Buscando todos dos filhos de um documento\n",
    "    return filtrar_df(df_id_relations, Batch=batch, Unique_ID=parent_unique_id)\n",
    "\n",
    "\n",
    "# 14. Pesquiso pai pelo Unique_ID e trago um dict\n",
    "def get_father_data_by_children_file(batch, file):\n",
    "    \n",
    "    src_result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    src_parent_unique_id = src_result['Parent_Unique_ID'].values[0]\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=src_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }\n",
    "\n",
    "\n",
    "# 15. Pesquiso pai pelo Unique_ID (document_parent_unique_id) e cria DICT\n",
    "def get_father_by_unique_id(batch, document_parent_unique_id):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=document_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }\n",
    "    \n",
    "# 16. Pesquiso pai pelo file do filho e cria DICT\n",
    "def get_father_data_by_children_file(batch, file):\n",
    "    \n",
    "    src_result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    src_parent_unique_id = src_result['Parent_Unique_ID'].values[0]\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=src_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }        \n",
    "        \n",
    "\n",
    "# 17. Busca o 'Unique_ID' para definir o Parent_Unique_ID sem considerar 'Level'\n",
    "def get_parent_unique_id(df_id_relations, batch_name, file, type):\n",
    "    try:\n",
    "        parent_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type)]['Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        parent_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return parent_unique_id\n",
    "\n",
    "\n",
    "# 18. funcao para trazer somente o 'Unique_ID'\n",
    "def get_document_unique_id(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)]['Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        document_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# 19. funcao para trazer somente o 'Parent_Unique_ID'\n",
    "def get_document_parent_unique_id(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_parent_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)]['Parent_Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        document_parent_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_parent_unique_id\n",
    "\n",
    "\n",
    "# 20. funçao para trazer toda a row de df_id_relations para o documento\n",
    "def get_document_id_relations(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_id_relations = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)].values[0]\n",
    "    except IndexError:\n",
    "        document_id_relations = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_id_relations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# EXEMPLOS de Pesquisa DFss\n",
    "    # get_document_unique_id(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "    # # Busca somente o 'Parent_Unique_ID'\n",
    "    # get_document_parent_unique_id(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "\n",
    "    # #Busca todos os dados da row do documento encontrado\n",
    "    # document_id_relations = get_document_id_relations(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "    # document_batch = document_id_relations[0]\n",
    "    # document_date = document_id_relations[1]\n",
    "    # document_name = document_id_relations[2]\n",
    "    # document_type = document_id_relations[3]\n",
    "    # document_level = document_id_relations[4]\n",
    "    # document_unique_id = document_id_relations[5]\n",
    "    # document_parent_unique_id = document_id_relations[6]\n",
    "    # document_hash = document_id_relations[7]\n",
    "    # document_path = document_id_relations[8]\n",
    "\n",
    "    # # Insercao de um registro pela func add_source_entry\n",
    "    # file_path = \"pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/SPA 15082023.rar\"\n",
    "\n",
    "    # file = os.path.basename(file_path)\n",
    "\n",
    "    # type = \"compressed_file_attachment\"\n",
    "\n",
    "    # level = 1\n",
    "\n",
    "    # parent_unique_id = ''\n",
    "\n",
    "    # # Adicionando um novo registro (substitua 'batch_name' e 'email' conforme necessário)\n",
    "    # new_entry = add_source_entry(batch_name, file_path, file, type, level, parent_unique_id)\n",
    "\n",
    "    # df_id_relations = df_id_relations.append(new_entry, ignore_index=True)\n",
    "\n",
    "    # df_id_relations\n",
    "\n",
    "\n",
    "# Busca proximo Batch caso nao esteja rodando email\n",
    "batch_name = busca_proximo_batch()\n",
    "\n",
    "# 1. Criaçao do DataFrame para armazenar as relações de Unique_ID e Parent_Unique_ID\n",
    "df_id_relations = pd.DataFrame(columns=['Batch', 'Data' ,'File', 'Type', 'Level', 'Unique_ID', 'Parent_Unique_ID', 'Hash', 'File_Path'])\n",
    "\n",
    "# 2. Criaçao do DataFrame para df_start_pipe:\n",
    "#df_start_pipe = pd.DataFrame(columns=['Batch', 'Data' ,'File', 'Type', 'Level', 'Unique_ID', 'dt_hora', 'de', 'assunto', 'email', 'Hash'])\n",
    "\n",
    "df_id_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementando o Root Unique_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>1.1</b> Funcoes importantes - algumas das quais estao sempre por ai  </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. XXX Analisa nro de paginas\n",
    "def analisa_nro_pages(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    pages = pdf_document.pages() # generator object\n",
    "\n",
    "    page_nro = []\n",
    "    for page in pages:\n",
    "        page_nro.append(page)\n",
    "        \n",
    "    nro_paginas = len(page_nro)    \n",
    "    if nro_paginas > 1:\n",
    "        doc_1_page = False\n",
    "        return doc_1_page, nro_paginas    \n",
    "    else:\n",
    "        doc_1_page = True\n",
    "        return doc_1_page, nro_paginas  \n",
    "    pdf_document.close()\n",
    "    \n",
    " \n",
    "\n",
    "# XXX FUNCAO DE SPLIT\n",
    "def split_documentos(qualquer_df, fase, atividade, status):\n",
    "    \n",
    "    documentos_splitados = []\n",
    "    doc_info = {}\n",
    "    rows_list = []\n",
    "    documentos = []\n",
    "    #output_dir = os.path.join(documentos_scan_path, batch_name)\n",
    "    num_linhas_df = qualquer_df.shape[0]\n",
    "\n",
    "    i = num_linhas_df + 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        message_erro = []\n",
    "        nun_pages = row['pages']\n",
    "        batch_name = row['batch']\n",
    "        original_file_name = row['original_file_name']\n",
    "        folder_name = row['directory']\n",
    "        file_path = row['file_path']\n",
    "        level = row['level']\n",
    "        document_type = row['document_type']\n",
    "        doc_action = row['doc_action']\n",
    "        document_unique_id = idx\n",
    "        new_level = level + 1\n",
    "        \n",
    "        if (doc_action == 'splitar') and (status == 'root_analise'):\n",
    "            if nun_pages > 1:\n",
    "                try:\n",
    "                    pdf = fitz.open(file_path)\n",
    "                    # Número total de páginas no PDF\n",
    "                    total_pages = len(pdf)\n",
    "                except Exception as e:\n",
    "                    print(f\"Nao congui abrir o PDF: {e}\")    \n",
    "\n",
    "                # Nome base para os arquivos de saída\n",
    "                base_name = file_path.split('.')[0]  # Remove a extensão do arquivo\n",
    "                file_to_delete = file_path\n",
    "                # Loop para criar um novo PDF para cada página\n",
    "                for page_num in range(total_pages):\n",
    "                    # Cria um novo objeto PDF\n",
    "                    new_pdf = fitz.open()\n",
    "                    # Adiciona a página atual ao novo PDF\n",
    "                    new_pdf.insert_pdf(pdf, from_page=page_num, to_page=page_num)\n",
    "                    # Nome do novo arquivo PDF\n",
    "                    new_pdf_name = f\"{base_name}_page_{page_num + 1}.pdf\"\n",
    "                    # Salva o novo PDF\n",
    "                    new_pdf.save(new_pdf_name)\n",
    "                    # Fecha o novo PDF\n",
    "                    new_pdf.close()\n",
    "                    rotulo = \"prov_nota_fiscal\"\n",
    "                    acao_sugerida = sugestoes_acao.get(rotulo, \"no_defined_action\")\n",
    "                    acao_executada = \"novo_doc_criado\"\n",
    "                    informations = (f'documento criado a partir do split do documento: {original_file_name}')  \n",
    "                    name_pdf_splited = os.path.basename(new_pdf_name)\n",
    "\n",
    "                    new_row = {\n",
    "                        \"seq\": i,\n",
    "                        \"date_time\": cron.timenow_pt_BR(),\n",
    "                        \"batch\": batch_name,\n",
    "                        \"fase_processo\": fase,\n",
    "                        \"nome_atividade\": atividade,\n",
    "                        \"status_documento\": status,\n",
    "                        \"acao_executada\": acao_executada,\n",
    "                        \"original_file_name\": new_pdf_name,\n",
    "                        \"directory\": folder_name,\n",
    "                        \"one_page\": True,\n",
    "                        \"pages\": 1,\n",
    "                        \"document_type\": rotulo,\n",
    "                        \"doc_action\": acao_sugerida,\n",
    "                        \"level\": level,\n",
    "                        \"document_unique_id\": generate_unique_id(),\n",
    "                        \"parent_document_unique_id\": document_unique_id,\n",
    "                        \"file_hash\": generate_file_hash(file_path),\n",
    "                        \"file_path\": file_path,\n",
    "                        \"informations\": informations,\n",
    "                    }\n",
    "                    rows_list.append(new_row)\n",
    "                    i += 1\n",
    "                qualquer_df.loc[idx, 'status_documento'] = \"NAO_PROCESSAR\" \n",
    "                qualquer_df.loc[idx, 'informations'] = \"Paginas splitada em multiplos documentos\" \n",
    "                qualquer_df.loc[idx, 'date_time'] = cron.timenow_pt_BR() \n",
    "    \n",
    "    total_split = i - 1        \n",
    "    df_split = pd.DataFrame(rows_list)\n",
    "    \n",
    "    \n",
    "    return df_split, rows_list\n",
    "\n",
    "\n",
    "# XXX Usando na criacao da imagem \n",
    "def conv_filename_no_ext(title):\n",
    "    \n",
    "    # Divida o título em nome e extensão (mas ignore a extensão)\n",
    "    name = title.rsplit('.', 1)[0] if '.' in title else title\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    return filename \n",
    "\n",
    "\n",
    "# XXX Funcao ajustada para convertere e resize\n",
    "def convertResize(doc2convert, document_path, image_resized_path):\n",
    "    \n",
    "    \n",
    "    name_image = conv_filename_no_ext(doc2convert)\n",
    "    \n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(name_image)}.jpg')\n",
    "    #print(f'image_resized_name: {image_resized_name}\\n')\n",
    "    # 3. Conversao para imagem\n",
    "    pages = convert_from_path(document_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((2067, 2923))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "        image_2work = resized_pages[0]\n",
    "        \n",
    "    return image_2work, image_resized_name\n",
    "\n",
    "\n",
    "\n",
    "def apagar_zone(documentos_extracao_path):\n",
    "    # Para apagar arquivos PDF:Zone\n",
    "    for root, dirs, files in os.walk(documentos_extracao_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            #print(file)\n",
    "            if \":Zone\" in file:\n",
    "                file_to_delete = file_path\n",
    "                os.remove(file_to_delete)\n",
    "                #print(file, \"termina, pode eliminar\")\n",
    "                \n",
    "                \n",
    "def confirma_pdf_pequisavel(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "    # Definir retângulo de interesse\n",
    "    try:\n",
    "        x0 = 0\n",
    "        y0 = 4\n",
    "        x1 = 600\n",
    "        y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "        # Extrair texto dentro do retângulo\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        if text:\n",
    "            page_number = 0\n",
    "            pdf_pequisavel = True\n",
    "        #print(page_number)\n",
    "        else:\n",
    "            page_number = 1\n",
    "            pdf_pequisavel = False\n",
    "        #print(page_number)\n",
    "    except Exception as e:\n",
    "        msg_error = (f\"Erro ao abrir pagina do PDF: {e}\")\n",
    "        pdf_pequisavel = False\n",
    "        pdf_document.close()   \n",
    "         \n",
    "        return pdf_pequisavel\n",
    "                   \n",
    "\n",
    "#generated_parent_document_unique_id = generate_unique_id()  \n",
    "\n",
    "# Processo de deleçao e atualizacao de documentos\n",
    "#e_deleta_peloamor(df_docs_splitados)\n",
    "\n",
    "#me_atualiza_logo_vai_2(novo_df)\n",
    "\n",
    "# apagar_zone(documentos_extracao_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fase = 'processamento' # 'pre-processamento'\n",
    "atividade = 'pre_process'\n",
    "status = 'root_analise'\n",
    "\n",
    "acao_sugerida = \"splitar\"\n",
    "\n",
    "\n",
    "df_root_pipe = split_documentos(df_root_pipe, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualquer_df = pd.concat([qualquer_df, df_split], ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template e dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nome_arquivo:                              batatinha_quando_nasce.pdf | palavra_chave:              default | rotulo: prov_nota_fiscal     | acao_sugerida: NO_PROCESS                    \n"
     ]
    }
   ],
   "source": [
    "nf_model_path = \"pipeline_extracao_documentos/6_geral_administacao/modelos/frames_nf_v9.xlsx\"\n",
    "\n",
    "# 11. path para datasets CNAE e Itens de Serviço\n",
    "nf_datasets_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria do DF\n",
    "frames_nf_v4_df = pd.read_excel(nf_model_path)\n",
    "\n",
    "# Cria dicionários para armazenar diferentes tipos de elementos do modelo\n",
    "document_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'document'].iloc[0]\n",
    "boundaries_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'boundaries']\n",
    "sections_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'section']\n",
    "frames_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'frame']\n",
    "sframe_fields_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'sframe_field']\n",
    "field_boxes_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'field_box']\n",
    "\n",
    "def define_dados_iniciais(texto_tratado):\n",
    "    \n",
    "    dados_iniciais_nf = {}\n",
    "\n",
    "    prefeitura_encontrada = None\n",
    "    de_para_encontrado = None\n",
    "\n",
    "    # 7. ZZZ Dicionário para mapear Prefeitura com sua sigla\n",
    "    de_para_prefeitura = {\n",
    "        \"PREFEITURA DA CIDADE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA DA CIDADE DE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA MUNICIPAL DE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\": \"PM_SPA\",\n",
    "        \"MUNICIPAL DE SAO PEDRO DA ALDEIA\": \"PM_SPA\",\n",
    "        \"PREFEITURA MUNICIPAL DE MESQUITA\": \"PM_MESQUITA\",\n",
    "        \"PREFEITURA MUNICIPAL DE DE MESQUITA\": \"PM_MESQUITA\",\n",
    "        # ... adicione \n",
    "    }\n",
    "\n",
    "    templates = {\n",
    "        (\"PM_MAGE\", \"30.693.231/0001-99\"): \"MAGE_MAICON\",\n",
    "        (\"PM_MAGE\", \"23.317.112/0001-76\"): \"MAGE_MFF\",\n",
    "        (\"PM_MAGE\", None): \"MAGE\",\n",
    "        (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", \"47.945.459/0001-21\"): \"SAO_PEDRO_GOAT\",\n",
    "        (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", \"68.687.722/0001-08\"): \"SAO_PEDRO_GM\",\n",
    "        (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", \"34.230.979/0038-06\"): \"SAO_PEDRO_SUPERMIX\",\n",
    "        (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", None): \"SAO_PEDRO\",\n",
    "        (\"Pague agora com o seu Pix\", None): \"NAO_PROCESSAR\",\n",
    "        # ... adicione outras combinações aqui\n",
    "    }\n",
    "\n",
    "    cnpj_encontrado = None\n",
    "    # Verifique cada linha do texto\n",
    "    for linha in texto_tratado:\n",
    "        for pref in de_para_prefeitura.keys():\n",
    "            if pref in linha:\n",
    "                #print(linha)\n",
    "                prefeitura_encontrada = pref\n",
    "                dados_iniciais_nf['prefeitura'] = prefeitura_encontrada\n",
    "                #print(prefeitura_encontrada)\n",
    "    # Saímos do loop, agora vamos verificar qual template usar\n",
    "    if prefeitura_encontrada:\n",
    "        de_para_pm = de_para_prefeitura.get(prefeitura_encontrada)\n",
    "        dados_iniciais_nf['de_para_pm'] = de_para_pm\n",
    "        #print(de_para_pm)\n",
    "        if not de_para_pm:\n",
    "            de_para_pm = de_para_prefeitura.get(prefeitura_encontrada, \"NAO_PROCESSAR\")\n",
    "            dados_iniciais_nf['de_para_pm'] = de_para_pm\n",
    "            #print(de_para_pm)\n",
    "    else:\n",
    "        de_para_pm = \"NAO_PROCESSAR\"\n",
    "        #print(de_para_pm)\n",
    "        \n",
    "        \n",
    "        # Verifique cada linha do texto\n",
    "    for linha in texto_tratado:\n",
    "        for de_para, cnpj in templates.keys():\n",
    "            if cnpj and cnpj in linha:\n",
    "                cnpj_encontrado = cnpj\n",
    "                dados_iniciais_nf['cnpj_encontrado'] = cnpj_encontrado\n",
    "                print(cnpj_encontrado)\n",
    "                \n",
    "    # Saímos do loop, agora vamos verificar qual template usar\n",
    "    if de_para_pm:\n",
    "        template_usar = templates.get((de_para_pm, cnpj_encontrado))\n",
    "        # print(template_usar)\n",
    "        dados_iniciais_nf['template_usar'] = template_usar\n",
    "        if not template_usar:\n",
    "            template_usar = templates.get((de_para_pm, None), \"TEMPLATE_NAO_ENCONTRADO\")\n",
    "            dados_iniciais_nf['template_usar'] = template_usar \n",
    "    else:\n",
    "        template_usar = \"TEMPLATE_NAO_ENCONTRADO\"\n",
    "        dados_iniciais_nf['template_usar'] = template_usar\n",
    "        \n",
    "    \n",
    "    #Confirmando se template existe em frames    \n",
    "    try:        \n",
    "        f_type = 'frame'\n",
    "        #template_usar = 'SAO_PEDRO_SUPERMIX'\n",
    "        result = filtrar_df(frames_nf_v4_df, type=f_type, de_para_pm=de_para_pm, model=template_usar)\n",
    "        model = result['model'].values[0]\n",
    "        if model:\n",
    "            template_oficial = model\n",
    "            if model == template_usar:\n",
    "                dados_iniciais_nf['model'] = template_oficial\n",
    "            else:    \n",
    "                template_usar = \"necessario cadastrar\"\n",
    "                dados_iniciais_nf['template_usar']\n",
    "                \n",
    "            dados_iniciais_nf['template_usar'] = template_usar\n",
    "        else:\n",
    "            template_usar = \"necessario cadastrar\"\n",
    "            dados_iniciais_nf['template_usar'] = template_usar\n",
    "            dados_iniciais_nf['model'] = template_usar\n",
    "                \n",
    "    except Exception as e:\n",
    "       error_msg = (f\"Erro busca cnae: {e}\") \n",
    "               \n",
    "        \n",
    "    return dados_iniciais_nf  \n",
    "\n",
    "\n",
    "\n",
    "# Dicionário para mapear palavras-chave a rótulos\n",
    "mapeamento_palavras_chave = {\n",
    "    \"relatorio\": \"prov_relatorio\",\n",
    "    \"listagem\": \"prov_listagem\",\n",
    "    \"NF\": \"prov_nota_fiscal\",\n",
    "    \"nf\": \"prov_nota_fiscal\",\n",
    "    \"relatorio\": \"prov_listagem\",\n",
    "    \"sintetico\": \"prov_listagem\",\n",
    "    \"livro\": \"prov_livro_registro\",\n",
    "    \"sintético\": \"prov_listagem\",\n",
    "    \"nota\": \"prov_nota_fiscal\",\n",
    "    \"zip\": \"doc_zip\",\n",
    "    \"rar\": \"doc_rar\",\n",
    "    \"valores\": \"prov_dinheiro\",\n",
    "}\n",
    "\n",
    "# Dicionário mapeando rótulos a ações sugeridas\n",
    "sugestoes_acao = {\n",
    "    \"prov_relatorio\": \"NO_PROCESS\",\n",
    "    \"prov_listagem\": \"NO_PROCESS\",\n",
    "    \"prov_nota_fiscal\": \"NO_PROCESS\",\n",
    "    \"sem_rotulo\": \"MANUAL_REV\",\n",
    "    \"prov_livro_registro\": \"NO_PROCESS\",\n",
    "    \"doc_nao_pdf\": \"verificar\",\n",
    "    \"nao_pdf\": \"NO_PROCESS\",\n",
    "    \"doc_zip\": \"NO_PROCESS\",\n",
    "    \"pdf_mul_paginas\": \"SPLIT\",\n",
    "}\n",
    "\n",
    "\n",
    "def define_rotulo_acao(nome_arquivo):\n",
    "    \n",
    "    for palavra_chave, rotulo in mapeamento_palavras_chave.items():\n",
    "        if palavra_chave.lower() in nome_arquivo.lower():\n",
    "            break\n",
    "    else:\n",
    "        rotulo = 'prov_nota_fiscal' #\"sem_rotulo\"\n",
    "        palavra_chave = 'default'\n",
    "        acao_sugerida = sugestoes_acao.get(rotulo, 'None')\n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "        # palavra_chave = 'None' #\"sem_palavra_chave\"\n",
    "        # acao_sugerida = 'None' #\"sem_acao_sugerida\"\n",
    "        \n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "        #print(f'nome_arquivo: {nome_arquivo} | rotulo: {rotulo}')\n",
    "    if rotulo != 'None': #\"sem_rotulo\"\n",
    "        acao_sugerida = sugestoes_acao.get(rotulo, 'None') # \"Ação não definida\"\n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "    \n",
    "    \n",
    "# 2.Testando\n",
    "nome_arquivo = 'batatinha_quando_nasce.pdf' # 'pre-processamento'\n",
    "\n",
    "palavra_chave, rotulo, acao_sugerida = define_rotulo_acao(nome_arquivo)\n",
    "\n",
    "print(f'nome_arquivo: {nome_arquivo:>55} | palavra_chave: {palavra_chave:>20} | rotulo: {rotulo:20} | acao_sugerida: {acao_sugerida:30}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcoes de OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Funcao de extracao\n",
    "def extract_text_PIL(image, coordinates):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    image_croped = image.crop((x0, y0, x1, y1))\n",
    "    texto_extraido = pytesseract.image_to_string(image_croped, lang='por')\n",
    "    return texto_extraido \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_text_from_coordinates(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    print(x0, y0, x1, y1)\n",
    "    frame_image\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=config).strip()\n",
    "    return extracted_text\n",
    "\n",
    "def extract_text_from_coordinates_2(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    print(x0, y0, x1, y1)\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config_1).strip()\n",
    "    return extracted_text\n",
    "\n",
    "\n",
    "def extract_fields_box(modelo, father_value, section):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    filtered_boxes_info = field_boxes_info[(field_boxes_info['father'] == father_value) & (field_boxes_info['model'] == modelo)]\n",
    "    # Iterate nas informações dos boxes de fields e extraia o texto de cada field\n",
    "    for index_field, row_field in filtered_boxes_info.iterrows():\n",
    "        x0, y0, x1, y1 = row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1']\n",
    "        extracted_text_box = extract_text_from_frame_2(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        #extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        \n",
    "        #print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference'], row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1'] ))\n",
    "        # Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "        value = extracted_text_box.split('\\n')[-1]\n",
    "        # Remova qualquer espaço em branco à esquerda ou à direita\n",
    "        value = value.strip()\n",
    "        if row_field['t_value'] == 'number':\n",
    "            # Formate o valor usando a função format_number\n",
    "            #print(\"vou verificar valor\")\n",
    "            value = format_number2(value)\n",
    "            #print(value)\n",
    "        # Armazene o texto extraído com o rótulo correspondente\n",
    "        label = row_field['label']\n",
    "        data_box_valores[label] = value\n",
    "        \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "# 3. Efetua OCR no documento (area parao do texto da NF)\n",
    "def ocr_RasterPDF(image_name):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = 406\n",
    "    y0 = 0\n",
    "    x1= 1540\n",
    "    y1 = 380\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "    return values, extracted_text_frame \n",
    "\n",
    "\n",
    "# 2. Efetua OCR no documento (area parao do texto da NF)\n",
    "def ocr_RasterPDF_free(image_name, vx0, vy0, vx1, vy1):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = vx0\n",
    "    y0 = vy0\n",
    "    x1= vx1\n",
    "    y1 = vy1\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "    return values, extracted_text_frame \n",
    "\n",
    " \n",
    "def extract_text_from_frame(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=config).strip()\n",
    "    return extracted_text \n",
    "\n",
    "\n",
    "def extract_fb_outras_inf(modelo, father_value, section):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    filtered_boxes_info = field_boxes_info[(field_boxes_info['father'] == father_value) & (field_boxes_info['model'] == model)]\n",
    "    # Iterate nas informações dos boxes de fields e extraia o texto de cada field\n",
    "    for index_field, row_field in filtered_boxes_info.iterrows():\n",
    "        \n",
    "        string_pesquisa = row_field['reference']\n",
    "        x0, y0, x1, y1 = row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        label = row_field['label']\n",
    "        #print(f'extracted_text_box {extracted_text_box}, label {label}')\n",
    "        text = extracted_text_box.replace('\\n', '')\n",
    "        if text.startswith(string_pesquisa):\n",
    "            #print(\"aqui:\", text)\n",
    "            text = text[len(label):].strip()\n",
    "            data_box_valores[label] = text\n",
    "    \n",
    "    return   data_box_valores  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcoes REGEX e ORganizacao TEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_number(number_str):\n",
    "    # Check for percentage and handle it\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # You can multiply by 100 here if needed\n",
    "\n",
    "    # Check if the string contains \"R$\" or a comma, indicating the original format\n",
    "    if 'R$' in number_str or ',' in number_str:\n",
    "        # Original format: Remove 'R$', replace dots with nothing, and replace commas with dots\n",
    "        number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    else:\n",
    "        # New format: Extract only the numeric part using regex\n",
    "        number_str = re.findall(r'[\\d\\.]+', number_str)[-1]\n",
    "\n",
    "    return float(number_str)\n",
    "\n",
    "# Funçao de formatacao de numeros\n",
    "def format_number2(number_str):\n",
    "    number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # multiplica por 100 para fields %\n",
    "    return float(number_str)\n",
    "\n",
    "\n",
    "\n",
    "#1. funcao: find_value_after_keyword_out_frame_up\n",
    "def find_value_after_keyword_out_frame_up(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "        if text_list[index + 1] not in default_keyword_list:\n",
    "            return text_list[index + 1]\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            for default_keyword in default_keyword_list:\n",
    "                if default_keyword in text_list:\n",
    "                    # Caso especial para 'Nome/Razão Social:'\n",
    "                    if keyword == 'Nome/Razão Social:':\n",
    "                        return text_list[0]\n",
    "        return None\n",
    "    \n",
    "#2. find_value_after_keyword_out_frame_down  \n",
    "def find_value_after_keyword_out_frame_down(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o índice seguinte está dentro da lista\n",
    "        if index + 1 < len(text_list):\n",
    "            # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            try:\n",
    "                index = text_list.index(default_keyword_list[-1])\n",
    "                return text_list[index - 1]\n",
    "            except ValueError:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "#3. find_value_after_keyword_fuzz\n",
    "def find_value_after_keyword_fuzz(keyword, text_list, default_keyword_list=None, fuzziness_threshold=80):\n",
    "    closest_match = None\n",
    "    closest_match_score = 0\n",
    "    \n",
    "    for i, text in enumerate(text_list):\n",
    "        score = fuzz.ratio(keyword, text)\n",
    "        \n",
    "        if score > closest_match_score:\n",
    "            closest_match_score = score\n",
    "            closest_match = text\n",
    "        \n",
    "        if closest_match_score > fuzziness_threshold:\n",
    "            break\n",
    "\n",
    "    if closest_match_score > fuzziness_threshold:\n",
    "        index = text_list.index(closest_match)\n",
    "        if index + 1 < len(text_list):\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "\n",
    "    \n",
    "def pesquisa_keyword(string_pesquisa, text_splited, keyword_list):\n",
    "\n",
    "    resultado_extraido_fuzz = find_value_after_keyword_fuzz(string_pesquisa, text_splited, keyword_list)\n",
    "\n",
    "    if resultado_extraido_fuzz == None:\n",
    "        resultado_extraido_frame_up = find_value_after_keyword_out_frame_up(string_pesquisa, text_splited, keyword_list)\n",
    "        if resultado_extraido_frame_up == None:\n",
    "            resultado_extraido_frame_down = find_value_after_keyword_out_frame_down(string_pesquisa, text_splited, keyword_list)\n",
    "            resultado_extraido = resultado_extraido_frame_down\n",
    "        else:\n",
    "            resultado_extraido = resultado_extraido_frame_up\n",
    "    else:\n",
    "        resultado_extraido = resultado_extraido_fuzz           \n",
    "    #print(resultado_extraido)\n",
    "    return resultado_extraido\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_fields_tomador_cnpj(text):\n",
    "    nf_data_tomador_cnpj = {}\n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in text:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "                        nf_data_tomador_cnpj['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                        nf_data_tomador_cnpj['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "\n",
    "    \n",
    "    # Extrair Telefone\n",
    "    telefone_match = re.search(r'Telefone:\\s+(.+)', text)\n",
    "    if telefone_match:\n",
    "        telefone_str = telefone_match.group(1)\n",
    "        if telefone_str == 'Inscrição Estadual:':\n",
    "            nf_data_tomador_cnpj['telefone'] = None  # Valor padrão quando não há correspondência\n",
    "        elif telefone_str == '':\n",
    "            nf_data_tomador_cnpj['telefone'] = None  # Valor padrão quando não há correspondência\n",
    "                    \n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['telefone'] = telefone_match.group(1)\n",
    "            \n",
    "    \n",
    "    # Extrair Inscrição Municipal\n",
    "    inscricao_municipal_match = re.search(r'Inscrição Municipal:\\s+(.+)', text)\n",
    "    if inscricao_municipal_match:\n",
    "        inscricao_municipal_str = inscricao_municipal_match.group(1)\n",
    "        if inscricao_municipal_str == \"Telefone:\": \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = inscricao_municipal_str\n",
    "    \n",
    "    insc_municipal_match = re.search(r'INSC:MUNICIPAL:\\s+(.+)', text)\n",
    "    if insc_municipal_match:\n",
    "        insc_municipal_str = insc_municipal_match.group(1)\n",
    "        if insc_municipal_str == \"Telefone:\":\n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = insc_municipal_str\n",
    "    else:\n",
    "        nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "            \n",
    "    \n",
    "    return nf_data_tomador_cnpj \n",
    "\n",
    "\n",
    "\n",
    "def extract_dados_comple_obs(modelo, frame_father, section):\n",
    "    \n",
    "    data_dados_complementares = {}\n",
    "    #frame_label = frame_father\n",
    "    \n",
    "    # 1. Filtrando o frames_info para buscar os dados de corte\n",
    "    filtered_frames_info = frames_info[(frames_info['label'] == frame_father) & (frames_info['model'] == modelo)]\n",
    "\n",
    "    # 2. Filtrando o sframe_fields_info para buscar os dados dos campos que estao nos frames\n",
    "    filtered_sframe_fields_info = sframe_fields_info[(sframe_fields_info['father'] == frame_father) & (sframe_fields_info['model'] == modelo)]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "        \n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        \n",
    "        print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_frame['seq'], row_frame['model'], row_frame['father'], row_frame['label'], row_frame['reference'], row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'] ))\n",
    "        for index_field, row_field in filtered_sframe_fields_info.iterrows():\n",
    "            #print(\"{:<5} {:<10} {:<30} {:<20} {:<20}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference']))\n",
    "            \n",
    "            if frame_father == \"5_frame_dados_complementares\":\n",
    "                nf_data_dados_complementares = {}\n",
    "                nf_data_dados_complementares['section'] = section\n",
    "                \n",
    "                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^DADOS COMPLEMENTARES', '', extracted_text_box, count=1)\n",
    "                if text == '':\n",
    "                    text = None\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text\n",
    "                else:    \n",
    "                    # Extrair texto dentro do retângulo\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "                    \n",
    "                return nf_data_dados_complementares                \n",
    "                \n",
    "            elif frame_father == \"5_frame_observacao\":\n",
    "                nf_data_observacao = {}\n",
    "                nf_data_observacao['section'] = section \n",
    "                                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^Observação:', '', extracted_text_box, count=1)\n",
    "\n",
    "                # Remover quebras de linha\n",
    "                text = text.replace('\\n', ' ')\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                nf_data_observacao['observacao'] = text.strip()\n",
    "                \n",
    "                return nf_data_observacao \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def processa_cnae_outros(text):\n",
    "    nf_data_CNAE_match = re.search(r'CNAE\\s+(.+)', text)\n",
    "    if nf_data_CNAE_match:\n",
    "        try:\n",
    "            # Remove a primeira ocorrência de \"CNAE:\"\n",
    "            nf_data_CNAE_str = re.sub(r'^CNAE - ', '', text, count=1)\n",
    "            # Remover quebras de linha\n",
    "            nf_data_CNAE_str = nf_data_CNAE_str.replace('\\n', ' ')\n",
    "            return nf_data_CNAE_str \n",
    "        except Exception as e:\n",
    "            print(f\"Erro busca cnae: {e}\") \n",
    "        \n",
    "    return None   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> Analise de Extracao de dados no Pipeline </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_name = \"Batch_20\" #Excepcionalmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_parent_document_unique_id = generate_unique_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importantando df_root_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. XXX SE deseja importar o DF df_analise_pipe\n",
    "\n",
    "df_root_pipe_path = \"df_root_analise.xlsx\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria df_documento_recebido\n",
    "df_root_pipe = pd.read_excel(df_root_pipe_path)\n",
    "\n",
    "# Ajusta o indice\n",
    "df_root_pipe.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.2 Funçao de Analise do Pipeline - gerar <mark> <b>df_root_pipe </b></mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.XXX  Acao 1 - Ler todo o pipeline de documentos recebidos - ESSA E A UNICA FUNCAO QUE ITERA NO DIRETORIO\n",
    "def scan_pipeline_documentos(documentos_extracao_path, batch_name, fase, atividade, status):\n",
    "    \n",
    "    doc_info = {}\n",
    "    resumo = {}\n",
    "    raw_document = []\n",
    "    \n",
    "    output_dir = os.path.join(documentos_extracao_path, batch_name)\n",
    "    i = 1\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        folder_name = os.path.basename(root)\n",
    "        #print(folder_name)\n",
    "        for file in files:\n",
    "            nome_arquivo = file\n",
    "            palavra_chave, rotulo, acao_sugerida = define_rotulo_acao(nome_arquivo)\n",
    "            acao_executada = \"Analise\"\n",
    "            informations = ' '    \n",
    "            file_name = file.lower()    \n",
    "            file_path = os.path.join(root, file)\n",
    "            new_path_name = os.path.join(output_dir, file)\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                doc_one_page, nro_pgs = analisa_nro_pages(file_path)\n",
    "            # if doc_one_page == False:\n",
    "            #             rotulo = 'pdf_mul_paginas'\n",
    "            level = 3\n",
    "            diretorio = os.path.basename(file_path)\n",
    "            if folder_name == batch_name:\n",
    "                folder_name = \"root_dir\"\n",
    "                \n",
    "            #print(f'nome_arquivo: {nome_arquivo:>55} | palavra_chave: {palavra_chave:>20} | rotulo: {rotulo:20} | acao_sugerida: {acao_sugerida:30}')    \n",
    "            \n",
    "            new_row = {\n",
    "                \"seq\": i,\n",
    "                \"date_time\": cron.timenow_pt_BR(),\n",
    "                \"batch\": batch_name,\n",
    "                \"fase_processo\": fase,\n",
    "                \"nome_atividade\": atividade,\n",
    "                \"status_documento\": status,\n",
    "                \"acao_executada\": acao_executada,\n",
    "                \"original_file_name\": file,\n",
    "                \"directory\": folder_name,\n",
    "                \"one_page\": doc_one_page,\n",
    "                \"pages\": nro_pgs,\n",
    "                \"palavra_chave\": palavra_chave,\n",
    "                \"document_tag\": rotulo,\n",
    "                \"action_item\": acao_sugerida,\n",
    "                \"level\": level,\n",
    "                \"document_unique_id\": generate_unique_id(),\n",
    "                \"parent_document_unique_id\": fake_parent_document_unique_id,\n",
    "                \"file_hash\": generate_file_hash(file_path),\n",
    "                \"file_path\": file_path,\n",
    "                \"informations\": informations,\n",
    "            }\n",
    "            raw_document.append(new_row)\n",
    "\n",
    "            \n",
    "            # print(f'seq: {i} | file: {file}'\n",
    "            i += 1\n",
    "    df_trans_pipe = pd.DataFrame(raw_document)\n",
    "      \n",
    "                \n",
    "    return df_trans_pipe, raw_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Crio o DF df_scan_pipe\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'scan_analise'\n",
    "status = 'root_analise'\n",
    "\n",
    "documentos = []\n",
    "\n",
    "#df_trans_pipe = pd.DataFrame()\n",
    "\n",
    "df_root_pipe, documentos = scan_pipeline_documentos(documentos_extracao_path, batch_name, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "df_root_pipe.query('one_page == False & palavra_chave == \"sem_palavra_chave\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. XXX Usando loc para Filtrar Baseado em one_page == False\n",
    "df_pages_2_split = df_root_pipe[df_root_pipe['one_page'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporando o processo de IteraÇao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta e o principio da melhor funcao do mundo\n",
    "def extracao_pipeline(qualquer_df, fase, atividade, status):\n",
    "    \n",
    "    linhas_analise = []\n",
    "    bloco_1_list = []\n",
    "    bloco_2_list = []\n",
    "    bloco_3_list = []\n",
    "    imagens_list = []  \n",
    "    new_data = [] \n",
    "    \n",
    "    pre_processo = ['nro_nota', 'competencia', 'dt_hr_emissao', 'codigo_verificacao', 'ajustar_nome', 'split_paginas', 'ajustar_imagem', 'buscar_nome_prefeitura', 'enviar_canceladas', 'enviar_listagens']\n",
    "    \n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    \n",
    "    fase_processo_atual = fase\n",
    "    atividade_processo_atual = atividade\n",
    "    status_documento_atual = status\n",
    "    \n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        message_erro = []\n",
    "        \n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        document_unique_id = idx\n",
    "        seq_df = row['seq']\n",
    "        batch_name = row['batch']\n",
    "        fase_processo = row['fase_processo']\n",
    "        nome_atividade = row['nome_atividade']\n",
    "        status_documento = row['status_documento']\n",
    "        original_file_name = row['original_file_name']\n",
    "        file_directory = row['directory']\n",
    "        level = row['level']\n",
    "        d_type = row['level']\n",
    "        document_type = row['document_type']\n",
    "        pdf_pesquisavel = row['pdf_pesquisavel']\n",
    "        one_page_doc = row['one_page']\n",
    "        modelo = row['modelo']\n",
    "        \n",
    "        prefeitura = row['prefeitura']\n",
    "        \n",
    "        parent_document_unique_id = row['parent_document_unique_id']\n",
    "        file_path = row['file_path']\n",
    "        file_hash = row['file_hash']\n",
    "        \n",
    "        # 2. Busca modelo\n",
    "        if (not status_documento == 'NAO_PROCESSAR') or (not document_type == 'outros') or (not one_page_doc == True):\n",
    "            \n",
    "            if atividade_processo_atual == 'extracao_prestador':\n",
    "            #print(f' 1 - seq: {seq_df} | file: {original_file_name} |status_documento: {status_documento} pdf_pesquisavel: {pdf_pesquisavel}\\n')\n",
    "                if status_documento == status_documento_atual:\n",
    "         \n",
    "                    print(f'seq_df: {seq_df} status_documento: {status_documento} | modelo: {modelo:>20} | file: {original_file_name:>40} | prefeitura: {prefeitura:>55} | {pdf_pesquisavel}\\n')\n",
    "                    \n",
    "                    result_list = []\n",
    "                    \n",
    "                    dfcnpj_prestador = {}\n",
    "                    dfincricao_prestador = {}\n",
    "                    dfdados_prestador = {}\n",
    "                        \n",
    "                    # doc2convert = original_file_name\n",
    "                    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "                    modelo = 'SAO_PEDRO_SUPERMIX'\n",
    "                    f_tipo = 'frame'\n",
    "                    \n",
    "                    dfcnpj_prestador, dfincricao_prestador, dfdados_prestador, textoextraido  = processar_dados_dados_documentos(row, original_file_name, file_path, pdf_pesquisavel, section, modelo, f_tipo)\n",
    "                    \n",
    "                    print(f'\\n {dfcnpj_prestador}\\n{dfincricao_prestador}\\n{dfdados_prestador}\\n')\n",
    "                    \n",
    "               \n",
    "      \n",
    "\n",
    "\n",
    "                i += 1\n",
    "              \n",
    "    return textoextraido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe.to_excel('df_root_analise.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se Quiser evoluir no doc Analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. XXX SE deseja importar o DF df_analise_pipe\n",
    "\n",
    "df_analise_pipe_path = \"Em_HOLD/df_mapeamento_e_analise2.xlsx\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria df_documento_recebido\n",
    "df_analise_pipe = pd.read_excel(df_analise_pipe_path)\n",
    "\n",
    "# Ajusta o indice\n",
    "df_analise_pipe.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Crio o DF df_scan_pipe\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'scan_analise'\n",
    "status = 'mapear'\n",
    "\n",
    "df_analise_pipe, documentos = scan_pipeline_documentos(documentos_scan_path, batch_name, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>2.x</b> Processo de mapeamento e criacao do  df_analise_pipe </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. XXX df_analise_pipe e o df oficial de analise do pipeline\n",
    "df_analise_pipe\n",
    "\n",
    "# 3. Numero de linhas do DF\n",
    "num_linhas_df = df_analise_pipe.shape[0]\n",
    "\n",
    "num_linhas_df # variavel para o numero de linhas do DF\n",
    "\n",
    "\n",
    "# 4. XXX Usando loc para Filtrar Baseado em one_page == False\n",
    "df_pages_2_split = df_analise_pipe[df_analise_pipe['one_page'] == False]\n",
    "\n",
    "num_docs_2_split = df_pages_2_split.shape[0]\n",
    "\n",
    "\n",
    "print(f'nro documentos em df_analise_pipe: {num_linhas_df} | nro documentos + 1 pagina: {df_pages_2_split.shape[0]}')\n",
    "    \n",
    "# 5. XXX Definimos o Index dos DFs\n",
    "df_analise_pipe.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "df_pages_2_split.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# 6. XXX Executo a criacao dos documentos split \n",
    "fase = 'pre-processamento'\n",
    "atividade = 'scan_analise'\n",
    "status = 'processar'\n",
    "\n",
    "# df_docs_splitados recebera o DF com os documentos splitados\n",
    "df_docs_splitados = split_documentos(df_pages_2_split, df_analise_pipe, num_linhas_df, fase, atividade, status)\n",
    "\n",
    "\n",
    "\n",
    "# 7. XXX Retiro o indice do DF - Resetando o índice e mantendo o índice original como uma nova coluna\n",
    "df_analise_pipe.reset_index(inplace=True)\n",
    "\n",
    "# 8. XXX Concatenando os DataFrames\n",
    "df_analise_pipe = pd.concat([df_analise_pipe, df_docs_splitados], ignore_index=True)\n",
    "\n",
    "# 9. XXX Volto novamente o indice do DF\n",
    "df_analise_pipe.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "# 10. XXX Salvo em Excel (pode ser feito durante fases)\n",
    "df_analise_pipe.to_excel(\"df_mapeamento_e_analise.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2B. XXX Efetuo a analise do pipeline de documentos e inicio da extracao de dados\n",
    "# analisar_pdf_pesquisavel\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'Reavaliar_PDF_Pesquisavel' #'pesquisar_prefeitura' # pesquisar_prefeitura  pesquisar_modelo    mensagem_status = \"Reavaliar_PDF_Pesquisavel\"\n",
    "status = 'mapear'\n",
    "\n",
    "imagens_list = analise_extracao_pipeline(subset_df_analise_pipe, df_analise_pipe, fase, atividade, status)\n",
    "\n",
    "\n",
    "if imagens_list:\n",
    "    remove_images(imagens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "subset_df_analise_pipe = df_analise_pipe.query('seq == 59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Salvando o DF (IMPORTANTE)\n",
    "df_analise_pipe.to_excel(\"df_mapeamento_e_analise2.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Criando uma nova coluna no DF\n",
    "df_analise_pipe.insert(loc=17, column='s_act', value=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustando dados do DF e efetuando queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise_pipe['pdf_pesquisavel'] = df_analise_pipe.apply(lambda row: row['coluna1'] * 2 if row['coluna2'] > 0 else row['coluna1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizei o DF baseado em condiçao de outra coluna\n",
    "df_analise_pipe['pdf_pesquisavel'] = df_analise_pipe.apply(lambda row: False if row['status_documento'] == \"Template_encontrado\" else row['pdf_pesquisavel'], axis=1)\n",
    "\n",
    "\n",
    "# Podemos chegar de forma mais rapida\n",
    "df_analise_pipe.loc[df_analise_pipe['status_documento'] == \"Template_encontrado\", 'pdf_pesquisavel'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df_analise_pipe.quert('nro_nota')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efetuando pesquisa no DF\n",
    "df_analise_pipe.query('pdf_pesquisavel == False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise_pipe.query('nro_nota == 13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Para Analise de PDF Pesquisavel, uei uma copia do df_analise_pipe\n",
    "subset_df_analise_pipe = df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "subset_df_analise_pipe = df_analise_pipe.query('seq < 21 & seq > 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Criando uma nova coluna no DF\n",
    "df_analise_pipe.insert(loc=13, column='model_frame', value=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>1.</b> Efetuando subset do df para pesquisa </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Usando iloc para Filtrar um Número Específico de Linhas\n",
    "subset_df_analise_pipe = df_analise_pipe.iloc[:10]\n",
    "\n",
    "subset_df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Usando loc para Filtrar Baseado em Condições\n",
    "subset_df = df_scan_pipe.loc[df_scan_pipe['directory'] == 'camaleao']\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert file_hash == file_hash2, \"Os arquivos são diferentes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Usando iloc para Filtrar um Número Específico de Linhas\n",
    "subset_df = df_scan_pipe.iloc[:10]\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "subset_df = df_scan_pipe.query('directory == \"root_dir\" & seq < 10 & seq > 7')\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Filtrando Linhas Baseadas em Valores em uma Lista\n",
    "\n",
    "valores = [11, 16, 30, 41]\n",
    "subset_df = df_scan_pipe[df_scan_pipe['seq'].isin(valores)]\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registro_específico = df_scan_pipe.loc['e1f4b1af-30f3-45d2-85a7-1bb895bd5325']\n",
    "\n",
    "registro_específico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Efetuando Extracao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcoes de extracao\n",
    "def processar_dados_iniciais(row, original_file_name, file_path):\n",
    "    \n",
    "    # lista_texto_extraido = []\n",
    "    nf_dados_doc = {}\n",
    "    pdf_pesquisavel = None\n",
    "    extracted_txt = pesquisa_prefeitura_pdf_pesquisavel(file_path)\n",
    "    \n",
    "    if extracted_txt:\n",
    "        pdf_pesquisavel = True\n",
    "    else:\n",
    "        pdf_pesquisavel = False \n",
    "       \n",
    "        x0 = 110\n",
    "        y0 = 0\n",
    "        x1= 1929\n",
    "        y1 = 786\n",
    "        \n",
    "        image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "        extracted_txt = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "    \n",
    "    nf_dados_doc['file_name'] = original_file_name    \n",
    "    nf_dados_doc['pdf_pesquisavel'] = pdf_pesquisavel \n",
    "    value = {}   \n",
    "    texto_tratado = texto_extraido(extracted_txt)\n",
    "    value = define_dados_iniciais(texto_tratado)\n",
    "    if value:\n",
    "        nf_dados_doc.update(value)\n",
    "   \n",
    "\n",
    "\n",
    "    return nf_dados_doc\n",
    "\n",
    "def pesquisa_prefeitura_pdf_pesquisavel(file_path):    \n",
    "    \n",
    "    # Carregar o arquivo PDF\n",
    "    pdf_document = fitz.open(file_path)\n",
    "\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 4\n",
    "    x1 = 600\n",
    "    y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    \n",
    "    #print(text)\n",
    "    \n",
    "    if text:\n",
    "       page_number = 0\n",
    "       #print(page_number)\n",
    "    else:\n",
    "       page_number = 1\n",
    "       #print(page_number)\n",
    "    \n",
    "    pdf_document.close()\n",
    "   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Funcoes de Regex - cabecalho - documento ped pesquisavel\n",
    "nf_data_servico = {}\n",
    "nf_data_erros = {}\n",
    "nf_lista_erros = []\n",
    "\n",
    "# 0. Pesquisa PDF\n",
    "def is_pdf_searchable(pdf_path):\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        is_searchable = all(page.get_text(\"text\") != \"\" for page in pdf_document)\n",
    "        pdf_document.close()\n",
    "        return is_searchable\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao verificar o PDF: {e}\")\n",
    "        return False\n",
    "\n",
    "# 1. CABECALHO\n",
    "def extract_fields_cabecalho(text):\n",
    "    nf_data_cabecalho = {}\n",
    "    # Extrair Nome da Prefeitura\n",
    "    nome_prefeitura_match = re.search(r'PREFEITURA (.+)', text)\n",
    "    if nome_prefeitura_match:\n",
    "        nome_prefeitura = \"PREFEITURA \" + nome_prefeitura_match.group(1)\n",
    "        nf_data_cabecalho['nome_prefeitura'] = nome_prefeitura\n",
    "    else:\n",
    "        nf_data_cabecalho['nome_prefeitura'] = None    \n",
    "\n",
    "    # Extrair Tipo de NF\n",
    "    tipo_nf_match = re.search(r'NOTA FISCAL (.+)', text)\n",
    "    if tipo_nf_match:\n",
    "        tipo_nf = \"NOTA FISCAL \" + tipo_nf_match.group(1)\n",
    "        nf_data_cabecalho['tipo_nota_fiscal'] = tipo_nf\n",
    "    else:\n",
    "        nf_data_cabecalho['tipo_nota_fiscal'] = None    \n",
    "    \n",
    "    # Extrair Número da Nota\n",
    "    numero_nota_match = re.search(r'Número da Nota:\\s+(\\d+)', text)\n",
    "    if numero_nota_match:\n",
    "        nr_nro_nf = numero_nota_match.group(1)\n",
    "        nf_data_cabecalho['numero_nota_fiscal'] = numero_nota_match.group(1)\n",
    "    else:\n",
    "        nf_data_cabecalho['numero_nota_fiscal'] = None\n",
    "        nf_data_erros['cabecalho'] = \"numero_nota\"\n",
    "        nf_lista_erros.append(nf_data_erros)        \n",
    "\n",
    "    # Extrair Competência\n",
    "    competencia_match = re.search(r'Competência:\\s+(.+)', text)\n",
    "    if competencia_match:\n",
    "        nf_data_cabecalho['competencia'] = competencia_match.group(1)\n",
    "    else:\n",
    "        nf_data_cabecalho['competencia'] = None\n",
    "        nf_data_erros['cabecalho'] = \"competencia\"\n",
    "        nf_lista_erros.append(nf_data_erros)\n",
    "            \n",
    "\n",
    "    # Extrair Data e Hora de Emissão\n",
    "    data_emissao_match = re.search(r'Data e Hora da Emissão:\\s+(.+)', text)\n",
    "    if data_emissao_match:\n",
    "        nf_data_cabecalho['dt_hr_emissao'] = data_emissao_match.group(1)\n",
    "    else:\n",
    "        nf_data_cabecalho['dt_hr_emissao'] = None    \n",
    "        \n",
    "    # Extrair Data e Hora de Emissão\n",
    "    codigo_verificacao_match = re.search(r'Código Verificação:\\s+(.+)', text)\n",
    "    if codigo_verificacao_match:\n",
    "        nf_data_cabecalho['codigo_verificacao'] = codigo_verificacao_match.group(1)\n",
    "    else:\n",
    "        nf_data_cabecalho['codigo_verificacao'] = None\n",
    "        nf_data_erros['cabecalho'] = \"codigo_verificacao\"\n",
    "        nf_lista_erros.append(nf_data_erros)        \n",
    "\n",
    "    return nf_data_cabecalho\n",
    "\n",
    "\n",
    "def processar_cabecalho_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    section = \"1 - CABECALHO\"\n",
    "    frame_type = 'frame'\n",
    "    f_frame_label = \"1_frame_dados_nf\"\n",
    "    data_box_valores = {'secao': section}\n",
    "    nf_data_cabecalho = {}\n",
    "    message_erro = []\n",
    "    nf_data_cabecalho['original_file_name'] = original_file_name\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == frame_type) & (frames_nf_v4_df['label'] == f_frame_label)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        frame_id = row_frame['id']\n",
    "        label = row_frame['label']\n",
    "        x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "        #print(f'doc {\"pdf_pesquisavel\" if pdf_pesquisavel else \"raster_pdf\"}: {label:>30} | id: {frame_id:>3} | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6} ')\n",
    "        \n",
    "        pdf_document = fitz.open(file_path)\n",
    "        # Página do PDF\n",
    "        page_number = 0  # Defina o número da página que deseja analisar\n",
    "        page = pdf_document[page_number]\n",
    "        #x0, y0, x1, y1 = (0, 0, 600, 110)\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        nf_data_cabecalho = extract_fields_cabecalho(text)\n",
    "        # nf_data_cabecalho['original_file_name'] = original_file_name\n",
    "        # nf_data_cabecalho['pdf_pesquisavel'] = pdf_pesquisavel_map\n",
    "        \n",
    "        pdf_document.close()\n",
    "        \n",
    "    return nf_data_cabecalho \n",
    "\n",
    "\n",
    "\n",
    "def processar_cabecalho_raster_pdf(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    section = \"1 - CABECALHO\"\n",
    "    pdf_pesquisavel = False\n",
    "    father = \"1_frame_prefeitura_nf\"\n",
    "    data_extrated_prefeitura = {}\n",
    "    tipo_2 = \"sframe_field\"\n",
    "    \n",
    "    data_extrated_prefeitura['pdf_pesquisavel'] = pdf_pesquisavel_map\n",
    "    data_extrated_prefeitura['original_file_name'] = original_file_name\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['section_json'] == section)]\n",
    "    filtered_sframe_field_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['father'] == father) & (frames_nf_v4_df['type'] == tipo_2)]\n",
    "    image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        label_value = row_frame['label']\n",
    "        reference_value = row_frame['reference']\n",
    "        f_type = row_frame['type']\n",
    "        frame_id = row_frame['id']\n",
    "        if row_frame['label']  == \"1_frame_prefeitura_nf\":\n",
    "            x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "            texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "            #print(texto_extraido)\n",
    "            label_value = row_frame['label']\n",
    "            #print(\"label_value\", label_value)\n",
    "            values = texto_extraido.split('\\n')\n",
    "            for index_sframe, row_sframe in filtered_sframe_field_nf_v4_df.iterrows():\n",
    "                label_value = row_sframe['label']\n",
    "                #print(\"label_value\", label_value)\n",
    "                if label_value == \"nome_prefeitura\":\n",
    "                    reference_value = row_sframe['reference']\n",
    "                    for value in values:\n",
    "                        result = process_line(value, reference_value, label_value)\n",
    "                        if result:\n",
    "                            data_extrated_prefeitura.update(result)\n",
    "                elif label_value == \"secretaria\":\n",
    "                    reference_value = row_sframe['reference']\n",
    "                    for value in values:\n",
    "                        result = process_line(value, reference_value, label_value)\n",
    "                        if result:\n",
    "                            data_extrated_prefeitura.update(result) \n",
    "                elif label_value == \"tipo_nota_fiscal\":\n",
    "                    reference_value = row_sframe['reference']  \n",
    "                    for value in values:\n",
    "                        result = process_line(value, reference_value, label_value)\n",
    "                        if result:\n",
    "                            data_extrated_prefeitura.update(result)\n",
    "        \n",
    "        if row_frame['label']  == \"1_frame_dados_nf\":\n",
    "            x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "            texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "            #print(texto_extraido)\n",
    "            text_splited = texto_extraido_cabecalho(texto_extraido)\n",
    "            keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "            string_pesquisa = \"Número da Nota:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "            data_extrated_prefeitura['numero_nota_fiscal'] = texto\n",
    "\n",
    "            string_pesquisa = \"Competência:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_extrated_prefeitura['competencia'] = texto\n",
    "\n",
    "            string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_extrated_prefeitura['dt_hr_emissao'] = texto\n",
    "\n",
    "            string_pesquisa = \"Código Verificação:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_extrated_prefeitura['codigo_verificacao'] = texto\n",
    "            #print(f'{frame_id:>5}  {f_type:>20} | {label_value:>30} |  ref:{reference_value:>30}  | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6}')\n",
    "\n",
    "    return data_extrated_prefeitura\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta e o principio da melhor funcao do mundo\n",
    "def extracao_pipeline(qualquer_df, fase, atividade, status):\n",
    "    \n",
    "    doc_info = {}\n",
    "    resumo = {}\n",
    "    row_teste_info = []\n",
    "    \n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    \n",
    "    func_fase = fase\n",
    "    func_atividade = atividade\n",
    "    func_status = status\n",
    "    lista_dicts = []\n",
    "   \n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        dados_iniciais = {}\n",
    "        # dados_root_pipe[idx] = row.to_dict()\n",
    "        row_info = row.to_dict()\n",
    "        message_erro = []\n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        map_document_unique_id = idx\n",
    "        #dados_root_pipe['document_unique_id'] = map_document_unique_id\n",
    "        \n",
    "        map_seq = row['seq']\n",
    "        #dados_root_pipe['seq'] = map_seq\n",
    "        map_batch_name = row['batch']\n",
    "        #dados_root_pipe['batch'] = map_batch_name\n",
    "        map_fase_processo = row['fase_processo']\n",
    "        #dados_root_pipe['fase_processo'] = map_fase_processo\n",
    "        map_nome_atividade = row['nome_atividade']\n",
    "        \n",
    "        map_status_documento = row['status_documento']\n",
    "        map_original_file_name = row['original_file_name']\n",
    "        map_directory = row['directory']\n",
    "        \n",
    "        map_one_page = row['one_page']\n",
    "        \n",
    "        map_palavra_chave = row['palavra_chave']\n",
    "        map_document_tag = row['document_tag']\n",
    "        map_action_item = row['action_item']\n",
    "        \n",
    "        map_level = row['level']\n",
    "        map_file_path = row['file_path']\n",
    "        \n",
    "        dados_iniciais['document_unique_id'] = map_document_unique_id\n",
    "        \n",
    "        file_path = map_file_path\n",
    "\n",
    "        if (map_status_documento != 'NO_PROCESS'):\n",
    "            #1. Buscar Prefeitura, de/para e modelo\n",
    "            if map_status_documento == 'PREPROCESS_EXTRACT':\n",
    "                \n",
    "                dados_iniciais = processar_dados_iniciais(row, map_original_file_name, map_file_path)\n",
    "                \n",
    "                \n",
    "                prefeitura_map = dados_iniciais['prefeitura']\n",
    "                pdf_pesquisavel_map = dados_iniciais['pdf_pesquisavel']\n",
    "                de_para_map = dados_iniciais['de_para_pm']\n",
    "                model_map = dados_iniciais['model']\n",
    "                #cnpj_map = dados_iniciais['cnpj']\n",
    "            \n",
    "                row_info['prefeitura'] = prefeitura_map\n",
    "                row_info['pdf_pesquisavel'] = pdf_pesquisavel_map    \n",
    "                row_info['de_para_pm'] = de_para_map\n",
    "                row_info['model'] = model_map\n",
    "                row_info['model'] = model_map\n",
    "                #row_info['cnpj'] = cnpj_map\n",
    "                data_cabecalho = {}\n",
    "                data_cabecalho['secao'] = \"1 - CABECALHO\"\n",
    "                \n",
    "                if pdf_pesquisavel_map:\n",
    "                   data_cabecalho = processar_cabecalho_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, map_original_file_name, file_path)\n",
    "                   \n",
    "                   nome_prefeitura_map = data_cabecalho['nome_prefeitura'] \n",
    "                   tipo_nota_fiscal_map = data_cabecalho['tipo_nota_fiscal'] \n",
    "                   nro_nota_map = data_cabecalho['numero_nota_fiscal']\n",
    "                   competencia_map = data_cabecalho['competencia']\n",
    "                   dt_hr_emissao_map = data_cabecalho['dt_hr_emissao']  \n",
    "                   codigo_verificacao_map = data_cabecalho['codigo_verificacao']   \n",
    "                   \n",
    "                   \n",
    "                else:\n",
    "                    data_cabecalho = processar_cabecalho_raster_pdf(row, pdf_pesquisavel_map, model_map, map_original_file_name, file_path)\n",
    "                    nome_prefeitura_map = data_cabecalho['nome_prefeitura']\n",
    "                    tipo_nota_fiscal_map = data_cabecalho['tipo_nota_fiscal']\n",
    "                    nro_nota_map = data_cabecalho['numero_nota_fiscal']\n",
    "                    competencia_map = data_cabecalho['competencia']\n",
    "                    dt_hr_emissao_map = data_cabecalho['dt_hr_emissao']  \n",
    "                    codigo_ver_map = data_cabecalho['codigo_verificacao']\n",
    "                    \n",
    "                row_teste_info.append(data_cabecalho)        \n",
    "                \n",
    "                \n",
    "                # print(f' 1 - seq: {map_seq:>5} Pesquisa? {pdf_pesquisavel_map} | {nro_nota_frame} | Pesq: {competencia_frame:>10} | dt_hr_emissao: {dt_hr_emissao_frame} | cod: {codigo_ver_frame} | file: {map_original_file_name} ')\n",
    "                  \n",
    "\n",
    "                lista_dicts.append(row_info)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    novo_df = pd.DataFrame(lista_dicts) \n",
    "                   \n",
    "              \n",
    "    return novo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3B. XXX Efetuo a extracao de dados\n",
    "# analisar_pdf_pesquisavel\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'PREPROCESS' \n",
    "status = 'PREPROCESS_EXTRACT'\n",
    "raw_document_list = []\n",
    "dados_iniciais_qr = {}\n",
    "\n",
    "#raw_texto = extracao_pipeline(df_analise_pipe, fase, atividade, status)subset_df\n",
    "# df_ini, dados_iniciais_qr = extracao_pipeline(df_root_pipe, fase, atividade, status)\n",
    "df = extracao_pipeline(df_root_pipe, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>date_time</th>\n",
       "      <th>batch</th>\n",
       "      <th>fase_processo</th>\n",
       "      <th>nome_atividade</th>\n",
       "      <th>status_documento</th>\n",
       "      <th>acao_executada</th>\n",
       "      <th>original_file_name</th>\n",
       "      <th>directory</th>\n",
       "      <th>one_page</th>\n",
       "      <th>...</th>\n",
       "      <th>action_item</th>\n",
       "      <th>level</th>\n",
       "      <th>parent_document_unique_id</th>\n",
       "      <th>file_hash</th>\n",
       "      <th>file_path</th>\n",
       "      <th>informations</th>\n",
       "      <th>prefeitura</th>\n",
       "      <th>pdf_pesquisavel</th>\n",
       "      <th>de_para_pm</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Nota Fiscal Eletrônica julho.pdf</td>\n",
       "      <td>1004268</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>3</td>\n",
       "      <td>67ea057e-09ab-4cc1-9ca5-6a80f15b260c</td>\n",
       "      <td>0c9302206ff324aac606554ecfa215cb7c955929ca4b8b...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MAGE</td>\n",
       "      <td>True</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>MAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>1.pdf</td>\n",
       "      <td>11790236</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>3</td>\n",
       "      <td>67ea057e-09ab-4cc1-9ca5-6a80f15b260c</td>\n",
       "      <td>66a7db9ee1500d5f9fa5da26563cfd7b68f1f5ba3daba2...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MAGE</td>\n",
       "      <td>False</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>MAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NF 55 JML.pdf</td>\n",
       "      <td>1004813</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>3</td>\n",
       "      <td>67ea057e-09ab-4cc1-9ca5-6a80f15b260c</td>\n",
       "      <td>c6fc93ab85e40f4804b1129827e96d087e0efcf09b4b64...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MAGE</td>\n",
       "      <td>False</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>MAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Digitalização 26 de jul de 2023(1).pdf</td>\n",
       "      <td>11779446</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>3</td>\n",
       "      <td>67ea057e-09ab-4cc1-9ca5-6a80f15b260c</td>\n",
       "      <td>6a96cd40e1a2f525a27e564b31099bda7d4ec035db62a6...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MAGE</td>\n",
       "      <td>False</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>MAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>JULH-2023-NotaFiscal 2.pdf</td>\n",
       "      <td>241538</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>3</td>\n",
       "      <td>67ea057e-09ab-4cc1-9ca5-6a80f15b260c</td>\n",
       "      <td>32016344bc6b954bc69fe5b065a93cb6261be8bfbc06b7...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MAGE</td>\n",
       "      <td>False</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>MAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Nota Fiscal Nº27 VITALE ECO.pdf</td>\n",
       "      <td>11778530</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>3</td>\n",
       "      <td>67ea057e-09ab-4cc1-9ca5-6a80f15b260c</td>\n",
       "      <td>00c372e293ffdfb8c04e984458940eaf265b8c5968e20f...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MAGE</td>\n",
       "      <td>True</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>MAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NFS.300 07.2023 FLOC TEXTIL.pdf</td>\n",
       "      <td>10597</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>3</td>\n",
       "      <td>67ea057e-09ab-4cc1-9ca5-6a80f15b260c</td>\n",
       "      <td>b2096003fcbd7e9c75aa782256d366793e22bb4b4aebfd...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MAGE</td>\n",
       "      <td>True</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>MAGE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq            date_time     batch fase_processo nome_atividade  \\\n",
       "0    7  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "1   14  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "2   18  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "3   22  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "4   23  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "5   24  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "6   31  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "\n",
       "     status_documento acao_executada                      original_file_name  \\\n",
       "0  PREPROCESS_EXTRACT        Analise        Nota Fiscal Eletrônica julho.pdf   \n",
       "1  PREPROCESS_EXTRACT        Analise                                   1.pdf   \n",
       "2  PREPROCESS_EXTRACT        Analise                           NF 55 JML.pdf   \n",
       "3  PREPROCESS_EXTRACT        Analise  Digitalização 26 de jul de 2023(1).pdf   \n",
       "4  PREPROCESS_EXTRACT        Analise              JULH-2023-NotaFiscal 2.pdf   \n",
       "5  PREPROCESS_EXTRACT        Analise         Nota Fiscal Nº27 VITALE ECO.pdf   \n",
       "6  PREPROCESS_EXTRACT        Analise         NFS.300 07.2023 FLOC TEXTIL.pdf   \n",
       "\n",
       "   directory  one_page  ...         action_item level  \\\n",
       "0    1004268     False  ...  PREPROCESS_EXTRACT     3   \n",
       "1   11790236      True  ...  PREPROCESS_EXTRACT     3   \n",
       "2    1004813      True  ...  PREPROCESS_EXTRACT     3   \n",
       "3   11779446      True  ...  PREPROCESS_EXTRACT     3   \n",
       "4     241538      True  ...  PREPROCESS_EXTRACT     3   \n",
       "5   11778530      True  ...  PREPROCESS_EXTRACT     3   \n",
       "6      10597      True  ...  PREPROCESS_EXTRACT     3   \n",
       "\n",
       "              parent_document_unique_id  \\\n",
       "0  67ea057e-09ab-4cc1-9ca5-6a80f15b260c   \n",
       "1  67ea057e-09ab-4cc1-9ca5-6a80f15b260c   \n",
       "2  67ea057e-09ab-4cc1-9ca5-6a80f15b260c   \n",
       "3  67ea057e-09ab-4cc1-9ca5-6a80f15b260c   \n",
       "4  67ea057e-09ab-4cc1-9ca5-6a80f15b260c   \n",
       "5  67ea057e-09ab-4cc1-9ca5-6a80f15b260c   \n",
       "6  67ea057e-09ab-4cc1-9ca5-6a80f15b260c   \n",
       "\n",
       "                                           file_hash  \\\n",
       "0  0c9302206ff324aac606554ecfa215cb7c955929ca4b8b...   \n",
       "1  66a7db9ee1500d5f9fa5da26563cfd7b68f1f5ba3daba2...   \n",
       "2  c6fc93ab85e40f4804b1129827e96d087e0efcf09b4b64...   \n",
       "3  6a96cd40e1a2f525a27e564b31099bda7d4ec035db62a6...   \n",
       "4  32016344bc6b954bc69fe5b065a93cb6261be8bfbc06b7...   \n",
       "5  00c372e293ffdfb8c04e984458940eaf265b8c5968e20f...   \n",
       "6  b2096003fcbd7e9c75aa782256d366793e22bb4b4aebfd...   \n",
       "\n",
       "                                           file_path informations  \\\n",
       "0  pipeline_extracao_documentos/2_documentos_para...          NaN   \n",
       "1  pipeline_extracao_documentos/2_documentos_para...          NaN   \n",
       "2  pipeline_extracao_documentos/2_documentos_para...          NaN   \n",
       "3  pipeline_extracao_documentos/2_documentos_para...          NaN   \n",
       "4  pipeline_extracao_documentos/2_documentos_para...          NaN   \n",
       "5  pipeline_extracao_documentos/2_documentos_para...          NaN   \n",
       "6  pipeline_extracao_documentos/2_documentos_para...          NaN   \n",
       "\n",
       "                     prefeitura pdf_pesquisavel  de_para_pm model  \n",
       "0  PREFEITURA MUNICIPAL DE MAGE            True     PM_MAGE  MAGE  \n",
       "1  PREFEITURA MUNICIPAL DE MAGE           False     PM_MAGE  MAGE  \n",
       "2  PREFEITURA MUNICIPAL DE MAGE           False     PM_MAGE  MAGE  \n",
       "3  PREFEITURA MUNICIPAL DE MAGE           False     PM_MAGE  MAGE  \n",
       "4  PREFEITURA MUNICIPAL DE MAGE           False     PM_MAGE  MAGE  \n",
       "5  PREFEITURA MUNICIPAL DE MAGE            True     PM_MAGE  MAGE  \n",
       "6  PREFEITURA MUNICIPAL DE MAGE            True     PM_MAGE  MAGE  \n",
       "\n",
       "[7 rows x 23 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['document_unique_id'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22980/2653187075.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'document_unique_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_format_argument_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallow_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   6008\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6009\u001b[0m                         \u001b[0mmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6011\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6012\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of {missing} are in the columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6015\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of ['document_unique_id'] are in the columns\""
     ]
    }
   ],
   "source": [
    "df.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>date_time</th>\n",
       "      <th>batch</th>\n",
       "      <th>fase_processo</th>\n",
       "      <th>nome_atividade</th>\n",
       "      <th>status_documento</th>\n",
       "      <th>acao_executada</th>\n",
       "      <th>original_file_name</th>\n",
       "      <th>directory</th>\n",
       "      <th>one_page</th>\n",
       "      <th>...</th>\n",
       "      <th>action_item</th>\n",
       "      <th>level</th>\n",
       "      <th>parent_document_unique_id</th>\n",
       "      <th>file_hash</th>\n",
       "      <th>file_path</th>\n",
       "      <th>informations</th>\n",
       "      <th>prefeitura</th>\n",
       "      <th>pdf_pesquisavel</th>\n",
       "      <th>de_para_pm</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Nota Fiscal Eletrônica julho.pdf</td>\n",
       "      <td>1004268</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>3</td>\n",
       "      <td>67ea057e-09ab-4cc1-9ca5-6a80f15b260c</td>\n",
       "      <td>0c9302206ff324aac606554ecfa215cb7c955929ca4b8b...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MAGE</td>\n",
       "      <td>True</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>MAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>1.pdf</td>\n",
       "      <td>11790236</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>3</td>\n",
       "      <td>67ea057e-09ab-4cc1-9ca5-6a80f15b260c</td>\n",
       "      <td>66a7db9ee1500d5f9fa5da26563cfd7b68f1f5ba3daba2...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MAGE</td>\n",
       "      <td>False</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>MAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NF 55 JML.pdf</td>\n",
       "      <td>1004813</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>3</td>\n",
       "      <td>67ea057e-09ab-4cc1-9ca5-6a80f15b260c</td>\n",
       "      <td>c6fc93ab85e40f4804b1129827e96d087e0efcf09b4b64...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MAGE</td>\n",
       "      <td>False</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>MAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Digitalização 26 de jul de 2023(1).pdf</td>\n",
       "      <td>11779446</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>3</td>\n",
       "      <td>67ea057e-09ab-4cc1-9ca5-6a80f15b260c</td>\n",
       "      <td>6a96cd40e1a2f525a27e564b31099bda7d4ec035db62a6...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MAGE</td>\n",
       "      <td>False</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>MAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>JULH-2023-NotaFiscal 2.pdf</td>\n",
       "      <td>241538</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>3</td>\n",
       "      <td>67ea057e-09ab-4cc1-9ca5-6a80f15b260c</td>\n",
       "      <td>32016344bc6b954bc69fe5b065a93cb6261be8bfbc06b7...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MAGE</td>\n",
       "      <td>False</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>MAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Nota Fiscal Nº27 VITALE ECO.pdf</td>\n",
       "      <td>11778530</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>3</td>\n",
       "      <td>67ea057e-09ab-4cc1-9ca5-6a80f15b260c</td>\n",
       "      <td>00c372e293ffdfb8c04e984458940eaf265b8c5968e20f...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MAGE</td>\n",
       "      <td>True</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>MAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NFS.300 07.2023 FLOC TEXTIL.pdf</td>\n",
       "      <td>10597</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>3</td>\n",
       "      <td>67ea057e-09ab-4cc1-9ca5-6a80f15b260c</td>\n",
       "      <td>b2096003fcbd7e9c75aa782256d366793e22bb4b4aebfd...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MAGE</td>\n",
       "      <td>True</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>MAGE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq            date_time     batch fase_processo nome_atividade  \\\n",
       "0    7  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "1   14  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "2   18  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "3   22  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "4   23  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "5   24  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "6   31  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "\n",
       "     status_documento acao_executada                      original_file_name  \\\n",
       "0  PREPROCESS_EXTRACT        Analise        Nota Fiscal Eletrônica julho.pdf   \n",
       "1  PREPROCESS_EXTRACT        Analise                                   1.pdf   \n",
       "2  PREPROCESS_EXTRACT        Analise                           NF 55 JML.pdf   \n",
       "3  PREPROCESS_EXTRACT        Analise  Digitalização 26 de jul de 2023(1).pdf   \n",
       "4  PREPROCESS_EXTRACT        Analise              JULH-2023-NotaFiscal 2.pdf   \n",
       "5  PREPROCESS_EXTRACT        Analise         Nota Fiscal Nº27 VITALE ECO.pdf   \n",
       "6  PREPROCESS_EXTRACT        Analise         NFS.300 07.2023 FLOC TEXTIL.pdf   \n",
       "\n",
       "   directory  one_page  ...         action_item level  \\\n",
       "0    1004268     False  ...  PREPROCESS_EXTRACT     3   \n",
       "1   11790236      True  ...  PREPROCESS_EXTRACT     3   \n",
       "2    1004813      True  ...  PREPROCESS_EXTRACT     3   \n",
       "3   11779446      True  ...  PREPROCESS_EXTRACT     3   \n",
       "4     241538      True  ...  PREPROCESS_EXTRACT     3   \n",
       "5   11778530      True  ...  PREPROCESS_EXTRACT     3   \n",
       "6      10597      True  ...  PREPROCESS_EXTRACT     3   \n",
       "\n",
       "              parent_document_unique_id  \\\n",
       "0  67ea057e-09ab-4cc1-9ca5-6a80f15b260c   \n",
       "1  67ea057e-09ab-4cc1-9ca5-6a80f15b260c   \n",
       "2  67ea057e-09ab-4cc1-9ca5-6a80f15b260c   \n",
       "3  67ea057e-09ab-4cc1-9ca5-6a80f15b260c   \n",
       "4  67ea057e-09ab-4cc1-9ca5-6a80f15b260c   \n",
       "5  67ea057e-09ab-4cc1-9ca5-6a80f15b260c   \n",
       "6  67ea057e-09ab-4cc1-9ca5-6a80f15b260c   \n",
       "\n",
       "                                           file_hash  \\\n",
       "0  0c9302206ff324aac606554ecfa215cb7c955929ca4b8b...   \n",
       "1  66a7db9ee1500d5f9fa5da26563cfd7b68f1f5ba3daba2...   \n",
       "2  c6fc93ab85e40f4804b1129827e96d087e0efcf09b4b64...   \n",
       "3  6a96cd40e1a2f525a27e564b31099bda7d4ec035db62a6...   \n",
       "4  32016344bc6b954bc69fe5b065a93cb6261be8bfbc06b7...   \n",
       "5  00c372e293ffdfb8c04e984458940eaf265b8c5968e20f...   \n",
       "6  b2096003fcbd7e9c75aa782256d366793e22bb4b4aebfd...   \n",
       "\n",
       "                                           file_path informations  \\\n",
       "0  pipeline_extracao_documentos/2_documentos_para...          NaN   \n",
       "1  pipeline_extracao_documentos/2_documentos_para...          NaN   \n",
       "2  pipeline_extracao_documentos/2_documentos_para...          NaN   \n",
       "3  pipeline_extracao_documentos/2_documentos_para...          NaN   \n",
       "4  pipeline_extracao_documentos/2_documentos_para...          NaN   \n",
       "5  pipeline_extracao_documentos/2_documentos_para...          NaN   \n",
       "6  pipeline_extracao_documentos/2_documentos_para...          NaN   \n",
       "\n",
       "                     prefeitura pdf_pesquisavel  de_para_pm model  \n",
       "0  PREFEITURA MUNICIPAL DE MAGE            True     PM_MAGE  MAGE  \n",
       "1  PREFEITURA MUNICIPAL DE MAGE           False     PM_MAGE  MAGE  \n",
       "2  PREFEITURA MUNICIPAL DE MAGE           False     PM_MAGE  MAGE  \n",
       "3  PREFEITURA MUNICIPAL DE MAGE           False     PM_MAGE  MAGE  \n",
       "4  PREFEITURA MUNICIPAL DE MAGE           False     PM_MAGE  MAGE  \n",
       "5  PREFEITURA MUNICIPAL DE MAGE            True     PM_MAGE  MAGE  \n",
       "6  PREFEITURA MUNICIPAL DE MAGE            True     PM_MAGE  MAGE  \n",
       "\n",
       "[7 rows x 23 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Consulta multiparametros\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pesquisando os dados para o processo de extracao - Raster\n",
    "\n",
    "#f_type = 'frame'\n",
    "f_type = 'field_box'\n",
    "\n",
    "filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['de_para_pm'] == de_para_pm) & (frames_nf_v4_df['model'] == model2search) & (frames_nf_v4_df['type'] == f_type)]\n",
    "\n",
    "for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "    seq = row_frame['seq']\n",
    "    de_para = row_frame['de_para_pm']\n",
    "    model = row_frame['model']  \n",
    "    print(f'{seq:>4} {de_para:>8} | {model:>10} | {row_frame[\"type\"]:>15} | {row_frame[\"label\"]:>35} | {row_frame[\"section_json\"]:>35} | {row_frame[\"x0\"]:>7} | {row_frame[\"y0\"]:>7} | {row_frame[\"x1\"]:>7} | {row_frame[\"y1\"]:>7}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pesquisando os dados para o processo de extracao - PDF Pesquisavel\n",
    "\n",
    "f_type = 'frame'\n",
    "\n",
    "filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['de_para_pm'] == de_para_pm) & (frames_nf_v4_df['model'] == model2search) & (frames_nf_v4_df['type'] == f_type)]\n",
    "\n",
    "\n",
    "for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "    seq = row_frame['seq']\n",
    "    de_para = row_frame['de_para_pm']\n",
    "    model = row_frame['model']  \n",
    "    print(f'{seq:>4} {de_para:>8} | {model:>10} | {row_frame[\"type\"]:>15} | {row_frame[\"label\"]:>35} | {row_frame[\"section_json\"]:>35} | {row_frame[\"x0_p\"]:>7} | {row_frame[\"y0_p\"]:>8} | {row_frame[\"x1_p\"]:>8} | {row_frame[\"y1_p\"]:>8}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model2search) & (frames_nf_v4_df['type'] == tipo)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Avaliar Funçao basica de modelo \n",
    "def pesquisa_model_frame_coordenadas(model_src, tipo, tipo_processo_pdf):\n",
    "\n",
    "    data_dados_model = {}\n",
    "    dados_modelo = []\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_src) & (frames_nf_v4_df['type'] == tipo)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        frame_model = row_frame['model']\n",
    "        frame_prefeitura = row_frame['prefeitura']\n",
    "        frame_label = row_frame['label']\n",
    "        frame_section_json = row_frame['section_json']\n",
    "        frame_cnpj = row_frame['cnpj']\n",
    "        \n",
    "        if frame_model:\n",
    "            modelo = True\n",
    "        \n",
    "        if tipo_processo_pdf == 'pdf_pesquisavel':\n",
    "            try:\n",
    "                # data for PDF Pesquisavel\n",
    "                x0, y0, x1, y1 = row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']\n",
    "                \n",
    "                data_dados_model = {\n",
    "                    \"prefeitura\": frame_prefeitura,\n",
    "                    \"model\": frame_model,\n",
    "                    \"label\": frame_label,\n",
    "                    \"section_json\": frame_section_json,\n",
    "                    \"cnpj\": frame_cnpj,\n",
    "                    \"x0\": x0,\n",
    "                    \"y0\": y0,\n",
    "                    \"x1\": x1,\n",
    "                    \"y1\": y1,\n",
    "                }\n",
    "                dados_modelo.append(data_dados_model)   \n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao carregar coordenadas do frame: {e}\")\n",
    "                x0, y0, x1, y1 = 0, 0, 0, 0 \n",
    "                \n",
    "\n",
    "        elif tipo_processo_pdf == 'raster_pdf':\n",
    "            try:\n",
    "                # data for Raster_PDF\n",
    "                x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "                \n",
    "                data_dados_model = {\n",
    "                    \"prefeitura\": frame_prefeitura,\n",
    "                    \"model\": frame_model,\n",
    "                    \"label\": frame_label,\n",
    "                    \"section_json\": frame_section_json,\n",
    "                    \"cnpj\": frame_cnpj,\n",
    "                    \"x0\": x0,\n",
    "                    \"y0\": y0,\n",
    "                    \"x1\": x1,\n",
    "                    \"y1\": y1,\n",
    "                }\n",
    "                dados_modelo.append(data_dados_model)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao carregar coordenadas do frame: {e}\")\n",
    "                x0, y0, x1, y1 = 0, 0, 0, 0 \n",
    "                  \n",
    "  \n",
    "    return dados_modelo, modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. funçao basica de modelo \n",
    "def pesquisa_model_frame(model_src, tipo):\n",
    "\n",
    "    data_dados_model = {}\n",
    "    dados_modelo = []\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_src) & (frames_nf_v4_df['type'] == tipo)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        frame_model = row_frame['model']\n",
    "        # frame_prefeitura = row_frame['prefeitura']\n",
    "        # frame_label = row_frame['label']\n",
    "        # frame_section_json = row_frame['section_json']\n",
    "        # frame_cnpj = row_frame['cnpj']\n",
    "        \n",
    "        # if frame_model:\n",
    "        #     modelo_frame = True\n",
    "        # else:    \n",
    "        #     modelo_frame = False\n",
    "\n",
    "                  \n",
    "  \n",
    "    return frame_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX funçao basica de modelo \n",
    "def pesquisa_frame_coordenadas_pdf_pesquisa(model_src, tipo, section, label):\n",
    "\n",
    "    tipo = tipo\n",
    "    data_dados_model = {}\n",
    "    dados_modelo = []\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_src) & (frames_nf_v4_df['type'] == tipo) & (frames_nf_v4_df['section_json'] == section) & (frames_nf_v4_df['label'] == label)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        frame_model = row_frame['model']\n",
    "        frame_label = row_frame['label']\n",
    " \n",
    "        # data for PDF Pesquisavel\n",
    "        x0, y0, x1, y1 = row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']\n",
    " \n",
    "    return x0, y0, x1, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao importante - process_line\n",
    "def process_line(value, reference, label):\n",
    "    name_match = re.search(fr'{reference} (.+)', value)\n",
    "    if name_match:\n",
    "        extracted_value = reference + \" \" + name_match.group(1)\n",
    "        return {label: extracted_value}\n",
    "    return None\n",
    "\n",
    "# Função para extrair número da string\n",
    "def extract_number(text):\n",
    "    match = re.search(r'\\b\\d+(\\.\\d+)?\\b', text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def extract_fields_cabecalho_raster_pdf(row, file_path, image_2work): \n",
    "  \n",
    "    message_erro = []\n",
    "  \n",
    "  \n",
    "    model_src = row['modelo']\n",
    "    original_file_name = row['original_file_name'] \n",
    "    \n",
    "    model = row['modelo']   \n",
    "    secao = \"1 - CABECALHO\"\n",
    "    section = \"1 - CABECALHO\"\n",
    "    f_frame_name = \"1_frame_dados_nf\"\n",
    "    f_tipo = 'frame'\n",
    "        \n",
    "    raw_texto_pil = executa_model_frame(model, secao, f_frame_name, f_tipo, image_2work) # executa_model_frame\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        text_splited = texto_extraido_cabecalho(raw_texto_pil)\n",
    "        keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "        string_pesquisa = \"Número da Nota:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "        nro_nota_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Competência:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        competencia_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        dt_hr_emissao_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Código Verificação:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        codigo_verificacao_frame = texto\n",
    "        \n",
    "    except Exception as e:\n",
    "        # erros_cabecalho = {}\n",
    "        msg = (f\"doc: {original_file_name} | {e}\")\n",
    "        nro_nota_frame = \" \"\n",
    "        competencia_frame = \" \"\n",
    "        dt_hr_emissao_frame = \" \"\n",
    "        codigo_verificacao_frame = \" \"\n",
    "        message_erro.append(msg)\n",
    "        \n",
    "        \n",
    "    return nro_nota_frame, competencia_frame, dt_hr_emissao_frame, codigo_verificacao_frame, message_erro, section, raw_texto_pil \n",
    "\n",
    "def extract_fields_cabecalho_pdf_pesquisavel(row, file_path):\n",
    "    \n",
    "    message_erro = [] \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "    \n",
    "    \n",
    "    model_src = row['modelo']\n",
    "    original_file_name = row['original_file_name']\n",
    "    tipo = \"frame\"\n",
    "    section = '1 - CABECALHO'\n",
    "    label = '1_frame_prefeitura_nf'\n",
    "\n",
    "    x0, y0, x1, y1 = pesquisa_frame_coordenadas_pdf_pesquisa(model_src, tipo, section, label)\n",
    "\n",
    "    try:\n",
    "        texto = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        \n",
    "        text_splited = texto_extraido_nf(texto)\n",
    "        keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "        string_pesquisa = \"Número da Nota:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "        nro_nota_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Competência:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        competencia_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        dt_hr_emissao_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Código Verificação:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        codigo_verificacao_frame = texto\n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} | {e}\")\n",
    "        nro_nota_frame = \" \"\n",
    "        competencia_frame = \" \"\n",
    "        dt_hr_emissao_frame = \" \"\n",
    "        codigo_verificacao_frame = \" \"\n",
    "        message_erro.append(msg)\n",
    "            \n",
    "    pdf_document.close()\n",
    "    #print(row)\n",
    "    return nro_nota_frame, competencia_frame, dt_hr_emissao_frame, codigo_verificacao_frame, message_erro, section\n",
    "\n",
    "\n",
    "# 5. XXX Ajusta texto\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF_Pesquisavel-  NO CABECALHO\n",
    "def texto_extraido_nf(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF RASTER NO CABECALHO\n",
    "def texto_extraido_cabecalho(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    text_splited = [s.replace(\")\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_raw = {}\n",
    "i = 0\n",
    "for  doc in raw_document_list:\n",
    "    dict_raw = raw_document_list[i]\n",
    "    print(i)\n",
    "    print(dict_raw['pdf_pesquisavel'])\n",
    "    print(dict_raw['original_file_name'])\n",
    "    print(dict_raw['texto_extraido'])\n",
    "    #print(f'\\nraw_document_list[i]: {raw_document_list[i]}\\n\\n')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields_cnpj(texto_extraido, original_file_name): \n",
    "  \n",
    "    message_erro = []\n",
    "    \n",
    "    data_nf_cnpj_prestador = {}\n",
    "      \n",
    "    print(f'2 extract_fields_cnpj:  {texto_extraido}')\n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in texto_extraido:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', texto_extraido)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "            data_nf_cnpj_prestador['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "            data_nf_cnpj_prestador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "    else:\n",
    "        data_nf_cnpj_prestador['cpf_cnpj_com_mascara'] = None\n",
    "        data_nf_cnpj_prestador['cpf_cnpj_sem_mascara'] = None            \n",
    "            \n",
    "            \n",
    "    telefone_str = None\n",
    "\n",
    "    #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "    telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', texto_extraido)\n",
    "    if telefone_match: \n",
    "        telefone_str = telefone_match.group(1)\n",
    "        # Remover quebras de linha\n",
    "        telefone_str = telefone_str.replace('.', '')\n",
    "        telefone_str = telefone_str.replace('\\n', '')\n",
    "                \n",
    "        data_nf_cnpj_prestador['telefone'] = telefone_str\n",
    "    else:\n",
    "        data_nf_cnpj_prestador['telefone'] = None \n",
    "    \n",
    "    #print(data_nf_cnpj_prestador)    \n",
    "\n",
    "             \n",
    "    return data_nf_cnpj_prestador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Criando um DataFrame de exemplo\n",
    "data = {'A': [1, 2], 'B': [3, 4], 'C': [5, 6]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Criando uma lista para armazenar os dicionários que representam cada linha\n",
    "lista_dicts = []\n",
    "\n",
    "# Iterando sobre as linhas do DataFrame e adicionando cada linha à lista\n",
    "for index, row in df.iterrows():\n",
    "    # Convertendo a linha a um dicionário\n",
    "    row_dict = row.to_dict()\n",
    "    \n",
    "    # Adicionando novas colunas ao dicionário\n",
    "    row_dict['D'] = row['A'] * row['B']\n",
    "    row_dict['E'] = row['B'] + row['C']\n",
    "    \n",
    "    # Adicionando o dicionário à lista\n",
    "    lista_dicts.append(row_dict)\n",
    "\n",
    "# Criando um novo DataFrame a partir da lista de dicionários\n",
    "novo_df = pd.DataFrame(lista_dicts)\n",
    "\n",
    "# Exibindo o novo DataFrame\n",
    "print(novo_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tables-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
