{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <mark>Novo Processo de Pipeline - 1_Extruturacao_Pipeline_docs.ipynb</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atual notebook com as funçoes para processamento de PDF Pesquisavel e Raster e a criaçao dos Dataframes de forma dependente e unica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1 Extração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1 - Modules e config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import platform\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from urllib import response\n",
    "\n",
    "from outlook_msg import Message\n",
    "import extract_msg\n",
    "import zipfile\n",
    "from pyunpack import Archive\n",
    "import py7zr\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "import locale\n",
    "import time, copy\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import cv2\n",
    "import fitz  # Módulo PyMuPDF\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import Image, ImageDraw\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar\n",
    "from pdf2image import convert_from_path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "# Modulos da solucao\n",
    "import modules.extrai_pdf_pesquisavel as Extc\n",
    "import modules.cronometro as cron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Config - E-mail\n",
    "\n",
    "# 1. Caminho do arquivo uma mensagem especifica\n",
    "msg_dir_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails'\n",
    "\n",
    "# 2. Path para arquivos atachados compactados\n",
    "msg_attachment_zip = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments'\n",
    "\n",
    "# 3. Path para arquivos recebidos manualmente\n",
    "arquivos_recebidos_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments'\n",
    "\n",
    "# 4. Path para documentos para extracao\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 6. Path para gestao de imagens resized\n",
    "image_resized_path = \"pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas\"\n",
    "\n",
    "\n",
    "# 1. Path para DFs e CSVs exportados\n",
    "export_path = \"pipeline_extracao_documentos/6_geral_administacao/exports\"\n",
    "\n",
    "# 2. Path para lixeira\n",
    "root_garbage_path = \"pipeline_extracao_documentos/0_lixeira\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### funcoes para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Funcao flush_documents\n",
    "def flush_documents(i, folder_name, file, file4garbage_path, new_path_file4garbage):\n",
    "    \n",
    "    # IMPORTANTE: Coloque aqui sua lógica para cada arquivo\n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    status_process = \"MOVIDO\"\n",
    "    \n",
    "    return {\n",
    "        'id': i,\n",
    "        'data_eliminacao':time_now, \n",
    "        'diretorio': folder_name,\n",
    "        'arquivo': file,\n",
    "        'path_origem': file4garbage_path,\n",
    "        'path_destino': new_path_file4garbage,\n",
    "        'status': status_process,\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "param_garbage_path = root_garbage_path\n",
    "# 2. FunÇao para mover documentos de um diretorio para a lixeira\n",
    "def garbage_pipe(docs4garbage_path, param_garbage_path=root_garbage_path):\n",
    "    rows_list = []   \n",
    "    i = 1\n",
    "    for root, dirs, files in os.walk(docs4garbage_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            # 1. Defino o path do arquivo que sera mandado para lixeira\n",
    "            file4garbage_path = os.path.join(root, file)\n",
    "            # 2. Defino dsubdiretorio para envio de documentos\n",
    "            garbage_subdir_path = os.path.join(param_garbage_path, folder_name)\n",
    "            \n",
    "            # 3. Crio o subdiretorio em garbage\n",
    "            if not os.path.exists(garbage_subdir_path):\n",
    "                os.makedirs(garbage_subdir_path)\n",
    "            \n",
    "            # 4. Defino o path final do arquivo para a lixeira\n",
    "            new_path_file4garbage = os.path.join(garbage_subdir_path, file)\n",
    "            # 5. Processo a transferencia\n",
    "            try:\n",
    "                shutil.move(file4garbage_path, new_path_file4garbage)\n",
    "                print(f'p{i:>3} | move: {file} | for: {new_path_file4garbage}')\n",
    "                new_row = flush_documents(i, folder_name, file, file4garbage_path, new_path_file4garbage)\n",
    "                rows_list.append(new_row)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao mover documento {file}: {e}\")\n",
    "            folder_to_remove = os.path.join(docs4garbage_path, folder_name)\n",
    "            i += 1 \n",
    "               \n",
    "    os.rmdir(folder_to_remove)\n",
    "    \n",
    "    df_garbage = pd.DataFrame(rows_list)\n",
    "    \n",
    "\n",
    "            \n",
    "            #print(\"1. path da lixeira:\", param_garbage_path)\n",
    "    print(i-1, \"Documentos movidos\")\n",
    "    \n",
    "    \n",
    "    # 2. Tratando nome de carga do df_extracao_files_Batch\n",
    "    df_garbage_file = \"df_garbage_\"\n",
    "    df_filename_wr = df_garbage_file + str(folder_name) + \".xlsx\"\n",
    "    path_to_export_df = os.path.join(export_path, df_filename_wr)\n",
    "    # Salvando o DF para excel\n",
    "    df_garbage.to_excel(path_to_export_df, index=False)\n",
    "    \n",
    "    return df_garbage\n",
    "\n",
    "# 3. Buscar o primeiro documento do diretorio EXTERNO para processar\n",
    "def busca_doc_test(src_dir_path, tipo_file, tgt_dir_path):\n",
    " \n",
    "    extensao = \".\" + str(tipo_file)\n",
    "    # Verifique se o diretório de destino está vazio\n",
    "    if os.listdir(tgt_dir_path):\n",
    "        raise Exception(\"O diretório de destino não está vazio!\")\n",
    "\n",
    "    for roots, directories, documents in os.walk(src_dir_path):\n",
    "        # Filtre os documentos para incluir apenas aqueles com extensão .pdf\n",
    "        doc_files = [doc for doc in documents if doc.lower().endswith(extensao)]\n",
    "        \n",
    "        if doc_files: # Verifique se há algum arquivo PDF no diretório\n",
    "            first_doc_file = doc_files[0] # Obtenha o primeiro arquivo PDF\n",
    "            source_path = os.path.join(roots, first_doc_file) # Construa o caminho completo para o primeiro arquivo PDF\n",
    "            print(f\"Encontrei o primeiro arquivo: {source_path}\")\n",
    "            \n",
    "            # Mova o arquivo para o diretório de destino\n",
    "            shutil.move(source_path, tgt_dir_path)\n",
    "            \n",
    "            break # Saia do loop após encontrar o primeiro arquivo PDF\n",
    "\n",
    "\n",
    "def move_pdf_processed_ok(document_path):\n",
    "    \n",
    "    source_path = document_path\n",
    "    destination_path = os.path.join(f'{nf_processada_path}/{str(doc2convert)}')\n",
    "    shutil.move(source_path, destination_path)\n",
    "\n",
    "# # 3. parametros e paths para processamento\n",
    "# param_garbage_path = root_garbage_path\n",
    "# docs4garbage_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "# df_garbage = garbage_pipe(docs4garbage_path, param_garbage_path=root_garbage_path)\n",
    "# df_garbage  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trazendo o primeiro e-mail para teste\n",
    "sorce_mail = \"pipeline_extracao_documentos/0_arquivos_teste_pipeline/emails\"\n",
    "target_dir_path = \"pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails\"\n",
    "\n",
    "tipo_file = \"msg\"\n",
    "busca_doc_test(sorce_mail, tipo_file, target_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1 - Funcoes para e-mail e extracao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>1.</b> Abrindo o pipeline de documentos do email </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geraçao do hash do arquivo\n",
    "def generate_file_hash(file_path):\n",
    "    # Abre o arquivo em modo de leitura de bytes\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # Lê o conteúdo do arquivo\n",
    "        file_data = f.read()\n",
    "        # Utiliza o algoritmo SHA-256 para gerar o hash\n",
    "        file_hash = hashlib.sha256(file_data).hexdigest()\n",
    "    return file_hash\n",
    "\n",
    "\n",
    "# Geraçao do Unique_id do arquivo\n",
    "def generate_unique_id(content):\n",
    "    return hashlib.md5(content.encode()).hexdigest()\n",
    "\n",
    "\n",
    "\n",
    "# 1. XXX Busca proximo Batch\n",
    "def busca_proximo_batch():\n",
    "    # Abre o arquivo Excel e lê a coluna 'batch'\n",
    "    df = pd.read_excel(\"pipeline_extracao_documentos/6_geral_administacao/exports/df_documento_recebido.xlsx\", usecols=[\"batch\"])\n",
    "    # Pega o último valor da coluna 'batch'\n",
    "    last_value = df.iloc[-1, 0]\n",
    "    \n",
    "    # Extraí o número do último batch e adiciona 1 para o próximo\n",
    "    last_number = int(last_value.split(\"_\")[1])\n",
    "    next_number = last_number + 1\n",
    "    \n",
    "    # Forma o nome do próximo batch\n",
    "    next_batch = f\"Batch_{next_number}\"\n",
    "    \n",
    "    return next_batch    \n",
    "\n",
    "\n",
    "# 4. XXX converte nome do arquivo\n",
    "def conv_filename(title):\n",
    "    \n",
    "    # Divide o título em nome e extensão\n",
    "    name, extension = title.rsplit('.', 1) if '.' in title else (title, \"\")\n",
    "\n",
    "    # Remove acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substiti espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remove quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converte para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    # Adiciona a extensão de volta, se houver\n",
    "    if extension:\n",
    "        filename += '.' + extension.lower()\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Batch_17'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Busca proximo Batch caso nao esteja rodando email\n",
    "batch_name = busca_proximo_batch()\n",
    "batch_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Função para verificar e criar a pasta se não existir\n",
    "def check_and_create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "# Função para processar o e-mail individualmente\n",
    "def process_single_email(msg, email_path, origem_docs, first_doc_file):\n",
    "    content = 'email'\n",
    "    new_unique_id = generate_unique_id(content)\n",
    "    hash_value = generate_file_hash(email_path)\n",
    "    original_date_str = msg.date\n",
    "    date_email = cron.convert_email_date(original_date_str)\n",
    "\n",
    "    return {\n",
    "        'dt_hora': date_email,\n",
    "        'origem': origem_docs,\n",
    "        'de': msg.sender.split('<')[0].strip(),\n",
    "        'assunto': msg.subject,\n",
    "        'Batch': busca_proximo_batch(),\n",
    "        'email': msg.sender.split('<')[-1].strip('<>'),\n",
    "        'msg_file': first_doc_file,\n",
    "        'Unique_Id': new_unique_id,\n",
    "        'hash_value': hash_value\n",
    "    }\n",
    "\n",
    "# Função principal para processar e-mails\n",
    "def email_pipe(msg_dir_path, msg_attachment_zip):\n",
    "    locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")\n",
    "    rows_list = []\n",
    "    \n",
    "    check_and_create_folder(msg_attachment_zip)\n",
    "    \n",
    "    for root, dirs, files in os.walk(msg_dir_path):\n",
    "        doc_files = [doc for doc in files if doc.lower().endswith(\".msg\")]\n",
    "        if doc_files:\n",
    "            first_doc_file = doc_files[0]\n",
    "            email_path = os.path.join(root, first_doc_file)\n",
    "            try:\n",
    "                msg = extract_msg.Message(email_path)\n",
    "                new_row = process_single_email(msg, email_path, 'email', first_doc_file)\n",
    "                rows_list.append(new_row)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao ler email: {e}\")\n",
    "\n",
    "    df_mail = pd.DataFrame(rows_list)\n",
    "    return df_mail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que msg_dir_path e msg_attachment_zip são suas variáveis de caminho\n",
    "df_mail = email_pipe(msg_dir_path, msg_attachment_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt_hora</th>\n",
       "      <th>origem</th>\n",
       "      <th>de</th>\n",
       "      <th>assunto</th>\n",
       "      <th>Batch</th>\n",
       "      <th>email</th>\n",
       "      <th>msg_file</th>\n",
       "      <th>Unique_Id</th>\n",
       "      <th>hash_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/08/2023 23:20:56</td>\n",
       "      <td>email</td>\n",
       "      <td>Verlânio Gallindo</td>\n",
       "      <td>Fwd: Notas Magé 2</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>verlanio@gmail.com</td>\n",
       "      <td>Fwd Notas Magé 2.msg</td>\n",
       "      <td>0c83f57c786a0b4a39efab23731c7ebc</td>\n",
       "      <td>38a8682b76d4456d529ba103930609fb16cde5a612c367...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dt_hora origem                 de            assunto     Batch  \\\n",
       "0  10/08/2023 23:20:56  email  Verlânio Gallindo  Fwd: Notas Magé 2  Batch_17   \n",
       "\n",
       "                email              msg_file                         Unique_Id  \\\n",
       "0  verlanio@gmail.com  Fwd Notas Magé 2.msg  0c83f57c786a0b4a39efab23731c7ebc   \n",
       "\n",
       "                                          hash_value  \n",
       "0  38a8682b76d4456d529ba103930609fb16cde5a612c367...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extrai documentos compactados\n",
    "def extract_documents(src_compressed_file_path, tgt_directory_path, batch, content, unique_id):\n",
    "    output_dir = os.path.join(tgt_directory_path, batch)\n",
    "    folder_file_dict = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(src_compressed_file_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            new_unique_id = generate_unique_id(content)\n",
    "\n",
    "            folder_file_dict[new_unique_id] = {\n",
    "                'filename': file,\n",
    "                'hash_value': generate_file_hash(file_path),\n",
    "                'Unique_Id': unique_id,\n",
    "            }\n",
    "\n",
    "            if file.lower().endswith('.zip'):\n",
    "                zip_file = file\n",
    "                zip_file_path = os.path.join(root, file)\n",
    "                # Obtém o nome base do arquivo ZIP para usar como subdiretório\n",
    "                zip_base_name = os.path.splitext(os.path.basename(zip_file_path))[0]\n",
    "                zip_basename = conv_filename(zip_base_name)\n",
    "                \n",
    "                # Cria o subdiretório com base no nome do arquivo ZIP\n",
    "                root_output_dir = os.path.join(output_dir, zip_basename)\n",
    "                \n",
    "                if not os.path.exists(root_output_dir):\n",
    "                    os.makedirs(root_output_dir) # estou criando o diretorio caso nao exista\n",
    "\n",
    "                # Abre o arquivo ZIP\n",
    "                with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                    for member in zip_ref.namelist():\n",
    "                        # Separa o nome da pasta e o nome do arquivo usando barra invertida como delimitador\n",
    "                        parts = member.rsplit('\\\\', 1)\n",
    "                        folder_name = parts[0] if len(parts) > 1 else ''\n",
    "                        #folder_name = conv_filename(folder_temp)\n",
    "                        filename = parts[-1]\n",
    "                        if filename:  # ignora diretórios\n",
    "                            # Adiciona ao dicionário\n",
    "                            #filename = conv_filename(filename)\n",
    "                            # Cria um subdiretório se ele não existir\n",
    "                            sub_dir = os.path.join(root_output_dir, folder_name)\n",
    "                            if not os.path.exists(sub_dir):\n",
    "                                os.makedirs(sub_dir)\n",
    "\n",
    "                            # Salva o arquivo no subdiretório especificado\n",
    "                            source = zip_ref.open(member)\n",
    "                            target_path = os.path.join(sub_dir, filename)\n",
    "                            \n",
    "                            with open(target_path, \"wb\") as target:\n",
    "                                target.write(source.read())\n",
    "                                dir_path = os.path.dirname(filename)\n",
    "                                \n",
    "            elif file.lower().endswith('.rar'):\n",
    "                rar_file = file\n",
    "                rar_file = conv_filename_no_ext(rar_file)\n",
    "                rar_file_path = os.path.join(root, file) \n",
    "                root_output_dir = os.path.join(output_dir, rar_file)\n",
    "                if not os.path.exists(root_output_dir):\n",
    "                    os.makedirs(root_output_dir)\n",
    "                Archive(rar_file_path).extractall(root_output_dir)  \n",
    "                \n",
    "            elif file.lower().endswith('.7z'):\n",
    "                sevenz_file = file\n",
    "                sevenz_file = conv_filename_no_ext(sevenz_file)\n",
    "                sevenz_file_path = os.path.join(root, file)\n",
    "                root_output_dir = os.path.join(output_dir, sevenz_file)                      \n",
    "                                \n",
    "                with py7zr.SevenZipFile(sevenz_file_path, mode='r') as z:\n",
    "                    z.extractall(root_output_dir)\n",
    "                    \n",
    "            elif file.lower().endswith('.pdf'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                new_path_name = os.path.join(output_dir, file)\n",
    "                if not os.path.exists(output_dir):\n",
    "                                os.makedirs(output_dir)\n",
    "                shutil.move(file_path, new_path_name)\n",
    "                \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para adicionar um novo registro em df_source\n",
    "def add_source_entry(batch_name, type='email'):\n",
    "    unique_id = generate_unique_id(batch_name)\n",
    "    data = {\n",
    "        'Unique_ID': unique_id,\n",
    "        'Type': type,\n",
    "        'Batch': batch_name,\n",
    "        'Data': pd.Timestamp.now()\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source = pd.DataFrame(columns=['Unique_ID', 'Type', 'Batch', 'Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando um novo registro (substitua 'batch_name' e 'email' conforme necessário)\n",
    "new_entry = add_source_entry(batch_name, type='email')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29631/3213830368.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_source = df_source.append(new_entry, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df_source = df_source.append(new_entry, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Batch</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7645834ecf8a0a0eee72b08c42e76996</td>\n",
       "      <td>email</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>2023-09-06 21:49:01.086234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Unique_ID   Type     Batch  \\\n",
       "0  7645834ecf8a0a0eee72b08c42e76996  email  Batch_17   \n",
       "\n",
       "                        Data  \n",
       "0 2023-09-06 21:49:01.086234  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_unique_id(batch_name, df_source, content):\n",
    "    try:\n",
    "        parent_unique_id = df_source[(df_source['Batch'] == batch_name) & (df_source['Type'] == content)]['Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        parent_unique_id = None\n",
    "        print(f\"Unique_ID for batch {batch_name} and content {content} not found in df_source.\")\n",
    "    return parent_unique_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "parente_unique_id = get_parent_unique_id(batch_name, df_source, 'email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7645834ecf8a0a0eee72b08c42e76996'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parente_unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Estabelece o pipeline de documentos\n",
    "def abertura_pipeline(index, batch_name, file, documentos_extracao_path, folder_name, file_path, content, parent_unique_id):\n",
    "    \n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    status_pro = \"para_analise\"\n",
    "    new_unique_id = generate_unique_id(content)\n",
    "    \n",
    "    return {\n",
    "        'index': index,\n",
    "        'data_processamento': time_now,\n",
    "        'Batch': batch_name,\n",
    "        'diretorio_origem': folder_name,\n",
    "        'nome_arquivo': file,\n",
    "        'diretorio_origem': file_path,\n",
    "        'status': status_pro,\n",
    "        'Unique_ID': new_unique_id,\n",
    "        'hash_value': generate_file_hash(file_path),\n",
    "        'Parent_Unique_ID': parent_unique_id\n",
    "    }\n",
    "\n",
    "\n",
    "# 3. função para ajustar documentos no pipeline antes de serem processados\n",
    "def pipeline_preprocessamento(documentos_extracao_path, content, unique_id):\n",
    "    rows_list = []\n",
    "    i = 0\n",
    "    for root, dirs, files in os.walk(documentos_extracao_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                i += 1\n",
    "                new_row = abertura_pipeline(i, batch_name, file, documentos_extracao_path, folder_name, file_path, content, parent_unique_id)\n",
    "                rows_list.append(new_row)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao criar nova row: {e}\")\n",
    "    \n",
    "    df_documento_recebido = pd.DataFrame(rows_list)\n",
    "    return df_documento_recebido    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Batch_17'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id = df_source[df_source['Batch'] == batch_name]['Unique_ID'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = 'compressed_file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_documents(msg_attachment_zip, documentos_extracao_path, batch_name, content, unique_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipeline_preprocessamento(documentos_extracao_path, content, unique_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>data_processamento</th>\n",
       "      <th>Batch</th>\n",
       "      <th>diretorio_origem</th>\n",
       "      <th>nome_arquivo</th>\n",
       "      <th>status</th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>hash_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>time_now_placeholder</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NFS-e 35.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "      <td>fcfaab93e2a5afc3cda74c9966559eed</td>\n",
       "      <td>3831e488a24090cc78faf10dd98a102323d954bf2e79c6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>time_now_placeholder</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NFS-e 37.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "      <td>fcfaab93e2a5afc3cda74c9966559eed</td>\n",
       "      <td>5a420e4de9e05e734d5306bf8f4094fbe82a987a24fb7e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>time_now_placeholder</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NFS-e 36 CANCELADA.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "      <td>fcfaab93e2a5afc3cda74c9966559eed</td>\n",
       "      <td>3c25ef5afa1af34119e473333da508b097600bf55f89cd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>time_now_placeholder</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NFS-e 38 CANCELADA.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "      <td>fcfaab93e2a5afc3cda74c9966559eed</td>\n",
       "      <td>f3b375f8362e0a93607fe938095a947ebc102b2af8e1da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>time_now_placeholder</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Doria Marinho 0297 Raquel.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "      <td>fcfaab93e2a5afc3cda74c9966559eed</td>\n",
       "      <td>f08d5c1e8583a5755cd9b9a328444132fbe127445f6960...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>time_now_placeholder</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Doria Marinho 0298 Marcelo.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "      <td>fcfaab93e2a5afc3cda74c9966559eed</td>\n",
       "      <td>88d764f214df1c1c915b8975eeba24d671ca33490e2ef0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>time_now_placeholder</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Doria Marinho 0295 Carlos Leandro.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "      <td>fcfaab93e2a5afc3cda74c9966559eed</td>\n",
       "      <td>53305daf18b4e42f36e27a8b900fae3a57cc033b402ba5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>time_now_placeholder</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Doria Marinho 0300 Vanisa.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "      <td>fcfaab93e2a5afc3cda74c9966559eed</td>\n",
       "      <td>becab429d6ec14ff2e42867f7eb4189bbe09a0c3df7503...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>time_now_placeholder</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Doria Marinho 0296 Vanisa (Cancelada).pdf</td>\n",
       "      <td>para_analise</td>\n",
       "      <td>fcfaab93e2a5afc3cda74c9966559eed</td>\n",
       "      <td>dac71a025f4d2d6b452f31f7573f2d182aee6b6a9f8eda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>time_now_placeholder</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Doria Marinho 0301 Ultrascan.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "      <td>fcfaab93e2a5afc3cda74c9966559eed</td>\n",
       "      <td>979071adf47addba5183e66c7cb8dc5ab1eec28cc91bb1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>time_now_placeholder</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Doria Marinho 0299 Luciana.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "      <td>fcfaab93e2a5afc3cda74c9966559eed</td>\n",
       "      <td>789163ae50d04d2b51b12ab3b3423dc2436293f7caf72a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index    data_processamento     Batch  \\\n",
       "0       1  time_now_placeholder  Batch_17   \n",
       "1       2  time_now_placeholder  Batch_17   \n",
       "2       3  time_now_placeholder  Batch_17   \n",
       "3       4  time_now_placeholder  Batch_17   \n",
       "4       5  time_now_placeholder  Batch_17   \n",
       "5       6  time_now_placeholder  Batch_17   \n",
       "6       7  time_now_placeholder  Batch_17   \n",
       "7       8  time_now_placeholder  Batch_17   \n",
       "8       9  time_now_placeholder  Batch_17   \n",
       "9      10  time_now_placeholder  Batch_17   \n",
       "10     11  time_now_placeholder  Batch_17   \n",
       "\n",
       "                                     diretorio_origem  \\\n",
       "0   pipeline_extracao_documentos/2_documentos_para...   \n",
       "1   pipeline_extracao_documentos/2_documentos_para...   \n",
       "2   pipeline_extracao_documentos/2_documentos_para...   \n",
       "3   pipeline_extracao_documentos/2_documentos_para...   \n",
       "4   pipeline_extracao_documentos/2_documentos_para...   \n",
       "5   pipeline_extracao_documentos/2_documentos_para...   \n",
       "6   pipeline_extracao_documentos/2_documentos_para...   \n",
       "7   pipeline_extracao_documentos/2_documentos_para...   \n",
       "8   pipeline_extracao_documentos/2_documentos_para...   \n",
       "9   pipeline_extracao_documentos/2_documentos_para...   \n",
       "10  pipeline_extracao_documentos/2_documentos_para...   \n",
       "\n",
       "                                 nome_arquivo        status  \\\n",
       "0                                NFS-e 35.pdf  para_analise   \n",
       "1                                NFS-e 37.pdf  para_analise   \n",
       "2                      NFS-e 36 CANCELADA.pdf  para_analise   \n",
       "3                      NFS-e 38 CANCELADA.pdf  para_analise   \n",
       "4               Doria Marinho 0297 Raquel.pdf  para_analise   \n",
       "5              Doria Marinho 0298 Marcelo.pdf  para_analise   \n",
       "6       Doria Marinho 0295 Carlos Leandro.pdf  para_analise   \n",
       "7               Doria Marinho 0300 Vanisa.pdf  para_analise   \n",
       "8   Doria Marinho 0296 Vanisa (Cancelada).pdf  para_analise   \n",
       "9            Doria Marinho 0301 Ultrascan.pdf  para_analise   \n",
       "10             Doria Marinho 0299 Luciana.pdf  para_analise   \n",
       "\n",
       "                           Unique_ID  \\\n",
       "0   fcfaab93e2a5afc3cda74c9966559eed   \n",
       "1   fcfaab93e2a5afc3cda74c9966559eed   \n",
       "2   fcfaab93e2a5afc3cda74c9966559eed   \n",
       "3   fcfaab93e2a5afc3cda74c9966559eed   \n",
       "4   fcfaab93e2a5afc3cda74c9966559eed   \n",
       "5   fcfaab93e2a5afc3cda74c9966559eed   \n",
       "6   fcfaab93e2a5afc3cda74c9966559eed   \n",
       "7   fcfaab93e2a5afc3cda74c9966559eed   \n",
       "8   fcfaab93e2a5afc3cda74c9966559eed   \n",
       "9   fcfaab93e2a5afc3cda74c9966559eed   \n",
       "10  fcfaab93e2a5afc3cda74c9966559eed   \n",
       "\n",
       "                                           hash_value  \n",
       "0   3831e488a24090cc78faf10dd98a102323d954bf2e79c6...  \n",
       "1   5a420e4de9e05e734d5306bf8f4094fbe82a987a24fb7e...  \n",
       "2   3c25ef5afa1af34119e473333da508b097600bf55f89cd...  \n",
       "3   f3b375f8362e0a93607fe938095a947ebc102b2af8e1da...  \n",
       "4   f08d5c1e8583a5755cd9b9a328444132fbe127445f6960...  \n",
       "5   88d764f214df1c1c915b8975eeba24d671ca33490e2ef0...  \n",
       "6   53305daf18b4e42f36e27a8b900fae3a57cc033b402ba5...  \n",
       "7   becab429d6ec14ff2e42867f7eb4189bbe09a0c3df7503...  \n",
       "8   dac71a025f4d2d6b452f31f7573f2d182aee6b6a9f8eda...  \n",
       "9   979071adf47addba5183e66c7cb8dc5ab1eec28cc91bb1...  \n",
       "10  789163ae50d04d2b51b12ab3b3423dc2436293f7caf72a...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Caminho do arquivo uma mensagem especifica\n",
    "msg_dir_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails'\n",
    "\n",
    "# 2. Path para arquivos atachados compactados\n",
    "msg_attachment_zip = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments'\n",
    "\n",
    "# 3. Path para arquivos recebidos manualmente\n",
    "arquivos_recebidos_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments'\n",
    "\n",
    "# 4. Path para documentos para extracao\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Processa e-mail e guarda seus anexos\n",
    "df_mail, batch_name = email_pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Efetua a extraçao de documento\n",
    "extract_documents(msg_attachment_zip, documentos_extracao_path, batch_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. XXX Estabelece o pipeline de documentos\n",
    "def abertura_pipeline(index, batch_name, file, documentos_extracao_path, folder_name, file_path):\n",
    "    action_itens = {}\n",
    "    \n",
    "    status_pro = \"para_analise\"\n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'index': index,\n",
    "        'data_processamento': time_now,\n",
    "        'Batch': batch_name,\n",
    "        'diretorio_origem': folder_name,\n",
    "        'nome_arquivo': file,\n",
    "        'diretorio_origem': file_path,\n",
    "        'status': status_pro,\n",
    "    }\n",
    "\n",
    "\n",
    "# 3. funcao para ajustar documentos no pipeline antes de serem processados\n",
    "def pipeline_preprocessamento():\n",
    "    rows_list = []\n",
    "    i = 0\n",
    "    for root, dirs, files in os.walk(documentos_extracao_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                i += 1\n",
    "                new_row = abertura_pipeline(i, batch_name, file, documentos_extracao_path, folder_name, file_path)\n",
    "                rows_list.append(new_row)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao criar nova row: {e}\") \n",
    "            \n",
    "    \n",
    "    df_documento_recebido = pd.DataFrame(rows_list)\n",
    "    \n",
    "    return df_documento_recebido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_documento_recebido = pipeline_preprocessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_documento_recebido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. XXX Analisa nro de paginas\n",
    "def analisa_nro_pages(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    pages = pdf_document.pages() # generator object\n",
    "\n",
    "    page_nro = []\n",
    "    for page in pages:\n",
    "        page_nro.append(page)\n",
    "        \n",
    "    nro_paginas = len(page_nro)    \n",
    "    if nro_paginas > 1:\n",
    "        doc_1_page = False\n",
    "        return doc_1_page, nro_paginas    \n",
    "    else:\n",
    "        doc_1_page = True\n",
    "        return doc_1_page, nro_paginas  \n",
    "    pdf_document.close() \n",
    "    \n",
    "    \n",
    "# 3. XXX Split de paginas\n",
    "def split_pdf_pages(file2split_path):\n",
    "    \n",
    "    try:\n",
    "        pdf = fitz.open(file2split_path)\n",
    "        # Número total de páginas no PDF\n",
    "        total_pages = len(pdf)\n",
    "    except Exception as e:\n",
    "        print(f\"Nao congui abrir o PDF: {e}\")    \n",
    "\n",
    "    # Nome base para os arquivos de saída\n",
    "    base_name = file2split_path.split('.')[0]  # Remove a extensão do arquivo\n",
    "    \n",
    "    file_to_delete = file2split_path\n",
    "\n",
    "    # Loop para criar um novo PDF para cada página\n",
    "    for page_num in range(total_pages):\n",
    "        # Cria um novo objeto PDF\n",
    "        new_pdf = fitz.open()\n",
    "        # Adiciona a página atual ao novo PDF\n",
    "        new_pdf.insert_pdf(pdf, from_page=page_num, to_page=page_num)\n",
    "        # Nome do novo arquivo PDF\n",
    "        new_pdf_name = f\"{base_name}_page_{page_num + 1}.pdf\"\n",
    "        # Salva o novo PDF\n",
    "        new_pdf.save(new_pdf_name)\n",
    "        # Fecha o novo PDF\n",
    "        new_pdf.close()\n",
    "    return new_pdf_name, file_to_delete\n",
    "    # Fecha o PDF original\n",
    "    pdf.close()   \n",
    "    \n",
    "    \n",
    "# 4. XXX Efetua a conversao e o resize pagina - NF (convem pensar noutro modelo para listagem) \n",
    "def convertResizeAnalise_1page(doc2convert, document_path, image_resized_path):\n",
    "    \n",
    "    # 1. construo um novo nome para o documento imagem\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(doc2convert)}.jpg')\n",
    "    \n",
    "    # 2. Conversao para imagem\n",
    "    pages = convert_from_path(document_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    # 3. Para cada pagina faco o resize (apesar de ser somente uma)\n",
    "    resized_pages = []\n",
    "    for page in pages:\n",
    "        resized_page = page.resize((2067, 2923))\n",
    "        resized_pages.append(resized_page)\n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "    return resized_pages[0], image_resized_name \n",
    "\n",
    "\n",
    "# 5. XXX Pesquisa prefeitura no documento (dando as coordenadas) e efetuando o OCR\n",
    "def pequisaTextoDoc(image_name):\n",
    "\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = 0\n",
    "    y0 = 0\n",
    "    x1= 2066\n",
    "    y1 = 2922\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "    \n",
    "    return extracted_text_frame \n",
    "\n",
    "# 6.1. XXX Ajusta texto\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "# 6.2. XXX Ajusta texto sem quebrar o \":\"\n",
    "def texto_extraido_nf(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_df_analise(df_documento_recebido):\n",
    "    linhas_df_analise = []\n",
    "    pre_processo = ['ajustar_nome', 'split_paginas', 'ajustar_imagem', 'buscar_nome_prefeitura', 'enviar_canceladas', 'enviar_listagens']\n",
    "    \n",
    "    for idx, row in df_documento_recebido.iterrows():\n",
    "        file_name = row['nome_arquivo']\n",
    "        file_path = row['diretorio_origem']\n",
    "        #file_path = f\"{row['diretorio_origem']}/{file_name}\"  # Ajustar como necessário\n",
    "\n",
    "        # Chama a função analisa_nro_pages\n",
    "        doc_1_page, nro_paginas = analisa_nro_pages(file_path)\n",
    "\n",
    "        # Chama a função conv_filename\n",
    "        new_name = conv_filename(file_name)\n",
    "\n",
    "        acoes_necessarias = {}\n",
    "\n",
    "        # Se o documento tem mais de uma página\n",
    "        if not doc_1_page:\n",
    "            acoes_necessarias['split_paginas'] = nro_paginas\n",
    "\n",
    "        # Se o nome do arquivo precisa ser ajustado\n",
    "        if new_name != file_name:\n",
    "            acoes_necessarias['ajustar_nome'] = new_name\n",
    "            \n",
    "        if \"cancelada\" in file_name.lower():\n",
    "            acoes_necessarias['enviar_canceladas'] = True\n",
    "            \n",
    "        if \"listagem\" in file_name.lower():\n",
    "            acoes_necessarias['enviar_listagens'] = True\n",
    "                \n",
    "\n",
    "        for action in pre_processo:\n",
    "            if action in acoes_necessarias:\n",
    "                info = acoes_necessarias[action]\n",
    "                seq = pre_processo.index(action) + 1  # Pega a sequência da ação baseada na lista pre_processo\n",
    "                \n",
    "                nova_linha = {\n",
    "                    'index': row['index'],\n",
    "                    'data_processamento': row['data_processamento'],\n",
    "                    'Batch': row['Batch'],\n",
    "                    'nome_arquivo': row['nome_arquivo'],\n",
    "                    'seq': seq,\n",
    "                    'action': action,\n",
    "                    'info_adicional': info,\n",
    "                    'diretorio_origem': row['diretorio_origem'],\n",
    "\n",
    "\n",
    "                }\n",
    "                linhas_df_analise.append(nova_linha)\n",
    "\n",
    "    df_analise = pd.DataFrame(linhas_df_analise)\n",
    "    return df_analise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise = criar_df_analise(df_documento_recebido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame de análise\n",
    "df_analise = pd.DataFrame(columns=['data_processamento', 'Batch', 'diretorio_origem', 'nome_arquivo', 'unique_ID', 'parent_ID', 'seq', 'action', 'nro_paginas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ao receber um novo documento\n",
    "new_unique_id = generate_unique_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame de documentos recebidos\n",
    "df_documento_recebido = pd.DataFrame(columns=['Batch', 'nome_arquivo', 'caminho_arquivo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preenche o df_documento_recebido com informações do novo documento\n",
    "df_documento_recebido = df_documento_recebido.append({'Batch': 'Batch_17', 'nome_arquivo': 'Exemplo.pdf', 'caminho_arquivo': '/caminho/Exemplo.pdf'}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preenche o df_analise com análises iniciais e o unique_ID\n",
    "df_analise = df_analise.append({\n",
    "    'data_processamento': pd.Timestamp.now(),\n",
    "    'Batch': 'Batch_17',\n",
    "    'diretorio_origem': '/caminho/',\n",
    "    'nome_arquivo': 'Exemplo.pdf',\n",
    "    'unique_ID': new_unique_id,\n",
    "    'parent_ID': None,\n",
    "    'seq': 1,\n",
    "    'action': 'recebido',\n",
    "    'nro_paginas': 2\n",
    "}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_17/fwdnotasfaltantesnosistemadeemisso/Doria Marinho 0295 Carlos Leandro.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path_2 = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_17/fwdnotasfaltantesnosistemadeemisso/Doria Marinho 0295 Carlos Leandro copy.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_path = \"pipeline_extracao_documentos/0_arquivos_teste_pipeline/emails/Notas Magé.msg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_value = generate_file_hash(doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"O hash do arquivo é: {hash_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_value_2 = generate_file_hash(doc_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"O hash do arquivo é: {hash_value_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_msg_value = generate_file_hash(msg_path)\n",
    "print(f\"O hash do arquivo é: {hash_msg_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = \"pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/fwdnotasfaltantesnosistemadeemisso.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_zip_value = generate_file_hash(zip_path)\n",
    "print(f\"O hash do arquivo é: {hash_zip_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar aqui um df_documento_recebido para teste\n",
    "# df_documento_recebido = ...\n",
    "\n",
    "# Chama a função e cria df_analise\n",
    "# df_analise = criar_df_analise(df_documento_recebido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada linha do df_documento_recebido\n",
    "for idx, row in df_documento_recebido.iterrows():\n",
    "    # Iterar sobre cada ação da lista pre_processo\n",
    "    file_name = row['nome_arquivo']\n",
    "    print(f'\\n\\nidx: {idx} | file_name: {file_name}\\n')\n",
    "    for seq, action in enumerate(pre_processo, start=1):\n",
    "        print(f'seq: {seq} | action: {action}')\n",
    "        nova_linha = {\n",
    "            'index': row['index'],\n",
    "            'data_processamento': row['data_processamento'],\n",
    "            'Batch': row['Batch'],\n",
    "            'diretorio_origem': row['diretorio_origem'],\n",
    "            'nome_arquivo': row['nome_arquivo'],\n",
    "            'seq': seq,\n",
    "            'action': action\n",
    "        }\n",
    "        \n",
    "        linhas_df_analise.append(nova_linha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o novo DataFrame\n",
    "df_analise = pd.DataFrame(linhas_df_analise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processo = ['ajustar_nome', 'split_paginas', 'ajustar_imagem', 'buscar_nome_prefeitura']\n",
    "\n",
    "# Lista vazia para armazenar as linhas do novo DataFrame\n",
    "linhas_df_analise = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_pdf(args):\n",
    "    # Lógica para analisar o PDF e retornar uma lista de ações necessárias\n",
    "    acoes = []\n",
    "    if args['numero_paginas'] > 1:\n",
    "        acoes.append('split_paginas')\n",
    "    if args['nome_precisa_ajuste']:\n",
    "        acoes.append('ajustar_nome')\n",
    "    # Adicionar outras verificações aqui\n",
    "    return acoes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linhas_df_analise = []\n",
    "\n",
    "for idx, row in df_documento_recebido.iterrows():\n",
    "    # Realizar a análise inicial para determinar quais ações são necessárias\n",
    "    acoes_necessarias = analisar_pdf(row)\n",
    "    \n",
    "    for seq, action in enumerate(acoes_necessarias, start=1):\n",
    "        nova_linha = {\n",
    "            'index': row['index'],\n",
    "            'data_processamento': row['data_processamento'],\n",
    "            'Batch': row['Batch'],\n",
    "            'diretorio_origem': row['diretorio_origem'],\n",
    "            'nome_arquivo': row['nome_arquivo'],\n",
    "            'seq': seq,\n",
    "            'action': action\n",
    "        }\n",
    "        linhas_df_analise.append(nova_linha)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ZZZ Estou analisando a quantidade de paginas \n",
    "one_page_doc, paginas = analisa_nro_pages(new_path_name)\n",
    "\n",
    "# 2. ZZZ Faço um If para tomar de decisao do documentos\n",
    "if not one_page_doc:\n",
    "    print(f'documento com varias paginas = {paginas} ser splitado')\n",
    "else:    \n",
    "    print(f'documento apenas {paginas} pagina e nao preciosa ser splitado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. XXX Estabelece o pipeline de documentos\n",
    "def ajusta_pipeline(index, batch_name, file, documentos_extracao_path, folder_name, file_path):\n",
    "    \n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    # 1. Processo de ajuste de nome do documento\n",
    "    new_name = conv_filename(file)\n",
    "    #print(f'file: {file} | new_name: {new_name}')\n",
    "    root_dir_batch = os.path.join(documentos_extracao_path, batch_name)\n",
    "    root_dir_batch_dir = os.path.join(root_dir_batch, folder_name)\n",
    "    new_name_file_path = os.path.join(root_dir_batch_dir, new_name)\n",
    "    if new_name != file:\n",
    "        #print(f'nome mudou | {folder_name} | file: {file} | new_name: {new_name}\\n{file_path}\\n{new_name_file_path}\\n')\n",
    "        status_pro = 'NOME AJUSTADO'\n",
    "        try:\n",
    "            shutil.move(file_path, new_name_file_path)\n",
    "            status_pro = 'NOME AJUSTADO'\n",
    "            new_path_name = new_name_file_path\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao mudar nome do arquivo: {e}\")  \n",
    "    else:\n",
    "        status_pro = 'processado'\n",
    "        #print(f'Nao mudou | {folder_name}  | file: {file} | new_name: {new_name}')\n",
    "        \n",
    "        new_path_name = file_path\n",
    "    \n",
    "    # 2. Verifica nro de paginas\n",
    "    one_page_doc, paginas = analisa_nro_pages(new_path_name)\n",
    "    # 2. ZZZ Faço um If para tomar de decisao do documentos\n",
    "    if not one_page_doc:\n",
    "        try:\n",
    "            print(f'documento com varias paginas = {paginas} ser splitado')\n",
    "            # 3. Processa o split de paginas\n",
    "            new_pdf_path_file, nome_file_deletar = split_pdf_pages(new_path_name)\n",
    "            \n",
    "            # 3.1 elimina documento com varias paginas\n",
    "            if new_pdf_path_file:\n",
    "                os.remove(nome_file_deletar)\n",
    "            \n",
    "            df_split = processa_pipeline()   \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao efetuar split de paginas: {e}\")       \n",
    "    else:    \n",
    "        print(f'documento apenas {paginas} pagina e nao preciosa ser splitado')\n",
    "\n",
    "    return {\n",
    "        'index': index,\n",
    "        'data_processamento': time_now,\n",
    "        'Batch': batch_name,\n",
    "        'diretorio_origem': folder_name,\n",
    "        'nome_arquivo_origem': file,\n",
    "        'nome_arquivo_destino': new_name,\n",
    "        'one_page_doc': one_page_doc,\n",
    "        'numero_paginas': paginas,\n",
    "        'status': status_pro,\n",
    "    }\n",
    "\n",
    "\n",
    "# 3. FunÇao para ajustar documentos no pipeline antes de serem processados\n",
    "def processa_pipeline():\n",
    "    rows_list = []\n",
    "    i = 1\n",
    "    for root, dirs, files in os.walk(documentos_extracao_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                new_row = ajusta_pipeline(i, batch_name, file, documentos_extracao_path, folder_name, file_path)\n",
    "                rows_list.append(new_row)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao criar nova row: {e}\") \n",
    "        i += 1\n",
    "    \n",
    "    df_processa_pipe = pd.DataFrame(rows_list)\n",
    "    \n",
    "    return df_processa_pipe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tables-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
