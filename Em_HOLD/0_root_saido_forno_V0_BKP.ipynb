{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <mark> Root Unique_ID - Onde Tudo Começa </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Atual notebook com as funçoes para processamento de PDF Pesquisavel e Raster e a criaçao dos Dataframes de forma dependente e unica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import platform\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from urllib import response\n",
    "\n",
    "from outlook_msg import Message\n",
    "import extract_msg\n",
    "import zipfile\n",
    "from pyunpack import Archive\n",
    "import py7zr\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "from fuzzywuzzy import fuzz\n",
    "import PyPDF2\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "import locale\n",
    "import time, copy\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import cv2\n",
    "import fitz  # Módulo PyMuPDF\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import Image, ImageDraw\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar\n",
    "from pdf2image import convert_from_path\n",
    "import matplotlib.pyplot as plt\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "# Modulos da solucao\n",
    "# import modules.extrai_pdf_pesquisavel as Extc\n",
    "import modules.cronometro as cron\n",
    "import modules.nova_extracao_pdf_pesquisavel as novaextra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt_BR.utf8'"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Config - E-mail\n",
    "# 1. Caminho do arquivo uma mensagem especifica\n",
    "msg_dir_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails'\n",
    "\n",
    "# 2. Path para arquivos atachados compactados\n",
    "msg_attachment_zip = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/13_attachments'\n",
    "\n",
    "\n",
    "#### Config - messages\n",
    "# 3. Caminho do arquivo uma mensagem especifica\n",
    "msg_outros_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/12_messages'\n",
    "\n",
    "# 4. Path para arquivos recebidos manualmente\n",
    "arquivos_recebidos_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/14_documentos_recebidos'\n",
    "\n",
    "\n",
    "####Config Processamento Pipeline\n",
    "\n",
    "# 5. Path para documentos para extracao\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "# 6. Path para gestao de imagens resized\n",
    "image_resized_path = \"pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas\"\n",
    "\n",
    "\n",
    "\n",
    "# 7. path para arquivos json\n",
    "json_path = \"pipeline_extracao_documentos/5_documentos_processados/jsons\"\n",
    "\n",
    "# 7. Path para DFs e CSVs exportados\n",
    "export_path = \"pipeline_extracao_documentos/6_geral_administacao/exports\"\n",
    "\n",
    "# 8. Path para lixeira\n",
    "root_garbage_path = \"pipeline_extracao_documentos/0_lixeira\"\n",
    "\n",
    "\n",
    "#### paths de objetos para criacao/gestao (dicionarios/datasets)\n",
    "cnae_dict_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets/CNAE_X_ITEM_SERVICO_PREFEITURAS.xlsx\"\n",
    "\n",
    "\n",
    "# 12. poppler path\n",
    "poppler_path = \"/home/dani-boy/miniconda3/envs/tables-detr/bin\"\n",
    "\n",
    "# 13. path para config Tesseract\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "#Modelo atual\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "# definindo localizadcao para pt_BR\n",
    "locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 1 - Funcoes para e-mail e extracao (rever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch</th>\n",
       "      <th>Data</th>\n",
       "      <th>File</th>\n",
       "      <th>Type</th>\n",
       "      <th>Level</th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Parent_Unique_ID</th>\n",
       "      <th>Hash</th>\n",
       "      <th>File_Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Batch, Data, File, Type, Level, Unique_ID, Parent_Unique_ID, Hash, File_Path]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Geraçao do hash do arquivo\n",
    "def generate_file_hash(file_path):\n",
    "    # Abre o arquivo em modo de leitura de bytes\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # Lê o conteúdo do arquivo\n",
    "        file_data = f.read()\n",
    "        # Utiliza o algoritmo SHA-256 para gerar o hash\n",
    "        file_hash = hashlib.sha256(file_data).hexdigest()\n",
    "    return file_hash\n",
    "\n",
    "# 2. Geraçao do Unique_id do arquivo\n",
    "def generate_unique_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# 3. XXX Busca proximo Batch\n",
    "def busca_proximo_batch():\n",
    "    # Abre o arquivo Excel e lê a coluna 'batch'\n",
    "    df = pd.read_excel(\"pipeline_extracao_documentos/6_geral_administacao/exports/df_documento_recebido.xlsx\", usecols=[\"batch\"])\n",
    "    # Pega o último valor da coluna 'batch'\n",
    "    last_value = df.iloc[-1, 0]\n",
    "    \n",
    "    # Extraí o número do último batch e adiciona 1 para o próximo\n",
    "    last_number = int(last_value.split(\"_\")[1])\n",
    "    next_number = last_number + 1\n",
    "    \n",
    "    # Forma o nome do próximo batch\n",
    "    next_batch = f\"Batch_{next_number}\"\n",
    "    \n",
    "    return next_batch    \n",
    "\n",
    "# 4. Função para verificar e criar a pasta se não existir\n",
    "def check_and_create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        \n",
    "# 5. converte nome do arquivo\n",
    "def conv_filename(title):\n",
    "    \n",
    "    # Divide o título em nome e extensão\n",
    "    name, extension = title.rsplit('.', 1) if '.' in title else (title, \"\")\n",
    "\n",
    "    # Remove acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substiti espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remove quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converte para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    # Adiciona a extensão de volta, se houver\n",
    "    if extension:\n",
    "        filename += '.' + extension.lower()\n",
    "\n",
    "    return filename\n",
    "\n",
    "# 6. converte nome do arquivo retirando extensao\n",
    "def conv_filename_no_ext(title):\n",
    "    \n",
    "    # Divida o título em nome e extensão (mas ignore a extensão)\n",
    "    name = title.rsplit('.', 1)[0] if '.' in title else title\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    return filename         \n",
    "\n",
    "# 7. funçao que MOVE documentos e gera add_log_transaction_entry para df_log_transctions\n",
    "def move_doc_processed_file(batch_name, src_path, tgt_path):\n",
    "    \n",
    "    function = \"move_doc_processed_file\"\n",
    "    source_path = src_path\n",
    "    file = os.path.basename(source_path)\n",
    "    sub_dir = os.path.join(tgt_path, batch_name)\n",
    "    destination_path = os.path.join(sub_dir, file)\n",
    "    document_action = \"move_processed_file\"\n",
    "    transaction_detail = (f'document {file} moved by: {function}')\n",
    "    df_move = pd.DataFrame()\n",
    "    try:\n",
    "        document_unique_id = get_document_id_by_file(batch_name, file)\n",
    "        check_and_create_folder(destination_path)\n",
    "        shutil.move(source_path, destination_path)\n",
    "        sucess = True\n",
    "        move_log = add_log_transaction_entry(document_unique_id, batch_name, file, document_action, src_path, tgt_path, transaction_detail, sucess)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao mover documento: {e}\")\n",
    "        sucess = False\n",
    "    \n",
    "    return move_log    \n",
    "\n",
    "# 8. Função para adicionar um novo registro em df_source\n",
    "def add_source_entry(batch_name, file_path, file, type, level, parent_unique_id):\n",
    "    #unique_id = generate_unique_id(type)\n",
    "    unique_id = generate_unique_id()\n",
    "    time_now = cron.timenow_pt_BR()   \n",
    "    file_hash = generate_file_hash(file_path) \n",
    "    if level == 1:\n",
    "        parent_unique_id = unique_id\n",
    "    data = {\n",
    "        'Batch': batch_name,\n",
    "        'Data': time_now,\n",
    "        'File': file,\n",
    "        'Type': type,\n",
    "        'Level': level,\n",
    "        'Unique_ID': unique_id,\n",
    "        'Parent_Unique_ID': parent_unique_id,\n",
    "        'Hash': file_hash,\n",
    "        'File_Path': file_path\n",
    "    }\n",
    "    return data\n",
    "\n",
    "# 9. Add nova linha para atualizar df_log_transctions\n",
    "def add_log_transaction_entry(document_unique_id,batch_name, file, document_action, src_path, tgt_path, transaction_detail, sucess=True):\n",
    "\n",
    "    data_log = {\n",
    "        'Dt_Time': cron.timenow_pt_BR(),\n",
    "        'Batch': batch_name,\n",
    "        'File' : file,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Action': document_action,\n",
    "        'Sorce': src_path,\n",
    "        'Target': tgt_path,\n",
    "        'Transction_Detail': transaction_detail,\n",
    "        'Sucess': sucess,    \n",
    "    }\n",
    "    \n",
    "        \n",
    "    return data_log\n",
    "\n",
    "\n",
    "# 10. Consulta multiparametros\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# 12. Busca filhos - simples\n",
    "def get_children(batch, file_path):\n",
    "    \n",
    "    file = os.path.basename(file_path)\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    # Buscando todos dos filhos de um documento\n",
    "    return filtrar_df(df_id_relations, Batch=batch, Parent_Unique_ID=document_unique_id)\n",
    "\n",
    "\n",
    "# 13. Busca pai -simples\n",
    "def get_father(batch, file_path):\n",
    "    \n",
    "    file = os.path.basename(file_path)\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    # Buscando todos dos filhos de um documento\n",
    "    return filtrar_df(df_id_relations, Batch=batch, Unique_ID=parent_unique_id)\n",
    "\n",
    "\n",
    "# 14. Pesquiso pai pelo Unique_ID e trago um dict\n",
    "def get_father_data_by_children_file(batch, file):\n",
    "    \n",
    "    src_result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    src_parent_unique_id = src_result['Parent_Unique_ID'].values[0]\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=src_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }\n",
    "\n",
    "\n",
    "# 15. Pesquiso pai pelo Unique_ID (document_parent_unique_id) e cria DICT\n",
    "def get_father_by_unique_id(batch, document_parent_unique_id):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=document_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }\n",
    "    \n",
    "# 16. Pesquiso pai pelo file do filho e cria DICT\n",
    "def get_father_data_by_children_file(batch, file):\n",
    "    \n",
    "    src_result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    src_parent_unique_id = src_result['Parent_Unique_ID'].values[0]\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=src_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }        \n",
    "        \n",
    "\n",
    "# 17. Busca o 'Unique_ID' para definir o Parent_Unique_ID sem considerar 'Level'\n",
    "def get_parent_unique_id(df_id_relations, batch_name, file, type):\n",
    "    try:\n",
    "        parent_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type)]['Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        parent_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return parent_unique_id\n",
    "\n",
    "\n",
    "# 18. funcao para trazer somente o 'Unique_ID'\n",
    "def get_document_unique_id(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)]['Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        document_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# 19. funcao para trazer somente o 'Parent_Unique_ID'\n",
    "def get_document_parent_unique_id(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_parent_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)]['Parent_Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        document_parent_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_parent_unique_id\n",
    "\n",
    "\n",
    "# 20. funçao para trazer toda a row de df_id_relations para o documento\n",
    "def get_document_id_relations(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_id_relations = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)].values[0]\n",
    "    except IndexError:\n",
    "        document_id_relations = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_id_relations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# EXEMPLOS de Pesquisa DFss\n",
    "    # get_document_unique_id(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "    # # Busca somente o 'Parent_Unique_ID'\n",
    "    # get_document_parent_unique_id(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "\n",
    "    # #Busca todos os dados da row do documento encontrado\n",
    "    # document_id_relations = get_document_id_relations(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "    # document_batch = document_id_relations[0]\n",
    "    # document_date = document_id_relations[1]\n",
    "    # document_name = document_id_relations[2]\n",
    "    # document_type = document_id_relations[3]\n",
    "    # document_level = document_id_relations[4]\n",
    "    # document_unique_id = document_id_relations[5]\n",
    "    # document_parent_unique_id = document_id_relations[6]\n",
    "    # document_hash = document_id_relations[7]\n",
    "    # document_path = document_id_relations[8]\n",
    "\n",
    "    # # Insercao de um registro pela func add_source_entry\n",
    "    # file_path = \"pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/SPA 15082023.rar\"\n",
    "\n",
    "    # file = os.path.basename(file_path)\n",
    "\n",
    "    # type = \"compressed_file_attachment\"\n",
    "\n",
    "    # level = 1\n",
    "\n",
    "    # parent_unique_id = ''\n",
    "\n",
    "    # # Adicionando um novo registro (substitua 'batch_name' e 'email' conforme necessário)\n",
    "    # new_entry = add_source_entry(batch_name, file_path, file, type, level, parent_unique_id)\n",
    "\n",
    "    # df_id_relations = df_id_relations.append(new_entry, ignore_index=True)\n",
    "\n",
    "    # df_id_relations\n",
    "\n",
    "\n",
    "# Busca proximo Batch caso nao esteja rodando email\n",
    "batch_name = busca_proximo_batch()\n",
    "\n",
    "# 1. Criaçao do DataFrame para armazenar as relações de Unique_ID e Parent_Unique_ID\n",
    "df_id_relations = pd.DataFrame(columns=['Batch', 'Data' ,'File', 'Type', 'Level', 'Unique_ID', 'Parent_Unique_ID', 'Hash', 'File_Path'])\n",
    "\n",
    "# 2. Criaçao do DataFrame para df_start_pipe:\n",
    "#df_start_pipe = pd.DataFrame(columns=['Batch', 'Data' ,'File', 'Type', 'Level', 'Unique_ID', 'dt_hora', 'de', 'assunto', 'email', 'Hash'])\n",
    "\n",
    "df_id_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b>2.0</b> Implementando o Root Unique_ID  </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.x Template e dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nome_arquivo:                              batatinha_quando_nasce.pdf | palavra_chave:              default | rotulo: prov_nota_fiscal     | acao_sugerida: NO_PROCESS                    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "nf_model_path = \"pipeline_extracao_documentos/6_geral_administacao/modelos/frames_nf_v9.xlsx\"\n",
    "\n",
    "# 11. path para datasets CNAE e Itens de Serviço\n",
    "nf_datasets_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria do DF\n",
    "frames_nf_v4_df = pd.read_excel(nf_model_path)\n",
    "\n",
    "# Cria dicionários para armazenar diferentes tipos de elementos do modelo\n",
    "document_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'document'].iloc[0]\n",
    "boundaries_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'boundaries']\n",
    "sections_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'section']\n",
    "frames_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'frame']\n",
    "sframe_fields_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'sframe_field']\n",
    "field_boxes_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'field_box']\n",
    "\n",
    "def define_dados_iniciais(texto_tratado):\n",
    "    \n",
    "    dados_iniciais_nf = {}\n",
    "\n",
    "    prefeitura_encontrada = None\n",
    "    de_para_encontrado = None\n",
    "\n",
    "    # 7. ZZZ Dicionário para mapear Prefeitura com sua sigla\n",
    "    de_para_prefeitura = {\n",
    "        \"PREFEITURA DA CIDADE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA DA CIDADE DE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA MUNICIPAL DE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\": \"PM_SPA\",\n",
    "        \"MUNICIPAL DE SAO PEDRO DA ALDEIA\": \"PM_SPA\",\n",
    "        \"PREFEITURA MUNICIPAL DE MESQUITA\": \"PM_MESQUITA\",\n",
    "        \"PREFEITURA MUNICIPAL DE DE MESQUITA\": \"PM_MESQUITA\",\n",
    "        # ... adicione \n",
    "    }\n",
    "\n",
    "    templates = {\n",
    "        (\"PM_MAGE\", \"30.693.231/0001-99\"): \"MAGE_MAICON\",\n",
    "        (\"PM_MAGE\", \"23.317.112/0001-76\"): \"MAGE_MFF\",   \n",
    "        (\"PM_MAGE\", None): \"MAGE\",\n",
    "        (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", \"47.945.459/0001-21\"): \"SAO_PEDRO_GOAT\",\n",
    "        (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", \"68.687.722/0001-08\"): \"SAO_PEDRO_GM\",\n",
    "        (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", \"34.230.979/0038-06\"): \"SAO_PEDRO_SUPERMIX\",\n",
    "        (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", None): \"SAO_PEDRO\",\n",
    "        (\"Pague agora com o seu Pix\", None): \"NAO_PROCESSAR\",\n",
    "        # ... adicione outras combinações aqui\n",
    "    }\n",
    "\n",
    "    cnpj_encontrado = None\n",
    "    # Verifique cada linha do texto\n",
    "    for linha in texto_tratado:\n",
    "        for pref in de_para_prefeitura.keys():\n",
    "            if pref in linha:\n",
    "                #print(linha)\n",
    "                prefeitura_encontrada = pref\n",
    "                dados_iniciais_nf['prefeitura'] = prefeitura_encontrada\n",
    "                #print(prefeitura_encontrada)\n",
    "    # Saímos do loop, agora vamos verificar qual template usar\n",
    "    if prefeitura_encontrada:\n",
    "        de_para_pm = de_para_prefeitura.get(prefeitura_encontrada)\n",
    "        dados_iniciais_nf['de_para_pm'] = de_para_pm\n",
    "        #print(de_para_pm)\n",
    "        if not de_para_pm:\n",
    "            de_para_pm = de_para_prefeitura.get(prefeitura_encontrada, \"NAO_PROCESSAR\")\n",
    "            dados_iniciais_nf['de_para_pm'] = de_para_pm\n",
    "            #print(de_para_pm)\n",
    "    else:\n",
    "        de_para_pm = \"NAO_PROCESSAR\"\n",
    "        #print(de_para_pm)\n",
    "        \n",
    "        \n",
    "        # Verifique cada linha do texto\n",
    "    for linha in texto_tratado:\n",
    "        for de_para, cnpj in templates.keys():\n",
    "            if cnpj and cnpj in linha:\n",
    "                cnpj_encontrado = cnpj\n",
    "                dados_iniciais_nf['cnpj_encontrado'] = cnpj_encontrado\n",
    "                print('usara template para: ', cnpj_encontrado)\n",
    "                \n",
    "    # Saímos do loop, agora vamos verificar qual template usar\n",
    "    if de_para_pm:\n",
    "        template_usar = templates.get((de_para_pm, cnpj_encontrado))\n",
    "        # print(template_usar)\n",
    "        dados_iniciais_nf['template_usar'] = template_usar\n",
    "        if not template_usar:\n",
    "            template_usar = templates.get((de_para_pm, None), \"TEMPLATE_NAO_ENCONTRADO\")\n",
    "            dados_iniciais_nf['template_usar'] = template_usar \n",
    "    else:\n",
    "        template_usar = \"TEMPLATE_NAO_ENCONTRADO\"\n",
    "        dados_iniciais_nf['template_usar'] = template_usar\n",
    "        \n",
    "    \n",
    "    #Confirmando se template existe em frames    \n",
    "    try:        \n",
    "        f_type = 'frame'\n",
    "        #template_usar = 'SAO_PEDRO_SUPERMIX'\n",
    "        result = filtrar_df(frames_nf_v4_df, type=f_type, de_para_pm=de_para_pm, model=template_usar)\n",
    "        model = result['model'].values[0]\n",
    "        if model:\n",
    "            template_oficial = model\n",
    "            if model == template_usar:\n",
    "                dados_iniciais_nf['model'] = template_oficial\n",
    "            else:    \n",
    "                template_usar = \"necessario cadastrar\"\n",
    "                dados_iniciais_nf['template_usar']\n",
    "                \n",
    "            dados_iniciais_nf['template_usar'] = template_usar\n",
    "        else:\n",
    "            template_usar = \"necessario cadastrar\"\n",
    "            dados_iniciais_nf['template_usar'] = template_usar\n",
    "            dados_iniciais_nf['model'] = template_usar\n",
    "                \n",
    "    except Exception as e:\n",
    "       error_msg = (f\"Erro busca cnae: {e}\") \n",
    "               \n",
    "        \n",
    "    return dados_iniciais_nf  \n",
    "\n",
    "\n",
    "\n",
    "# Dicionário para mapear palavras-chave a rótulos\n",
    "mapeamento_palavras_chave = {\n",
    "    \"relatorio\": \"prov_relatorio\",\n",
    "    \"listagem\": \"prov_listagem\",\n",
    "    \"NF\": \"prov_nota_fiscal\",\n",
    "    \"nf\": \"prov_nota_fiscal\",\n",
    "    \"relatorio\": \"prov_listagem\",\n",
    "    \"sintetico\": \"prov_listagem\",\n",
    "    \"livro\": \"prov_livro_registro\",\n",
    "    \"sintético\": \"prov_listagem\",\n",
    "    \"nota\": \"prov_nota_fiscal\",\n",
    "    \"zip\": \"doc_zip\",\n",
    "    \"rar\": \"doc_rar\",\n",
    "    \"valores\": \"prov_dinheiro\",\n",
    "}\n",
    "\n",
    "# Dicionário mapeando rótulos a ações sugeridas\n",
    "sugestoes_acao = {\n",
    "    \"prov_relatorio\": \"NO_PROCESS\",\n",
    "    \"prov_listagem\": \"NO_PROCESS\",\n",
    "    \"prov_nota_fiscal\": \"NO_PROCESS\",\n",
    "    \"sem_rotulo\": \"MANUAL_REV\",\n",
    "    \"prov_livro_registro\": \"NO_PROCESS\",\n",
    "    \"doc_nao_pdf\": \"verificar\",\n",
    "    \"nao_pdf\": \"NO_PROCESS\",\n",
    "    \"doc_zip\": \"NO_PROCESS\",\n",
    "    \"pdf_mul_paginas\": \"SPLIT\",\n",
    "}\n",
    "\n",
    "\n",
    "def define_rotulo_acao(nome_arquivo):\n",
    "    \n",
    "    for palavra_chave, rotulo in mapeamento_palavras_chave.items():\n",
    "        if palavra_chave.lower() in nome_arquivo.lower():\n",
    "            break\n",
    "    else:\n",
    "        rotulo = 'prov_nota_fiscal' #\"sem_rotulo\"\n",
    "        palavra_chave = 'default'\n",
    "        acao_sugerida = sugestoes_acao.get(rotulo, 'None')\n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "        # palavra_chave = 'None' #\"sem_palavra_chave\"\n",
    "        # acao_sugerida = 'None' #\"sem_acao_sugerida\"\n",
    "        \n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "        #print(f'nome_arquivo: {nome_arquivo} | rotulo: {rotulo}')\n",
    "    if rotulo != 'None': #\"sem_rotulo\"\n",
    "        acao_sugerida = sugestoes_acao.get(rotulo, 'None') # \"Ação não definida\"\n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "    \n",
    "    \n",
    "# 2.Testando\n",
    "nome_arquivo = 'batatinha_quando_nasce.pdf' # 'pre-processamento'\n",
    "\n",
    "palavra_chave, rotulo, acao_sugerida = define_rotulo_acao(nome_arquivo)\n",
    "\n",
    "print(f'nome_arquivo: {nome_arquivo:>55} | palavra_chave: {palavra_chave:>20} | rotulo: {rotulo:20} | acao_sugerida: {acao_sugerida:30}')    \n",
    "\n",
    "\n",
    "\n",
    "# 1 XXX Crio o DF  cnae_x_item_servico_df\n",
    "cnae_x_item_servico_df = pd.read_excel(cnae_dict_path)\n",
    "\n",
    "# Mapeando prefeitura e CNAE para a descrição do CNAE\n",
    "cnae_dict = dict(zip(zip(cnae_x_item_servico_df['PREFE'], cnae_x_item_servico_df['CNA_NUMERO']), cnae_x_item_servico_df['CNA_NOME']))\n",
    "\n",
    "# Mapeando prefeitura e item de serviço para a descrição do item de serviço e o CNAE associado\n",
    "item_servico_dict = dict(zip(zip(cnae_x_item_servico_df['PREFE'], cnae_x_item_servico_df['ATV_CODIGO']), zip(cnae_x_item_servico_df['ATV_DESCRICAO'], cnae_x_item_servico_df['CNA_NUMERO'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> ImportaÇao do df_root_pipe </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_name = \"Batch_20\" #Excepcionalmente\n",
    "fake_parent_document_unique_id = generate_unique_id()\n",
    "\n",
    "nome_formado_json = batch_name +\".json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. XXX SE deseja importar o DF df_analise_pipe\n",
    "\n",
    "df_root_pipe_path = \"df_root_analis.xlsx\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria df_documento_recebido\n",
    "df_root_pipe = pd.read_excel(df_root_pipe_path)\n",
    "\n",
    "# Ajusta o indice\n",
    "df_root_pipe.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b>2.1</b> Principais Funcoes Utilizadas  </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Funcoes de Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXXpara buscar melhor as coordendas dos FRAMES\n",
    "def get_coordinates_filter(pdf_pesquisavel_map, model, tipo, label, section):\n",
    "    \n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model, type=tipo, label=label, section_json=section)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [((row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p']) if pdf_pesquisavel_map else (row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1']))]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates\n",
    "\n",
    "\n",
    "def get_coordinates_filter_pdf_pesquisavel(model_map, tipo, label, section):\n",
    "    \n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model_map, type=tipo, label=label, section_json=section)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [(row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p'])]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates\n",
    "\n",
    "\n",
    "# # XXX Dados para testes de cooredenadas dos Frames\n",
    "# pdf_pesquisavel_map = True\n",
    "# section = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "# model = \"MAGE\"\n",
    "# tipo = \"frame\"\n",
    "# label = \"4_frame_descricao_totais\"\n",
    "\n",
    "# coordinates = get_coordinates_filter(model, tipo, label, section)\n",
    "# x0, y0, x1, y1 = coordinates[0]\n",
    "# get_coordinates_filter(pdf_pesquisavel_map, model, tipo, label, section)\n",
    "# print(f'model: {model} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6} ')\n",
    "\n",
    "\n",
    "# file_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_20/MAGE_PDF_31282023_2254/1004813/NF 55 JML.pdf\"\n",
    "# original_file_name = \"NF 55 JML.pdf\"\n",
    "\n",
    "\n",
    "# model = \"MAGE\"\n",
    "# tipo = \"frame\"\n",
    "# label = \"2_frame_cnpj_prestador\"\n",
    "# section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "# pdf_pesquisavel_map = False\n",
    "\n",
    "\n",
    "# XXX Criando uma lista de frames para buscar coordenadas\n",
    "i_frame = 0\n",
    "\n",
    "# frames_processo = []\n",
    "# # Filtrar o DataFrame para incluir apenas linhas onde a coluna \"model\" oriundo de: modelo\n",
    "# filtered_frames_info = frames_info[frames_info['model'] == model]\n",
    "# for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "#     frame_name = row_frame['label']\n",
    "#     frames_processo.append(frame_name)\n",
    "    \n",
    "\n",
    "# ## IMPORTANTE - GESTAO DO ESCOPO DAS VARRIAVEIS NO FRAME    \n",
    "# pdf_pesquisavel_map = True\n",
    "# for frame in frames_processo:\n",
    "#     model = \"MAGE\"\n",
    "#     label = frame\n",
    "#     coordinates = get_coordinates_filter(model, tipo, label, section)\n",
    "#     x0, y0, x1, y1 = coordinates[0]\n",
    "#     print(f'model: {model} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6} ') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>2.x.x</b> Funcoes importantes - algumas das quais estao sempre por ai  </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. XXX Analisa nro de paginas\n",
    "def analisa_nro_pages(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    pages = pdf_document.pages() # generator object\n",
    "\n",
    "    page_nro = []\n",
    "    for page in pages:\n",
    "        page_nro.append(page)\n",
    "        \n",
    "    nro_paginas = len(page_nro)    \n",
    "    if nro_paginas > 1:\n",
    "        doc_1_page = False\n",
    "        return doc_1_page, nro_paginas    \n",
    "    else:\n",
    "        doc_1_page = True\n",
    "        return doc_1_page, nro_paginas  \n",
    "    pdf_document.close()\n",
    "    \n",
    " \n",
    "\n",
    "# XXX FUNCAO DE SPLIT\n",
    "def split_documentos(qualquer_df, fase, atividade, status):\n",
    "    \n",
    "    documentos_splitados = []\n",
    "    doc_info = {}\n",
    "    rows_list = []\n",
    "    documentos = []\n",
    "    #output_dir = os.path.join(documentos_scan_path, batch_name)\n",
    "    num_linhas_df = qualquer_df.shape[0]\n",
    "\n",
    "    i = num_linhas_df + 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        message_erro = []\n",
    "        nun_pages = row['pages']\n",
    "        batch_name = row['batch']\n",
    "        original_file_name = row['original_file_name']\n",
    "        folder_name = row['directory']\n",
    "        file_path = row['file_path']\n",
    "        level = row['level']\n",
    "        document_type = row['document_type']\n",
    "        doc_action = row['doc_action']\n",
    "        document_unique_id = idx\n",
    "        new_level = level + 1\n",
    "        \n",
    "        if (doc_action == 'splitar') and (status == 'root_analise'):\n",
    "            if nun_pages > 1:\n",
    "                try:\n",
    "                    pdf = fitz.open(file_path)\n",
    "                    # Número total de páginas no PDF\n",
    "                    total_pages = len(pdf)\n",
    "                except Exception as e:\n",
    "                    print(f\"Nao congui abrir o PDF: {e}\")    \n",
    "\n",
    "                # Nome base para os arquivos de saída\n",
    "                base_name = file_path.split('.')[0]  # Remove a extensão do arquivo\n",
    "                file_to_delete = file_path\n",
    "                # Loop para criar um novo PDF para cada página\n",
    "                for page_num in range(total_pages):\n",
    "                    # Cria um novo objeto PDF\n",
    "                    new_pdf = fitz.open()\n",
    "                    # Adiciona a página atual ao novo PDF\n",
    "                    new_pdf.insert_pdf(pdf, from_page=page_num, to_page=page_num)\n",
    "                    # Nome do novo arquivo PDF\n",
    "                    new_pdf_name = f\"{base_name}_page_{page_num + 1}.pdf\"\n",
    "                    # Salva o novo PDF\n",
    "                    new_pdf.save(new_pdf_name)\n",
    "                    # Fecha o novo PDF\n",
    "                    new_pdf.close()\n",
    "                    rotulo = \"prov_nota_fiscal\"\n",
    "                    acao_sugerida = sugestoes_acao.get(rotulo, \"no_defined_action\")\n",
    "                    acao_executada = \"novo_doc_criado\"\n",
    "                    informations = (f'documento criado a partir do split do documento: {original_file_name}')  \n",
    "                    name_pdf_splited = os.path.basename(new_pdf_name)\n",
    "\n",
    "                    new_row = {\n",
    "                        \"seq\": i,\n",
    "                        \"date_time\": cron.timenow_pt_BR(),\n",
    "                        \"batch\": batch_name,\n",
    "                        \"fase_processo\": fase,\n",
    "                        \"nome_atividade\": atividade,\n",
    "                        \"status_documento\": status,\n",
    "                        \"acao_executada\": acao_executada,\n",
    "                        \"original_file_name\": new_pdf_name,\n",
    "                        \"directory\": folder_name,\n",
    "                        \"one_page\": True,\n",
    "                        \"pages\": 1,\n",
    "                        \"document_type\": rotulo,\n",
    "                        \"doc_action\": acao_sugerida,\n",
    "                        \"level\": level,\n",
    "                        \"document_unique_id\": generate_unique_id(),\n",
    "                        \"parent_document_unique_id\": document_unique_id,\n",
    "                        \"file_hash\": generate_file_hash(file_path),\n",
    "                        \"file_path\": file_path,\n",
    "                        \"informations\": informations,\n",
    "                    }\n",
    "                    rows_list.append(new_row)\n",
    "                    i += 1\n",
    "                qualquer_df.loc[idx, 'status_documento'] = \"NAO_PROCESSAR\" \n",
    "                qualquer_df.loc[idx, 'informations'] = \"Paginas splitada em multiplos documentos\" \n",
    "                qualquer_df.loc[idx, 'date_time'] = cron.timenow_pt_BR() \n",
    "    \n",
    "    total_split = i - 1        \n",
    "    df_split = pd.DataFrame(rows_list)\n",
    "    \n",
    "    \n",
    "    return df_split, rows_list\n",
    "\n",
    "\n",
    "# XXX Usando na criacao da imagem \n",
    "def conv_filename_no_ext(title):\n",
    "    \n",
    "    # Divida o título em nome e extensão (mas ignore a extensão)\n",
    "    name = title.rsplit('.', 1)[0] if '.' in title else title\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    return filename \n",
    "\n",
    "\n",
    "# XXX Funcao ajustada para convertere e resize\n",
    "def convertResize(doc2convert, document_path, image_resized_path):\n",
    "    \n",
    "    \n",
    "    name_image = conv_filename_no_ext(doc2convert)\n",
    "    \n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(name_image)}.jpg')\n",
    "    #print(f'image_resized_name: {image_resized_name}\\n')\n",
    "    # 3. Conversao para imagem\n",
    "    pages = convert_from_path(document_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((2067, 2923))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "        image_2work = resized_pages[0]\n",
    "        \n",
    "    return image_2work, image_resized_name\n",
    "\n",
    "\n",
    "\n",
    "def apagar_zone(documentos_extracao_path):\n",
    "    # Para apagar arquivos PDF:Zone\n",
    "    for root, dirs, files in os.walk(documentos_extracao_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            #print(file)\n",
    "            if \":Zone\" in file:\n",
    "                file_to_delete = file_path\n",
    "                os.remove(file_to_delete)\n",
    "                #print(file, \"termina, pode eliminar\")\n",
    "                \n",
    "                \n",
    "def confirma_pdf_pequisavel(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "    # Definir retângulo de interesse\n",
    "    try:\n",
    "        x0 = 0\n",
    "        y0 = 4\n",
    "        x1 = 600\n",
    "        y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "        # Extrair texto dentro do retângulo\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        if text:\n",
    "            page_number = 0\n",
    "            pdf_pequisavel = True\n",
    "        #print(page_number)\n",
    "        else:\n",
    "            page_number = 1\n",
    "            pdf_pequisavel = False\n",
    "        #print(page_number)\n",
    "    except Exception as e:\n",
    "        msg_error = (f\"Erro ao abrir pagina do PDF: {e}\")\n",
    "        pdf_pequisavel = False\n",
    "        pdf_document.close()   \n",
    "         \n",
    "        return pdf_pequisavel\n",
    "                   \n",
    "\n",
    "\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "#generated_parent_document_unique_id = generate_unique_id()  \n",
    "\n",
    "# Processo de deleçao e atualizacao de documentos\n",
    "#e_deleta_peloamor(df_docs_splitados)\n",
    "\n",
    "#me_atualiza_logo_vai_2(novo_df)\n",
    "\n",
    "# apagar_zone(documentos_extracao_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Funcoes de OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Funcao de extracao\n",
    "def extract_text_PIL(image, coordinates):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    image_croped = image.crop((x0, y0, x1, y1))\n",
    "    texto_extraido = pytesseract.image_to_string(image_croped, lang='por')\n",
    "    return texto_extraido \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_text_from_coordinates(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    print(x0, y0, x1, y1)\n",
    "    frame_image\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=config).strip()\n",
    "    return extracted_text\n",
    "\n",
    "def extract_text_from_coordinates_2(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    print(x0, y0, x1, y1)\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config_1).strip()\n",
    "    return extracted_text\n",
    "\n",
    "\n",
    "def extract_fields_box(modelo, father_value, section):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    filtered_boxes_info = field_boxes_info[(field_boxes_info['father'] == father_value) & (field_boxes_info['model'] == modelo)]\n",
    "    # Iterate nas informações dos boxes de fields e extraia o texto de cada field\n",
    "    for index_field, row_field in filtered_boxes_info.iterrows():\n",
    "        x0, y0, x1, y1 = row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1']\n",
    "        extracted_text_box = extract_text_from_frame_2(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        #extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        \n",
    "        #print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference'], row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1'] ))\n",
    "        # Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "        value = extracted_text_box.split('\\n')[-1]\n",
    "        # Remova qualquer espaço em branco à esquerda ou à direita\n",
    "        value = value.strip()\n",
    "        if row_field['t_value'] == 'number':\n",
    "            # Formate o valor usando a função format_number\n",
    "            #print(\"vou verificar valor\")\n",
    "            value = format_number2(value)\n",
    "            #print(value)\n",
    "        # Armazene o texto extraído com o rótulo correspondente\n",
    "        label = row_field['label']\n",
    "        data_box_valores[label] = value\n",
    "        \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "# 3. Efetua OCR no documento (area parao do texto da NF)\n",
    "def ocr_RasterPDF(image_name):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = 406\n",
    "    y0 = 0\n",
    "    x1= 1540\n",
    "    y1 = 380\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "    return values, extracted_text_frame \n",
    "\n",
    "\n",
    "# 2. Efetua OCR no documento (area parao do texto da NF)\n",
    "def ocr_RasterPDF_free(image_name, vx0, vy0, vx1, vy1):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = vx0\n",
    "    y0 = vy0\n",
    "    x1= vx1\n",
    "    y1 = vy1\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "    return values, extracted_text_frame \n",
    "\n",
    " \n",
    "def extract_text_from_frame(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=config).strip()\n",
    "    return extracted_text \n",
    "\n",
    "\n",
    "def extract_fb_outras_inf(modelo, father_value, section):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    filtered_boxes_info = field_boxes_info[(field_boxes_info['father'] == father_value) & (field_boxes_info['model'] == model)]\n",
    "    # Iterate nas informações dos boxes de fields e extraia o texto de cada field\n",
    "    for index_field, row_field in filtered_boxes_info.iterrows():\n",
    "        \n",
    "        string_pesquisa = row_field['reference']\n",
    "        x0, y0, x1, y1 = row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        label = row_field['label']\n",
    "        #print(f'extracted_text_box {extracted_text_box}, label {label}')\n",
    "        text = extracted_text_box.replace('\\n', '')\n",
    "        if text.startswith(string_pesquisa):\n",
    "            #print(\"aqui:\", text)\n",
    "            text = text[len(label):].strip()\n",
    "            data_box_valores[label] = text\n",
    "    \n",
    "    return   data_box_valores  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Funcoes REGEX e ORganizacao TEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao importante - process_line\n",
    "def process_line(value, reference, label):\n",
    "    name_match = re.search(fr'{reference} (.+)', value)\n",
    "    if name_match:\n",
    "        extracted_value = reference + \" \" + name_match.group(1)\n",
    "        return {label: extracted_value}\n",
    "    return None\n",
    "\n",
    "\n",
    "# 5. XXX Ajusta texto\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF_Pesquisavel-  NO CABECALHO\n",
    "def texto_extraido_nf(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF RASTER NO CABECALHO\n",
    "def texto_extraido_cabecalho(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    text_splited = [s.replace(\")\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "    \n",
    "\n",
    "\n",
    "def format_number(number_str):\n",
    "    # Check for percentage and handle it\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # You can multiply by 100 here if needed\n",
    "\n",
    "    # Check if the string contains \"R$\" or a comma, indicating the original format\n",
    "    if 'R$' in number_str or ',' in number_str:\n",
    "        # Original format: Remove 'R$', replace dots with nothing, and replace commas with dots\n",
    "        number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    else:\n",
    "        # New format: Extract only the numeric part using regex\n",
    "        number_str = re.findall(r'[\\d\\.]+', number_str)[-1]\n",
    "\n",
    "    return float(number_str)\n",
    "\n",
    "# Funçao de formatacao de numeros\n",
    "def format_number2(number_str):\n",
    "    number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # multiplica por 100 para fields %\n",
    "    return float(number_str)\n",
    "\n",
    "\n",
    "\n",
    "#1. funcao: find_value_after_keyword_out_frame_up\n",
    "def find_value_after_keyword_out_frame_up(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "        if text_list[index + 1] not in default_keyword_list:\n",
    "            return text_list[index + 1]\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            for default_keyword in default_keyword_list:\n",
    "                if default_keyword in text_list:\n",
    "                    # Caso especial para 'Nome/Razão Social:'\n",
    "                    if keyword == 'Nome/Razão Social:':\n",
    "                        return text_list[0]\n",
    "        return None\n",
    "    \n",
    "#2. find_value_after_keyword_out_frame_down  \n",
    "def find_value_after_keyword_out_frame_down(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o índice seguinte está dentro da lista\n",
    "        if index + 1 < len(text_list):\n",
    "            # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            try:\n",
    "                index = text_list.index(default_keyword_list[-1])\n",
    "                return text_list[index - 1]\n",
    "            except ValueError:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "#3. find_value_after_keyword_fuzz\n",
    "def find_value_after_keyword_fuzz(keyword, text_list, default_keyword_list=None, fuzziness_threshold=80):\n",
    "    closest_match = None\n",
    "    closest_match_score = 0\n",
    "    \n",
    "    for i, text in enumerate(text_list):\n",
    "        score = fuzz.ratio(keyword, text)\n",
    "        \n",
    "        if score > closest_match_score:\n",
    "            closest_match_score = score\n",
    "            closest_match = text\n",
    "        \n",
    "        if closest_match_score > fuzziness_threshold:\n",
    "            break\n",
    "\n",
    "    if closest_match_score > fuzziness_threshold:\n",
    "        index = text_list.index(closest_match)\n",
    "        if index + 1 < len(text_list):\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "\n",
    "    \n",
    "def pesquisa_keyword(string_pesquisa, text_splited, keyword_list):\n",
    "\n",
    "    resultado_extraido_fuzz = find_value_after_keyword_fuzz(string_pesquisa, text_splited, keyword_list)\n",
    "\n",
    "    if resultado_extraido_fuzz == None:\n",
    "        resultado_extraido_frame_up = find_value_after_keyword_out_frame_up(string_pesquisa, text_splited, keyword_list)\n",
    "        if resultado_extraido_frame_up == None:\n",
    "            resultado_extraido_frame_down = find_value_after_keyword_out_frame_down(string_pesquisa, text_splited, keyword_list)\n",
    "            resultado_extraido = resultado_extraido_frame_down\n",
    "        else:\n",
    "            resultado_extraido = resultado_extraido_frame_up\n",
    "    else:\n",
    "        resultado_extraido = resultado_extraido_fuzz           \n",
    "    #print(resultado_extraido)\n",
    "    return resultado_extraido\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_fields_tomador_cnpj(text):\n",
    "    nf_data_tomador_cnpj = {}\n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in text:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "                        nf_data_tomador_cnpj['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                        nf_data_tomador_cnpj['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "\n",
    "    \n",
    "    # Extrair Telefone\n",
    "    telefone_match = re.search(r'Telefone:\\s+(.+)', text)\n",
    "    if telefone_match:\n",
    "        telefone_str = telefone_match.group(1)\n",
    "        if telefone_str == 'Inscrição Estadual:':\n",
    "            nf_data_tomador_cnpj['telefone'] = None  # Valor padrão quando não há correspondência\n",
    "        elif telefone_str == '':\n",
    "            nf_data_tomador_cnpj['telefone'] = None  # Valor padrão quando não há correspondência\n",
    "                    \n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['telefone'] = telefone_match.group(1)\n",
    "            \n",
    "    \n",
    "    # Extrair Inscrição Municipal\n",
    "    inscricao_municipal_match = re.search(r'Inscrição Municipal:\\s+(.+)', text)\n",
    "    if inscricao_municipal_match:\n",
    "        inscricao_municipal_str = inscricao_municipal_match.group(1)\n",
    "        if inscricao_municipal_str == \"Telefone:\": \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = inscricao_municipal_str\n",
    "    \n",
    "    insc_municipal_match = re.search(r'INSC:MUNICIPAL:\\s+(.+)', text)\n",
    "    if insc_municipal_match:\n",
    "        insc_municipal_str = insc_municipal_match.group(1)\n",
    "        if insc_municipal_str == \"Telefone:\":\n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = insc_municipal_str\n",
    "    else:\n",
    "        nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "            \n",
    "    \n",
    "    return nf_data_tomador_cnpj \n",
    "\n",
    "\n",
    "\n",
    "def extract_dados_comple_obs(modelo, frame_father, section):\n",
    "    \n",
    "    data_dados_complementares = {}\n",
    "    #frame_label = frame_father\n",
    "    \n",
    "    # 1. Filtrando o frames_info para buscar os dados de corte\n",
    "    filtered_frames_info = frames_info[(frames_info['label'] == frame_father) & (frames_info['model'] == modelo)]\n",
    "\n",
    "    # 2. Filtrando o sframe_fields_info para buscar os dados dos campos que estao nos frames\n",
    "    filtered_sframe_fields_info = sframe_fields_info[(sframe_fields_info['father'] == frame_father) & (sframe_fields_info['model'] == modelo)]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "        \n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        \n",
    "        print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_frame['seq'], row_frame['model'], row_frame['father'], row_frame['label'], row_frame['reference'], row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'] ))\n",
    "        for index_field, row_field in filtered_sframe_fields_info.iterrows():\n",
    "            #print(\"{:<5} {:<10} {:<30} {:<20} {:<20}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference']))\n",
    "            \n",
    "            if frame_father == \"5_frame_dados_complementares\":\n",
    "                nf_data_dados_complementares = {}\n",
    "                nf_data_dados_complementares['section'] = section\n",
    "                \n",
    "                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^DADOS COMPLEMENTARES', '', extracted_text_box, count=1)\n",
    "                if text == '':\n",
    "                    text = None\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text\n",
    "                else:    \n",
    "                    # Extrair texto dentro do retângulo\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "                    \n",
    "                return nf_data_dados_complementares                \n",
    "                \n",
    "            elif frame_father == \"5_frame_observacao\":\n",
    "                nf_data_observacao = {}\n",
    "                nf_data_observacao['section'] = section \n",
    "                                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^Observação:', '', extracted_text_box, count=1)\n",
    "\n",
    "                # Remover quebras de linha\n",
    "                text = text.replace('\\n', ' ')\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                nf_data_observacao['observacao'] = text.strip()\n",
    "                \n",
    "                return nf_data_observacao \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def processa_cnae_outros(text):\n",
    "    nf_data_CNAE_match = re.search(r'CNAE\\s+(.+)', text)\n",
    "    if nf_data_CNAE_match:\n",
    "        try:\n",
    "            # Remove a primeira ocorrência de \"CNAE:\"\n",
    "            nf_data_CNAE_str = re.sub(r'^CNAE - ', '', text, count=1)\n",
    "            # Remover quebras de linha\n",
    "            nf_data_CNAE_str = nf_data_CNAE_str.replace('\\n', ' ')\n",
    "            return nf_data_CNAE_str \n",
    "        except Exception as e:\n",
    "            print(f\"Erro busca cnae: {e}\") \n",
    "        \n",
    "    return None   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b>2.1</b> Criando O Root Pipeline </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.XXX  Acao 1 - Ler todo o pipeline de documentos recebidos - ESSA E A UNICA FUNCAO QUE ITERA NO DIRETORIO\n",
    "def scan_pipeline_documentos(documentos_extracao_path, batch_name, fase, atividade, status):\n",
    "    \n",
    "    doc_info = {}\n",
    "    resumo = {}\n",
    "    raw_document = []\n",
    "    \n",
    "    output_dir = os.path.join(documentos_extracao_path, batch_name)\n",
    "    i = 1\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        folder_name = os.path.basename(root)\n",
    "        #print(folder_name)\n",
    "        for file in files:\n",
    "            nome_arquivo = file\n",
    "            palavra_chave, rotulo, acao_sugerida = define_rotulo_acao(nome_arquivo)\n",
    "            acao_executada = \"Analise\"\n",
    "            informations = ' '    \n",
    "            file_name = file.lower()    \n",
    "            file_path = os.path.join(root, file)\n",
    "            new_path_name = os.path.join(output_dir, file)\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                doc_one_page, nro_pgs = analisa_nro_pages(file_path)\n",
    "            # if doc_one_page == False:\n",
    "            #             rotulo = 'pdf_mul_paginas'\n",
    "            level = 3\n",
    "            diretorio = os.path.basename(file_path)\n",
    "            if folder_name == batch_name:\n",
    "                folder_name = \"root_dir\"\n",
    "                \n",
    "            #print(f'nome_arquivo: {nome_arquivo:>55} | palavra_chave: {palavra_chave:>20} | rotulo: {rotulo:20} | acao_sugerida: {acao_sugerida:30}')    \n",
    "            \n",
    "            new_row = {\n",
    "                \"seq\": i,\n",
    "                \"date_time\": cron.timenow_pt_BR(),\n",
    "                \"batch\": batch_name,\n",
    "                \"fase_processo\": fase,\n",
    "                \"nome_atividade\": atividade,\n",
    "                \"status_documento\": status,\n",
    "                \"acao_executada\": acao_executada,\n",
    "                \"original_file_name\": file,\n",
    "                \"directory\": folder_name,\n",
    "                \"one_page\": doc_one_page,\n",
    "                \"pages\": nro_pgs,\n",
    "                \"palavra_chave\": palavra_chave,\n",
    "                \"document_tag\": rotulo,\n",
    "                \"action_item\": acao_sugerida,\n",
    "                \"level\": level,\n",
    "                \"document_unique_id\": generate_unique_id(),\n",
    "                \"parent_document_unique_id\": fake_parent_document_unique_id,\n",
    "                \"file_hash\": generate_file_hash(file_path),\n",
    "                \"file_path\": file_path,\n",
    "                \"informations\": informations,\n",
    "            }\n",
    "            raw_document.append(new_row)\n",
    "\n",
    "            \n",
    "            # print(f'seq: {i} | file: {file}'\n",
    "            i += 1\n",
    "    df_trans_pipe = pd.DataFrame(raw_document)\n",
    "      \n",
    "                \n",
    "    return df_trans_pipe, raw_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Crio o DF df_scan_pipe\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'scan_analise'\n",
    "status = 'root_analise'\n",
    "\n",
    "documentos = []\n",
    "\n",
    "#df_trans_pipe = pd.DataFrame()\n",
    "\n",
    "df_root_pipe, documentos = scan_pipeline_documentos(documentos_extracao_path, batch_name, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "df_root_pipe.query('one_page == False & palavra_chave == \"sem_palavra_chave\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. XXX Usando loc para Filtrar Baseado em one_page == False\n",
    "df_pages_2_split = df_root_pipe[df_root_pipe['one_page'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. XXX SE deseja importar o DF df_analise_pipe\n",
    "file_path = \"df_root_analis.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Le a planilha e cria df_documento_recebido\n",
    "df_root_pipe = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ajusta o indice\n",
    "df_root_pipe.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. XXX SE deseja importar o DF df_analise_pipe\n",
    "\n",
    "df_analise_pipe_path = \"Em_HOLD/df_mapeamento_e_analise2.xlsx\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria df_documento_recebido\n",
    "df_analise_pipe = pd.read_excel(df_analise_pipe_path)\n",
    "\n",
    "# Ajusta o indice\n",
    "df_analise_pipe.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Crio o DF df_scan_pipe\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'scan_analise'\n",
    "status = 'mapear'\n",
    "\n",
    "df_analise_pipe, documentos = scan_pipeline_documentos(documentos_scan_path, batch_name, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2B. XXX Efetuo a analise do pipeline de documentos e inicio da extracao de dados\n",
    "# analisar_pdf_pesquisavel\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'Reavaliar_PDF_Pesquisavel' #'pesquisar_prefeitura' # pesquisar_prefeitura  pesquisar_modelo    mensagem_status = \"Reavaliar_PDF_Pesquisavel\"\n",
    "status = 'mapear'\n",
    "\n",
    "imagens_list = analise_extracao_pipeline(subset_df_analise_pipe, df_analise_pipe, fase, atividade, status)\n",
    "\n",
    "\n",
    "if imagens_list:\n",
    "    remove_images(imagens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "subset_df_analise_pipe = df_analise_pipe.query('seq == 59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Salvando o DF (IMPORTANTE)\n",
    "df_analise_pipe.to_excel(\"df_mapeamento_e_analise2.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Criando uma nova coluna no DF\n",
    "df_analise_pipe.insert(loc=17, column='s_act', value=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>1.</b> Efetuando subset do df para pesquisa </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. XXX df_analise_pipe e o df oficial de analise do pipeline\n",
    "df_analise_pipe\n",
    "\n",
    "# 3. Numero de linhas do DF\n",
    "num_linhas_df = df_analise_pipe.shape[0]\n",
    "\n",
    "num_linhas_df # variavel para o numero de linhas do DF\n",
    "\n",
    "\n",
    "# 4. XXX Usando loc para Filtrar Baseado em one_page == False\n",
    "df_pages_2_split = df_analise_pipe[df_analise_pipe['one_page'] == False]\n",
    "\n",
    "num_docs_2_split = df_pages_2_split.shape[0]\n",
    "\n",
    "\n",
    "print(f'nro documentos em df_analise_pipe: {num_linhas_df} | nro documentos + 1 pagina: {df_pages_2_split.shape[0]}')\n",
    "    \n",
    "# 5. XXX Definimos o Index dos DFs\n",
    "df_analise_pipe.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "df_pages_2_split.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# 6. XXX Executo a criacao dos documentos split \n",
    "fase = 'pre-processamento'\n",
    "atividade = 'scan_analise'\n",
    "status = 'processar'\n",
    "\n",
    "# df_docs_splitados recebera o DF com os documentos splitados\n",
    "df_docs_splitados = split_documentos(df_pages_2_split, df_analise_pipe, num_linhas_df, fase, atividade, status)\n",
    "\n",
    "\n",
    "\n",
    "# 7. XXX Retiro o indice do DF - Resetando o índice e mantendo o índice original como uma nova coluna\n",
    "df_analise_pipe.reset_index(inplace=True)\n",
    "\n",
    "# 8. XXX Concatenando os DataFrames\n",
    "df_analise_pipe = pd.concat([df_analise_pipe, df_docs_splitados], ignore_index=True)\n",
    "\n",
    "# 9. XXX Volto novamente o indice do DF\n",
    "df_analise_pipe.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "# 10. XXX Salvo em Excel (pode ser feito durante fases)\n",
    "df_analise_pipe.to_excel(\"df_mapeamento_e_analise.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Usando iloc para Filtrar um Número Específico de Linhas\n",
    "subset_df_analise_pipe = df_analise_pipe.iloc[:10]\n",
    "\n",
    "subset_df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Usando loc para Filtrar Baseado em Condições\n",
    "subset_df = df_scan_pipe.loc[df_scan_pipe['directory'] == 'camaleao']\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert file_hash == file_hash2, \"Os arquivos são diferentes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Usando iloc para Filtrar um Número Específico de Linhas\n",
    "subset_df = df_scan_pipe.iloc[:10]\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "subset_df = df_scan_pipe.query('directory == \"root_dir\" & seq < 10 & seq > 7')\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Filtrando Linhas Baseadas em Valores em uma Lista\n",
    "\n",
    "valores = [11, 16, 30, 41]\n",
    "subset_df = df_scan_pipe[df_scan_pipe['seq'].isin(valores)]\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registro_específico = df_scan_pipe.loc['e1f4b1af-30f3-45d2-85a7-1bb895bd5325']\n",
    "\n",
    "registro_específico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise_pipe['pdf_pesquisavel'] = df_analise_pipe.apply(lambda row: row['coluna1'] * 2 if row['coluna2'] > 0 else row['coluna1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizei o DF baseado em condiçao de outra coluna\n",
    "df_analise_pipe['pdf_pesquisavel'] = df_analise_pipe.apply(lambda row: False if row['status_documento'] == \"Template_encontrado\" else row['pdf_pesquisavel'], axis=1)\n",
    "\n",
    "\n",
    "# Podemos chegar de forma mais rapida\n",
    "df_analise_pipe.loc[df_analise_pipe['status_documento'] == \"Template_encontrado\", 'pdf_pesquisavel'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df_analise_pipe.quert('nro_nota')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efetuando pesquisa no DF\n",
    "df_analise_pipe.query('pdf_pesquisavel == False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise_pipe.query('nro_nota == 13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Para Analise de PDF Pesquisavel, uei uma copia do df_analise_pipe\n",
    "subset_df_analise_pipe = df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "subset_df_analise_pipe = df_analise_pipe.query('seq < 21 & seq > 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Criando uma nova coluna no DF\n",
    "df_analise_pipe.insert(loc=13, column='model_frame', value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe.to_excel('df_root_analise.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fase = 'processamento' # 'pre-processamento'\n",
    "atividade = 'pre_process'\n",
    "status = 'root_analise'\n",
    "\n",
    "acao_sugerida = \"splitar\"\n",
    "\n",
    "\n",
    "df_root_pipe = split_documentos(df_root_pipe, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualquer_df = pd.concat([qualquer_df, df_split], ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b>2.2</b> FunÇoes de Extracao de dados no Pipeline </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Pequenos mas poderosos\n",
    "def extract_text_PIL(image, coordinates):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    image_croped = image.crop((x0, y0, x1, y1))\n",
    "    texto_extraido = pytesseract.image_to_string(image_croped, lang='por')\n",
    "    return texto_extraido \n",
    "\n",
    "\n",
    "# 10. Consulta multiparametros\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# Funcao importante - process_line\n",
    "def process_line(value, reference, label):\n",
    "    name_match = re.search(fr'{reference} (.+)', value)\n",
    "    if name_match:\n",
    "        extracted_value = reference + \" \" + name_match.group(1)\n",
    "        return {label: extracted_value}\n",
    "    return None\n",
    "\n",
    "# Função para extrair número da string\n",
    "def extract_number(text):\n",
    "    match = re.search(r'\\b\\d+(\\.\\d+)?\\b', text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# 5. XXX Ajusta texto\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF_Pesquisavel-  NO CABECALHO\n",
    "def texto_extraido_nf(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF RASTER NO CABECALHO\n",
    "def texto_extraido_cabecalho(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    text_splited = [s.replace(\")\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. INFOMACOES INICIAIS\n",
    "def processar_dados_iniciais(row, original_file_name, file_path):\n",
    "    \n",
    "    # lista_texto_extraido = []\n",
    "    nf_dados_doc = {}\n",
    "    pdf_pesquisavel = None\n",
    "    extracted_txt = pesquisa_prefeitura_pdf_pesquisavel(file_path)\n",
    "    \n",
    "    if extracted_txt:\n",
    "        pdf_pesquisavel = True\n",
    "    else:\n",
    "        pdf_pesquisavel = False \n",
    "       \n",
    "        x0 = 110\n",
    "        y0 = 0\n",
    "        x1= 1929\n",
    "        y1 = 786\n",
    "        \n",
    "        image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "        extracted_txt = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "    \n",
    "    nf_dados_doc['file_name'] = original_file_name    \n",
    "    nf_dados_doc['pdf_pesquisavel'] = pdf_pesquisavel \n",
    "    value = {}   \n",
    "    texto_tratado = texto_extraido(extracted_txt)\n",
    "    value = define_dados_iniciais(texto_tratado)\n",
    "    if value:\n",
    "        nf_dados_doc.update(value)\n",
    "   \n",
    "\n",
    "\n",
    "    return nf_dados_doc\n",
    "\n",
    "# Funcoes de extracao - pdf pesquisavel\n",
    "def pesquisa_prefeitura_pdf_pesquisavel(file_path):    \n",
    "    \n",
    "    # Carregar o arquivo PDF\n",
    "    pdf_document = fitz.open(file_path)\n",
    "\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 4\n",
    "    x1 = 600\n",
    "    y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    \n",
    "    #print(text)\n",
    "    \n",
    "    if text:\n",
    "       page_number = 0\n",
    "       #print(page_number)\n",
    "    else:\n",
    "       page_number = 1\n",
    "       #print(page_number)\n",
    "    \n",
    "    pdf_document.close()\n",
    "   \n",
    "    return text\n",
    "\n",
    "# XXX Funcoes de Regex - cabecalho - documento pdf pesquisavel\n",
    "nf_data_servico = {}\n",
    "nf_data_erros = {}\n",
    "nf_lista_erros = []\n",
    "\n",
    "# 0. Pesquisa PDF\n",
    "def is_pdf_searchable(pdf_path):\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        is_searchable = all(page.get_text(\"text\") != \"\" for page in pdf_document)\n",
    "        pdf_document.close()\n",
    "        return is_searchable\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao verificar o PDF: {e}\")\n",
    "        return False\n",
    "\n",
    "def extrai_cabecalho_PDF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path):\n",
    "    \n",
    "    nf_data_cabecalho = {}\n",
    "    section = \"1 - CABECALHO\"\n",
    "    pdf_pesquisavel_map = True\n",
    "    lista_erros = []\n",
    "    \n",
    "    label = \"1_frame_dados_nf\"\n",
    "\n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    tipo = \"frame\"\n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    #print(label)\n",
    "    #print(x0,x1,y0,y1)\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    nf_data_cabecalho = novaextra.extract_fields_cabecalho(text, original_file_name)\n",
    "    \n",
    "    \n",
    "    pdf_document.close()\n",
    "    return nf_data_cabecalho\n",
    "\n",
    "# 1.B CABECALHO XXX Funcoes de extracao -cabecalho Raster\n",
    "def processar_cabecalho_raster_pdf(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    section = \"1 - CABECALHO\"\n",
    "    pdf_pesquisavel = False\n",
    "    father = \"1_frame_prefeitura_nf\"\n",
    "    data_extrated_prefeitura = {}\n",
    "    tipo_2 = \"sframe_field\"\n",
    "    \n",
    "    data_extrated_prefeitura['pdf_pesquisavel'] = pdf_pesquisavel_map\n",
    "    data_extrated_prefeitura['original_file_name'] = original_file_name\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['section_json'] == section)]\n",
    "    filtered_sframe_field_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['father'] == father) & (frames_nf_v4_df['type'] == tipo_2)]\n",
    "    image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        label_value = row_frame['label']\n",
    "        reference_value = row_frame['reference']\n",
    "        f_type = row_frame['type']\n",
    "        frame_id = row_frame['id']\n",
    "        if row_frame['label']  == \"1_frame_prefeitura_nf\":\n",
    "            x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "            texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "            #print(texto_extraido)\n",
    "            label_value = row_frame['label']\n",
    "            #print(\"label_value\", label_value)\n",
    "            values = texto_extraido.split('\\n')\n",
    "            for index_sframe, row_sframe in filtered_sframe_field_nf_v4_df.iterrows():\n",
    "                label_value = row_sframe['label']\n",
    "                #print(\"label_value\", label_value)\n",
    "                if label_value == \"nome_prefeitura\":\n",
    "                    reference_value = row_sframe['reference']\n",
    "                    for value in values:\n",
    "                        result = process_line(value, reference_value, label_value)\n",
    "                        if result:\n",
    "                            data_extrated_prefeitura.update(result)\n",
    "                elif label_value == \"secretaria\":\n",
    "                    reference_value = row_sframe['reference']\n",
    "                    for value in values:\n",
    "                        result = process_line(value, reference_value, label_value)\n",
    "                        if result:\n",
    "                            data_extrated_prefeitura.update(result) \n",
    "                elif label_value == \"tipo_nota_fiscal\":\n",
    "                    reference_value = row_sframe['reference']  \n",
    "                    for value in values:\n",
    "                        result = process_line(value, reference_value, label_value)\n",
    "                        if result:\n",
    "                            data_extrated_prefeitura.update(result)\n",
    "        \n",
    "        if row_frame['label']  == \"1_frame_dados_nf\":\n",
    "            x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "            texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "            #print(texto_extraido)\n",
    "            text_splited = texto_extraido_cabecalho(texto_extraido)\n",
    "            keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "            string_pesquisa = \"Número da Nota:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "            data_extrated_prefeitura['numero_nota_fiscal'] = texto\n",
    "\n",
    "            string_pesquisa = \"Competência:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_extrated_prefeitura['competencia'] = texto\n",
    "\n",
    "            string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_extrated_prefeitura['dt_hr_emissao'] = texto\n",
    "\n",
    "            string_pesquisa = \"Código Verificação:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_extrated_prefeitura['codigo_verificacao'] = texto\n",
    "            #print(f'{frame_id:>5}  {f_type:>20} | {label_value:>30} |  ref:{reference_value:>30}  | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6}')\n",
    "\n",
    "    return data_extrated_prefeitura\n",
    "\n",
    "\n",
    "# 2. PRESTADOR DE SERVIÇO\n",
    "def extrai_prestador_PDF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path):\n",
    "    \n",
    "    nf_data_prestador = {}\n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    label = \"2_frame_cnpj_prestador\"\n",
    "\n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    tipo = \"frame\"\n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    #print(label)\n",
    "    #print(x0,x1,y0,y1)\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    nf_data_prestador = novaextra.extract_fields_prestador(text)\n",
    "\n",
    "           \n",
    "\n",
    "    pdf_document.close()\n",
    "\n",
    "    return nf_data_prestador \n",
    "\n",
    "# 2.B PRESTADOR DE SERVIÇO - RASTER\n",
    "def processar_prestador_raster_pdf(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = False\n",
    "    \n",
    "    tipo= \"frame\"\n",
    "    message_erro = []\n",
    "    nf_dados_prestador = {}\n",
    "    model = row['model']\n",
    "    print(model)\n",
    "\n",
    "    process_prestador = ['2_frame_cnpj_prestador', '2_frame_inscricao_prestador', '2_frame_dados_prestador']\n",
    "    image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "    for father in process_prestador:\n",
    "            label = father\n",
    "            if label == \"2_frame_cnpj_prestador\": \n",
    "                coordinates = get_coordinates_filter(model_map, tipo, label, section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                extract_text = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                #text_splited = texto_extraido_nf(extract_text)\n",
    "                if \"CPF/CNPJ:\" in extract_text:\n",
    "                    cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', extract_text)\n",
    "                    if cpf_cnpj_formatado_match:\n",
    "                        texto = cpf_cnpj_formatado_match.group(1)\n",
    "                    \n",
    "                        nf_dados_prestador['cpf_cnpj_com_mascara'] = texto\n",
    "                        nf_dados_prestador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "                else:\n",
    "                    nf_dados_prestador['cpf_cnpj_com_mascara'] = 'None'\n",
    "                    nf_dados_prestador['cpf_cnpj_sem_mascara'] = 'None'           \n",
    "                        \n",
    "                telefone_str = None\n",
    "                #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "                telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', extract_text)\n",
    "                if telefone_match: \n",
    "                    telefone_str = telefone_match.group(1)\n",
    "                    # Remover quebras de linha\n",
    "                    telefone_str = telefone_str.replace('.', '')\n",
    "                    telefone_str = telefone_str.replace('\\n', '')\n",
    "                            \n",
    "                    nf_dados_prestador['telefone'] = telefone_str\n",
    "                else:\n",
    "                    nf_dados_prestador['telefone'] = 'None'\n",
    "                    \n",
    "            # if father_to_process == \"2_frame_cnpj_prestador\":\n",
    "            elif label == '2_frame_inscricao_prestador':\n",
    "                coordinates = get_coordinates_filter(model_map, tipo, label, section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                extract_text = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                \n",
    "                text_splited = texto_extraido_nf(extract_text)\n",
    "                \n",
    "                \n",
    "                # Extrair Inscrição Estadual\n",
    "                inscricao_estadual_match = re.search(r'Inscrição Estadual:\\s+(.+)', extract_text)\n",
    "                if inscricao_estadual_match:\n",
    "                    inscricao_estadual_str = inscricao_estadual_match.group(1)\n",
    "                    if inscricao_estadual_str == 'Nome/Razão Social:':\n",
    "                        nf_inscr_estadual = 'None'\n",
    "                        nf_dados_prestador['inscricao_estadual'] = \"NONE\"\n",
    "                    else:\n",
    "                        nf_inscr_estadual = inscricao_estadual_match.group(1)\n",
    "                        nf_dados_prestador['inscricao_estadual'] = inscricao_estadual_match.group(1) \n",
    "                \n",
    "\n",
    "            elif label == '2_frame_dados_prestador':\n",
    "                dados_prestador = {}\n",
    "                coordinates = get_coordinates_filter(model_map, tipo, label, section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                \n",
    "                extract_text = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                dados_prestador = extract_fields_prestador(extract_text)\n",
    "                \n",
    "                text_splited = texto_extraido_nf(extract_text)\n",
    "                keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "                string_pesquisa = \"Nome/Razão Social:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            \n",
    "                nf_dados_prestador['razao_social'] = texto\n",
    "\n",
    "                string_pesquisa = \"Nome de Fantasia:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                nf_dados_prestador['nome_fantasia'] = texto\n",
    "                \n",
    "                string_pesquisa = \"Endereço:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                nf_dados_prestador['endereco'] = texto\n",
    "                \n",
    "                string_pesquisa = \"E-mail:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                nf_dados_prestador['email'] = texto\n",
    "                \n",
    "    return nf_dados_prestador  \n",
    "\n",
    "# 3. TOMADOR DE SERVIÇO\n",
    "def extrai_tomador_PDF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path):\n",
    "\n",
    "    nf_data_tomador = {}\n",
    "    section = \"3. TOMADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = True\n",
    "\n",
    "    label = \"3_frame_cnpj_tomador\"\n",
    "\n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    tipo = \"frame\"\n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    #print(label)\n",
    "    #print(x0,x1,y0,y1)\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    nf_data_tomador = novaextra.extract_fields_tomador(text)\n",
    "           \n",
    "\n",
    "    pdf_document.close()\n",
    "\n",
    "    return nf_data_tomador \n",
    "\n",
    "\n",
    "# 4. DESCRIMINACAO DOS SERVIÇOS - VErificar substituicao\n",
    "def processar_servicos_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    section = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "    frame_type = 'frame'\n",
    "    f_frame_label = \"4_frame_descricao_totais\"\n",
    "    data_box_valores = {'secao': section}\n",
    "    nf_data_servico = {}\n",
    "    message_erro = []\n",
    "    nf_data_servico['original_file_name'] = original_file_name\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == frame_type) & (frames_nf_v4_df['label'] == f_frame_label)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        frame_id = row_frame['id']\n",
    "        label = row_frame['label']\n",
    "        x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "        #print(f'model: {model_map} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: {label:>30} | id: {frame_id:>3} | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6} ')\n",
    "        \n",
    "        pdf_document = fitz.open(file_path)\n",
    "        # Página do PDF\n",
    "        page_number = 0  # Defina o número da página que deseja analisar\n",
    "        page = pdf_document[page_number]\n",
    "        #x0, y0, x1, y1 = (0, 0, 600, 110)\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        \n",
    "        # Remover quebras de linha e rótulo\n",
    "        text = text.replace('\\n', ' ')\n",
    "        label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "        if text.startswith(label):\n",
    "            text = text[len(label):].strip()\n",
    "\n",
    "\n",
    "        try:\n",
    "            discrimanacao_servico = text   \n",
    "        except Exception as e:\n",
    "            msg = (f\"doc: | {e}\")\n",
    "            discrimanacao_servico = \"Descricao nao encontrada\"\n",
    "\n",
    "        # Atribuir texto ao dicionário\n",
    "        nf_data_servico['discriminacao_servicos'] = discrimanacao_servico\n",
    "        \n",
    "        pdf_document.close()\n",
    "        \n",
    "    return nf_data_servico \n",
    "            \n",
    "                \n",
    "# 5. VALOR TOTAL\n",
    "def processar_valor_total_PDF_P(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    nf_data_valor_total = {}\n",
    "    \n",
    "    process = ['4_frame_valor_total']\n",
    "    \n",
    "    section = \"5. VALOR TOTAL\"\n",
    "    pdf_pesquisavel_map = True\n",
    "        \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "        \n",
    "    tipo= \"frame\"\n",
    "    message_erro = []\n",
    "    nf_dados_prestador = {}\n",
    "    # model = row['model']\n",
    "    for father in process:\n",
    "        label = father\n",
    "        if label == \"4_frame_valor_total\": \n",
    "            #print(model_map)\n",
    "            coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            #print(f'model: {model_map} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6}\\n {text}')\n",
    "            valor_total_match = re.search(r'R\\$ ([\\d,.]+)', text)\n",
    "            if valor_total_match:\n",
    "                valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "                valor_total_float = float(valor_total_sem_formatacao)\n",
    "                #nf_data_valor_total['valor_total_nota'] =\n",
    "                \n",
    "    pdf_document.close()            \n",
    "                \n",
    "    return valor_total_float  \n",
    "\n",
    "\n",
    "# 6. CNAE e Item da Lista de Serviços\n",
    "def extrai_consiste_cnae_PDF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0_cnae, f_1_cnae, f_0_it, f_1_it, original_file_name, file_path):\n",
    "\n",
    "    section = \"6. CNAE e Item da Lista de Serviços\"\n",
    "    nf_data_CNAE = {}\n",
    "    message_erro = []\n",
    "    cnae_value = None\n",
    "    item_servico_value = None\n",
    "    \n",
    "    nf_data_CNAE['Secao'] = \"6. CNAE e Item da Lista de Serviços\"\n",
    "    \n",
    "    process = [\"cnae\", 'item_lista_servicos']\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    tipo = \"sframe_field\"\n",
    "    \n",
    "    print(f'\\n item: {original_file_name}\\n')\n",
    "    # model = row['model']\n",
    "    for father in process:\n",
    "        cnae_value = None\n",
    "        item_servico_value = None\n",
    "        label = father\n",
    "        if label == \"cnae\": \n",
    "            tipo = \"sframe_field\"\n",
    "            coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            print(f'1.A TRATAMENTO CNAE : (coordenadas originais):  x0:{x0} y0:{y0} x1:{x1} y1:{y1}')\n",
    "            #print(f'1. label: {label} | coordenadas originais: x0:{x0} y0:{y0} x1:{x1} y1:{y1} item:')\n",
    "            y0 = y0 * f_0_cnae\n",
    "            y1 = y1 * f_1_cnae\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            print(f'1.A TRATAMENTO CNAE - texto extraido em  coordenadas: x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0_cnae: {f_0_cnae}, f_1_cnae: {f_1_cnae} TEXT:\\n{text}\\n')\n",
    "            cnae_processado = processa_cnae_dict(text, de_para_pm)\n",
    "            print(f'\\n1.B CNAE PROCESSADO: {cnae_processado}\\n')\n",
    "            \n",
    "        elif label == \"item_lista_servicos\":\n",
    "            tipo = \"sframe_field\"\n",
    "            #print(model_map)\n",
    "            coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            print(f'2.A TRATAMENTO ITENS DE SERVICO:(coordenadas originais):  x0:{x0} y0:{y0} x1:{x1} y1:{y1}')\n",
    "            y0 = y0 * f_0_it\n",
    "            y1 = y1 * f_1_it\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            print(f'2.A TRATAMENTO ITENS DE SERVICO -  texto extraido em  coordenadas originais: x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0_it:{f_0_it}, f_1_it: {f_1_it} TEXT:\\n{text}\\n')\n",
    "            #print(text)\n",
    "            item_servico_processado = processa_itens_servico_dict(text, de_para_pm)\n",
    "            print(f' 2.B ITEM DE SERVICO PROCESSADO: {item_servico_processado} \\n\\n')\n",
    "\n",
    "    pdf_document.close()\n",
    "    return cnae_processado, item_servico_processado    \n",
    "\n",
    "\n",
    "# 6A. XXX  Tratando o CNAE com dict criado\n",
    "def processa_cnae_dict(Texto_extraido, de_para_pm):\n",
    "\n",
    "    text_splited = Texto_extraido.split('\\n')\n",
    "    # Processando CNAE\n",
    "    cnae_lines = [line for line in text_splited if 'CNAE' in line]\n",
    "    if cnae_lines:\n",
    "        cnae_line = cnae_lines[0]\n",
    "        cnae_number = int(extract_number(cnae_line))\n",
    "        cnae_value = cnae_dict.get((de_para_pm, cnae_number),(\"Valor não encontrado\"))\n",
    "        if cnae_value != \"Valor não encontrado\":\n",
    "            cnae_value = cnae_value.upper()\n",
    "            cnae_value = str(cnae_number) + \" - \" + cnae_value\n",
    "            return cnae_value\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        cnae_value = processa_cnae_outros(Texto_extraido)\n",
    "        cnae_number = int(extract_number(cnae_line))\n",
    "        cnae_value = cnae_dict.get((de_para_pm, cnae_number),(\"Valor não encontrado\"))\n",
    "        if cnae_value != \"Valor não encontrado\":\n",
    "            cnae_value = cnae_value.upper()\n",
    "            cnae_value = str(cnae_number) + \" - \" + cnae_value\n",
    "            return cnae_value\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "     \n",
    "\n",
    "# 2. XXX  Tratando Item de Servico com dict criado\n",
    "def processa_itens_servico_dict(Texto_extraido, de_para_pm):\n",
    "    \n",
    "    text_splited = Texto_extraido.split('\\n')\n",
    "    # Encontrando a linha que contém o texto desejado\n",
    "    item_servico_lines = [line for line in text_splited if 'Item da Lista de Serviços' in line]\n",
    "    print(f'item_servico_lines (fora do if): {item_servico_lines}')\n",
    "    # Verificando se encontramos uma linha válida\n",
    "    if item_servico_lines:\n",
    "        print(f'item_servico_lines: {item_servico_lines}')\n",
    "        item_servico_line = item_servico_lines[0]\n",
    "        item_servico_cod = float(extract_number(item_servico_line))\n",
    "        item_servico, cnae_associado = item_servico_dict.get((de_para_pm, item_servico_cod), (\"Valor não encontrado\", None))\n",
    "        item_servico = item_servico.upper()\n",
    "        item_servico_value = str(item_servico_cod) + \" - \" + item_servico\n",
    "        return item_servico_value\n",
    "    \n",
    "    else:\n",
    "        #print(\"Linha com 'Item da Lista de Serviços' não encontrada\")\n",
    "        item_servico_line = processa_item_sevico_outros(Texto_extraido)\n",
    "        if item_servico_line:\n",
    "            item_servico_cod = float(extract_number(item_servico_line))\n",
    "            item_servico, cnae_associado = item_servico_dict.get((de_para_pm, item_servico_cod), (\"Valor não encontrado\", None))\n",
    "            item_servico = item_servico.upper()\n",
    "            item_servico_value = str(item_servico_cod) + \" - \" + item_servico\n",
    "            return item_servico_value\n",
    "        \n",
    "        else:\n",
    "            return None\n",
    "        #return None\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def processa_cnae_outros(text):\n",
    "    \n",
    "    nf_data_CNAE_match = re.search(r'CNAE\\s+(.+)', text)\n",
    "    if nf_data_CNAE_match:\n",
    "        try:\n",
    "            # Remove a primeira ocorrência de \"CNAE:\"\n",
    "            nf_data_CNAE_str = re.sub(r'^CNAE - ', '', text, count=1)\n",
    "            # Remover quebras de linha\n",
    "            nf_data_CNAE_str = nf_data_CNAE_str.replace('\\n', ' ')\n",
    "            return nf_data_CNAE_str \n",
    "        except Exception as e:\n",
    "            print(f\"Erro busca cnae: {e}\") \n",
    "        \n",
    "    return None \n",
    "\n",
    "\n",
    "\n",
    "def processa_item_sevico_outros(text):\n",
    "\n",
    "    nf_item_lista_servicos_match = re.search(r'Item da Lista de Serviços\\s+(.+)', text)\n",
    "    if nf_item_lista_servicos_match:\n",
    "        try:\n",
    "            # Remove a primeira ocorrência de \"Item de Servico:\"\n",
    "            nf_item_lista_servicos_str = re.sub(r'^Item da Lista de Serviços - ', '', text, count=1) \n",
    "            # Remover quebras de linha\n",
    "            #nf_item_lista_servicos_str = nf_item_lista_servicos_str.replace('\\n \\n', '')\n",
    "            nf_item_lista_servicos_str = nf_item_lista_servicos_str.replace('\\n', ' ')\n",
    "            return nf_item_lista_servicos_str\n",
    "        except Exception as e:\n",
    "            print(f\"Erro busca cnae: {e}\")\n",
    "            \n",
    "    return None        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 7. VALORES E IMPOSTOS\n",
    "def extrai_valores_impostos_PDF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path):\n",
    "\n",
    "    nf_data_valores = {}\n",
    "    \n",
    "    section = \"8. DADOS COMPLEMENTARES\"\n",
    "    f_frame_label = \"5_frame_valores_impostos\"\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    tipo = \"frame\"\n",
    "    message_erro = []\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number] \n",
    "    \n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=f_frame_label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    y0_tpt = y0\n",
    "    y1_tpt = y1\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    #print(f'Importos: labe: {label:>30} | template:  x0:{x0:>6} | y0:{y0_tpt:>6} | x1:{x1:>6} | y1:{y1_tpt:>6} | ajuste: y0: {y0} | y1: {y1} \\n {text}')\n",
    "    \n",
    "    nf_data_valores = novaextra.extract_fields_impostos(text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    #print(text)\n",
    "    \n",
    "    pdf_document.close()\n",
    "    \n",
    "    return  nf_data_valores\n",
    "    \n",
    "\n",
    "\n",
    "# 8. DADOS COMPLEMENTARES\n",
    "def extrai_dados_complementares_PDF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path):\n",
    "    \n",
    "    section = \"8. DADOS COMPLEMENTARES\"\n",
    "    f_frame_label = \"5_frame_dados_complementares\"\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    tipo = \"frame\"\n",
    "    message_erro = []\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number] \n",
    "    \n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=f_frame_label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    y0_tpt = y0\n",
    "    y1_tpt = y1\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    #print(f'Considerar: label: {f_frame_label:>30} | templt.x0:{x0:>6} | y0:{y0_tpt:>6} | x1:{x1:>6} | y1:{y1_tpt:>6} | ajuste: y0: {y0} | y1: {y1} \\n {text}')\n",
    "    \n",
    "    nf_data_dados_complementares = {}\n",
    "    nf_data_dados_complementares['secao'] = \"8. DADOS COMPLEMENTARES\"\n",
    "\n",
    "    text = re.sub(r'^DADOS COMPLEMENTARES', '', text, count=1)\n",
    "    if text == \" \":\n",
    "        text = \"NONE\"\n",
    "        nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "    else:    \n",
    "        # Extrair texto dentro do retângulo\n",
    "        nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "    \n",
    "    pdf_document.close()\n",
    "                        \n",
    "    return nf_data_dados_complementares  \n",
    "\n",
    "\n",
    "# 9. OUTRAS INFORMAÇOES / CRITICAS \n",
    "def extrai_outras_informacoes_PSF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, map_original_file_name, file_path):\n",
    "    \n",
    "    nf_data_outras_informacoes = {}\n",
    "    section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "    f_frame_label = \"5_frame_inf_criticas\"\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    tipo = \"frame\"\n",
    "    message_erro = []\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number] \n",
    "    \n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=f_frame_label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    y0_tpt = y0\n",
    "    y1_tpt = y1\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    #print(f'Considerar: label: {f_frame_label:>30} | templt.x0:{x0:>6} | y0:{y0_tpt:>6} | x1:{x1:>6} | y1:{y1_tpt:>6} | ajuste: y0: {y0} | y1: {y1} \\n {text}')\n",
    "    nf_data_outras_informacoes = novaextra.extract_fields_outras_info(text)\n",
    "    \n",
    "    pdf_document.close()\n",
    "    \n",
    "    return nf_data_outras_informacoes       \n",
    "    \n",
    " \n",
    "# 10. OBSERVACOES \n",
    "def extrai_data_observacao_PSF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path):\n",
    "    \n",
    "    nf_data_observacao   = {}\n",
    "    section = \"10. OBSERVACOES\"\n",
    "    f_frame_label = \"5_frame_observacao\"\n",
    "    pdf_pesquisavel_map = True\n",
    "\n",
    "    tipo = \"frame\"\n",
    "    message_erro = []\n",
    "\n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number] \n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=f_frame_label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    y0_tpt = y0\n",
    "    y1_tpt = y1\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    \n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    #print(f'Considerar: label: {f_frame_label:>30} | templt.x0:{x0:>6} | y0:{y0_tpt:>6} | x1:{x1:>6} | y1:{y1_tpt:>6} | ajuste: y0: {y0} | y1: {y1} \\n {text}')\n",
    "    # Remove a primeira ocorrência de \"Observação:\"\n",
    "    text = re.sub(r'^Observação:', '', text, count=1)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    nf_data_observacao['observacao'] = text.strip()\n",
    "\n",
    "\n",
    "    pdf_document.close()\n",
    "    \n",
    "    return nf_data_observacao\n",
    "   \n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b>2.3</b> ExecuÇao do Pipeline de Extracao </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta e o principio da melhor funcao do mundo\n",
    "def extracao_pipeline(qualquer_df, fase, atividade, status):\n",
    "    \n",
    "    doc_info = {}\n",
    "    resumo = {}\n",
    "    row_teste_info = []\n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    func_fase = fase\n",
    "    func_atividade = atividade\n",
    "    func_status = status\n",
    "    lista_dicts = []\n",
    "   \n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        dados_iniciais = {}\n",
    "        # dados_root_pipe[idx] = row.to_dict()\n",
    "        row_info = row.to_dict()\n",
    "        message_erro = []\n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        map_document_unique_id = idx\n",
    "        #dados_root_pipe['document_unique_id'] = map_document_unique_id\n",
    "        map_seq = row['seq']\n",
    "        #dados_root_pipe['seq'] = map_seq\n",
    "        map_batch_name = row['batch']\n",
    "        #dados_root_pipe['batch'] = map_batch_name\n",
    "        map_fase_processo = row['fase_processo']\n",
    "        #dados_root_pipe['fase_processo'] = map_fase_processo\n",
    "        map_nome_atividade = row['nome_atividade']\n",
    "        map_status_documento = row['status_documento']\n",
    "        map_original_file_name = row['original_file_name']\n",
    "        map_directory = row['directory']\n",
    "        map_one_page = row['one_page']\n",
    "        map_palavra_chave = row['palavra_chave']\n",
    "        map_document_tag = row['document_tag']\n",
    "        map_action_item = row['action_item']\n",
    "        map_level = row['level']\n",
    "        file_path = row['file_path']\n",
    "        if (map_status_documento != 'NO_PROCESS'):\n",
    "            #1. Buscar Prefeitura, de/para e modelo\n",
    "            if map_status_documento == 'PREPROCESS_EXTRACT':\n",
    "                \n",
    "                dados_iniciais = processar_dados_iniciais(row, map_original_file_name, file_path)\n",
    "                \n",
    "                prefeitura_map = dados_iniciais['prefeitura']\n",
    "                pdf_pesquisavel_map = dados_iniciais['pdf_pesquisavel']\n",
    "                de_para_map = dados_iniciais['de_para_pm']\n",
    "                model_map = dados_iniciais['model']\n",
    "                #cnpj_map = dados_iniciais['cnpj']\n",
    "                \n",
    "                #print(f'\\nde_para_map: {de_para_map} | model: {model_map} | PM: {prefeitura_map}\\n')\n",
    "                \n",
    "                row_info['document_unique_id'] = map_document_unique_id\n",
    "                row_info['prefeitura'] = prefeitura_map\n",
    "                row_info['pdf_pesquisavel'] = pdf_pesquisavel_map    \n",
    "                row_info['de_para_pm'] = de_para_map\n",
    "                row_info['model'] = model_map\n",
    "                \n",
    "                #print(de_para_map)\n",
    "                \n",
    "                #row_info['cnpj'] = cnpj_map\n",
    "                \n",
    "                # 1. CABECALHO\n",
    "                valores = {}\n",
    "                erros = []\n",
    "                erros_processo = {}\n",
    "                data_cabecalho = {}\n",
    "                data_cabecalho['secao'] = \"1 - CABECALHO\"\n",
    "                f_0 = 1\n",
    "                f_1 = 1\n",
    "                \n",
    "                if pdf_pesquisavel_map:\n",
    "                    valores = extrai_cabecalho_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "                    if valores:\n",
    "                       data_cabecalho.update(valores)\n",
    "                else:\n",
    "                    data_cabecalho = processar_cabecalho_raster_pdf(row, pdf_pesquisavel_map, model_map, map_original_file_name, file_path)\n",
    "                \n",
    "                nome_prefeitura_map = data_cabecalho['nome_prefeitura']\n",
    "                tipo_nota_fiscal_map = data_cabecalho['tipo_nota_fiscal']\n",
    "                nro_nota_map = data_cabecalho['numero_nota_fiscal']\n",
    "                competencia_map = data_cabecalho['competencia']\n",
    "                dt_hr_emissao_map = data_cabecalho['dt_hr_emissao']  \n",
    "                codigo_ver_map = data_cabecalho['codigo_verificacao']\n",
    "                #erros_cabecalho = data_cabecalho['erros_cabecalho']\n",
    "                #erros_processo.update(data_cabecalho['erros_cabecalho'])\n",
    "                row_info['prefeitura'] = nome_prefeitura_map \n",
    "                row_info['tipo_nota_fiscal'] = tipo_nota_fiscal_map\n",
    "                row_info['numero_nota_fiscal'] = nro_nota_map\n",
    "                row_info['competencia'] = competencia_map\n",
    "                row_info['dt_hr_emissao'] = dt_hr_emissao_map\n",
    "                row_info['codigo_verificacao'] = codigo_ver_map\n",
    "                \n",
    "                # 2. PRESTADOR DE SERVIÇO\n",
    "                valores = {}\n",
    "                erros = []\n",
    "                data_tomador = {}\n",
    "                f_0 = 1\n",
    "                f_1 = 1\n",
    "                \n",
    "                if pdf_pesquisavel_map:\n",
    "                   valores = extrai_prestador_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "                   if valores:\n",
    "                       row_info.update(valores)\n",
    "\n",
    "                # 3. TOMADOR DE SERVIÇO\n",
    "                valores = {}\n",
    "                erros = []\n",
    "                data_tomador = {}\n",
    "                f_0 = 1\n",
    "                f_1 = 1\n",
    "                \n",
    "                if pdf_pesquisavel_map:\n",
    "                   valores = extrai_tomador_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "                   row_info.update(valores)\n",
    "    \n",
    "                \n",
    "                # 4. DESCRIMINACAO DOS SERVIÇOS\n",
    "                valores = {}\n",
    "                nf_data_servicor = {}    \n",
    "                if pdf_pesquisavel_map:\n",
    "                   nf_data_servicor = processar_servicos_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, map_original_file_name, file_path)\n",
    "                          \n",
    "                try:\n",
    "                    discrimanacao_servico = nf_data_servicor['discriminacao_servicos'] \n",
    "                    row_info['discriminacao_servicos'] = discrimanacao_servico \n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "                    discrimanacao_servico = \"Descricao nao encontrada\"\n",
    "                    row_info['discriminacao_servicos'] = discrimanacao_servico\n",
    "\n",
    "                \n",
    "                \n",
    "                # 5. VALOR TOTAL\n",
    "                valor_total = None\n",
    "                if pdf_pesquisavel_map:\n",
    "                    valor_total = processar_valor_total_PDF_P(row, pdf_pesquisavel_map, model_map, map_original_file_name, file_path)\n",
    "                \n",
    "                try:\n",
    "                    row_info['valor_total_nota'] = valor_total\n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "                    row_info['valor_total_nota'] = None\n",
    "                \n",
    "  \n",
    "                # 6. CNAE e Item da Lista de Serviços \n",
    "                f_0_cnae = 1\n",
    "                f_1_cnae = 1.15\n",
    "                f_0_it = 0.95     #0.95\n",
    "                f_1_it = 1.15    # 1\n",
    "                \n",
    "                if pdf_pesquisavel_map:\n",
    "                    cnae_value, item_servico_value = extrai_consiste_cnae_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0_cnae, f_1_cnae, f_0_it, f_1_it, map_original_file_name, file_path)\n",
    "                try:\n",
    "                    row_info['cnae'] = cnae_value\n",
    "                    row_info['item_lista_servicos'] = item_servico_value\n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "                    row_info['cnae'] = None\n",
    "                    row_info['item_lista_servicos'] = None\n",
    "                    \n",
    "                \n",
    "                # 7. VALORES E IMPOSTOS\n",
    "                valores = {}\n",
    "                nf_data_valores = {}\n",
    "                lista_impostos = []\n",
    "                f_0 = 1\n",
    "                f_1 = 1\n",
    "                if pdf_pesquisavel_map:\n",
    "                    valores = extrai_valores_impostos_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "                    row_info.update(valores)\n",
    "                  \n",
    "                        \n",
    "                # 8. DADOS COMPLEMENTARES\n",
    "                nf_data_dados_complementares = {}\n",
    "                f_0 = 1\n",
    "                f_1 = 1\n",
    "                if pdf_pesquisavel_map:\n",
    "                    nf_data_valores = extrai_dados_complementares_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "                \n",
    "                try:\n",
    "                    row_info['dados_complementares'] = nf_data_dados_complementares['dados_complementares']\n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "                    row_info['dados_complementares'] = None\n",
    "                \n",
    "                \n",
    "                \n",
    "                # 9. OUTRAS INFORMAÇOES / CRITICAS\n",
    "                valores = {} \n",
    "                nf_data_outras_informacoes = {}\n",
    "                f_0 = 1\n",
    "                f_1 = 1\n",
    "                if pdf_pesquisavel_map:\n",
    "                    valores = extrai_outras_informacoes_PSF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "                    if valores:\n",
    "                        row_info.update(valores)\n",
    "                \n",
    "                \n",
    "                # 10. OBSERVACOES    \n",
    "                data_observacao = {}\n",
    "                f_0 = 0.9\n",
    "                f_1 = 1.1\n",
    "                if pdf_pesquisavel_map:\n",
    "                    data_observacao = extrai_data_observacao_PSF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "\n",
    "                try:\n",
    "                    row_info['observacao'] = data_observacao['observacao']\n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "                    row_info['observacao'] = None\n",
    "                    \n",
    "                #erros.append(erros_processo)\n",
    "                lista_dicts.append(row_info)\n",
    "        i += 1\n",
    "\n",
    "    novo_df = pd.DataFrame(lista_dicts)\n",
    "     \n",
    "    return novo_df\n",
    "    #return prestador_lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " item: NF 11 IGREJA EVANGÉLICA BATISTA ETERNA VIDA.pdf\n",
      "\n",
      "1.A TRATAMENTO CNAE : (coordenadas originais):  x0:0.0 y0:530.0 x1:600.0 y1:540.0\n",
      "1.A TRATAMENTO CNAE - texto extraido em  coordenadas: x0:0.0 y0:530.0 x1:600.0 y1:621.0: f_0_cnae: 1, f_1_cnae: 1.15 TEXT:\n",
      "CNAE - 7111100 - SERVIÇOS DE ARQUITETURA\n",
      " \n",
      "Item da Lista de Serviços - 7.01 - ENGENHARIA, AGRONOMIA, AGRIMENSURA, ARQUITETURA, GEOLOGIA, URBANISMO, PAISAGISMO E CONGÊNERES.\n",
      " \n",
      "VALOR SERVIÇOS:\n",
      "R$ 1.000,00\n",
      "VALOR\n",
      "DEDUÇÃO:\n",
      "R$ 0,00\n",
      "DESC. INCOND:\n",
      "R$ 0,00\n",
      "BASE DE\n",
      "CÁLCULO:\n",
      "R$ 1.000,00\n",
      "ALÍQUOTA:\n",
      "2,17%\n",
      "VALOR ISS:\n",
      "R$ 21,70\n",
      "VALOR ISS\n",
      "RETIDO:\n",
      "R$ 0,00\n",
      "DESC. COND:\n",
      "R$ 0,00\n",
      "____________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "1.B CNAE PROCESSADO: 7111100 - SERVIÇOS DE ARQUITETURA                                                                                                                                                                                                                                                                                     \n",
      "\n",
      "2.A TRATAMENTO ITENS DE SERVICO:(coordenadas originais):  x0:0.0 y0:540.0 x1:600.0 y1:560.0\n",
      "2.A TRATAMENTO ITENS DE SERVICO -  texto extraido em  coordenadas originais: x0:0.0 y0:513.0 x1:600.0 y1:644.0: f_0_it:0.95, f_1_it: 1.15 TEXT:\n",
      "VALOR TOTAL DA NOTA: R$ 1.000,00\n",
      "CNAE - 7111100 - SERVIÇOS DE ARQUITETURA\n",
      " \n",
      "Item da Lista de Serviços - 7.01 - ENGENHARIA, AGRONOMIA, AGRIMENSURA, ARQUITETURA, GEOLOGIA, URBANISMO, PAISAGISMO E CONGÊNERES.\n",
      " \n",
      "VALOR SERVIÇOS:\n",
      "R$ 1.000,00\n",
      "VALOR\n",
      "DEDUÇÃO:\n",
      "R$ 0,00\n",
      "DESC. INCOND:\n",
      "R$ 0,00\n",
      "BASE DE\n",
      "CÁLCULO:\n",
      "R$ 1.000,00\n",
      "ALÍQUOTA:\n",
      "2,17%\n",
      "VALOR ISS:\n",
      "R$ 21,70\n",
      "VALOR ISS\n",
      "RETIDO:\n",
      "R$ 0,00\n",
      "DESC. COND:\n",
      "R$ 0,00\n",
      "____________________________________________________________________\n",
      "VALOR PIS:\n",
      "R$ 0,00\n",
      "VALOR COFINS:\n",
      "R$ 0,00\n",
      "VALOR IR:\n",
      "R$ 0,00\n",
      "VALOR INSS:\n",
      "R$ 0,00\n",
      "VALOR CSLL:\n",
      "R$ 0,00\n",
      "OUTRAS RETENÇÕES:\n",
      "R$ 0,00\n",
      "VALOR LÍQUIDO:\n",
      "R$ 1.000,00\n",
      "\n",
      "\n",
      "item_servico_lines (fora do if): ['Item da Lista de Serviços - 7.01 - ENGENHARIA, AGRONOMIA, AGRIMENSURA, ARQUITETURA, GEOLOGIA, URBANISMO, PAISAGISMO E CONGÊNERES.']\n",
      "item_servico_lines: ['Item da Lista de Serviços - 7.01 - ENGENHARIA, AGRONOMIA, AGRIMENSURA, ARQUITETURA, GEOLOGIA, URBANISMO, PAISAGISMO E CONGÊNERES.']\n",
      " 2.B ITEM DE SERVICO PROCESSADO: 7.01 - ENGENHARIA, AGRONOMIA, AGRIMENSURA, ARQUITETURA, GEOLOGIA, URBANISMO, PAISAGISMO E CONGÊNERES.                                                                                                                                                                                                               \n",
      "\n",
      "\n",
      "\n",
      " item: NF 29 GRI.pdf\n",
      "\n",
      "1.A TRATAMENTO CNAE : (coordenadas originais):  x0:0.0 y0:530.0 x1:600.0 y1:540.0\n",
      "1.A TRATAMENTO CNAE - texto extraido em  coordenadas: x0:0.0 y0:530.0 x1:600.0 y1:621.0: f_0_cnae: 1, f_1_cnae: 1.15 TEXT:\n",
      "CNAE - 8592999 - ENSINO DE ARTE E CULTURA NÃO ESPECIFICADO ANTERIORMENTE\n",
      "Item da Lista de Serviços - 8.02 - INSTRUÇÃO, TREINAMENTO, ORIENTAÇÃO PEDAGÓGICA E EDUCACIONAL, AVALIAÇÃO DE CONHECIMENTOS DE\n",
      "QUALQUER NATUREZA.\n",
      " \n",
      "VALOR SERVIÇOS:\n",
      "R$ 895,00\n",
      "VALOR\n",
      "DEDUÇÃO:\n",
      "R$ 0,00\n",
      "DESC. INCOND:\n",
      "R$ 0,00\n",
      "BASE DE\n",
      "CÁLCULO:\n",
      "R$ 895,00\n",
      "ALÍQUOTA:\n",
      "0%\n",
      "VALOR ISS:\n",
      "R$ 0,00\n",
      "VALOR ISS\n",
      "RETIDO:\n",
      "R$ 0,00\n",
      "DESC. COND:\n",
      "R$ 0,00\n",
      "____________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "1.B CNAE PROCESSADO: 8592999 - ENSINO DE ARTE E CULTURA NÃO ESPECIFICADO ANTERIORMENTE                                                                                                                                                                                                                                                     \n",
      "\n",
      "2.A TRATAMENTO ITENS DE SERVICO:(coordenadas originais):  x0:0.0 y0:540.0 x1:600.0 y1:560.0\n",
      "2.A TRATAMENTO ITENS DE SERVICO -  texto extraido em  coordenadas originais: x0:0.0 y0:513.0 x1:600.0 y1:644.0: f_0_it:0.95, f_1_it: 1.15 TEXT:\n",
      "VALOR TOTAL DA NOTA: R$ 895,00\n",
      "CNAE - 8592999 - ENSINO DE ARTE E CULTURA NÃO ESPECIFICADO ANTERIORMENTE\n",
      "Item da Lista de Serviços - 8.02 - INSTRUÇÃO, TREINAMENTO, ORIENTAÇÃO PEDAGÓGICA E EDUCACIONAL, AVALIAÇÃO DE CONHECIMENTOS DE\n",
      "QUALQUER NATUREZA.\n",
      " \n",
      "VALOR SERVIÇOS:\n",
      "R$ 895,00\n",
      "VALOR\n",
      "DEDUÇÃO:\n",
      "R$ 0,00\n",
      "DESC. INCOND:\n",
      "R$ 0,00\n",
      "BASE DE\n",
      "CÁLCULO:\n",
      "R$ 895,00\n",
      "ALÍQUOTA:\n",
      "0%\n",
      "VALOR ISS:\n",
      "R$ 0,00\n",
      "VALOR ISS\n",
      "RETIDO:\n",
      "R$ 0,00\n",
      "DESC. COND:\n",
      "R$ 0,00\n",
      "____________________________________________________________________\n",
      "VALOR PIS:\n",
      "R$ 0,00\n",
      "VALOR COFINS:\n",
      "R$ 0,00\n",
      "VALOR IR:\n",
      "R$ 0,00\n",
      "VALOR INSS:\n",
      "R$ 0,00\n",
      "VALOR CSLL:\n",
      "R$ 0,00\n",
      "OUTRAS RETENÇÕES:\n",
      "R$ 0,00\n",
      "VALOR LÍQUIDO:\n",
      "R$ 895,00\n",
      "\n",
      "\n",
      "item_servico_lines (fora do if): ['Item da Lista de Serviços - 8.02 - INSTRUÇÃO, TREINAMENTO, ORIENTAÇÃO PEDAGÓGICA E EDUCACIONAL, AVALIAÇÃO DE CONHECIMENTOS DE']\n",
      "item_servico_lines: ['Item da Lista de Serviços - 8.02 - INSTRUÇÃO, TREINAMENTO, ORIENTAÇÃO PEDAGÓGICA E EDUCACIONAL, AVALIAÇÃO DE CONHECIMENTOS DE']\n",
      " 2.B ITEM DE SERVICO PROCESSADO: 8.02 - INSTRUÇÃO, TREINAMENTO, ORIENTAÇÃO PEDAGÓGICA E EDUCACIONAL, AVALIAÇÃO DE CONHECIMENTOS DE QUALQUER NATUREZA.                                                                                                                                                                                                \n",
      "\n",
      "\n",
      "\n",
      " item: NF 19 CONDOMÍNIO EDIFÍCIO MENDES CAMPOS.pdf\n",
      "\n",
      "1.A TRATAMENTO CNAE : (coordenadas originais):  x0:0.0 y0:530.0 x1:600.0 y1:540.0\n",
      "1.A TRATAMENTO CNAE - texto extraido em  coordenadas: x0:0.0 y0:530.0 x1:600.0 y1:621.0: f_0_cnae: 1, f_1_cnae: 1.15 TEXT:\n",
      "CNAE - 4399103 - OBRAS DE ALVENARIA\n",
      "Item da Lista de Serviços - 7.02 - EXECUÇÃO, POR ADMINISTRAÇÃO, EMPREITADA OU SUBEMPREITADA, DE OBRAS DE CONSTRUÇÃO CIVIL, HIDRÁULICA\n",
      "OU ELÉTRICA E DE OUTRAS OBRAS SEMELHANTES, INCLUSIVE SONDAGEM, PERFURAÇÃO DE POÇOS, ESCAVAÇÃO, DRENAG\n",
      " \n",
      "VALOR SERVIÇOS:\n",
      "R$ 1.000,00\n",
      "VALOR\n",
      "DEDUÇÃO:\n",
      "R$ 0,00\n",
      "DESC. INCOND:\n",
      "R$ 0,00\n",
      "BASE DE\n",
      "CÁLCULO:\n",
      "R$ 1.000,00\n",
      "ALÍQUOTA:\n",
      "0%\n",
      "VALOR ISS:\n",
      "R$ 0,00\n",
      "VALOR ISS\n",
      "RETIDO:\n",
      "R$ 0,00\n",
      "DESC. COND:\n",
      "R$ 0,00\n",
      "____________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "1.B CNAE PROCESSADO: 4399103 - OBRAS DE ALVENARIA                                                                                                                                                                                                                                                                                          \n",
      "\n",
      "2.A TRATAMENTO ITENS DE SERVICO:(coordenadas originais):  x0:0.0 y0:540.0 x1:600.0 y1:560.0\n",
      "2.A TRATAMENTO ITENS DE SERVICO -  texto extraido em  coordenadas originais: x0:0.0 y0:513.0 x1:600.0 y1:644.0: f_0_it:0.95, f_1_it: 1.15 TEXT:\n",
      "VALOR TOTAL DA NOTA: R$ 1.000,00\n",
      "CNAE - 4399103 - OBRAS DE ALVENARIA\n",
      "Item da Lista de Serviços - 7.02 - EXECUÇÃO, POR ADMINISTRAÇÃO, EMPREITADA OU SUBEMPREITADA, DE OBRAS DE CONSTRUÇÃO CIVIL, HIDRÁULICA\n",
      "OU ELÉTRICA E DE OUTRAS OBRAS SEMELHANTES, INCLUSIVE SONDAGEM, PERFURAÇÃO DE POÇOS, ESCAVAÇÃO, DRENAG\n",
      " \n",
      "VALOR SERVIÇOS:\n",
      "R$ 1.000,00\n",
      "VALOR\n",
      "DEDUÇÃO:\n",
      "R$ 0,00\n",
      "DESC. INCOND:\n",
      "R$ 0,00\n",
      "BASE DE\n",
      "CÁLCULO:\n",
      "R$ 1.000,00\n",
      "ALÍQUOTA:\n",
      "0%\n",
      "VALOR ISS:\n",
      "R$ 0,00\n",
      "VALOR ISS\n",
      "RETIDO:\n",
      "R$ 0,00\n",
      "DESC. COND:\n",
      "R$ 0,00\n",
      "____________________________________________________________________\n",
      "VALOR PIS:\n",
      "R$ 0,00\n",
      "VALOR COFINS:\n",
      "R$ 0,00\n",
      "VALOR IR:\n",
      "R$ 0,00\n",
      "VALOR INSS:\n",
      "R$ 0,00\n",
      "VALOR CSLL:\n",
      "R$ 0,00\n",
      "OUTRAS RETENÇÕES:\n",
      "R$ 0,00\n",
      "VALOR LÍQUIDO:\n",
      "R$ 1.000,00\n",
      "\n",
      "\n",
      "item_servico_lines (fora do if): ['Item da Lista de Serviços - 7.02 - EXECUÇÃO, POR ADMINISTRAÇÃO, EMPREITADA OU SUBEMPREITADA, DE OBRAS DE CONSTRUÇÃO CIVIL, HIDRÁULICA']\n",
      "item_servico_lines: ['Item da Lista de Serviços - 7.02 - EXECUÇÃO, POR ADMINISTRAÇÃO, EMPREITADA OU SUBEMPREITADA, DE OBRAS DE CONSTRUÇÃO CIVIL, HIDRÁULICA']\n",
      " 2.B ITEM DE SERVICO PROCESSADO: 7.02 - EXECUÇÃO, POR ADMINISTRAÇÃO, EMPREITADA OU SUBEMPREITADA, DE OBRAS DE CONSTRUÇÃO CIVIL, HIDRÁULICA OU ELÉTRICA E DE OUTRAS OBRAS SEMELHANTES, INCLUSIVE SONDAGEM, PERFURAÇÃO DE POÇOS, ESCAVAÇÃO, DRENAGEM E IRRIGAÇÃO, TERRAPLANAGEM, PAVIMENTAÇÃO, CONCRETAGE                                              \n",
      "\n",
      "\n",
      "\n",
      " item: NFS.299 07.2023 FLOC.pdf\n",
      "\n",
      "1.A TRATAMENTO CNAE : (coordenadas originais):  x0:0.0 y0:530.0 x1:600.0 y1:540.0\n",
      "1.A TRATAMENTO CNAE - texto extraido em  coordenadas: x0:0.0 y0:530.0 x1:600.0 y1:621.0: f_0_cnae: 1, f_1_cnae: 1.15 TEXT:\n",
      "CNAE - 8299799 - OUTRAS ATIVIDADES DE SERVIÇOS PRESTADOS PRINCIPALMENTE ÀS EMPRESAS NÃO ESPECIFICADAS ANTERIORMENTE\n",
      "Item da Lista de Serviços - 11.02 - VIGILÂNCIA, SEGURANÇA OU MONITORAMENTO DE BENS, PESSOAS E SEMOVENTES.\n",
      "VALOR SERVIÇOS:\n",
      "R$ 5.610,72\n",
      "VALOR\n",
      "DEDUÇÃO:\n",
      "R$ 0,00\n",
      "DESC. INCOND:\n",
      "R$ 0,00\n",
      "BASE DE\n",
      "CÁLCULO:\n",
      "R$ 5.610,72\n",
      "ALÍQUOTA:\n",
      "2%\n",
      "VALOR ISS:\n",
      "R$ 0,00\n",
      "VALOR ISS\n",
      "RETIDO:\n",
      "R$ 112,21\n",
      "DESC. COND:\n",
      "R$ 0,00\n",
      "\n",
      "\n",
      "\n",
      "1.B CNAE PROCESSADO: 8299799 - OUTRAS ATIVIDADES DE SERVIÇOS PRESTADOS PRINCIPALMENTE ÀS EMPRESAS NÃO ESPECIFICADAS ANTERIORMENTE                                                                                                                                                                                                          \n",
      "\n",
      "2.A TRATAMENTO ITENS DE SERVICO:(coordenadas originais):  x0:0.0 y0:540.0 x1:600.0 y1:560.0\n",
      "2.A TRATAMENTO ITENS DE SERVICO -  texto extraido em  coordenadas originais: x0:0.0 y0:513.0 x1:600.0 y1:644.0: f_0_it:0.95, f_1_it: 1.15 TEXT:\n",
      "VALOR TOTAL DA NOTA: R$ 5.610,72\n",
      "CNAE - 8299799 - OUTRAS ATIVIDADES DE SERVIÇOS PRESTADOS PRINCIPALMENTE ÀS EMPRESAS NÃO ESPECIFICADAS ANTERIORMENTE\n",
      "Item da Lista de Serviços - 11.02 - VIGILÂNCIA, SEGURANÇA OU MONITORAMENTO DE BENS, PESSOAS E SEMOVENTES.\n",
      "VALOR SERVIÇOS:\n",
      "R$ 5.610,72\n",
      "VALOR\n",
      "DEDUÇÃO:\n",
      "R$ 0,00\n",
      "DESC. INCOND:\n",
      "R$ 0,00\n",
      "BASE DE\n",
      "CÁLCULO:\n",
      "R$ 5.610,72\n",
      "ALÍQUOTA:\n",
      "2%\n",
      "VALOR ISS:\n",
      "R$ 0,00\n",
      "VALOR ISS\n",
      "RETIDO:\n",
      "R$ 112,21\n",
      "DESC. COND:\n",
      "R$ 0,00\n",
      "____________________________________________________________________\n",
      "VALOR PIS:\n",
      "VALOR COFINS:\n",
      "VALOR IR:\n",
      "VALOR INSS:\n",
      "VALOR CSLL:\n",
      "OUTRAS RETENÇÕES:\n",
      "VALOR LÍQUIDO:\n",
      "\n",
      "\n",
      "item_servico_lines (fora do if): ['Item da Lista de Serviços - 11.02 - VIGILÂNCIA, SEGURANÇA OU MONITORAMENTO DE BENS, PESSOAS E SEMOVENTES.']\n",
      "item_servico_lines: ['Item da Lista de Serviços - 11.02 - VIGILÂNCIA, SEGURANÇA OU MONITORAMENTO DE BENS, PESSOAS E SEMOVENTES.']\n",
      " 2.B ITEM DE SERVICO PROCESSADO: 11.02 - VIGILÂNCIA, SEGURANÇA OU MONITORAMENTO DE BENS, PESSOAS E SEMOVENTES.                                                                                                                                                                                                                                        \n",
      "\n",
      "\n",
      "\n",
      " item: NFS.300 07.2023 FLOC TEXTIL.pdf\n",
      "\n",
      "1.A TRATAMENTO CNAE : (coordenadas originais):  x0:0.0 y0:530.0 x1:600.0 y1:540.0\n",
      "1.A TRATAMENTO CNAE - texto extraido em  coordenadas: x0:0.0 y0:530.0 x1:600.0 y1:621.0: f_0_cnae: 1, f_1_cnae: 1.15 TEXT:\n",
      "CNAE - 8299799 - OUTRAS ATIVIDADES DE SERVIÇOS PRESTADOS PRINCIPALMENTE ÀS EMPRESAS NÃO ESPECIFICADAS ANTERIORMENTE\n",
      "Item da Lista de Serviços - 11.02 - VIGILÂNCIA, SEGURANÇA OU MONITORAMENTO DE BENS, PESSOAS E SEMOVENTES.\n",
      "VALOR SERVIÇOS:\n",
      "R$ 5.610,72\n",
      "VALOR\n",
      "DEDUÇÃO:\n",
      "R$ 0,00\n",
      "DESC. INCOND:\n",
      "R$ 0,00\n",
      "BASE DE\n",
      "CÁLCULO:\n",
      "R$ 5.610,72\n",
      "ALÍQUOTA:\n",
      "2%\n",
      "VALOR ISS:\n",
      "R$ 0,00\n",
      "VALOR ISS\n",
      "RETIDO:\n",
      "R$ 112,21\n",
      "DESC. COND:\n",
      "R$ 0,00\n",
      "\n",
      "\n",
      "\n",
      "1.B CNAE PROCESSADO: 8299799 - OUTRAS ATIVIDADES DE SERVIÇOS PRESTADOS PRINCIPALMENTE ÀS EMPRESAS NÃO ESPECIFICADAS ANTERIORMENTE                                                                                                                                                                                                          \n",
      "\n",
      "2.A TRATAMENTO ITENS DE SERVICO:(coordenadas originais):  x0:0.0 y0:540.0 x1:600.0 y1:560.0\n",
      "2.A TRATAMENTO ITENS DE SERVICO -  texto extraido em  coordenadas originais: x0:0.0 y0:513.0 x1:600.0 y1:644.0: f_0_it:0.95, f_1_it: 1.15 TEXT:\n",
      "VALOR TOTAL DA NOTA: R$ 5.610,72\n",
      "CNAE - 8299799 - OUTRAS ATIVIDADES DE SERVIÇOS PRESTADOS PRINCIPALMENTE ÀS EMPRESAS NÃO ESPECIFICADAS ANTERIORMENTE\n",
      "Item da Lista de Serviços - 11.02 - VIGILÂNCIA, SEGURANÇA OU MONITORAMENTO DE BENS, PESSOAS E SEMOVENTES.\n",
      "VALOR SERVIÇOS:\n",
      "R$ 5.610,72\n",
      "VALOR\n",
      "DEDUÇÃO:\n",
      "R$ 0,00\n",
      "DESC. INCOND:\n",
      "R$ 0,00\n",
      "BASE DE\n",
      "CÁLCULO:\n",
      "R$ 5.610,72\n",
      "ALÍQUOTA:\n",
      "2%\n",
      "VALOR ISS:\n",
      "R$ 0,00\n",
      "VALOR ISS\n",
      "RETIDO:\n",
      "R$ 112,21\n",
      "DESC. COND:\n",
      "R$ 0,00\n",
      "____________________________________________________________________\n",
      "VALOR PIS:\n",
      "VALOR COFINS:\n",
      "VALOR IR:\n",
      "VALOR INSS:\n",
      "VALOR CSLL:\n",
      "OUTRAS RETENÇÕES:\n",
      "VALOR LÍQUIDO:\n",
      "\n",
      "\n",
      "item_servico_lines (fora do if): ['Item da Lista de Serviços - 11.02 - VIGILÂNCIA, SEGURANÇA OU MONITORAMENTO DE BENS, PESSOAS E SEMOVENTES.']\n",
      "item_servico_lines: ['Item da Lista de Serviços - 11.02 - VIGILÂNCIA, SEGURANÇA OU MONITORAMENTO DE BENS, PESSOAS E SEMOVENTES.']\n",
      " 2.B ITEM DE SERVICO PROCESSADO: 11.02 - VIGILÂNCIA, SEGURANÇA OU MONITORAMENTO DE BENS, PESSOAS E SEMOVENTES.                                                                                                                                                                                                                                        \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3B. XXX Efetuo a extracao de dados\n",
    "# analisar_pdf_pesquisavel\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'PREPROCESS' \n",
    "status = 'PREPROCESS_EXTRACT'\n",
    "raw_document_list = []\n",
    "dados_iniciais_qr = {}\n",
    "incricao_prest = {}\n",
    "dados_prest = {}\n",
    "\n",
    "#raw_texto = extracao_pipeline(df_analise_pipe, fase, atividade, status)subset_df\n",
    "# df_ini, dados_iniciais_qr = extracao_pipeline(df_root_pipe, fase, atividade, status)\n",
    "df = extracao_pipeline(df_root_pipe, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numero_nota_fiscal</th>\n",
       "      <th>codigo_verificacao</th>\n",
       "      <th>valor_total_nota</th>\n",
       "      <th>cnae</th>\n",
       "      <th>item_lista_servicos</th>\n",
       "      <th>directory</th>\n",
       "      <th>original_file_name</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74212c9a-0c35-4665-83e7-d48eeaaa5891</th>\n",
       "      <td>11</td>\n",
       "      <td>3CFDF62AA</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>7111100 - SERVIÇOS DE ARQUITETURA             ...</td>\n",
       "      <td>7.01 - ENGENHARIA, AGRONOMIA, AGRIMENSURA, ARQ...</td>\n",
       "      <td>1004403.0</td>\n",
       "      <td>NF 11 IGREJA EVANGÉLICA BATISTA ETERNA VIDA.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a707ef12-fce9-4e24-8ed4-fc9418497a2f</th>\n",
       "      <td>29</td>\n",
       "      <td>F3DF71EB9</td>\n",
       "      <td>895.00</td>\n",
       "      <td>8592999 - ENSINO DE ARTE E CULTURA NÃO ESPECIF...</td>\n",
       "      <td>8.02 - INSTRUÇÃO, TREINAMENTO, ORIENTAÇÃO PEDA...</td>\n",
       "      <td>1005075.0</td>\n",
       "      <td>NF 29 GRI.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e919cfbe-5384-4785-be7a-a4e7dfb5c43c</th>\n",
       "      <td>19</td>\n",
       "      <td>3319A8092</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>4399103 - OBRAS DE ALVENARIA                  ...</td>\n",
       "      <td>7.02 - EXECUÇÃO, POR ADMINISTRAÇÃO, EMPREITADA...</td>\n",
       "      <td>1004847.0</td>\n",
       "      <td>NF 19 CONDOMÍNIO EDIFÍCIO MENDES CAMPOS.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2155b43-b584-4a40-9039-2a1d5ecb7e40</th>\n",
       "      <td>299</td>\n",
       "      <td>8D50FD8F2</td>\n",
       "      <td>5610.72</td>\n",
       "      <td>8299799 - OUTRAS ATIVIDADES DE SERVIÇOS PRESTA...</td>\n",
       "      <td>11.02 - VIGILÂNCIA, SEGURANÇA OU MONITORAMENTO...</td>\n",
       "      <td>10597.0</td>\n",
       "      <td>NFS.299 07.2023 FLOC.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4728795-76e6-4b5f-b422-bc664825360b</th>\n",
       "      <td>300</td>\n",
       "      <td>A3EE7C7A9</td>\n",
       "      <td>5610.72</td>\n",
       "      <td>8299799 - OUTRAS ATIVIDADES DE SERVIÇOS PRESTA...</td>\n",
       "      <td>11.02 - VIGILÂNCIA, SEGURANÇA OU MONITORAMENTO...</td>\n",
       "      <td>10597.0</td>\n",
       "      <td>NFS.300 07.2023 FLOC TEXTIL.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     numero_nota_fiscal codigo_verificacao  \\\n",
       "document_unique_id                                                           \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891                 11          3CFDF62AA   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f                 29          F3DF71EB9   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c                 19          3319A8092   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40                299          8D50FD8F2   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b                300          A3EE7C7A9   \n",
       "\n",
       "                                      valor_total_nota  \\\n",
       "document_unique_id                                       \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891           1000.00   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f            895.00   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c           1000.00   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40           5610.72   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b           5610.72   \n",
       "\n",
       "                                                                                   cnae  \\\n",
       "document_unique_id                                                                        \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891  7111100 - SERVIÇOS DE ARQUITETURA             ...   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f  8592999 - ENSINO DE ARTE E CULTURA NÃO ESPECIF...   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c  4399103 - OBRAS DE ALVENARIA                  ...   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40  8299799 - OUTRAS ATIVIDADES DE SERVIÇOS PRESTA...   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b  8299799 - OUTRAS ATIVIDADES DE SERVIÇOS PRESTA...   \n",
       "\n",
       "                                                                    item_lista_servicos  \\\n",
       "document_unique_id                                                                        \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891  7.01 - ENGENHARIA, AGRONOMIA, AGRIMENSURA, ARQ...   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f  8.02 - INSTRUÇÃO, TREINAMENTO, ORIENTAÇÃO PEDA...   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c  7.02 - EXECUÇÃO, POR ADMINISTRAÇÃO, EMPREITADA...   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40  11.02 - VIGILÂNCIA, SEGURANÇA OU MONITORAMENTO...   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b  11.02 - VIGILÂNCIA, SEGURANÇA OU MONITORAMENTO...   \n",
       "\n",
       "                                      directory  \\\n",
       "document_unique_id                                \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891  1004403.0   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f  1005075.0   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c  1004847.0   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40    10597.0   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b    10597.0   \n",
       "\n",
       "                                                                   original_file_name  \\\n",
       "document_unique_id                                                                      \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891  NF 11 IGREJA EVANGÉLICA BATISTA ETERNA VIDA.pdf   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f                                    NF 29 GRI.pdf   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c      NF 19 CONDOMÍNIO EDIFÍCIO MENDES CAMPOS.pdf   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40                         NFS.299 07.2023 FLOC.pdf   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b                  NFS.300 07.2023 FLOC TEXTIL.pdf   \n",
       "\n",
       "                                                                              file_path  \n",
       "document_unique_id                                                                       \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891  pipeline_extracao_documentos/2_documentos_para...  \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f  pipeline_extracao_documentos/2_documentos_para...  \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c  pipeline_extracao_documentos/2_documentos_para...  \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40  pipeline_extracao_documentos/2_documentos_para...  \n",
       "e4728795-76e6-4b5f-b422-bc664825360b  pipeline_extracao_documentos/2_documentos_para...  "
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alterando a ordem das colunas\n",
    "df_conf = df[['numero_nota_fiscal', 'codigo_verificacao', 'valor_total_nota', 'cnae', 'item_lista_servicos','directory','original_file_name','file_path']]\n",
    "df_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>date_time</th>\n",
       "      <th>batch</th>\n",
       "      <th>fase_processo</th>\n",
       "      <th>nome_atividade</th>\n",
       "      <th>status_documento</th>\n",
       "      <th>acao_executada</th>\n",
       "      <th>original_file_name</th>\n",
       "      <th>directory</th>\n",
       "      <th>one_page</th>\n",
       "      <th>...</th>\n",
       "      <th>outras_retencoes</th>\n",
       "      <th>valor_liquido</th>\n",
       "      <th>dados_complementares</th>\n",
       "      <th>exigibilidade_iss</th>\n",
       "      <th>regime_tributacao</th>\n",
       "      <th>simples_nacional</th>\n",
       "      <th>issqn_retido</th>\n",
       "      <th>local_prestacao_servico</th>\n",
       "      <th>local_incidencia</th>\n",
       "      <th>observacao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NF 11 IGREJA EVANGÉLICA BATISTA ETERNA VIDA.pdf</td>\n",
       "      <td>1004403.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>None</td>\n",
       "      <td>Exigivel</td>\n",
       "      <td>Microempresa municipal</td>\n",
       "      <td>Sim ( 2,17% )</td>\n",
       "      <td>Não</td>\n",
       "      <td>Magé - RJ</td>\n",
       "      <td>Magé - RJ</td>\n",
       "      <td>DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NF 29 GRI.pdf</td>\n",
       "      <td>1005075.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>895.00</td>\n",
       "      <td>None</td>\n",
       "      <td>Exigivel</td>\n",
       "      <td>Microempresário Individual</td>\n",
       "      <td>Sim ( 0% )</td>\n",
       "      <td>Não</td>\n",
       "      <td>LOCAL INCIDÊNCIA</td>\n",
       "      <td>Magé - RJ</td>\n",
       "      <td>DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.0</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NF 19 CONDOMÍNIO EDIFÍCIO MENDES CAMPOS.pdf</td>\n",
       "      <td>1004847.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>None</td>\n",
       "      <td>Exigivel</td>\n",
       "      <td>Microempresário Individual</td>\n",
       "      <td>Sim ( 0% )</td>\n",
       "      <td>Não</td>\n",
       "      <td>Magé - RJ</td>\n",
       "      <td>Magé - RJ</td>\n",
       "      <td>DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NFS.299 07.2023 FLOC.pdf</td>\n",
       "      <td>10597.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5498.51</td>\n",
       "      <td>None</td>\n",
       "      <td>Exigivel</td>\n",
       "      <td>Microempresa municipal</td>\n",
       "      <td>Sim ( 2% )</td>\n",
       "      <td>Sim</td>\n",
       "      <td>LOCAL INCIDÊNCIA</td>\n",
       "      <td>Magé - RJ</td>\n",
       "      <td>DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.0</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NFS.300 07.2023 FLOC TEXTIL.pdf</td>\n",
       "      <td>10597.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5498.51</td>\n",
       "      <td>None</td>\n",
       "      <td>Exigivel</td>\n",
       "      <td>Microempresa municipal</td>\n",
       "      <td>Sim ( 2% )</td>\n",
       "      <td>Sim</td>\n",
       "      <td>LOCAL INCIDÊNCIA</td>\n",
       "      <td>Magé - RJ</td>\n",
       "      <td>DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    seq            date_time     batch fase_processo nome_atividade  \\\n",
       "0   5.0  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "1  15.0  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "2  17.0  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "3  30.0  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "4  31.0  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "\n",
       "     status_documento acao_executada  \\\n",
       "0  PREPROCESS_EXTRACT        Analise   \n",
       "1  PREPROCESS_EXTRACT        Analise   \n",
       "2  PREPROCESS_EXTRACT        Analise   \n",
       "3  PREPROCESS_EXTRACT        Analise   \n",
       "4  PREPROCESS_EXTRACT        Analise   \n",
       "\n",
       "                                original_file_name  directory  one_page  ...  \\\n",
       "0  NF 11 IGREJA EVANGÉLICA BATISTA ETERNA VIDA.pdf  1004403.0       1.0  ...   \n",
       "1                                    NF 29 GRI.pdf  1005075.0       1.0  ...   \n",
       "2      NF 19 CONDOMÍNIO EDIFÍCIO MENDES CAMPOS.pdf  1004847.0       1.0  ...   \n",
       "3                         NFS.299 07.2023 FLOC.pdf    10597.0       1.0  ...   \n",
       "4                  NFS.300 07.2023 FLOC TEXTIL.pdf    10597.0       1.0  ...   \n",
       "\n",
       "   outras_retencoes valor_liquido dados_complementares exigibilidade_iss  \\\n",
       "0               0.0       1000.00                 None          Exigivel   \n",
       "1               0.0        895.00                 None          Exigivel   \n",
       "2               0.0       1000.00                 None          Exigivel   \n",
       "3               0.0       5498.51                 None          Exigivel   \n",
       "4               0.0       5498.51                 None          Exigivel   \n",
       "\n",
       "            regime_tributacao simples_nacional issqn_retido  \\\n",
       "0      Microempresa municipal    Sim ( 2,17% )          Não   \n",
       "1  Microempresário Individual       Sim ( 0% )          Não   \n",
       "2  Microempresário Individual       Sim ( 0% )          Não   \n",
       "3      Microempresa municipal       Sim ( 2% )          Sim   \n",
       "4      Microempresa municipal       Sim ( 2% )          Sim   \n",
       "\n",
       "  local_prestacao_servico local_incidencia  \\\n",
       "0               Magé - RJ        Magé - RJ   \n",
       "1        LOCAL INCIDÊNCIA        Magé - RJ   \n",
       "2               Magé - RJ        Magé - RJ   \n",
       "3        LOCAL INCIDÊNCIA        Magé - RJ   \n",
       "4        LOCAL INCIDÊNCIA        Magé - RJ   \n",
       "\n",
       "                                          observacao  \n",
       "0  DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...  \n",
       "1  DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...  \n",
       "2  DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...  \n",
       "3  DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...  \n",
       "4  DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>date_time</th>\n",
       "      <th>batch</th>\n",
       "      <th>fase_processo</th>\n",
       "      <th>nome_atividade</th>\n",
       "      <th>status_documento</th>\n",
       "      <th>acao_executada</th>\n",
       "      <th>original_file_name</th>\n",
       "      <th>directory</th>\n",
       "      <th>one_page</th>\n",
       "      <th>...</th>\n",
       "      <th>outras_retencoes</th>\n",
       "      <th>valor_liquido</th>\n",
       "      <th>dados_complementares</th>\n",
       "      <th>exigibilidade_iss</th>\n",
       "      <th>regime_tributacao</th>\n",
       "      <th>simples_nacional</th>\n",
       "      <th>issqn_retido</th>\n",
       "      <th>local_prestacao_servico</th>\n",
       "      <th>local_incidencia</th>\n",
       "      <th>observacao</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74212c9a-0c35-4665-83e7-d48eeaaa5891</th>\n",
       "      <td>5.0</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NF 11 IGREJA EVANGÉLICA BATISTA ETERNA VIDA.pdf</td>\n",
       "      <td>1004403.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>None</td>\n",
       "      <td>Exigivel</td>\n",
       "      <td>Microempresa municipal</td>\n",
       "      <td>Sim ( 2,17% )</td>\n",
       "      <td>Não</td>\n",
       "      <td>Magé - RJ</td>\n",
       "      <td>Magé - RJ</td>\n",
       "      <td>DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a707ef12-fce9-4e24-8ed4-fc9418497a2f</th>\n",
       "      <td>15.0</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NF 29 GRI.pdf</td>\n",
       "      <td>1005075.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>895.00</td>\n",
       "      <td>None</td>\n",
       "      <td>Exigivel</td>\n",
       "      <td>Microempresário Individual</td>\n",
       "      <td>Sim ( 0% )</td>\n",
       "      <td>Não</td>\n",
       "      <td>LOCAL INCIDÊNCIA</td>\n",
       "      <td>Magé - RJ</td>\n",
       "      <td>DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e919cfbe-5384-4785-be7a-a4e7dfb5c43c</th>\n",
       "      <td>17.0</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NF 19 CONDOMÍNIO EDIFÍCIO MENDES CAMPOS.pdf</td>\n",
       "      <td>1004847.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>None</td>\n",
       "      <td>Exigivel</td>\n",
       "      <td>Microempresário Individual</td>\n",
       "      <td>Sim ( 0% )</td>\n",
       "      <td>Não</td>\n",
       "      <td>Magé - RJ</td>\n",
       "      <td>Magé - RJ</td>\n",
       "      <td>DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2155b43-b584-4a40-9039-2a1d5ecb7e40</th>\n",
       "      <td>30.0</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NFS.299 07.2023 FLOC.pdf</td>\n",
       "      <td>10597.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5498.51</td>\n",
       "      <td>None</td>\n",
       "      <td>Exigivel</td>\n",
       "      <td>Microempresa municipal</td>\n",
       "      <td>Sim ( 2% )</td>\n",
       "      <td>Sim</td>\n",
       "      <td>LOCAL INCIDÊNCIA</td>\n",
       "      <td>Magé - RJ</td>\n",
       "      <td>DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4728795-76e6-4b5f-b422-bc664825360b</th>\n",
       "      <td>31.0</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NFS.300 07.2023 FLOC TEXTIL.pdf</td>\n",
       "      <td>10597.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5498.51</td>\n",
       "      <td>None</td>\n",
       "      <td>Exigivel</td>\n",
       "      <td>Microempresa municipal</td>\n",
       "      <td>Sim ( 2% )</td>\n",
       "      <td>Sim</td>\n",
       "      <td>LOCAL INCIDÊNCIA</td>\n",
       "      <td>Magé - RJ</td>\n",
       "      <td>DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       seq            date_time     batch  \\\n",
       "document_unique_id                                                          \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891   5.0  13/09/2023 08:32:22  Batch_20   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f  15.0  13/09/2023 08:32:22  Batch_20   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c  17.0  13/09/2023 08:32:22  Batch_20   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40  30.0  13/09/2023 08:32:22  Batch_20   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b  31.0  13/09/2023 08:32:22  Batch_20   \n",
       "\n",
       "                                     fase_processo nome_atividade  \\\n",
       "document_unique_id                                                  \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891       analise   scan_analise   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f       analise   scan_analise   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c       analise   scan_analise   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40       analise   scan_analise   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b       analise   scan_analise   \n",
       "\n",
       "                                        status_documento acao_executada  \\\n",
       "document_unique_id                                                        \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891  PREPROCESS_EXTRACT        Analise   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f  PREPROCESS_EXTRACT        Analise   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c  PREPROCESS_EXTRACT        Analise   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40  PREPROCESS_EXTRACT        Analise   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b  PREPROCESS_EXTRACT        Analise   \n",
       "\n",
       "                                                                   original_file_name  \\\n",
       "document_unique_id                                                                      \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891  NF 11 IGREJA EVANGÉLICA BATISTA ETERNA VIDA.pdf   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f                                    NF 29 GRI.pdf   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c      NF 19 CONDOMÍNIO EDIFÍCIO MENDES CAMPOS.pdf   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40                         NFS.299 07.2023 FLOC.pdf   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b                  NFS.300 07.2023 FLOC TEXTIL.pdf   \n",
       "\n",
       "                                      directory  one_page  ...  \\\n",
       "document_unique_id                                         ...   \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891  1004403.0       1.0  ...   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f  1005075.0       1.0  ...   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c  1004847.0       1.0  ...   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40    10597.0       1.0  ...   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b    10597.0       1.0  ...   \n",
       "\n",
       "                                      outras_retencoes valor_liquido  \\\n",
       "document_unique_id                                                     \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891               0.0       1000.00   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f               0.0        895.00   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c               0.0       1000.00   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40               0.0       5498.51   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b               0.0       5498.51   \n",
       "\n",
       "                                     dados_complementares exigibilidade_iss  \\\n",
       "document_unique_id                                                            \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891                 None          Exigivel   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f                 None          Exigivel   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c                 None          Exigivel   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40                 None          Exigivel   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b                 None          Exigivel   \n",
       "\n",
       "                                               regime_tributacao  \\\n",
       "document_unique_id                                                 \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891      Microempresa municipal   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f  Microempresário Individual   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c  Microempresário Individual   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40      Microempresa municipal   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b      Microempresa municipal   \n",
       "\n",
       "                                     simples_nacional issqn_retido  \\\n",
       "document_unique_id                                                   \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891    Sim ( 2,17% )          Não   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f       Sim ( 0% )          Não   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c       Sim ( 0% )          Não   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40       Sim ( 2% )          Sim   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b       Sim ( 2% )          Sim   \n",
       "\n",
       "                                     local_prestacao_servico local_incidencia  \\\n",
       "document_unique_id                                                              \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891               Magé - RJ        Magé - RJ   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f        LOCAL INCIDÊNCIA        Magé - RJ   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c               Magé - RJ        Magé - RJ   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40        LOCAL INCIDÊNCIA        Magé - RJ   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b        LOCAL INCIDÊNCIA        Magé - RJ   \n",
       "\n",
       "                                                                             observacao  \n",
       "document_unique_id                                                                       \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891  DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...  \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f  DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...  \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c  DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...  \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40  DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...  \n",
       "e4728795-76e6-4b5f-b422-bc664825360b  DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRIT...  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "subset_df_analise_pipe = df_analise_pipe.query('seq == 59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Salvando o DF (IMPORTANTE)\n",
    "df.to_excel(\"df2.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['seq', 'date_time', 'batch', 'fase_processo', 'nome_atividade',\n",
       "       'status_documento', 'acao_executada', 'original_file_name', 'directory',\n",
       "       'one_page', 'pages', 'palavra_chave', 'document_tag', 'action_item',\n",
       "       'level', 'parent_document_unique_id', 'file_hash', 'file_path',\n",
       "       'informations', 'prefeitura', 'pdf_pesquisavel', 'de_para_pm', 'model',\n",
       "       'tipo_nota_fiscal', 'numero_nota_fiscal', 'competencia',\n",
       "       'dt_hr_emissao', 'codigo_verificacao', 'secao',\n",
       "       'p_cpf_cnpj_com_mascara', 'p_cpf_cnpj_sem_mascara',\n",
       "       'p_inscricao_municipal', 'p_inscricao_estadual', 'p_telefone',\n",
       "       'p_razao_social', 'p_nome_fantasia', 'p_endereco', 'p_email',\n",
       "       't_cpf_cnpj_com_mascara', 't_cpf_cnpj_sem_mascara', 't_rg',\n",
       "       't_telefone', 't_inscricao_municipal', 't_inscricao_estadual',\n",
       "       't_razao_social', 't_endereco', 't_email', 'discriminacao_servicos',\n",
       "       'valor_total_nota', 'cnae', 'item_lista_servicos', 'valor_servicos',\n",
       "       'valor_deducao', 'desc_incond', 'base_calculo', 'aliquota', 'valor_iss',\n",
       "       'valor_iss_retido', 'desc_cond', 'valor_pis', 'valor_cofins',\n",
       "       'valor_ir', 'valor_inss', 'valor_csll', 'outras_retencoes',\n",
       "       'valor_liquido', 'dados_complementares', 'exigibilidade_iss',\n",
       "       'regime_tributacao', 'simples_nacional', 'issqn_retido',\n",
       "       'local_prestacao_servico', 'local_incidencia', 'observacao'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>2.3.1</b> ExportaÇao do Json </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = os.path.join(json_path, nome_formado_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As informações foram salvas em pipeline_extracao_documentos/5_documentos_processados/jsons/Batch_20.json\n"
     ]
    }
   ],
   "source": [
    "# Lista para armazenar os dicionários\n",
    "dados_json = {}\n",
    "\n",
    "# Iterar sobre cada linha no DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # dados_df e o dicionario para armazenar os dados da nota fiscal atual\n",
    "    #diretorio = str(row['directory'])\n",
    "    dados_nf = {\n",
    "                \"dados_NF_PDF\": {\n",
    "                                \"data_cabecalho\": {\n",
    "                                    \"secao\": \"1 - CABECALHO\",\n",
    "                                    \"nome_prefeitura\": row['prefeitura'],\n",
    "                                    \"tipo_nota_fiscal\": row['tipo_nota_fiscal'],\n",
    "                                    \"numero_nota_fiscal\": row['numero_nota_fiscal'],\n",
    "                                    \"competencia\": row['competencia'],\n",
    "                                    \"dt_hr_emissoa\": row['dt_hr_emissao'],\n",
    "                                    \"codigo_verificacao\": row['codigo_verificacao']\n",
    "                                },\n",
    "                                \"data_prestador\": {\n",
    "                                    \"secao\": \"2. PRESTADOR DE SERVIÇO\",\n",
    "                                    \"cpf_cnpj_com_mascara\": row['p_cpf_cnpj_com_mascara'],\n",
    "                                    \"cpf_cnpj_sem_mascara\": row['p_cpf_cnpj_sem_mascara'],\n",
    "                                    \"inscricao_municipal\": row['p_inscricao_municipal'],\n",
    "                                    \"inscricao_estadual\": row['p_inscricao_estadual'],\n",
    "                                    \"telefone\": row['p_telefone'],\n",
    "                                    \"razao_social\": row['p_razao_social'],\n",
    "                                    \"nome_fantasia\": row['p_nome_fantasia'],\n",
    "                                    \"endereco\": row['p_endereco'],\n",
    "                                    \"email\": row['p_email']\n",
    "                                },\n",
    "                                \"data_tomador\": {\n",
    "                                    \"secao\": \"3. TOMADOR DE SERVIÇO\",\n",
    "                                    \"cpf_cnpj_com_mascara\": row['t_cpf_cnpj_com_mascara'],\n",
    "                                    \"cpf_cnpj_sem_mascara\": row['t_cpf_cnpj_sem_mascara'],\n",
    "                                    \"rg\": row['t_rg'],\n",
    "                                    \"inscricao_municipal\": row['t_inscricao_municipal'],\n",
    "                                    \"inscricao_estadual\": row['t_inscricao_estadual'],\n",
    "                                    \"telefone\": row['t_telefone'],\n",
    "                                    \"razao_social\": row['t_razao_social'],\n",
    "                                    \"endereco\": row['t_endereco'],\n",
    "                                    \"email\": row['t_email']\n",
    "                                },\n",
    "                                \"data_servico\": {\n",
    "                                    \"secao\": \"4. DESCRIMINACAO DOS SERVIÇOS\",\n",
    "                                    \"discriminacao_servicos\": row['discriminacao_servicos']\n",
    "                                },\n",
    "                                \"data_valor_total\": {\n",
    "                                    \"secao\": \"5. VALOR TOTAL\",\n",
    "                                    \"valor_total_nota\": row['valor_total_nota']\n",
    "                                },\n",
    "                                \"data_CNAE\": {\n",
    "                                    \"secao\": \"6. CNAE e Item da Lista de Serviços\",\n",
    "                                    \"cnae\": row['cnae'],\n",
    "                                    \"item_lista_servicos\": row['item_lista_servicos']\n",
    "                                },\n",
    "                                \"data_valores\": {\n",
    "                                    \"secao\": \"7. VALORES E IMPOSTOS\",\n",
    "                                    \"valor_servicos\": row['valor_servicos'],\n",
    "                                    \"valor_deducao\": row['valor_deducao'],\n",
    "                                    \"desc_incond\" : row['desc_incond'],\n",
    "                                    \"base_calculo\": row['base_calculo'],\n",
    "                                    \"aliquota\": row['aliquota'],\n",
    "                                    \"valor_iss\": row['valor_iss'],\n",
    "                                    \"valor_iss_retido\": row['valor_iss_retido'],\n",
    "                                    \"desc_cond\": row['desc_cond'],\n",
    "                                    \"valor_pis\": row['valor_pis'],\n",
    "                                    \"valor_cofins\": row['valor_cofins'],\n",
    "                                    \"valor_ir\": row['valor_ir'],\n",
    "                                    \"valor_inss\": row['valor_inss'],\n",
    "                                    \"valor_csll\": row['valor_csll'],\n",
    "                                    \"outras_retencoes\": row['outras_retencoes'],\n",
    "                                    \"valor_liquido\": row['valor_liquido']\n",
    "                                },\n",
    "                                \"data_dados_complementares\": {\n",
    "                                    \"secao\": \"8. DADOS COMPLEMENTARES\",\n",
    "                                    \"dados_complementares\": row['dados_complementares']\n",
    "                                },\n",
    "                                \"data_outras_informacoes\": {\n",
    "                                    \"secao\": \"9. OUTRAS INFORMAÇOES / CRITICAS\",\n",
    "                                    \"exigibilidade_iss\": row['exigibilidade_iss'],\n",
    "                                    \"regime_tributacao\": row['regime_tributacao'],\n",
    "                                    \"simples_nacional\": row['simples_nacional'],\n",
    "                                    \"issqn_retido\": row['issqn_retido'],\n",
    "                                    \"local_prestacao_servico\": row['local_prestacao_servico'],\n",
    "                                    \"local_incidencia\": row['local_incidencia']\n",
    "                                },\n",
    "                                \"data_observacao\": {\n",
    "                                    \"secao\": \"10. OBSERVACOES\",\n",
    "                                    \"observacao\": row['observacao']\n",
    "                                },\n",
    "                            },\n",
    "                            \"batch\": row['batch'],    \n",
    "                            \"diretorio\": str(row['directory']),\n",
    "                            \"nome_arquivo\": row['original_file_name'],\n",
    "                            \"pdf_pesquisavel\": row['pdf_pesquisavel'],\n",
    "                            \"modelo\": row['model'],   \n",
    "                            \"document_unique_id\": index,\n",
    "                    }        \n",
    "                \n",
    "    \n",
    "    numero_nota_fiscal = str(row['numero_nota_fiscal'])\n",
    "    dados_json[numero_nota_fiscal] = dados_nf\n",
    "\n",
    "# Salvando em formato JSON\n",
    "json_file_path = os.path.join(json_path, nome_formado_json)\n",
    "with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dados_json, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "print(f\"As informações foram salvas em {json_file_path}\")  \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> Por este decreto fica esta area liberada para composicao de testes  </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_file_name</th>\n",
       "      <th>directory</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74212c9a-0c35-4665-83e7-d48eeaaa5891</th>\n",
       "      <td>NF 11 IGREJA EVANGÉLICA BATISTA ETERNA VIDA.pdf</td>\n",
       "      <td>1004403.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a707ef12-fce9-4e24-8ed4-fc9418497a2f</th>\n",
       "      <td>NF 29 GRI.pdf</td>\n",
       "      <td>1005075.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e919cfbe-5384-4785-be7a-a4e7dfb5c43c</th>\n",
       "      <td>NF 19 CONDOMÍNIO EDIFÍCIO MENDES CAMPOS.pdf</td>\n",
       "      <td>1004847.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2155b43-b584-4a40-9039-2a1d5ecb7e40</th>\n",
       "      <td>NFS.299 07.2023 FLOC.pdf</td>\n",
       "      <td>10597.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4728795-76e6-4b5f-b422-bc664825360b</th>\n",
       "      <td>NFS.300 07.2023 FLOC TEXTIL.pdf</td>\n",
       "      <td>10597.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   original_file_name  \\\n",
       "document_unique_id                                                                      \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891  NF 11 IGREJA EVANGÉLICA BATISTA ETERNA VIDA.pdf   \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f                                    NF 29 GRI.pdf   \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c      NF 19 CONDOMÍNIO EDIFÍCIO MENDES CAMPOS.pdf   \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40                         NFS.299 07.2023 FLOC.pdf   \n",
       "e4728795-76e6-4b5f-b422-bc664825360b                  NFS.300 07.2023 FLOC TEXTIL.pdf   \n",
       "\n",
       "                                      directory  \n",
       "document_unique_id                               \n",
       "74212c9a-0c35-4665-83e7-d48eeaaa5891  1004403.0  \n",
       "a707ef12-fce9-4e24-8ed4-fc9418497a2f  1005075.0  \n",
       "e919cfbe-5384-4785-be7a-a4e7dfb5c43c  1004847.0  \n",
       "c2155b43-b584-4a40-9039-2a1d5ecb7e40    10597.0  \n",
       "e4728795-76e6-4b5f-b422-bc664825360b    10597.0  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conf = df[['original_file_name','directory']]\n",
    "df_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_file_name = \"NF 11 IGREJA EVANGÉLICA BATISTA ETERNA VIDA.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Item da Lista de Serviços - 7.01 - ENGENHARIA, AGRONOMIA, AGRIMENSURA, ARQUITETURA, GEOLOGIA, URBANISMO, PAISAGISMO E CONGÊNERES.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_20/MAGE_PDF_31282023_2254/1004847/NF 19 CONDOMÍNIO EDIFÍCIO MENDES CAMPOS.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Dados para testes de cooredenadas dos Frames\n",
    "pdf_pesquisavel_map = True\n",
    "section = \"6. CNAE e Item da Lista de Serviços\"\n",
    "model = \"MAGE\"\n",
    "tipo = \"sframe_field\"\n",
    "label = \"cnae\"\n",
    "\n",
    "coordinates = get_coordinates_filter_pdf_pesquisavel(model, tipo, label, section)\n",
    "x0, y0, x1, y1 = coordinates[0]\n",
    "get_coordinates_filter(pdf_pesquisavel_map, model, tipo, label, section)\n",
    "print(f'model: {model} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = [\"cnae\", 'item_lista_servicos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splited = Texto_extraido.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_extraido_itens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_servico_value, cnae_associado = item_servico_dict.get((de_para_pm, item_servico_cod), (\"Valor não encontrado\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnae_associado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_servico_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_para_pm = \"PM_MAGE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_extraido_cnae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnae_valor = processa_cnae_dict(text_extraido_cnae, de_para_pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise_pipe['pdf_pesquisavel'] = df_analise_pipe.apply(lambda row: row['coluna1'] * 2 if row['coluna2'] > 0 else row['coluna1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizei o DF baseado em condiçao de outra coluna\n",
    "df_analise_pipe['pdf_pesquisavel'] = df_analise_pipe.apply(lambda row: False if row['status_documento'] == \"Template_encontrado\" else row['pdf_pesquisavel'], axis=1)\n",
    "\n",
    "\n",
    "# Podemos chegar de forma mais rapida\n",
    "df_analise_pipe.loc[df_analise_pipe['status_documento'] == \"Template_encontrado\", 'pdf_pesquisavel'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df_analise_pipe.quert('nro_nota')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efetuando pesquisa no DF\n",
    "df_analise_pipe.query('pdf_pesquisavel == False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise_pipe.query('nro_nota == 13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Para Analise de PDF Pesquisavel, uei uma copia do df_analise_pipe\n",
    "subset_df_analise_pipe = df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "subset_df_analise_pipe = df_analise_pipe.query('seq < 21 & seq > 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Criando uma nova coluna no DF\n",
    "df_analise_pipe.insert(loc=13, column='model_frame', value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe.to_excel('df_root_analise.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fase = 'processamento' # 'pre-processamento'\n",
    "atividade = 'pre_process'\n",
    "status = 'root_analise'\n",
    "\n",
    "acao_sugerida = \"splitar\"\n",
    "\n",
    "\n",
    "df_root_pipe = split_documentos(df_root_pipe, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta e o principio da melhor funcao do mundo - SIM ELA E MAS PRECISA FICAR AQUI?\n",
    "def extracao_pipeline(qualquer_df, fase, atividade, status):\n",
    "    \n",
    "    linhas_analise = []\n",
    "    bloco_1_list = []\n",
    "    bloco_2_list = []\n",
    "    bloco_3_list = []\n",
    "    imagens_list = []  \n",
    "    new_data = [] \n",
    "    \n",
    "    pre_processo = ['nro_nota', 'competencia', 'dt_hr_emissao', 'codigo_verificacao', 'ajustar_nome', 'split_paginas', 'ajustar_imagem', 'buscar_nome_prefeitura', 'enviar_canceladas', 'enviar_listagens']\n",
    "    \n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    \n",
    "    fase_processo_atual = fase\n",
    "    atividade_processo_atual = atividade\n",
    "    status_documento_atual = status\n",
    "    \n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        message_erro = []\n",
    "        \n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        document_unique_id = idx\n",
    "        seq_df = row['seq']\n",
    "        batch_name = row['batch']\n",
    "        fase_processo = row['fase_processo']\n",
    "        nome_atividade = row['nome_atividade']\n",
    "        status_documento = row['status_documento']\n",
    "        original_file_name = row['original_file_name']\n",
    "        file_directory = row['directory']\n",
    "        level = row['level']\n",
    "        d_type = row['level']\n",
    "        document_type = row['document_type']\n",
    "        pdf_pesquisavel = row['pdf_pesquisavel']\n",
    "        one_page_doc = row['one_page']\n",
    "        modelo = row['modelo']\n",
    "        \n",
    "        prefeitura = row['prefeitura']\n",
    "        \n",
    "        parent_document_unique_id = row['parent_document_unique_id']\n",
    "        file_path = row['file_path']\n",
    "        file_hash = row['file_hash']\n",
    "        \n",
    "        # 2. Busca modelo\n",
    "        if (not status_documento == 'NAO_PROCESSAR') or (not document_type == 'outros') or (not one_page_doc == True):\n",
    "            \n",
    "            if atividade_processo_atual == 'extracao_prestador':\n",
    "            #print(f' 1 - seq: {seq_df} | file: {original_file_name} |status_documento: {status_documento} pdf_pesquisavel: {pdf_pesquisavel}\\n')\n",
    "                if status_documento == status_documento_atual:\n",
    "         \n",
    "                    print(f'seq_df: {seq_df} status_documento: {status_documento} | modelo: {modelo:>20} | file: {original_file_name:>40} | prefeitura: {prefeitura:>55} | {pdf_pesquisavel}\\n')\n",
    "                    \n",
    "                    result_list = []\n",
    "                    \n",
    "                    dfcnpj_prestador = {}\n",
    "                    dfincricao_prestador = {}\n",
    "                    dfdados_prestador = {}\n",
    "                        \n",
    "                    # doc2convert = original_file_name\n",
    "                    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "                    modelo = 'SAO_PEDRO_SUPERMIX'\n",
    "                    f_tipo = 'frame'\n",
    "                    \n",
    "                    dfcnpj_prestador, dfincricao_prestador, dfdados_prestador, textoextraido  = processar_dados_dados_documentos(row, original_file_name, file_path, pdf_pesquisavel, section, modelo, f_tipo)\n",
    "                    \n",
    "                    print(f'\\n {dfcnpj_prestador}\\n{dfincricao_prestador}\\n{dfdados_prestador}\\n')\n",
    "                    \n",
    "               \n",
    "      \n",
    "\n",
    "\n",
    "                i += 1\n",
    "              \n",
    "    return textoextraido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    secao = \"6. CNAE e Item da Lista de Serviços\"\n",
    "    try:\n",
    "        nd_data_CNAE = {}\n",
    "        nd_data_CNAE['secao'] = secao\n",
    "        f_frame_name = \"4_frame_cnae_itens_servico\"   \n",
    "        Texto_extraido = executa_model_frame(model, secao, f_frame_name)\n",
    "        text_splited = Texto_extraido.split('\\n')\n",
    "        # Processando CNAE\n",
    "        cnae_line = [line for line in text_splited if 'CNAE' in line][0]\n",
    "        cnae_number = int(extract_number(cnae_line))\n",
    "        cnae_value = cnae_dict.get(cnae_number, \"Valor não encontrado\")\n",
    "        if cnae_value == 'Valor não encontrado':\n",
    "            cnae_value = processa_cnae_outros(cnae_line)\n",
    "            cnae_value = cnae_value.upper()\n",
    "            nd_data_CNAE['cnae'] = cnae_value\n",
    "        else:\n",
    "            cnae_value = cnae_value.upper()\n",
    "            cnae_value = str(cnae_number) + \" - \" + cnae_value\n",
    "            nd_data_CNAE['cnae'] = cnae_value\n",
    "            nd_data_CNAE['item_lista_servicos'] = item_servico_value\n",
    "    except Exception as e:\n",
    "        print(f\"Erro busca cnae: {e}\")    \n",
    "\n",
    "    try:\n",
    "        item_servico_line = [line for line in text_splited if 'Item da Lista de Serviços' in line][0]\n",
    "        item_servico_number = float(extract_number(item_servico_line))\n",
    "        item_servico_value = item_servico_dict.get(item_servico_number, \"Valor não encontrado\")\n",
    "        item_servico_value = item_servico_value.upper()\n",
    "        item_servico_value = str(item_servico_number) + \" - \" + item_servico_value\n",
    "        nd_data_CNAE['item_lista_servicos'] = item_servico_value\n",
    "    except Exception as e:\n",
    "        print(f\"Erro busca Itens de servico: {e}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                # try:   \n",
    "                #     row_info['t_cpf_cnpj_com_mascara'] = data_tomador['cpf_cnpj_com_mascara']\n",
    "                #     row_info['t_cpf_cnpj_sem_mascara'] = data_tomador['cpf_cnpj_sem_mascara']\n",
    "                #     row_info['t_rg'] = data_tomador['rg']\n",
    "                #     row_info['t_telefone'] = data_tomador['telefone']\n",
    "                #     row_info['t_inscricao_municipal'] = data_tomador['inscricao_municipal']\n",
    "                #     row_info['t_inscricao_estadual'] =  data_tomador['inscricao_estadual']\n",
    "                #     row_info['t_razao_social'] = data_tomador['razao_social']\n",
    "                #     row_info['t_endereco'] = data_tomador['endereco']\n",
    "                #     row_info['t_email'] = data_tomador['email']\n",
    "                # except Exception as e:\n",
    "                #     msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "                #     row_info['t_cpf_cnpj_com_mascara'] = \" \"\n",
    "                #     row_info['t_cpf_cnpj_sem_mascara'] = \" \"\n",
    "                #     row_info['t_rg'] = \" \"\n",
    "                #     row_info['t_telefone'] = \" \"\n",
    "                #     row_info['t_inscricao_municipal'] = \" \"\n",
    "                #     row_info['t_inscricao_estadual'] = \" \"\n",
    "                #     row_info['t_razao_social'] = \" \"\n",
    "                #     row_info['t_endereco'] = \" \"\n",
    "                #     row_info['t_email'] = \" \"\n",
    "                #     message_erro.append(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    # try:\n",
    "                        \n",
    "                    #     regime_tributacao_map = nf_data_outras_informacoes['regime_tributacao']\n",
    "                    #     simples_nacional_map = nf_data_outras_informacoes['simples_nacional']\n",
    "                    #     issqn_retido_map = nf_data_outras_informacoes['issqn_retido']        \n",
    "                    #     local_prestacao_servico_map = nf_data_outras_informacoes['local_prestacao_servico']\n",
    "                    #     local_incidencia_map = nf_data_outras_informacoes['local_incidencia']\n",
    "                    #     #row_info['exigibilidade_iss'] = exigibilidade_iss_map\n",
    "                    # except Exception as e:\n",
    "                    #     msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "                    #     exigibilidade_iss_map = None\n",
    "                    #     regime_tributacao_map = None\n",
    "                    #     simples_nacional_map = None\n",
    "                    #     issqn_retido_map = None        \n",
    "                    #     local_prestacao_servico_map = None\n",
    "                    #     local_incidencia_map = None\n",
    "                    \n",
    "                    # row_info['exigibilidade_iss'] = exigibilidade_iss_map\n",
    "                    # row_info['regime_tributacao'] = regime_tributacao_map\n",
    "                    # row_info['simples_nacional'] = simples_nacional_map\n",
    "                    # row_info['issqn_retido'] = issqn_retido_map        \n",
    "                    # row_info['local_prestacao_servico'] = local_prestacao_servico_map\n",
    "                    # row_info['local_incidencia'] = local_incidencia_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                try:   \n",
    "                    row_info['p_cpf_cnpj_com_mascara'] = data_prestador['cpf_cnpj_com_mascara']\n",
    "                    row_info['p_cpf_cnpj_sem_mascara'] = data_prestador['cpf_cnpj_sem_mascara']\n",
    "                    row_info['p_telefone'] = data_prestador['telefone']\n",
    "                    row_info['p_inscricao_municipal'] = data_prestador['inscricao_municipal']\n",
    "                    row_info['p_inscricao_estadual'] =  data_prestador['inscricao_estadual']\n",
    "                    row_info['p_razao_social'] = data_prestador['razao_social']\n",
    "                    row_info['p_nome_fantasia']= data_prestador['nome_fantasia']\n",
    "                    row_info['p_endereco'] = data_prestador['endereco']\n",
    "                    row_info['p_email'] = data_prestador['email']\n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "                    row_info['p_cpf_cnpj_com_mascara'] = \" \"\n",
    "                    row_info['p_cpf_cnpj_sem_mascara'] = \" \"\n",
    "                    row_info['p_telefone'] = \" \"\n",
    "                    row_info['p_inscricao_municipal'] = \" \"\n",
    "                    row_info['p_inscricao_estadual'] = \" \"\n",
    "                    row_info['p_razao_social'] = \" \"\n",
    "                    row_info['p_nome_fantasia'] = \" \"\n",
    "                    row_info['p_endereco'] = \" \"\n",
    "                    row_info['p_email'] = \" \"\n",
    "                    message_erro.append(msg)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. VALOR TOTAL\n",
    "def processar_valor_total_PDF_P(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    nf_data_valor_total = {}\n",
    "    \n",
    "    process = ['4_frame_valor_total']\n",
    "    \n",
    "    section = \"5. VALOR TOTAL\"\n",
    "    pdf_pesquisavel_map = True\n",
    "        \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "        \n",
    "    tipo= \"frame\"\n",
    "    message_erro = []\n",
    "    nf_dados_prestador = {}\n",
    "    # model = row['model']\n",
    "    for father in process:\n",
    "        label = father\n",
    "        if label == \"4_frame_valor_total\": \n",
    "            print(model_map)\n",
    "            coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            print(f'model: {model_map} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6}\\n {text}')\n",
    "            valor_total_match = re.search(r'R\\$ ([\\d,.]+)', text)\n",
    "            if valor_total_match:\n",
    "                valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "                valor_total_float = float(valor_total_sem_formatacao)\n",
    "                #nf_data_valor_total['valor_total_nota'] =\n",
    "                \n",
    "    pdf_document.close()            \n",
    "                \n",
    "    return valor_total_float  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   nf_data_CNAE = {}\n",
    "    nf_data_CNAE['Secao'] = \"6. CNAE e Item da Lista de Serviços\"\n",
    "    \n",
    "    process = [\"cnae\", 'item_lista_servicos']\n",
    "    \n",
    "    section = \"6. CNAE e Item da Lista de Serviços\"\n",
    "    pdf_pesquisavel_map = True\n",
    "        \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "        \n",
    "    tipo = \"sframe_field\"\n",
    "    message_erro = []\n",
    "    nf_dados_prestador = {}\n",
    "    # model = row['model']\n",
    "    for father in process:\n",
    "        label = father\n",
    "        if label == \"cnae\": \n",
    "            tipo = \"sframe_field\"\n",
    "            print(model_map)\n",
    "            coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            print(f'model: {model_map} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6}\\n {text}')\n",
    "        \n",
    "        elif label == \"item_lista_servicos\":\n",
    "            tipo = \"sframe_field\"\n",
    "            print(model_map)\n",
    "            coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            print(f'model: {model_map} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6}\\n {text}')\n",
    "                \n",
    "\n",
    "           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tables-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
