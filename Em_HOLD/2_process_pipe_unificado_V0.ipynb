{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <mark>Processa pipeline models</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "\n",
    "import platform\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "\n",
    "\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import re\n",
    "import cv2\n",
    "import fitz  # Módulo PyMuPDF\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "import PyPDF2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import locale\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import time, copy\n",
    "\n",
    "\n",
    "from outlook_msg import Message\n",
    "import extract_msg\n",
    "import zipfile\n",
    "from pyunpack import Archive\n",
    "import py7zr\n",
    "\n",
    "\n",
    "from pytz import timezone\n",
    "from urllib import response\n",
    "\n",
    "\n",
    "\n",
    "import modules.extrai_pdf_pesquisavel as Extc\n",
    "import modules.cronometro as cron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_test = 16\n",
    "\n",
    "### PRESTAR ATENCAO\n",
    "#model = 'mage'\n",
    "model = 'mesquita'\n",
    "#model = 'pedro_aldeia'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Nome Batch\n",
    "batch_name = \"Batch_\" + str(i_test)\n",
    "\n",
    "root_doc_analise = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "image_resized_path = \"pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas\"\n",
    "\n",
    "######### PATHS\n",
    "#1. path formado para busca de pdfs recursiva\n",
    "root_doc_analise = os.path.join(root_doc_analise, batch_name)\n",
    "\n",
    "# 7. path para arquivos json\n",
    "json_path = \"pipeline_extracao_documentos/5_documentos_processados/jsons\"\n",
    "\n",
    "# Nome do arquivo json\n",
    "nome_formado_json = batch_name +\".json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. path para documentos PDF (omelhor se estiverem dentro de um unico diretorio)\n",
    "root_pdf_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "# 2. path para documentos PDF que podem estar aguardando para serem processados\n",
    "root_pdf_aguardando_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "# 3. path para documentos PDF externos para serem processados\n",
    "root_external_pdf_path = \"content_from_pdftool/data/data_pdf/NF_para_processamento/NFRJ_PDF_para _ocr\"\n",
    "# 4. path para documentos PDF PESQUISAVEIS externos para serem processados\n",
    "root_external_pdf_pesquisavel_path = \"content_from_pdftool/data/data_pdf/NF_processadas/NFRJ/fwdnotasfiscaisemitidaslmpadalegal\"\n",
    "\n",
    "# 5. path para imagem padrao\n",
    "image_resized_path = 'pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas'\n",
    "\n",
    "# 6. path para log\n",
    "log_path = 'pipeline_extracao_documentos/6_geral_administacao/logs'\n",
    "\n",
    "# 8. path para NFs processadas\n",
    "nf_processada_path = \"pipeline_extracao_documentos/5_documentos_processados\"\n",
    "\n",
    "#### paths de objetos para criacao/gestao (dicionarios/datasets)\n",
    "cnae_dict_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets/MAGE_CNAE_X_ITEM_SERVICO_V1.xlsx\"\n",
    "\n",
    "# VERIFICAR\n",
    "tgt_imagens = \"pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas\"\n",
    "\n",
    "# 12. poppler path\n",
    "poppler_path = \"/home/dani-boy/miniconda3/envs/tables-detr/bin\"\n",
    "\n",
    "# 13. path para config Tesseract\n",
    "tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt_BR.utf8'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao\"\n",
    "\n",
    "# 1. Caminho do arquivo uma mensagem especifica\n",
    "msg_dir_path = 'pipeline_extracao_documentos/1_emails'\n",
    "\n",
    "# 2. Path para arquivos atachados compactados\n",
    "msg_attachment_zip = 'pipeline_extracao_documentos/1_emails/attachments'\n",
    "\n",
    "# 3. Path para documentos atachados:\n",
    "documentos_extracao_path = 'pipeline_extracao_documentos/2_documentos_para_extracao'\n",
    "\n",
    "\n",
    "# definindo localizadcao para pt_BR\n",
    "locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")\n",
    "\n",
    "# Nosso timezone\n",
    "#local_tz = pytz.timezone('America/Sao_Paulo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "i_test: 16 | model: mesquita | batch_name: Batch_16 | root_doc_analise: pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'\\ni_test: {i_test} | model: {model} | batch_name: {batch_name} | root_doc_analise: {root_doc_analise}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tratando nome de carga do df_processamento\n",
    "dataset_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets\"\n",
    "\n",
    "df_processamento_file = \"df_processamento_\"\n",
    "\n",
    "df_processamento_file_read = df_processamento_file + str(i_test - 1) + \".xlsx\"\n",
    "\n",
    "# 2. Tratando nome de carga do df_extracao_files_Batch\n",
    "df_extracao_files_Batch_file = \"df_extracao_files_Batch_\"\n",
    "\n",
    "df_extracao_files_Batch_file_read = df_extracao_files_Batch_file + str(i_test - 1) + \".xlsx\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pipeline_extracao_documentos/6_geral_administacao/datasets/df_processamento_15.xlsx'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processamento_file_read_path = os.path.join(dataset_path, df_processamento_file_read)\n",
    "\n",
    "df_processamento_file_read_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pipeline_extracao_documentos/6_geral_administacao/datasets/df_extracao_files_Batch_15.xlsx'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extracao_files_Batch_file_read_path = os.path.join(dataset_path, df_extracao_files_Batch_file_read)\n",
    "\n",
    "df_extracao_files_Batch_file_read_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processamento = pd.read_excel(df_processamento_file_read_path) \n",
    "\n",
    "df_processamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extracao_files = pd.read_excel(df_extracao_files_Batch_file_read_path) \n",
    "\n",
    "df_extracao_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trata dicionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. path para models\n",
    "nf_model_path = \"pipeline_extracao_documentos/6_geral_administacao/modelos/frames_nf_v8.xlsx\"\n",
    "\n",
    "# 11. path para datasets CNAE e Itens de Serviço\n",
    "nf_datasets_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria do DF\n",
    "frames_nf_v4_df = pd.read_excel(nf_model_path)\n",
    "\n",
    "# Cria dicionários para armazenar diferentes tipos de elementos do modelo\n",
    "document_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'document'].iloc[0]\n",
    "boundaries_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'boundaries']\n",
    "sections_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'section']\n",
    "frames_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'frame']\n",
    "sframe_fields_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'sframe_field']\n",
    "field_boxes_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'field_box']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mage_cnae_x_item_servico_df = pd.read_excel(cnae_dict_path)\n",
    "\n",
    "# Creating a dictionary for CNAE codes and descriptions\n",
    "cnae_dict = dict(zip(mage_cnae_x_item_servico_df['cnae'], mage_cnae_x_item_servico_df['descricao_cnae']))\n",
    "item_servico_dict = dict(zip(mage_cnae_x_item_servico_df['item_servico'], mage_cnae_x_item_servico_df['descricao_item_servico']))\n",
    "\n",
    "\n",
    "    \n",
    "# 2. Leitura do arquivo CSV e criação do dicionário modelos\n",
    "def create_model_dictionary(model_dict_path):\n",
    "    model_dictionary = {}\n",
    "    with open(model_dict_path, 'r') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            prefeitura_name = row['prefeitura']\n",
    "            model_name = row['model']\n",
    "\n",
    "            if prefeitura_name not in model_dictionary:\n",
    "                model_dictionary[prefeitura_name] = model_name\n",
    "            \n",
    "            #model_dictionary[prefeitura_name].append(model_name)\n",
    "    \n",
    "    return model_dictionary    \n",
    "    \n",
    "    frames_nf_v4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As informações foram salvas em pipeline_extracao_documentos/5_documentos_processados/jsons/Batch_20_A.json\n"
     ]
    }
   ],
   "source": [
    "nome_arquivo_json = batch_name\n",
    "rows_list = []\n",
    "dict_df = {}\n",
    "nf_data_servico = {}\n",
    "pdf_info = {}  # Dicionário para armazenar informações sobre PDFs\n",
    "\n",
    "i = 1\n",
    "for root, dirs, files in os.walk(root_directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        \n",
    "        if file.lower().endswith('.pdf'):\n",
    "            if is_pdf_searchable(file_path):\n",
    "                \n",
    "                status = \"O PDF é pesquisável\"\n",
    "                nro_nota = 0\n",
    "                nr_nro_nf = 0\n",
    "                \n",
    "                pdf_document = fitz.open(file_path)\n",
    "                # Página do PDF\n",
    "                page_number = 0  # Defina o número da página que deseja analisar\n",
    "                page = pdf_document[page_number]\n",
    "\n",
    "                x0, y0, x1, y1 = (0, 4, 600, 200)\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "                if text:\n",
    "                    page_number = 0\n",
    "                else:\n",
    "                    page_number = 1\n",
    "                # 1 - cabecalho\n",
    "                page = pdf_document[page_number]\n",
    "                x0, y0, x1, y1 = (0, 0, 600, 110)\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "                nf_data_cabecalho = extract_fields_cabecalho(text)\n",
    "                #dict_df.update(nf_data_cabecalho)\n",
    "                nro_nota = nf_data_cabecalho['numero_nota_fiscal']\n",
    "                prefeitura = nf_data_cabecalho['nome_prefeitura']\n",
    "                \n",
    "                dict_df['numero_nota_fiscal'] = nf_data_cabecalho['numero_nota_fiscal']\n",
    "                dict_df['nome_prefeitura'] = nf_data_cabecalho['nome_prefeitura']\n",
    "                dict_df['tipo_nota_fiscal'] = nf_data_cabecalho['tipo_nota_fiscal']\n",
    "                dict_df['competencia'] = nf_data_cabecalho['competencia'] \n",
    "                dict_df['dt_hr_emissao'] = nf_data_cabecalho['dt_hr_emissao']\n",
    "                dict_df['codigo_verificacao'] = nf_data_cabecalho['codigo_verificacao']\n",
    "\n",
    "                try:\n",
    "                    nr_nro_nf = nf_data_cabecalho['numero_nota_fiscal']\n",
    "                except Exception as e:\n",
    "                     print(f\"Erro ao verificar o PDF: {e}\")\n",
    "                     \n",
    "                # 2. PRESTADOR DE SERVIÇO\n",
    "    \n",
    "                x0, y0, x1, y1 = (0, 100, 600, 236)\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "                \n",
    "                nf_data_prestador = extract_fields_prestador(text)\n",
    "                \n",
    "                dict_df['cnpj_mascara_prestador'] = nf_data_prestador['cpf_cnpj_com_mascara']\n",
    "                dict_df['inscr_munic_prestador'] = nf_data_prestador['inscricao_municipal']\n",
    "                dict_df['inscr_estadual_prestador'] = nf_data_prestador['inscricao_estadual']\n",
    "                dict_df['telefone_prestador'] = nf_data_prestador['telefone']\n",
    "                dict_df['razao_social_prestador'] = nf_data_prestador['razao_social']\n",
    "                dict_df['nome_fantasia_prestador'] = nf_data_prestador['nome_fantasia']\n",
    "                dict_df['endereco_prestador'] = nf_data_prestador['endereco']\n",
    "                # dict_df.update(nf_data_prestador)\n",
    "                # 3. TOMADOR DE SERVIÇO\n",
    "                \n",
    "                x0, y0, x1, y1 = (0, 210, 600, 340)\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "                nf_data_tomador = extract_fields_tomador(text)\n",
    "                dict_df.update(nf_data_tomador)\n",
    "                \n",
    "                # 4. DESCRIMINACAO DOS SERVIÇOS\n",
    "                nf_data_servico = {}\n",
    "                nf_data_servico['secao'] = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "\n",
    "                x0, y0, x1, y1 = (0, 340, 600, 500)\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "                # Remover quebras de linha e rótulo\n",
    "                text = text.replace('\\n', ' ')\n",
    "                label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "                if text.startswith(label):\n",
    "                    text = text[len(label):].strip()\n",
    "\n",
    "                # Atribuir texto ao dicionário\n",
    "                nf_data_servico['discriminacao_servicos'] = text\n",
    "                dict_df.update(nf_data_servico)\n",
    "         \n",
    "                # 5. VALOR TOTAL\n",
    "                nf_data_valor_total = {}\n",
    "                nf_data_valor_total['secao'] = \"5. VALOR TOTAL\"\n",
    "\n",
    "                x0, y0, x1, y1 = (0, 520, 600, 550)\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "                # Usar expressão regular para extrair apenas os caracteres numéricos e pontos decimais\n",
    "                valor_total_match = re.search(r'R\\$ ([\\d,.]+)', text)\n",
    "                if valor_total_match:\n",
    "                    valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "                    nf_data_valor_total['valor_total_nota'] = float(valor_total_sem_formatacao)\n",
    "                else:\n",
    "                    nf_data_valor_total['valor_total_nota'] = text    \n",
    "                \n",
    "                dict_df['valor_total_nota'] = nf_data_valor_total['valor_total_nota']\n",
    "                \n",
    "                # 6. CNAE e Item da Lista de Serviços\n",
    "                nf_data_CNAE = {}\n",
    "                nf_data_CNAE['Secao'] = \"6. CNAE e Item da Lista de Serviços\"\n",
    "\n",
    "                x0, y0, x1, y1 = (0, 540, 600, 570)\n",
    "                # Extrair texto dentro do retângulo\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "                # Extrair CNAE\n",
    "                nf_data_CNAE_match = re.search(r'CNAE\\s+(.+)', text)\n",
    "                if nf_data_CNAE_match:\n",
    "                    # Remove a primeira ocorrência de \"CNAE:\"\n",
    "                    nf_data_CNAE_str = re.sub(r'^CNAE - ', '', text, count=1)\n",
    "                    # Remover quebras de linha\n",
    "                    nf_data_CNAE_str = nf_data_CNAE_str.replace('\\n', ' ')\n",
    "                    nf_data_CNAE['cnae'] = nf_data_CNAE_str\n",
    "                else:\n",
    "                    nf_data_CNAE['cnae'] = ' '     \n",
    "                \n",
    "                dict_df['cnae'] = nf_data_CNAE['cnae']\n",
    "                \n",
    "                # Extrair Item da Lista de Serviços\n",
    "                x0, y0, x1, y1 = (0, 550, 600, 580)\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))     \n",
    "                nf_item_lista_servicos_match = re.search(r'Item da Lista de Serviços\\s+(.+)', text)\n",
    "                if nf_item_lista_servicos_match:\n",
    "                    nf_item_lista_servicos_str = re.sub(r'^Item da Lista de Serviços - ', '', text, count=1) \n",
    "                    # Remover quebras de linha\n",
    "                    #nf_item_lista_servicos_str = nf_item_lista_servicos_str.replace('\\n \\n', '')\n",
    "                    nf_item_lista_servicos_str = nf_item_lista_servicos_str.replace('\\n', ' ')\n",
    "                    nf_data_CNAE['item_lista_servicos'] = nf_item_lista_servicos_str\n",
    "                else:\n",
    "                    nf_data_CNAE['item_lista_servicos'] = ' '\n",
    "                dict_df['item_lista_servicos'] = nf_data_CNAE['item_lista_servicos']\n",
    "                        \n",
    "                # 7. VALORES E IMPOSTOS\n",
    "                x0, y0, x1, y1 = (0, 550, 600, 650)\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "                \n",
    "\n",
    "                # Extrair campos e valores\n",
    "                nf_data_valores = extract_fields_impostos(text)\n",
    "                dict_df.update(nf_data_valores)\n",
    "                \n",
    "                # 8. DADOS COMPLEMENTARES\n",
    "                nf_data_dados_complementares = {}\n",
    "                nf_data_dados_complementares['secao'] = \"8. DADOS COMPLEMENTARES\"\n",
    "\n",
    "                x0, y0, x1, y1 = (0, 650, 600, 680)\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^DADOS COMPLEMENTARES', '', text, count=1)\n",
    "                if text == \" \":\n",
    "                    text = \"NONE\"\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "                else:    \n",
    "                    # Extrair texto dentro do retângulo\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "                \n",
    "                dict_df.update(nf_data_dados_complementares)\n",
    "\n",
    "                # 9. OUTRAS INFORMAÇOES / CRITICAS  \n",
    "                x0, y0, x1, y1 = (0, 680, 600, 725)\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "                # Extrair campos e valores\n",
    "                nf_data_outras_informacoes = extract_fields_outras_info(text)\n",
    "                \n",
    "                dict_df.update(nf_data_outras_informacoes)\n",
    "                \n",
    "                # 10. OBSERVACOES\n",
    "                nf_data_observacao = {}\n",
    "                nf_data_observacao['secao'] = \"10. OBSERVACOES\"\n",
    "\n",
    "                x0, y0, x1, y1 = (0, 725, 600, 760)\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^Observação:', '', text, count=1)\n",
    "                text = text.replace('\\n', ' ')\n",
    "                nf_data_observacao['observacao'] = text.strip()\n",
    "                \n",
    "                nr_nro_nf = nro_nota\n",
    "                nome_arquivo = file\n",
    "                nome_formado_json = nome_arquivo_json + \".json\" \n",
    "                arquivo_zip = \"MAGE_PDF_31282023_2254\"\n",
    "                titulo = (f'Processamento {batch_name} - {prefeitura} - arq:{arquivo_zip}')\n",
    "                \n",
    "                pdf_info[\"title\"] = titulo\n",
    "                pdf_info[nr_nro_nf] = {\n",
    "                    \"dados_NF_PDF\": {\n",
    "                        \"data_cabecalho\": nf_data_cabecalho,\n",
    "                        \"data_prestador\": nf_data_prestador,\n",
    "                        \"data_tomador\": nf_data_tomador,\n",
    "                        \"data_servico\": nf_data_servico,\n",
    "                        \"data_valor_total\": nf_data_valor_total,\n",
    "                        \"data_CNAE\": nf_data_CNAE,\n",
    "                        \"data_valores\": nf_data_valores,\n",
    "                        \"data_dados_complementares\": nf_data_dados_complementares,\n",
    "                        \"data_outras_informacoes\": nf_data_outras_informacoes,\n",
    "                        \"data_observacao\": nf_data_observacao,\n",
    "                    },\n",
    "                    \"batch_name\": batch_name,\n",
    "                    \"diretorio\": os.path.basename(root),\n",
    "                    \"nome_arquivo\": nome_arquivo,\n",
    "                }\n",
    "                dict_df['batch_name'] = batch_name\n",
    "                dict_df['data_processamento'] = cron.timenow_pt_BR()\n",
    "                dict_df['nome_json'] = nome_formado_json\n",
    "                dict_df['nome_arquivo'] = file\n",
    "                dict_df['file_path'] = file_path\n",
    "                dict_df['diretorio'] = os.path.basename(root)\n",
    "                dict_df['document_unique_id'] = generate_unique_id()\n",
    "                dict_df['file_hash'] = generate_file_hash(file_path)\n",
    "\n",
    "                rows_list.append(dict_df.copy())\n",
    "                pdf_document.close()\n",
    "                \n",
    "df_data = pd.DataFrame(rows_list)                \n",
    "                \n",
    "\n",
    "# Salvando as informações em um arquivo JSON\n",
    "json_file_path = os.path.join(json_path, nome_formado_json)\n",
    "with open(json_file_path, \"w\", encoding='utf-8') as json_file:\n",
    "    json.dump(pdf_info, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"As informações foram salvas em {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trata e-mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt_BR.utf8'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao\"\n",
    "\n",
    "# 1. Caminho do arquivo uma mensagem especifica\n",
    "msg_dir_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails'\n",
    "\n",
    "# 2. Path para arquivos atachados compactados\n",
    "msg_attachment_zip = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/13_attachments'\n",
    "\n",
    "# 3. Path para documentos atachados:\n",
    "documentos_extracao_path = 'pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento'\n",
    "\n",
    "\n",
    "# definindo localizadcao para pt_BR\n",
    "locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")\n",
    "\n",
    "# Nosso timezone\n",
    "#local_tz = pytz.timezone('America/Sao_Paulo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para armazenar as linhas\n",
    "rows_list = []\n",
    "\n",
    "# Função recursiva para adicionar linha\n",
    "def add_row_recursively(rows_list, \n",
    "                        index, \n",
    "                        Batch, \n",
    "                        diretorio_ori, \n",
    "                        arquivo_origem, \n",
    "                        arquivo_destino, \n",
    "                        data_hora, \n",
    "                        tipo_doc_pdf, \n",
    "                        qtd_paginas\n",
    "                        ):\n",
    "    if index == 0:\n",
    "        return rows_list\n",
    "    else:\n",
    "        new_row = {\n",
    "                    'index': index,\n",
    "                    'Batch': Batch,\n",
    "                    'diretorio_origem': diretorio_ori,\n",
    "                    'nome_arquivo_origem': arquivo_origem,\n",
    "                    'nome_arquivo_destino': arquivo_destino,\n",
    "                    'data_processamento': data_hora,\n",
    "                    'tipo_pdf': tipo_doc_pdf,\n",
    "                    'qut_paginas': qtd_paginas\n",
    "                    }\n",
    "        rows_list.append(new_row)\n",
    "        \n",
    "        return add_row_recursively(rows_list, index-1, Batch, diretorio_ori, arquivo_origem, arquivo_destino, data_hora, tipo_doc_pdf, qtd_paginas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Funcoes necessarias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. converte nome do arquivo\n",
    "def conv_filename(title):\n",
    "    \n",
    "    # Divide o título em nome e extensão\n",
    "    name, extension = title.rsplit('.', 1) if '.' in title else (title, \"\")\n",
    "\n",
    "    # Remove acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substiti espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remove quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converte para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    # Adiciona a extensão de volta, se houver\n",
    "    if extension:\n",
    "        filename += '.' + extension.lower()\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "\n",
    "def conv_filename_no_ext(title):\n",
    "    \n",
    "    # Divida o título em nome e extensão (mas ignore a extensão)\n",
    "    name = title.rsplit('.', 1)[0] if '.' in title else title\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    return filename \n",
    "\n",
    "\n",
    "# 3. move NF processadas ok\n",
    "def move_raster_pdf(document_path, raster_pdf_path, batch_name, doc2convert):\n",
    "    # Determine the destination directory\n",
    "    destination_dir = os.path.join(raster_pdf_path, batch_name)\n",
    "\n",
    "    # Check if the destination directory exists; if not, create it\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "\n",
    "    # Determine the destination path including the filename\n",
    "    destination_path = os.path.join(destination_dir, os.path.basename(document_path))\n",
    "\n",
    "    # Move the file from the source path to the destination path\n",
    "    try:\n",
    "        shutil.move(document_path, destination_path)\n",
    "        print(f\"Sucesso ao mover: {document_path} para: {destination_path}\")\n",
    "        return True, destination_path, None  # Success, destination path, no error\n",
    "    except Exception as e:\n",
    "        error_message = f\"Erro ao mover: {document_path} para: {destination_path}: {str(e)}\"\n",
    "        print(error_message)\n",
    "        return False, None, error_message  # Failure, no destination path, error message\n",
    "\n",
    "# 5. Verifica se PDF e pesquisavel ou nao e grava metadados dele\n",
    "def is_pdf_searchable_analise(pdf_path):\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        pages = pdf_document.page_count\n",
    "        is_searchable = all(page.get_text(\"text\") != \"\" for page in pdf_document)\n",
    "        dados_pdf = pdf_document.metadata\n",
    "        pdf_document.close()\n",
    "        return is_searchable, dados_pdf, pages\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao verificar o PDF: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Lendo o email e anexos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email: Nota mesquita.msg, email_path: pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails/Nota mesquita.msg\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(msg_dir_path):\n",
    "    #print(f'{root}  | {dirs} | {document} | {files}\\n')\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        if file.lower().endswith('.msg'):\n",
    "            email_message = file\n",
    "            email_path = os.path.join(root, file)\n",
    "            print(f'email: {email_message}, email_path: {email_path}')\n",
    "            \n",
    "            \n",
    "msg = extract_msg.Message(email_path)\n",
    "\n",
    "msg_raw_sender = msg.sender\n",
    "\n",
    "parts = msg_raw_sender.rsplit('<', 1)\n",
    "\n",
    "msg_email_address = parts[1].strip('<>')\n",
    "\n",
    "msg_sender = parts[0].strip(' ')\n",
    "\n",
    "msg_subject = msg.subject\n",
    "\n",
    "\n",
    "msg_body = msg.body\n",
    "\n",
    "# Defina a localização para pt_BR\n",
    "locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")\n",
    "\n",
    "# String original\n",
    "original_date_str = msg.date\n",
    "\n",
    "date_email = cron.convert_email_date(original_date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_email = cron.convert_email_date(original_date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'31/08/2023 23:01:14'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo anexos e gravando em attachments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "pipeline_extracao_documentos/1_emails_documentos_recebidos/13_attachments/MESQUITA_PDF_31282023_2258.zip\n"
     ]
    }
   ],
   "source": [
    "# Caminho da pasta onde você quer salvar os anexos\n",
    "pasta_destino = msg_attachment_zip\n",
    "\n",
    "# Verifica se a pasta existe; se não, cria ela\n",
    "if not os.path.exists(pasta_destino):\n",
    "    os.makedirs(pasta_destino)\n",
    "\n",
    "with open(email_path) as msg_file:\n",
    "    msg = Message(msg_file)\n",
    "\n",
    "# Contents are the plaintext body of the email\n",
    "#contents = msg.body\n",
    "\n",
    "\n",
    "total_attch = len(msg.attachments)\n",
    "\n",
    "print(total_attch)\n",
    "\n",
    "arquivos_zip = []\n",
    "arquivos = []\n",
    "i = 0\n",
    "# Loop para salvar cada anexo\n",
    "for i in range(total_attch):\n",
    "    attachment = msg.attachments[i]\n",
    "    caminho_completo_anexo = os.path.join(pasta_destino, attachment.filename)\n",
    "    if file.lower().endswith('.zip') or file.lower().endswith('.rar') or file.lower().endswith('.7z'):\n",
    "        arquivos_zip.append(attachment.filename)\n",
    "    else:\n",
    "        arquivos.append(attachment.filename)\n",
    "      \n",
    "    print(caminho_completo_anexo)\n",
    "    with attachment.open() as attachment_fp, open(caminho_completo_anexo, 'wb') as output_fp:\n",
    "        output_fp.write(attachment_fp.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 Salvando os attachments do e-mail </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> Extraindo documentos do ZIP </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório onde você quer salvar os arquivos extraídos\n",
    "output_dir = os.path.join(documentos_extracao_path, batch_name)\n",
    "\n",
    "folder_file_dict = {}\n",
    "\n",
    "for root, dirs, files in os.walk(msg_attachment_zip):\n",
    "    #print(f'{root}  | {dirs} | {document} | {files}\\n')\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        if file.lower().endswith('.zip'):\n",
    "            zip_file = file\n",
    "            zip_file_path = os.path.join(root, file)\n",
    "            # Obtém o nome base do arquivo ZIP para usar como subdiretório\n",
    "            zip_base_name = os.path.splitext(os.path.basename(zip_file_path))[0]\n",
    "            zip_basename = conv_filename(zip_base_name)\n",
    "            \n",
    "            # Cria o subdiretório com base no nome do arquivo ZIP\n",
    "            root_output_dir = os.path.join(output_dir, zip_basename)\n",
    "            \n",
    "            if not os.path.exists(root_output_dir):\n",
    "                os.makedirs(root_output_dir) # estou criando o diretorio caso nao exista\n",
    "\n",
    "            # Abre o arquivo ZIP\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                for member in zip_ref.namelist():\n",
    "                    # Separa o nome da pasta e o nome do arquivo usando barra invertida como delimitador\n",
    "                    parts = member.rsplit('\\\\', 1)\n",
    "                    folder_name = parts[0] if len(parts) > 1 else ''\n",
    "                    #folder_name = conv_filename(folder_temp)\n",
    "                    filename = parts[-1]\n",
    "                    if filename:  # ignora diretórios\n",
    "                        # Adiciona ao dicionário\n",
    "                        #filename = conv_filename(filename)\n",
    "                        # Cria um subdiretório se ele não existir\n",
    "                        sub_dir = os.path.join(root_output_dir, folder_name)\n",
    "                        if not os.path.exists(sub_dir):\n",
    "                            os.makedirs(sub_dir)\n",
    "\n",
    "                        # Salva o arquivo no subdiretório especificado\n",
    "                        source = zip_ref.open(member)\n",
    "                        target_path = os.path.join(sub_dir, filename)\n",
    "                        \n",
    "                        with open(target_path, \"wb\") as target:\n",
    "                            target.write(source.read())\n",
    "                            dir_path = os.path.dirname(filename)\n",
    "                            \n",
    "        elif file.lower().endswith('.rar'):\n",
    "            rar_file = file\n",
    "            rar_file = conv_filename_no_ext(rar_file)\n",
    "            rar_file_path = os.path.join(root, file) \n",
    "            root_output_dir = os.path.join(output_dir, rar_file)\n",
    "            if not os.path.exists(root_output_dir):\n",
    "                os.makedirs(root_output_dir)\n",
    "            Archive(rar_file_path).extractall(root_output_dir)  \n",
    "            \n",
    "        elif file.lower().endswith('.7z'):\n",
    "            sevenz_file = file\n",
    "            sevenz_file = conv_filename_no_ext(sevenz_file)\n",
    "            sevenz_file_path = os.path.join(root, file)\n",
    "            root_output_dir = os.path.join(output_dir, sevenz_file)                      \n",
    "                            \n",
    "            with py7zr.SevenZipFile(sevenz_file_path, mode='r') as z:\n",
    "                z.extractall(root_output_dir)\n",
    "                \n",
    "        elif file.lower().endswith('.pdf'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            new_path_name = os.path.join(output_dir, file)\n",
    "            if not os.path.exists(output_dir):\n",
    "                            os.makedirs(output_dir)\n",
    "            shutil.move(file_path, new_path_name)\n",
    "            \n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ajustando o filename e criando o dicionario\n",
    "folder_file_dict = {}\n",
    "rows_list = []\n",
    "output_dir = os.path.join(documentos_extracao_path, batch_name)\n",
    "i = 0\n",
    "for root, dirs, files in os.walk(\"pipeline_extracao_documentos/2_documentos_para_extracao\"):\n",
    "    folder_name = os.path.basename(root)\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        pesquisavel, metadados, paginas = is_pdf_searchable_analise(file_path)\n",
    "\n",
    "        new_name = conv_filename(file)\n",
    "        new_path_name = os.path.join(root, new_name)\n",
    "        #print(f'\\nfile: {file} | new_name: {new_name} ')\n",
    "        shutil.move(file_path, new_path_name)\n",
    "        folder_file_dict.setdefault(folder_name, []).append(new_name)\n",
    "        time_now = cron.timenow_pt_BR()\n",
    "        new_row = {\n",
    "                    'index': i,\n",
    "                    'Batch': batch_name,\n",
    "                    'diretorio_origem': folder_name,\n",
    "                    'nome_arquivo_origem': file,\n",
    "                    'nome_arquivo_destino': new_name,\n",
    "                    'data_processamento': time_now,\n",
    "                    'tipo_pdf': pesquisavel,\n",
    "                    'qut_paginas': paginas\n",
    "                    }\n",
    "        rows_list.append(new_row)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_extracao_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/dani-boy/extractNF/2_process_pipe_unificado_V0.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/dani-boy/extractNF/2_process_pipe_unificado_V0.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Anexando o novo DataFrame ao original\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/dani-boy/extractNF/2_process_pipe_unificado_V0.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m df_extracao_files \u001b[39m=\u001b[39m df_extracao_files\u001b[39m.\u001b[39mappend(df_files, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_extracao_files' is not defined"
     ]
    }
   ],
   "source": [
    "# Anexando o novo DataFrame ao original\n",
    "df_extracao_files = df_extracao_files.append(df_files, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_extracao_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/dani-boy/extractNF/2_process_pipe_unificado_V0.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/dani-boy/extractNF/2_process_pipe_unificado_V0.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df_extracao_files\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_extracao_files' is not defined"
     ]
    }
   ],
   "source": [
    "df_extracao_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fwdnotasfaltantesnosistemadeemissoim20734_106187': ['doria_marinho_0301_ultrascan.pdf',\n",
       "  'doria_marinho_0300_vanisa.pdf',\n",
       "  'doria_marinho_0295_carlos_leandro.pdf',\n",
       "  'doria_marinho_0299_luciana.pdf',\n",
       "  'doria_marinho_0296_vanisa_cancelada.pdf',\n",
       "  'doria_marinho_0297_raquel.pdf',\n",
       "  'doria_marinho_0298_marcelo.pdf'],\n",
       " '115964': ['livro_de_registro_do_issqn.pdf'],\n",
       " '159871': ['2023__5.pdf',\n",
       "  '2023__7.pdf',\n",
       "  '2023__4.pdf',\n",
       "  '2023__6.pdf',\n",
       "  '2023__3.pdf',\n",
       "  '2023__8.pdf'],\n",
       " '160014': ['31_07.pdf',\n",
       "  'acfrogblgyewspqaweud3qjkpdqn5kp2dfiynq7d6wjcrymgxkby0xaq7m2xyrrh8asjkxsfk1z9f4bsqat1di5gppkc3ahrhnhavaawbjuamkpiluuxpydd2ovrxzk.pdf'],\n",
       " '126623': ['41c46d8f_73ab_4906_a4c6_c7dc92c05828.pdf'],\n",
       " '138565': ['b4066c58_f309_42e4_a992_55eb8961211e.pdf']}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_file_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dicionarios e Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho de folder_names: 6\n",
      "Tamanho de file_counts: 6\n",
      "Tamanho de file_names: 6\n"
     ]
    }
   ],
   "source": [
    "folder_names = []\n",
    "file_counts = []\n",
    "file_names = []\n",
    "\n",
    "# Iterar sobre o dicionário para coletar informações\n",
    "for folder, files in folder_file_dict.items():\n",
    "    folder_names.append(folder)\n",
    "    file_counts.append(len(files))\n",
    "    file_names.append(files)\n",
    "    \n",
    "    \n",
    "# Suponha que folder_file_dict é algo como {'pasta1': 'arquivo1.pdf', 'pasta2': 'arquivo2.pdf'}\n",
    "folder_names = list(folder_file_dict.keys())\n",
    "file_names = list(folder_file_dict.values())    \n",
    "\n",
    "\n",
    "print(\"Tamanho de folder_names:\", len(folder_names))\n",
    "print(\"Tamanho de file_counts:\", len(file_counts))\n",
    "print(\"Tamanho de file_names:\", len(file_names))\n",
    "# ... e assim por diante para as demais variáveis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nome dos arquivos para salvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tratando nome de carga do df_processamento\n",
    "dataset_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets\"\n",
    "\n",
    "df_processamento_file = \"df_processamento_\"\n",
    "\n",
    "df_processamento_file_write = df_processamento_file + str(i_test) + \".xlsx\"\n",
    "\n",
    "# 2. Tratando nome de carga do df_extracao_files_Batch\n",
    "df_extracao_files_Batch_file = \"df_extracao_files_Batch_\"\n",
    "\n",
    "df_extracao_files_Batch_file_write = df_extracao_files_Batch_file + str(i_test) + \".xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 1, 6, 2, 1, 1]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {} \n",
    "# Criar o DataFrame do batch\n",
    "df_batch = pd.DataFrame({\n",
    "    \"Dt_hora\": [date_email],\n",
    "    \"Assunto\": [msg_subject],\n",
    "    \"Arquivos_zip\": [arquivos_zip],\n",
    "    \"Quantidade de Documentos\": [file_counts],\n",
    "    \"De\": [msg_sender],\n",
    "    \"batch\": [batch_name],\n",
    "    \"email\": [msg_email_address],\n",
    "\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32751/1808731608.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_processamento = df_processamento.append(df_batch, ignore_index=True)\n"
     ]
    },
    {
     "ename": "InvalidIndexError",
     "evalue": "Reindexing only valid with uniquely valued Index objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/dani-boy/extractNF/2_process_pipe_unificado_V0.ipynb Cell 42\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/dani-boy/extractNF/2_process_pipe_unificado_V0.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Anexando o novo DataFrame ao original\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/dani-boy/extractNF/2_process_pipe_unificado_V0.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m df_processamento \u001b[39m=\u001b[39m df_processamento\u001b[39m.\u001b[39;49mappend(df_batch, ignore_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:9768\u001b[0m, in \u001b[0;36mDataFrame.append\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   9665\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   9666\u001b[0m \u001b[39mAppend rows of `other` to the end of caller, returning a new object.\u001b[39;00m\n\u001b[1;32m   9667\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9758\u001b[0m \u001b[39m4  4\u001b[39;00m\n\u001b[1;32m   9759\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   9760\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   9761\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mThe frame.append method is deprecated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   9762\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mand will be removed from pandas in a future version. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9765\u001b[0m     stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   9766\u001b[0m )\n\u001b[0;32m-> 9768\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_append(other, ignore_index, verify_integrity, sort)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:9808\u001b[0m, in \u001b[0;36mDataFrame._append\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   9805\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   9806\u001b[0m     to_concat \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m, other]\n\u001b[0;32m-> 9808\u001b[0m result \u001b[39m=\u001b[39m concat(\n\u001b[1;32m   9809\u001b[0m     to_concat,\n\u001b[1;32m   9810\u001b[0m     ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m   9811\u001b[0m     verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m   9812\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m   9813\u001b[0m )\n\u001b[1;32m   9814\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mappend\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/concat.py:381\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39mConcatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    368\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    369\u001b[0m     objs,\n\u001b[1;32m    370\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[1;32m    379\u001b[0m )\n\u001b[0;32m--> 381\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/concat.py:612\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    610\u001b[0m         obj_labels \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39maxes[\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m ax]\n\u001b[1;32m    611\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m new_labels\u001b[39m.\u001b[39mequals(obj_labels):\n\u001b[0;32m--> 612\u001b[0m             indexers[ax] \u001b[39m=\u001b[39m obj_labels\u001b[39m.\u001b[39;49mget_indexer(new_labels)\n\u001b[1;32m    614\u001b[0m     mgrs_indexers\u001b[39m.\u001b[39mappend((obj\u001b[39m.\u001b[39m_mgr, indexers))\n\u001b[1;32m    616\u001b[0m new_data \u001b[39m=\u001b[39m concatenate_managers(\n\u001b[1;32m    617\u001b[0m     mgrs_indexers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_axes, concat_axis\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbm_axis, copy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[1;32m    618\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3904\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[1;32m   3903\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 3904\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidIndexError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requires_unique_msg)\n\u001b[1;32m   3906\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(target) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   3907\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([], dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
     ]
    }
   ],
   "source": [
    "# Anexando o novo DataFrame ao original\n",
    "df_processamento = df_processamento.append(df_batch, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processamento_file_write_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pipeline_extracao_documentos/6_geral_administacao/datasets/df_processamento_16.xlsx'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processamento_file_write_path = os.path.join(dataset_path, df_processamento_file_write)\n",
    "\n",
    "df_processamento_file_write_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pipeline_extracao_documentos/6_geral_administacao/datasets/df_extracao_files_Batch_16.xlsx'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extracao_files_Batch_file_write_path = os.path.join(dataset_path, df_extracao_files_Batch_file_write)\n",
    "\n",
    "df_extracao_files_Batch_file_write_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o DF para excel\n",
    "df_extracao_files.to_excel(df_extracao_files_Batch_file_write_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o DF para excel\n",
    "df_processamento.to_excel(df_processamento_file_write_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcoes modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Funcao de conversao e resize do documento\n",
    "def convertResize(doc2convert, document_path, image_resized_path):\n",
    "    \n",
    "    \"\"\"# 1. remocao do sufixo .pdf\n",
    "    if doc2convert.split(\".\")[1].islower():\n",
    "        nameImage= doc2convert.removesuffix(\".pdf\")\n",
    "    else:\n",
    "        nameImage= doc2convert.removesuffix(\".PDF\")\"\"\"\n",
    "    \n",
    "    # 2. construo um novo nome para o documento imagem\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(doc2convert)}.jpg')\n",
    "    \n",
    "    # 3. Conversao para imagem\n",
    "    pages = convert_from_path(document_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((2067, 2923))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "    return resized_pages[0], image_resized_name\n",
    "\n",
    "\n",
    "def convertResizeAnalise_1page(doc2convert, document_path, image_resized_path):\n",
    "    \n",
    "    \"\"\"# 1. remocao do sufixo .pdf\n",
    "    if doc2convert.split(\".\")[1].islower():\n",
    "        nameImage= doc2convert.removesuffix(\".pdf\")\n",
    "    else:\n",
    "        nameImage= doc2convert.removesuffix(\".PDF\")\"\"\"\n",
    "    nome_convertido = conv_filename_no_ext(doc2convert)\n",
    "    # 2. construo um novo nome para o documento imagem\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(nome_convertido)}.jpg')\n",
    "    \n",
    "    # 3. Conversao para imagem\n",
    "    pages = convert_from_path(document_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    resized_pages = []\n",
    "    for page in pages:\n",
    "        resized_page = page.resize((2067, 2923))\n",
    "        resized_pages.append(resized_page)\n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "    return resized_pages[0], image_resized_name\n",
    "\n",
    "\n",
    "# Funcao de conversao e resize do documento\n",
    "def convertResize_analise(nome_documento, document_path, image_resized_path):\n",
    "    \n",
    "    \"\"\"# 1. remocao do sufixo .pdf\n",
    "    if doc2convert.split(\".\")[1].islower():\n",
    "        nameImage= doc2convert.removesuffix(\".pdf\")\n",
    "    else:\n",
    "        nameImage= doc2convert.removesuffix(\".PDF\")\"\"\"\n",
    "    \n",
    "    # 2. construo um novo nome para o documento imagem\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(nome_documento)}.jpg')\n",
    "    \n",
    "    # 3. Conversao para imagem\n",
    "    pages = convert_from_path(document_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((2067, 2923))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "    return resized_pages[0], image_resized_name\n",
    "\n",
    "\n",
    "\n",
    "# Trata Ocr\n",
    "def extract_fields_box(modelo, father_value, section):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    filtered_boxes_info = field_boxes_info[(field_boxes_info['father'] == father_value) & (field_boxes_info['model'] == modelo)]\n",
    "    # Iterate nas informações dos boxes de fields e extraia o texto de cada field\n",
    "    for index_field, row_field in filtered_boxes_info.iterrows():\n",
    "        x0, y0, x1, y1 = row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        #print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference'], row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1'] ))\n",
    "        # Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "        value = extracted_text_box.split('\\n')[-1]\n",
    "        # Remova qualquer espaço em branco à esquerda ou à direita\n",
    "        value = value.strip()\n",
    "        if row_field['t_value'] == 'number':\n",
    "            # Formate o valor usando a função format_number\n",
    "            #print(\"vou verificar valor\")\n",
    "            value = format_number2(value)\n",
    "            #print(value)\n",
    "        # Armazene o texto extraído com o rótulo correspondente\n",
    "        label = row_field['label']\n",
    "        data_box_valores[label] = value\n",
    "        \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "# 3. Efetua OCR no documento (area parao do texto da NF)\n",
    "def ocr_RasterPDF(image_name):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = 406\n",
    "    y0 = 0\n",
    "    x1= 1540\n",
    "    y1 = 380\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "    return values, extracted_text_frame \n",
    "\n",
    "\n",
    "\n",
    "# 2. Efetua OCR no documento (area parao do texto da NF)\n",
    "def ocr_RasterPDF_free(image_name, vx0, vy0, vx1, vy1):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = vx0\n",
    "    y0 = vy0\n",
    "    x1= vx1\n",
    "    y1 = vy1\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "    return values, extracted_text_frame \n",
    "\n",
    "\n",
    "# 2. Pesquisa prefeitura no documento\n",
    "def pequisaModel(image_name):\n",
    "\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = 406\n",
    "    y0 = 0\n",
    "    x1= 1540\n",
    "    y1 = 380\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "\n",
    "    # 5. Interacao para pesquisar prefeitura\n",
    "    for value in values:\n",
    "        nome_prefeitura_match = re.search(r'PREFEITURA (.+)', value)\n",
    "        if nome_prefeitura_match:\n",
    "            nome_prefeitura = \"PREFEITURA \" + nome_prefeitura_match.group(1) \n",
    "            return  nome_prefeitura        \n",
    "     \n",
    "\n",
    "# 1. Interacao para pesquisar prefeitura\n",
    "def pesquisa_texto(texto):\n",
    "    nome_prefeitura_match = re.search(r'PREFEITURA (.+)', texto)\n",
    "    if nome_prefeitura_match:\n",
    "        is_prefeitura = \"PREFEITURA \" + nome_prefeitura_match.group(1)\n",
    "        \n",
    "        return  is_prefeitura\n",
    "    else:\n",
    "        raise ValueError(\"Nao consegui pesquisar\")\n",
    "    \n",
    "    \n",
    "    # 5. Verifica se PDF e pesquisavel ou nao e grava metadados dele\n",
    "def is_pdf_searchable_analise(pdf_path):\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        pages = pdf_document.page_count\n",
    "        is_searchable = all(page.get_text(\"text\") != \"\" for page in pdf_document)\n",
    "        dados_pdf = pdf_document.metadata\n",
    "        pdf_document.close()\n",
    "        return is_searchable, dados_pdf, pages\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao verificar o PDF: {e}\")\n",
    "        return False\n",
    "    \n",
    "    \n",
    "# 2. Extracao OCR\n",
    "def extract_text_from_coordinates(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=config).strip()\n",
    "    return extracted_text\n",
    "\n",
    "# 1. funçao basica de modelo \n",
    "def executa_model_frame(model, secao, father_name):\n",
    "\n",
    "    data_dados_frame = {}\n",
    "    \n",
    "    tipo = \"frame\"\n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model) & (frames_nf_v4_df['label'] == f_frame_name) & (frames_nf_v4_df['type'] == tipo)]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        extracted_text_frame = extract_text_from_coordinates(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        \n",
    "        frame_seq = row_frame['seq']\n",
    "        frame_model = row_frame['model']\n",
    "        frame_label = row_frame['label']\n",
    "        frame_type = row_frame['type']\n",
    "        frame_section = row_frame['section_json']\n",
    "        frame_reference = row_frame['reference']\n",
    "        frame_father = row_frame['father']\n",
    "        frame_id = row_frame['id']\n",
    "        #print(f'\\fid: {frame_id:>3} | seq: {frame_seq:>3} | model: {frame_model:>8} | type: {frame_type:>15} | Father: {frame_father} label: {frame_label:>30} | section: {frame_section:>20} {frame_reference:>30}')\n",
    "        \n",
    "    return extracted_text_frame\n",
    "\n",
    "\n",
    "\n",
    "# Sao iguais \n",
    "def extract_text_from_frame(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=config).strip()\n",
    "    return extracted_text \n",
    "\n",
    "\n",
    "\n",
    "# Trata files\n",
    "# 3. Ajusta o filename tirando caracteres especiais \n",
    "def conv_filename(title):\n",
    "    \n",
    "    # Divida o título em nome e extensão\n",
    "    name, extension = title.rsplit('.', 1) if '.' in title else (title, \"\")\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    # Adicione a extensão de volta, se houver\n",
    "    if extension:\n",
    "        filename += '.' + extension.lower()\n",
    "\n",
    "    return filename\n",
    "\n",
    "# 4. Ajusta o filename tirando caracteres especiais e a\n",
    "def conv_filename_no_ext(title):\n",
    "    # Divida o título em nome e extensão (mas ignore a extensão)\n",
    "    name = title.rsplit('.', 1)[0] if '.' in title else title\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    return filename  \n",
    "\n",
    "\n",
    "# move NF processadas ok\n",
    "def move_raster_pdf(document_path, raster_pdf_path, batch_name, doc2convert):\n",
    "    # Determine the destination directory\n",
    "    destination_dir = os.path.join(raster_pdf_path, batch_name)\n",
    "\n",
    "    # Check if the destination directory exists; if not, create it\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "\n",
    "    # Determine the destination path including the filename\n",
    "    destination_path = os.path.join(destination_dir, os.path.basename(document_path))\n",
    "\n",
    "    # Move the file from the source path to the destination path\n",
    "    try:\n",
    "        shutil.move(document_path, destination_path)\n",
    "        print(f\"Sucesso ao mover: {document_path} para: {destination_path}\")\n",
    "        return True, destination_path, None  # Success, destination path, no error\n",
    "    except Exception as e:\n",
    "        error_message = f\"Erro ao mover: {document_path} para: {destination_path}: {str(e)}\"\n",
    "        print(error_message)\n",
    "        return False, None, error_message  # Failure, no destination path, error message  \n",
    "    \n",
    "    \n",
    "    # Trata texto extraido\n",
    "    # Função para extrair número da string\n",
    "def extract_number(text):\n",
    "    match = re.search(r'\\b\\d+(\\.\\d+)?\\b', text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "# Funcao importante - process_line\n",
    "def process_line(value, reference, label):\n",
    "    name_match = re.search(fr'{reference} (.+)', value)\n",
    "    if name_match:\n",
    "        extracted_value = reference + \" \" + name_match.group(1)\n",
    "        return {label: extracted_value}\n",
    "    return None\n",
    "\n",
    "\n",
    "def format_number(number_str):\n",
    "    # Check for percentage and handle it\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # You can multiply by 100 here if needed\n",
    "\n",
    "    # Check if the string contains \"R$\" or a comma, indicating the original format\n",
    "    if 'R$' in number_str or ',' in number_str:\n",
    "        # Original format: Remove 'R$', replace dots with nothing, and replace commas with dots\n",
    "        number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    else:\n",
    "        # New format: Extract only the numeric part using regex\n",
    "        number_str = re.findall(r'[\\d\\.]+', number_str)[-1]\n",
    "\n",
    "    return float(number_str)\n",
    "\n",
    "# Funçao de formatacao de numeros\n",
    "def format_number2(number_str):\n",
    "    number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # multiplica por 100 para fields %\n",
    "    return float(number_str)\n",
    "\n",
    "\n",
    "\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "#1. funcao: find_value_after_keyword_out_frame_up\n",
    "def find_value_after_keyword_out_frame_up(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "        if text_list[index + 1] not in default_keyword_list:\n",
    "            return text_list[index + 1]\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            for default_keyword in default_keyword_list:\n",
    "                if default_keyword in text_list:\n",
    "                    # Caso especial para 'Nome/Razão Social:'\n",
    "                    if keyword == 'Nome/Razão Social:':\n",
    "                        return text_list[0]\n",
    "        return None\n",
    "    \n",
    "#2. find_value_after_keyword_out_frame_down  \n",
    "def find_value_after_keyword_out_frame_down(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o índice seguinte está dentro da lista\n",
    "        if index + 1 < len(text_list):\n",
    "            # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            try:\n",
    "                index = text_list.index(default_keyword_list[-1])\n",
    "                return text_list[index - 1]\n",
    "            except ValueError:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "#3. find_value_after_keyword_fuzz\n",
    "def find_value_after_keyword_fuzz(keyword, text_list, default_keyword_list=None, fuzziness_threshold=80):\n",
    "    closest_match = None\n",
    "    closest_match_score = 0\n",
    "    \n",
    "    for i, text in enumerate(text_list):\n",
    "        score = fuzz.ratio(keyword, text)\n",
    "        \n",
    "        if score > closest_match_score:\n",
    "            closest_match_score = score\n",
    "            closest_match = text\n",
    "        \n",
    "        if closest_match_score > fuzziness_threshold:\n",
    "            break\n",
    "\n",
    "    if closest_match_score > fuzziness_threshold:\n",
    "        index = text_list.index(closest_match)\n",
    "        if index + 1 < len(text_list):\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def pesquisa_keyword(string_pesquisa, text_splited, keyword_list):\n",
    "\n",
    "    resultado_extraido_fuzz = find_value_after_keyword_fuzz(string_pesquisa, text_splited, keyword_list)\n",
    "\n",
    "    if resultado_extraido_fuzz == None:\n",
    "        resultado_extraido_frame_up = find_value_after_keyword_out_frame_up(string_pesquisa, text_splited, keyword_list)\n",
    "        if resultado_extraido_frame_up == None:\n",
    "            resultado_extraido_frame_down = find_value_after_keyword_out_frame_down(string_pesquisa, text_splited, keyword_list)\n",
    "            resultado_extraido = resultado_extraido_frame_down\n",
    "        else:\n",
    "            resultado_extraido = resultado_extraido_frame_up\n",
    "    else:\n",
    "        resultado_extraido = resultado_extraido_fuzz           \n",
    "    #print(resultado_extraido)\n",
    "    return resultado_extraido\n",
    "\n",
    "\n",
    "def cabecalho_prefeitura():\n",
    "    valor_dict = {}\n",
    "    dados_prefeitura = {}\n",
    "    f_frame_name = \"1_frame_prefeitura_nf\"\n",
    "    # 1. funçao basica de modelo \n",
    "    texto = executa_model_frame(model, secao, f_frame_name)\n",
    "    text_splited = texto.split('\\n')\n",
    "    \n",
    "    valor_dict = extract_prefeitura(model, f_frame_name, text_splited)\n",
    "    if valor_dict:\n",
    "        dados_prefeitura.update(valor_dict)\n",
    "        \n",
    "        \n",
    "    return dados_prefeitura \n",
    "                \n",
    "def cabecalho_dados():\n",
    "\n",
    "    valor = {}   \n",
    "    f_frame_name = \"1_frame_dados_nf\"\n",
    "    \n",
    "    dadinho_dados_nf = {}\n",
    "    \n",
    "    # 1. funçao basica de modelo \n",
    "    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "    text_splited = texto_extraido(texto)\n",
    "    keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "    string_pesquisa = \"Número da Nota:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "    dadinho_dados_nf['numero_nota_fiscal'] = texto\n",
    "\n",
    "\n",
    "    string_pesquisa = \"Competência:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['competencia'] = texto\n",
    "    \n",
    "    \n",
    "    string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['dt_hr_emissao'] = texto\n",
    "    \n",
    "    \n",
    "    string_pesquisa = \"Código Verificação:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['codigo_verificacao'] = texto\n",
    "    \n",
    "    return dadinho_dados_nf   \n",
    "\n",
    "\n",
    "def extract_fields_prestador_cnpj(text): # Função para extrair campos e valores dentro de um retângulo\n",
    "    \n",
    "    \n",
    "    nf_data_prestador_cnpj = {}\n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in text:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "                        nf_data_prestador_cnpj['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                        nf_data_prestador_cnpj['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "\n",
    "\n",
    "    # Extrair Telefone\n",
    "    telefone_str = None\n",
    "    \n",
    "    #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "    telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', text)\n",
    "    if telefone_match: \n",
    "        telefone_str = telefone_match.group(1)\n",
    "        # Remover quebras de linha\n",
    "        telefone_str = telefone_str.replace('.', '')\n",
    "        telefone_str = telefone_str.replace('\\n', '')\n",
    "                \n",
    "        nf_data_prestador_cnpj['telefone'] = telefone_str\n",
    "    else:\n",
    "        nf_data_prestador_cnpj['telefone'] = None   \n",
    "    \n",
    "    \n",
    "    return nf_data_prestador_cnpj \n",
    "\n",
    "\n",
    "def extract_fields_tomador_cnpj(text):\n",
    "    nf_data_tomador_cnpj = {}\n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in text:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "                        nf_data_tomador_cnpj['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                        nf_data_tomador_cnpj['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "\n",
    "    \n",
    "    # Extrair Telefone\n",
    "    telefone_match = re.search(r'Telefone:\\s+(.+)', text)\n",
    "    if telefone_match:\n",
    "        telefone_str = telefone_match.group(1)\n",
    "        if telefone_str == 'Inscrição Estadual:':\n",
    "            nf_data_tomador_cnpj['telefone'] = None  # Valor padrão quando não há correspondência\n",
    "        elif telefone_str == '':\n",
    "            nf_data_tomador_cnpj['telefone'] = None  # Valor padrão quando não há correspondência\n",
    "                    \n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['telefone'] = telefone_match.group(1)\n",
    "            \n",
    "    \n",
    "    # Extrair Inscrição Municipal\n",
    "    inscricao_municipal_match = re.search(r'Inscrição Municipal:\\s+(.+)', text)\n",
    "    if inscricao_municipal_match:\n",
    "        inscricao_municipal_str = inscricao_municipal_match.group(1)\n",
    "        if inscricao_municipal_str == \"Telefone:\": \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = inscricao_municipal_str\n",
    "    \n",
    "    insc_municipal_match = re.search(r'INSC:MUNICIPAL:\\s+(.+)', text)\n",
    "    if insc_municipal_match:\n",
    "        insc_municipal_str = insc_municipal_match.group(1)\n",
    "        if insc_municipal_str == \"Telefone:\":\n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = insc_municipal_str\n",
    "    else:\n",
    "        nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "            \n",
    "    \n",
    "    return nf_data_tomador_cnpj \n",
    "\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "def extract_dados_comple_obs(modelo, frame_father, section):\n",
    "    \n",
    "    data_dados_complementares = {}\n",
    "    #frame_label = frame_father\n",
    "    \n",
    "    # 1. Filtrando o frames_info para buscar os dados de corte\n",
    "    filtered_frames_info = frames_info[(frames_info['label'] == frame_father) & (frames_info['model'] == modelo)]\n",
    "\n",
    "    # 2. Filtrando o sframe_fields_info para buscar os dados dos campos que estao nos frames\n",
    "    filtered_sframe_fields_info = sframe_fields_info[(sframe_fields_info['father'] == frame_father) & (sframe_fields_info['model'] == modelo)]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "        \n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        \n",
    "        print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_frame['seq'], row_frame['model'], row_frame['father'], row_frame['label'], row_frame['reference'], row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'] ))\n",
    "        for index_field, row_field in filtered_sframe_fields_info.iterrows():\n",
    "            #print(\"{:<5} {:<10} {:<30} {:<20} {:<20}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference']))\n",
    "            \n",
    "            if frame_father == \"5_frame_dados_complementares\":\n",
    "                nf_data_dados_complementares = {}\n",
    "                nf_data_dados_complementares['section'] = section\n",
    "                \n",
    "                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^DADOS COMPLEMENTARES', '', extracted_text_box, count=1)\n",
    "                if text == '':\n",
    "                    text = None\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text\n",
    "                else:    \n",
    "                    # Extrair texto dentro do retângulo\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "                    \n",
    "                return nf_data_dados_complementares                \n",
    "                \n",
    "            elif frame_father == \"5_frame_observacao\":\n",
    "                nf_data_observacao = {}\n",
    "                nf_data_observacao['section'] = section \n",
    "                                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^Observação:', '', extracted_text_box, count=1)\n",
    "\n",
    "                # Remover quebras de linha\n",
    "                text = text.replace('\\n', ' ')\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                nf_data_observacao['observacao'] = text.strip()\n",
    "                \n",
    "                return nf_data_observacao \n",
    "            \n",
    "            \n",
    "secao = \"1 - CABECALHO\"\n",
    "f_frame_name = \"1_frame_prefeitura_nf\"\n",
    "\n",
    "\n",
    "#4. Extrai prefeitura\n",
    "def extract_prefeitura(model, father, values):\n",
    "    \n",
    "    tipo = \"sframe_field\"\n",
    "    data_extrated_prefeitura = {}\n",
    "    #print(tipo)\n",
    "\n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model) & (frames_nf_v4_df['father'] == father) & (frames_nf_v4_df['type'] == tipo)]\n",
    "\n",
    "    for index_sframe, row_sframe in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        label_value = row_sframe['label']\n",
    "        \n",
    "        #print(\"label_value\", label_value)\n",
    "        \n",
    "        if label_value == \"nome_prefeitura\":\n",
    "            reference_value = row_sframe['reference']\n",
    "            for value in values:\n",
    "                result = process_line(value, reference_value, label_value)\n",
    "                if result:\n",
    "                    data_extrated_prefeitura.update(result)\n",
    "        elif label_value == \"secretaria\":\n",
    "            reference_value = row_sframe['reference']\n",
    "            for value in values:\n",
    "                result = process_line(value, reference_value, label_value)\n",
    "                if result:\n",
    "                    data_extrated_prefeitura.update(result) \n",
    "        elif label_value == \"tipo_nota_fiscal\":\n",
    "            reference_value = row_sframe['reference']  \n",
    "            for value in values:\n",
    "                result = process_line(value, reference_value, label_value)\n",
    "                if result:\n",
    "                    data_extrated_prefeitura.update(result)\n",
    "                    \n",
    "    return data_extrated_prefeitura\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def processa_cnae_outros(text):\n",
    "    nf_data_CNAE_match = re.search(r'CNAE\\s+(.+)', text)\n",
    "    if nf_data_CNAE_match:\n",
    "        try:\n",
    "            # Remove a primeira ocorrência de \"CNAE:\"\n",
    "            nf_data_CNAE_str = re.sub(r'^CNAE - ', '', text, count=1)\n",
    "            # Remover quebras de linha\n",
    "            nf_data_CNAE_str = nf_data_CNAE_str.replace('\\n', ' ')\n",
    "            return nf_data_CNAE_str \n",
    "        except Exception as e:\n",
    "            print(f\"Erro busca cnae: {e}\") \n",
    "        \n",
    "    return None   \n",
    "\n",
    "\n",
    "\n",
    "def extract_fb_outras_inf(modelo, father_value, section):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    filtered_boxes_info = field_boxes_info[(field_boxes_info['father'] == father_value) & (field_boxes_info['model'] == model)]\n",
    "    # Iterate nas informações dos boxes de fields e extraia o texto de cada field\n",
    "    for index_field, row_field in filtered_boxes_info.iterrows():\n",
    "        \n",
    "        string_pesquisa = row_field['reference']\n",
    "        x0, y0, x1, y1 = row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        label = row_field['label']\n",
    "        #print(f'extracted_text_box {extracted_text_box}, label {label}')\n",
    "        text = extracted_text_box.replace('\\n', '')\n",
    "        if text.startswith(string_pesquisa):\n",
    "            #print(\"aqui:\", text)\n",
    "            text = text[len(label):].strip()\n",
    "            data_box_valores[label] = text\n",
    "    \n",
    "    return   data_box_valores      \n",
    "                 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## | REALMENTE O PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processa_raster_pdf(image_2work):\n",
    "    \n",
    "    secao = \"1 - CABECALHO\"\n",
    "    try:\n",
    "        nro_nota = 0\n",
    "        nd_data_cabecalho = {}\n",
    "        nd_data_cabecalho['secao'] = secao\n",
    "        valor_dict = {}\n",
    "        dados_prefeitura = {}\n",
    "        f_frame_name = \"1_frame_prefeitura_nf\"\n",
    "        # 1. funçao basica de modelo \n",
    "        texto = executa_model_frame(model, secao, f_frame_name)\n",
    "        text_splited = texto.split('\\n')\n",
    "        \n",
    "        valor_dict = extract_prefeitura(model, f_frame_name, text_splited)\n",
    "        if valor_dict:\n",
    "            dados_prefeitura.update(valor_dict)\n",
    "        valor = {}   \n",
    "        f_frame_name = \"1_frame_dados_nf\"\n",
    "        dadinho_dados_nf = {}\n",
    "        # 1. funçao basica de modelo \n",
    "        texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        text_splited = texto_extraido(texto)\n",
    "        keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "        string_pesquisa = \"Número da Nota:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "        dadinho_dados_nf['numero_nota_fiscal'] = texto\n",
    "        nro_nota = texto\n",
    "        \n",
    "        string_pesquisa = \"Competência:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        dadinho_dados_nf['competencia'] = texto\n",
    "        \n",
    "        string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        dadinho_dados_nf['dt_hr_emissao'] = texto\n",
    "        \n",
    "        string_pesquisa = \"Código Verificação:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        dadinho_dados_nf['codigo_verificacao'] = texto\n",
    "        \n",
    "        nd_data_cabecalho.update(dados_prefeitura)\n",
    "        nd_data_cabecalho.update(dadinho_dados_nf)\n",
    "    except Exception as e:\n",
    "        # erros_cabecalho = {}\n",
    "        err_msg = f\"Erro de processo cabecalho: {e}\"\n",
    "        print(err_msg)\n",
    "        # erros['documento'] = file\n",
    "        # erros_cabecalho['secao'] = secao\n",
    "        # erros_cabecalho['erro'] = err_msg\n",
    "        # erros.update(erros_cabecalho)                \n",
    "    \n",
    "    secao = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    try:\n",
    "        f_frame_name = \"2_frame_cnpj_prestador\"\n",
    "        nd_data_prestador = {}\n",
    "        prestador_inscricao = {}\n",
    "        nd_data_prestador['secao'] = secao\n",
    "        prestador_cnpj_value = {}\n",
    "        texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        #text_splited = texto_extraido(texto)\n",
    "        prestador_cnpj_value = extract_fields_prestador_cnpj(texto)\n",
    "        if prestador_cnpj_value:\n",
    "            nd_data_prestador.update(prestador_cnpj_value)\n",
    "    except Exception as e:\n",
    "        # erros_cnpj_prestador = {}\n",
    "        err_msg = (f\"Erro prestador cnpj: {e}\")\n",
    "        print(err_msg)\n",
    "        \n",
    "        # erros_cnpj_prestador['secao'] = secao\n",
    "        # erros_cnpj_prestador['erro'] = err_msg\n",
    "        # erros.update(erros_cnpj_prestador)       \n",
    "        \n",
    "    try:\n",
    "        f_frame_name = \"2_frame_inscricao_prestador\" \n",
    "        texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        text_splited = texto_extraido(texto)\n",
    "        keyword_list = ['Inscrição Municipal:', 'Inscrição Estadual:']\n",
    "        string_pesquisa = \"Inscrição Municipal:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        prestador_inscricao['prestador_inscricao'] = texto\n",
    "        \n",
    "        string_pesquisa = \"Inscrição Estadual:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        prestador_inscricao['inscricao_estadual'] = texto\n",
    "        nd_data_prestador.update(prestador_inscricao)\n",
    "    except Exception as e:\n",
    "        # erros_inscricao_prestador = {}\n",
    "        err_msg = (f\"Erro de processo inscricao prestador: {e}\")\n",
    "        print(err_msg)\n",
    "        # erros_inscricao_prestador['secao'] = secao\n",
    "        # erros_inscricao_prestador['erro'] = err_msg\n",
    "        # erros.update(erros_inscricao_prestador)\n",
    "\n",
    "    try:\n",
    "        f_frame_name = \"2_frame_dados_prestador\"\n",
    "        prestador_dados_value = {}\n",
    "        \n",
    "        keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "        string_pesquisa = \"Nome/Razão Social:\"\n",
    "        texto = executa_model_frame(model, secao, f_frame_name)\n",
    "        text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        prestador_dados_value['razao_social'] = texto\n",
    "\n",
    "        string_pesquisa = \"Nome de Fantasia:\"\n",
    "        #texto = executa_model_frame(model, secao, f_frame_name)\n",
    "        #text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        prestador_dados_value['nome_fantasia'] = texto\n",
    "        \n",
    "        string_pesquisa = \"Endereço:\"\n",
    "        #texto = executa_model_frame(model, secao, f_frame_name)\n",
    "        #text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        prestador_dados_value['endereco'] = texto\n",
    "        \n",
    "        string_pesquisa = \"E-mail:\"\n",
    "        #texto = executa_model_frame(model, secao, f_frame_name)\n",
    "        #text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        prestador_dados_value['email'] = texto\n",
    "        nd_data_prestador.update(prestador_dados_value)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro prestador dados: {e}\") \n",
    "        \n",
    "    \n",
    "    secao = \"3. TOMADOR DE SERVIÇO\"\n",
    "    try:\n",
    "        nd_data_tomador = {}\n",
    "        tomador_cnpj_value = {}\n",
    "        nd_data_tomador['secao'] = secao\n",
    "        f_frame_name = \"3_frame_cnpj_tomador\"\n",
    "        texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        text_splited = texto_extraido(texto) \n",
    "        tomador_cnpj_value = extract_fields_tomador_cnpj(texto)\n",
    "        if tomador_cnpj_value:\n",
    "            nd_data_tomador.update(tomador_cnpj_value)                  \n",
    "    except Exception as e:\n",
    "        print(f\"Erro tomador cnpj: {e}\")\n",
    "        \n",
    "    f_frame_name = \"3_frame_inscricao_tomador\"    \n",
    "    try:\n",
    "        data_tomador_inscricao = {}\n",
    "        keyword_list = ['RG:', 'Inscrição Estadual:']\n",
    "        string_pesquisa = \"RG:\"\n",
    "        texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        data_tomador_inscricao['rg'] = texto\n",
    "        \n",
    "        string_pesquisa = \"Inscrição Estadual:\"\n",
    "        #texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        #text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        data_tomador_inscricao['inscricao_estadual'] = texto\n",
    "        nd_data_tomador.update(data_tomador_inscricao)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro tomador inscricao: {e}\")\n",
    "        \n",
    "    f_frame_name = \"3_frame_dados_tomador\"\n",
    "    try: \n",
    "        data_tomador_dados = {}   \n",
    "        keyword_list = ['Nome/Razão Social:', 'Endereço:', 'E-mail']\n",
    "        string_pesquisa = \"Nome/Razão Social:\"\n",
    "        texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        data_tomador_dados['razao_social'] = texto\n",
    "        \n",
    "        string_pesquisa = \"Endereço:\"\n",
    "        #texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        #text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        data_tomador_dados['endereco'] = texto\n",
    "        \n",
    "        string_pesquisa = \"E-mail\"\n",
    "        #texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        #text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        data_tomador_dados['email'] = texto\n",
    "        \n",
    "        nd_data_tomador.update(data_tomador_dados)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro tomador dados: {e}\") \n",
    "        \n",
    "    secao = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "    try:\n",
    "        nd_data_servico = {}\n",
    "        nd_data_servico['secao'] = secao\n",
    "        f_frame_name = \"4_frame_descricao_totais\"\n",
    "        texto = executa_model_frame(model, secao, f_frame_name)\n",
    "        text = texto.replace('\\n', ' ')\n",
    "        label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "        if text.startswith(label):\n",
    "            text = text[len(label):].strip()\n",
    "        nd_data_servico['discriminacao_servicos'] = text \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro descricao servico: {e}\")\n",
    "            \n",
    "    secao = \"5. VALOR TOTAL\"\n",
    "    try:\n",
    "        nd_data_valor_total = {}\n",
    "        data_valor_total['secao'] = secao\n",
    "        f_frame_name = \"4_frame_valor_total\"   \n",
    "        text = executa_model_frame(model, secao, f_frame_name)  \n",
    "        valor_total_match = re.search(r'R\\$ ([\\d,.]+)', text)\n",
    "        if valor_total_match:\n",
    "            valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "            nd_data_valor_total['valor_total_nota'] = float(valor_total_sem_formatacao)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro valor total: {e}\")\n",
    "\n",
    "    secao = \"6. CNAE e Item da Lista de Serviços\"\n",
    "    try:\n",
    "        nd_data_CNAE = {}\n",
    "        nd_data_CNAE['secao'] = secao\n",
    "        f_frame_name = \"4_frame_cnae_itens_servico\"   \n",
    "        Texto_extraido = executa_model_frame(model, secao, f_frame_name)\n",
    "        text_splited = Texto_extraido.split('\\n')\n",
    "        # Processando CNAE\n",
    "        cnae_line = [line for line in text_splited if 'CNAE' in line][0]\n",
    "        cnae_number = int(extract_number(cnae_line))\n",
    "        cnae_value = cnae_dict.get(cnae_number, \"Valor não encontrado\")\n",
    "        if cnae_value == 'Valor não encontrado':\n",
    "            cnae_value = processa_cnae_outros(cnae_line)\n",
    "            cnae_value = cnae_value.upper()\n",
    "            nd_data_CNAE['cnae'] = cnae_value\n",
    "        else:\n",
    "            cnae_value = cnae_value.upper()\n",
    "            cnae_value = str(cnae_number) + \" - \" + cnae_value\n",
    "            nd_data_CNAE['cnae'] = cnae_value\n",
    "            nd_data_CNAE['item_lista_servicos'] = item_servico_value\n",
    "    except Exception as e:\n",
    "        print(f\"Erro busca cnae: {e}\")    \n",
    "\n",
    "    try:\n",
    "        item_servico_line = [line for line in text_splited if 'Item da Lista de Serviços' in line][0]\n",
    "        item_servico_number = float(extract_number(item_servico_line))\n",
    "        item_servico_value = item_servico_dict.get(item_servico_number, \"Valor não encontrado\")\n",
    "        item_servico_value = item_servico_value.upper()\n",
    "        item_servico_value = str(item_servico_number) + \" - \" + item_servico_value\n",
    "        nd_data_CNAE['item_lista_servicos'] = item_servico_value\n",
    "    except Exception as e:\n",
    "        print(f\"Erro busca Itens de servico: {e}\")  \n",
    "\n",
    "    secao = \"8. DADOS COMPLEMENTARES\"\n",
    "    try:\n",
    "        nd_data_valores = {}\n",
    "        nd_data_valores['secao'] = secao\n",
    "        f_frame_name = \"5_frame_valores_impostos\"   \n",
    "        \n",
    "        result = extract_fields_box(model, f_frame_name, secao)\n",
    "        if result:\n",
    "            nd_data_valores.update(result)\n",
    "\n",
    "        # secao: 8 - DADOS COMPLEMENTARES\"\n",
    "        nd_data_dados_complementares = {}\n",
    "        f_frame_name  = \"5_frame_dados_complementares\"\n",
    "        section = \"8. DADOS COMPLEMENTARES\"\n",
    "\n",
    "        nd_data_dados_complementares = extract_dados_comple_obs(model, f_frame_name, section)                                           \n",
    "                                \n",
    "                                \n",
    "        # secao: 9 - OUTRAS INFORMAÇOES / CRITICAS\n",
    "        nd_data_outras_informacoes = {}\n",
    "        father_value = \"5_frame_inf_criticas\"\n",
    "        section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "        \n",
    "        result = extract_fb_outras_inf(model, father_value, section)\n",
    "        if result:\n",
    "            nd_data_outras_informacoes.update(result)                        \n",
    "                            \n",
    "        # secao: 10. OBSERVACOES\n",
    "        nd_data_observacao = {}\n",
    "        f_father = \"5_frame_observacao\"\n",
    "        section = \"10. OBSERVACOES\"\n",
    "\n",
    "        nd_data_observacao = extract_dados_comple_obs(model, f_father, section)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro valores complementares:\")  \n",
    "        \n",
    "    \n",
    "    return nro_nota, nd_data_cabecalho, nd_data_prestador, nd_data_tomador, nd_data_servico, nd_data_valor_total, nd_data_CNAE, nd_data_valores, nd_data_dados_complementares, nd_data_outras_informacoes, nd_data_observacao      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processa_pdf_pesquisavel(file_path):\n",
    "    \n",
    "   \n",
    "    status = \"O PDF é pesquisável\"\n",
    "    # Carregar o arquivo PDF\n",
    "    pdf_document = fitz.open(file_path)\n",
    "\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 4\n",
    "    x1 = 600\n",
    "    y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    \n",
    "    print(text)\n",
    "    \n",
    "  \n",
    "    if text:\n",
    "       page_number = 0\n",
    "       print(page_number)\n",
    "    else:\n",
    "       page_number = 1\n",
    "       print(page_number)\n",
    "    \n",
    "    nf_data_cabecalho = {}\n",
    "    \n",
    "    page = pdf_document[page_number]\n",
    "    x0 = 0\n",
    "    y0 = 0\n",
    "    x1 = 600\n",
    "    y1 = 110\n",
    "\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    nf_data_cabecalho = Extc.extract_fields_cabecalho(text)\n",
    "    #nf_data_cabecalho = extract_fields_cabecalho(text)\n",
    "    \n",
    "    nro_nota = nf_data_cabecalho['numero_nota_fiscal']\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2. PRESTADOR DE SERVIÇO\n",
    "    # Definir retângulo de interesse\n",
    "    nf_data_prestador = {}\n",
    "    x0 = 0\n",
    "    y0 = 100\n",
    "    x1 = 600\n",
    "    y1 = 236  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    \n",
    "    nf_data_prestador = Extc.extract_fields_prestador(text)\n",
    "    #nf_data_prestador = extract_fields_prestador(text)\n",
    "    \n",
    "    # 3. TOMADOR DE SERVIÇO\n",
    "    # Definir retângulo de interesse\n",
    "    nf_data_tomador = {}\n",
    "    x0 = 0\n",
    "    y0 = 210\n",
    "    x1 = 600\n",
    "    y1 = 340  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    \n",
    "    \n",
    "    nf_data_tomador = Extc.extract_fields_tomador(text)\n",
    "    #nf_data_tomador = extract_fields_tomador(text)\n",
    "    \n",
    "    \n",
    "    # 4. DESCRIMINACAO DOS SERVIÇOS\n",
    "    nf_data_servico = {}\n",
    "    nf_data_servico['secao'] = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 330\n",
    "    x1 = 600\n",
    "    y1 = 500  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "    # Remover quebras de linha e rótulo\n",
    "    text = text.replace('\\n', ' ')\n",
    "    label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "    if text.startswith(label):\n",
    "        text = text[len(label):].strip()\n",
    "\n",
    "    # Atribuir texto ao dicionário\n",
    "    nf_data_servico['discriminacao_servicos'] = text\n",
    "    \n",
    "    \n",
    "    # 5. VALOR TOTAL\n",
    "    nf_data_valor_total = {}\n",
    "    nf_data_valor_total['secao'] = \"5. VALOR TOTAL\"\n",
    "\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 500\n",
    "    x1 = 600\n",
    "    y1 = 550  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "    # Usar expressão regular para extrair apenas os caracteres numéricos e pontos decimais\n",
    "    valor_total_match = re.search(r'R\\$ ([\\d,.]+)', text)\n",
    "    if valor_total_match:\n",
    "        valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "        nf_data_valor_total['valor_total_nota'] = float(valor_total_sem_formatacao)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    # 6. CNAE e Item da Lista de Serviços\n",
    "    nf_data_CNAE = {}\n",
    "    nf_data_CNAE['Secao'] = \"6. CNAE e Item da Lista de Serviços\"\n",
    "\n",
    "    # Definir retângulo de interesse CNAE\n",
    "    x0 = 0\n",
    "    y0 = 530\n",
    "    x1 = 600\n",
    "    y1 = 540  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "\n",
    "    # Extrair CNAE\n",
    "    nf_data_CNAE_match = re.search(r'CNAE\\s+(.+)', text)\n",
    "    if nf_data_CNAE_match:\n",
    "        # Remove a primeira ocorrência de \"CNAE:\"\n",
    "        nf_data_CNAE_str = re.sub(r'^CNAE - ', '', text, count=1)\n",
    "        # Remover quebras de linha\n",
    "        nf_data_CNAE_str = nf_data_CNAE_str.replace('\\n', ' ')\n",
    "        nf_data_CNAE['cnae'] = nf_data_CNAE_str\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Item da Lista de Serviços    \n",
    "    # Definir retângulo de interesse - Item da Lista de Serviços\n",
    "    x0 = 0\n",
    "    y0 = 545\n",
    "    x1 = 600\n",
    "    y1 = 560  # Ajuste este valor para delimitar a região vertical    \n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))     \n",
    "        \n",
    "    # Extrair Item da Lista de Serviços\n",
    "    nf_item_lista_servicos_match = re.search(r'Item da Lista de Serviços\\s+(.+)', text)\n",
    "    if nf_item_lista_servicos_match:\n",
    "        nf_item_lista_servicos_str = re.sub(r'^Item da Lista de Serviços - ', '', text, count=1) \n",
    "        # Remover quebras de linha\n",
    "        #nf_item_lista_servicos_str = nf_item_lista_servicos_str.replace('\\n \\n', '')\n",
    "        nf_item_lista_servicos_str = nf_item_lista_servicos_str.replace('\\n', ' ')\n",
    "        nf_data_CNAE['item_lista_servicos'] = nf_item_lista_servicos_str\n",
    "        \n",
    "    \n",
    "    # 7. VALORES E IMPOSTOS\n",
    "    # Definir retângulo de interesse\n",
    "    nf_data_valores = {}\n",
    "    \n",
    "    x0 = 0\n",
    "    y0 = 550\n",
    "    x1 = 600\n",
    "    y1 = 650  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "    # Extrair campos e valores\n",
    "    nf_data_valores = Extc.extract_fields_impostos(text)\n",
    "    #nf_data_valores = extract_fields_impostos(text)\n",
    "    \n",
    "    # 8. DADOS COMPLEMENTARES\n",
    "    nf_data_dados_complementares = {}\n",
    "    nf_data_dados_complementares['secao'] = \"8. DADOS COMPLEMENTARES\"\n",
    "\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 650\n",
    "    x1 = 600\n",
    "    y1 = 680  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    # Remove a primeira ocorrência de \"Observação:\"\n",
    "    text = re.sub(r'^DADOS COMPLEMENTARES', '', text, count=1)\n",
    "    if text == \" \":\n",
    "        text = \"NONE\"\n",
    "        nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "    else:    \n",
    "        # Extrair texto dentro do retângulo\n",
    "        nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 9. OUTRAS INFORMAÇOES / CRITICAS  \n",
    "    # Definir retângulo de interesse\n",
    "    nf_data_outras_informacoes = {}\n",
    "    x0 = 0\n",
    "    y0 = 680\n",
    "    x1 = 600\n",
    "    y1 = 725  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "    # Extrair campos e valores\n",
    "    nf_data_outras_informacoes = Extc.extract_fields_outras_info(text)\n",
    "    #nf_data_outras_informacoes = extract_fields_outras_info(text)\n",
    "    \n",
    "    \n",
    "    # 10. OBSERVACOES\n",
    "    nf_data_observacao = {}\n",
    "    nf_data_observacao['secao'] = \"10. OBSERVACOES\"\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 725\n",
    "    x1 = 600\n",
    "    y1 = 760  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "    # Remove a primeira ocorrência de \"Observação:\"\n",
    "    text = re.sub(r'^Observação:', '', text, count=1)\n",
    "\n",
    "    # Remover quebras de linha\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    nf_data_observacao['observacao'] = text.strip()\n",
    "    \n",
    "    # try:\n",
    "    #     nr_nro_nf = nro_nota\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Nao encontrado nro da NF: {e}\")       \n",
    "    \n",
    "\n",
    "    return nro_nota, nf_data_cabecalho, nf_data_prestador, nf_data_tomador, nf_data_servico, nf_data_valor_total, nf_data_CNAE, nf_data_valores, nf_data_dados_complementares, nf_data_outras_informacoes, nf_data_observacao       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Processo RASTER PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pipeline_extracao_documentos/6_geral_administacao/images/processadas'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_resized_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "processo = \"raster_PDF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erros = {}\n",
    "\n",
    "# 1. Leitura recursiva de diretorios e arquivos a partir de root\n",
    "pdf_info = {}  # Dicionário para armazenar informações sobre PDFs\n",
    "\n",
    "nf_data_servico = {}#VERIFICAR\n",
    "analise_doc_nf = {} #VERIFICAR\n",
    "file_data = [] #VERIFICAR\n",
    "\n",
    "list_document_pages = []\n",
    "#nro_nota = 0\n",
    "# TEMP\n",
    "# Nome do arquivo json\n",
    "nome_formado_json = batch_name +\".json\"\n",
    "#3. path formado para nome do arquivo json\n",
    "json_file_path = os.path.join(json_path, nome_formado_json)\n",
    "#root_doc_analise = os.path.join(documentos_extracao_path, batch_name)\n",
    "#print(root_doc_analise)\n",
    "i = 1\n",
    "for root, dirs, files in os.walk(root_doc_analise):\n",
    "    dir_name = os.path.basename(root)\n",
    "    #print(dir_name)\n",
    "    for file in files:\n",
    "        \n",
    "        if file.lower().endswith('.pdf'):\n",
    "            doc2convert = file\n",
    "            document_path_1 = os.path.join(root, file)\n",
    "            pdf_document = fitz.open(document_path_1)\n",
    "            #page_number = 0  # Defina o número da página que deseja analisar\n",
    "            #page = pdf_document[page_number]\n",
    "            \n",
    "            documento_pdf = True\n",
    "            pesquisavel, metadados, paginas = is_pdf_searchable_analise(document_path_1)\n",
    "            \n",
    "            \n",
    "            if documento_pdf:\n",
    "                print(f'\\nTeste nro: {i} | doc: {file} | pdf?: {documento_pdf} | pesquisavel?: {pesquisavel} | paginas: {paginas}\\n')\n",
    "                processo = \"PDF_Raster\"\n",
    "                \n",
    "                image_2work, name_image_2work = convertResizeAnalise_1page(file, document_path_1, image_resized_path)\n",
    "                \n",
    "                secao = \"1 - CABECALHO\"\n",
    "                try:\n",
    "                    nro_nota = 0\n",
    "                    data_cabecalho = {}\n",
    "                    data_cabecalho['secao'] = secao\n",
    "                    valor_dict = {}\n",
    "                    dados_prefeitura = {}\n",
    "                    f_frame_name = \"1_frame_prefeitura_nf\"\n",
    "                    # 1. funçao basica de modelo \n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    text_splited = texto.split('\\n')\n",
    "                    \n",
    "                    valor_dict = extract_prefeitura(model, f_frame_name, text_splited)\n",
    "                    if valor_dict:\n",
    "                        dados_prefeitura.update(valor_dict)\n",
    "                    valor = {}   \n",
    "                    f_frame_name = \"1_frame_dados_nf\"\n",
    "                    dadinho_dados_nf = {}\n",
    "                    # 1. funçao basica de modelo \n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    text_splited = texto_extraido(texto)\n",
    "                    keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "                    string_pesquisa = \"Número da Nota:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "                    dadinho_dados_nf['numero_nota_fiscal'] = texto\n",
    "                    nro_nota = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Competência:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    dadinho_dados_nf['competencia'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    dadinho_dados_nf['dt_hr_emissao'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Código Verificação:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    dadinho_dados_nf['codigo_verificacao'] = texto\n",
    "                    \n",
    "                    data_cabecalho.update(dados_prefeitura)\n",
    "                    data_cabecalho.update(dadinho_dados_nf)\n",
    "                except Exception as e:\n",
    "                    erros_cabecalho = {}\n",
    "                    err_msg = f\"Erro de processo cabecalho: {e}\"\n",
    "                    erros['documento'] = file\n",
    "                    erros_cabecalho['secao'] = secao\n",
    "                    erros_cabecalho['erro'] = err_msg\n",
    "                    erros.update(erros_cabecalho)                \n",
    "               \n",
    "                \n",
    "                \n",
    "                \n",
    "                secao = \"2. PRESTADOR DE SERVIÇO\"\n",
    "                try:\n",
    "                    f_frame_name = \"2_frame_cnpj_prestador\"\n",
    "                    data_prestador = {}\n",
    "                    prestador_inscricao = {}\n",
    "                    data_prestador['secao'] = secao\n",
    "                    prestador_cnpj_value = {}\n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #text_splited = texto_extraido(texto)\n",
    "                    prestador_cnpj_value = extract_fields_prestador_cnpj(texto)\n",
    "                    if prestador_cnpj_value:\n",
    "                        data_prestador.update(prestador_cnpj_value)\n",
    "                except Exception as e:\n",
    "                    erros_cnpj_prestador = {}\n",
    "                    err_msg = (f\"Erro prestador cnpj: {e}\")\n",
    "                    erros_cnpj_prestador['secao'] = secao\n",
    "                    erros_cnpj_prestador['erro'] = err_msg\n",
    "                    erros.update(erros_cnpj_prestador)       \n",
    "                    \n",
    "                try:\n",
    "                    f_frame_name = \"2_frame_inscricao_prestador\" \n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    text_splited = texto_extraido(texto)\n",
    "                    keyword_list = ['Inscrição Municipal:', 'Inscrição Estadual:']\n",
    "                    string_pesquisa = \"Inscrição Municipal:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    prestador_inscricao['prestador_inscricao'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Inscrição Estadual:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    prestador_inscricao['inscricao_estadual'] = texto\n",
    "                    data_prestador.update(prestador_inscricao)\n",
    "                except Exception as e:\n",
    "                    erros_inscricao_prestador = {}\n",
    "                    err_msg = (f\"Erro de processo inscricao prestador: {e}\")\n",
    "                    erros_inscricao_prestador['secao'] = secao\n",
    "                    erros_inscricao_prestador['erro'] = err_msg\n",
    "                    erros.update(erros_inscricao_prestador)\n",
    "\n",
    "                try:\n",
    "                    f_frame_name = \"2_frame_dados_prestador\"\n",
    "                    prestador_dados_value = {}\n",
    "                    \n",
    "                    keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "                    string_pesquisa = \"Nome/Razão Social:\"\n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    prestador_dados_value['razao_social'] = texto\n",
    "\n",
    "                    string_pesquisa = \"Nome de Fantasia:\"\n",
    "                    #texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    #text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    prestador_dados_value['nome_fantasia'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Endereço:\"\n",
    "                    #texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    #text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    prestador_dados_value['endereco'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"E-mail:\"\n",
    "                    #texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    #text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    prestador_dados_value['email'] = texto\n",
    "                    data_prestador.update(prestador_dados_value)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Erro prestador dados: {e}\") \n",
    "                    \n",
    "                \n",
    "                secao = \"3. TOMADOR DE SERVIÇO\"\n",
    "                try:\n",
    "                    data_tomador = {}\n",
    "                    tomador_cnpj_value = {}\n",
    "                    data_tomador['secao'] = secao\n",
    "                    f_frame_name = \"3_frame_cnpj_tomador\"\n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    text_splited = texto_extraido(texto) \n",
    "                    tomador_cnpj_value = extract_fields_tomador_cnpj(texto)\n",
    "                    if tomador_cnpj_value:\n",
    "                        data_tomador.update(tomador_cnpj_value)                  \n",
    "                except Exception as e:\n",
    "                    print(f\"Erro tomador cnpj: {e}\")\n",
    "                    \n",
    "                f_frame_name = \"3_frame_inscricao_tomador\"    \n",
    "                try:\n",
    "                    data_tomador_inscricao = {}\n",
    "                    keyword_list = ['RG:', 'Inscrição Estadual:']\n",
    "                    string_pesquisa = \"RG:\"\n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    data_tomador_inscricao['rg'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Inscrição Estadual:\"\n",
    "                    #texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    data_tomador_inscricao['inscricao_estadual'] = texto\n",
    "                    data_tomador.update(data_tomador_inscricao)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro tomador inscricao: {e}\")\n",
    "                    \n",
    "                f_frame_name = \"3_frame_dados_tomador\"\n",
    "                try: \n",
    "                    data_tomador_dados = {}   \n",
    "                    keyword_list = ['Nome/Razão Social:', 'Endereço:', 'E-mail']\n",
    "                    string_pesquisa = \"Nome/Razão Social:\"\n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    data_tomador_dados['razao_social'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Endereço:\"\n",
    "                    #texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    data_tomador_dados['endereco'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"E-mail\"\n",
    "                    #texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    data_tomador_dados['email'] = texto\n",
    "                    \n",
    "                    data_tomador.update(data_tomador_dados)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro tomador dados: {e}\") \n",
    "                    \n",
    "                secao = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "                try:\n",
    "                    data_servico = {}\n",
    "                    data_servico['secao'] = secao\n",
    "                    f_frame_name = \"4_frame_descricao_totais\"\n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    text = texto.replace('\\n', ' ')\n",
    "                    label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "                    if text.startswith(label):\n",
    "                        text = text[len(label):].strip()\n",
    "                    data_servico['discriminacao_servicos'] = text \n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro descricao servico: {e}\")\n",
    "                     \n",
    "                secao = \"5. VALOR TOTAL\"\n",
    "                try:\n",
    "                    data_valor_total = {}\n",
    "                    data_valor_total['secao'] = secao\n",
    "                    f_frame_name = \"4_frame_valor_total\"   \n",
    "                    text = executa_model_frame(model, secao, f_frame_name)  \n",
    "                    valor_total_match = re.search(r'R\\$ ([\\d,.]+)', text)\n",
    "                    if valor_total_match:\n",
    "                        valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "                        data_valor_total['valor_total_nota'] = float(valor_total_sem_formatacao)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro valor total: {e}\")\n",
    "\n",
    "                secao = \"6. CNAE e Item da Lista de Serviços\"\n",
    "                try:\n",
    "                    data_CNAE = {}\n",
    "                    data_CNAE['secao'] = secao\n",
    "                    f_frame_name = \"4_frame_cnae_itens_servico\"   \n",
    "                    Texto_extraido = executa_model_frame(model, secao, f_frame_name)\n",
    "                    text_splited = Texto_extraido.split('\\n')\n",
    "                    # Processando CNAE\n",
    "                    cnae_line = [line for line in text_splited if 'CNAE' in line][0]\n",
    "                    cnae_number = int(extract_number(cnae_line))\n",
    "                    cnae_value = cnae_dict.get(cnae_number, \"Valor não encontrado\")\n",
    "                    if cnae_value == 'Valor não encontrado':\n",
    "                        cnae_value = processa_cnae_outros(cnae_line)\n",
    "                        cnae_value = cnae_value.upper()\n",
    "                        data_CNAE['cnae'] = cnae_value\n",
    "                    else:\n",
    "                        cnae_value = cnae_value.upper()\n",
    "                        cnae_value = str(cnae_number) + \" - \" + cnae_value\n",
    "                        data_CNAE['cnae'] = cnae_value\n",
    "                        data_CNAE['item_lista_servicos'] = item_servico_value\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro busca cnae: {e}\")    \n",
    "          \n",
    "                try:\n",
    "                    item_servico_line = [line for line in text_splited if 'Item da Lista de Serviços' in line][0]\n",
    "                    item_servico_number = float(extract_number(item_servico_line))\n",
    "                    item_servico_value = item_servico_dict.get(item_servico_number, \"Valor não encontrado\")\n",
    "                    item_servico_value = item_servico_value.upper()\n",
    "                    item_servico_value = str(item_servico_number) + \" - \" + item_servico_value\n",
    "                    data_CNAE['item_lista_servicos'] = item_servico_value\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro busca Itens de servico: {e}\")  \n",
    "\n",
    "                secao = \"8. DADOS COMPLEMENTARES\"\n",
    "                try:\n",
    "                    data_valores = {}\n",
    "                    data_valores['secao'] = secao\n",
    "                    f_frame_name = \"5_frame_valores_impostos\"   \n",
    "                    \n",
    "                    result = extract_fields_box(model, f_frame_name, secao)\n",
    "                    if result:\n",
    "                        data_valores.update(result)\n",
    "            \n",
    "                    # secao: 8 - DADOS COMPLEMENTARES\"\n",
    "                    data_dados_complementares = {}\n",
    "                    f_frame_name  = \"5_frame_dados_complementares\"\n",
    "                    section = \"8. DADOS COMPLEMENTARES\"\n",
    "\n",
    "                    data_dados_complementares = extract_dados_comple_obs(model, f_frame_name, section)                                           \n",
    "                                            \n",
    "                                            \n",
    "                    # secao: 9 - OUTRAS INFORMAÇOES / CRITICAS\n",
    "                    data_outras_informacoes = {}\n",
    "                    father_value = \"5_frame_inf_criticas\"\n",
    "                    section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "                    \n",
    "                    result = extract_fb_outras_inf(model, father_value, section)\n",
    "                    if result:\n",
    "                        data_outras_informacoes.update(result)                        \n",
    "                                        \n",
    "                    # secao: 10. OBSERVACOES\n",
    "                    data_observacao = {}\n",
    "                    f_father = \"5_frame_observacao\"\n",
    "                    section = \"10. OBSERVACOES\"\n",
    "\n",
    "                    data_observacao = extract_dados_comple_obs(model, f_father, section)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro valores complementares: {e}\")   \n",
    "                \n",
    "                nome_arquivo = file\n",
    "                #pdf_info[\"diretorio\"] = os.path.basename(root)\n",
    "                try:\n",
    "                    pdf_info[nro_nota] = {\n",
    "                    \"dados_NF_PDF\": {\n",
    "                        \"data_cabecalho\": data_cabecalho,\n",
    "                        \"data_prestador\": data_prestador,\n",
    "                        \"data_tomador\": data_tomador,\n",
    "                        \"data_servico\": data_servico,\n",
    "                        \"data_valor_total\": data_valor_total,\n",
    "                        \"data_CNAE\": data_CNAE,\n",
    "                        \"data_valores\": data_valores,\n",
    "                        \"data_dados_complementares\": data_dados_complementares,\n",
    "                        \"data_outras_informacoes\": data_outras_informacoes,\n",
    "                        \"data_observacao\": data_observacao,\n",
    "                    },\n",
    "                    \"diretorio\": dir_name, #os.path.basename(root)\n",
    "                    \"nome_arquivo\": nome_arquivo,\n",
    "                    \"Batch\": batch_name,\n",
    "                    \"modelo\": model,\n",
    "                    \"processo\": processo,\n",
    "                }\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao gerar o json: {e}\")\n",
    "                pdf_document.close()\n",
    "                \n",
    "                #print(pdf_info)\n",
    "                #if paginas == 1:\n",
    "                if paginas > 1000:\n",
    "                    if i == 1000: #Define quantidade de tratamento de documentos raster PDF\n",
    "                        break\n",
    "            i +=1 \n",
    "                \n",
    "# Nome do arquivo json\n",
    "nome_formado_json = batch_name + \"_\" + processo + \".json\"\n",
    "\n",
    "json_file_path = os.path.join(json_path, nome_formado_json )\n",
    "\n",
    "\n",
    "# Salvando as informações em um arquivo JSON (novo formato nome arquivo V2)\n",
    "with open(json_file_path, \"w\", encoding='utf-8') as json_file:\n",
    "    json.dump(pdf_info, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"As informações foram salvas em {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Processo PDF Pesquisavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_16'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_doc_analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "no such file: 'pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_16/mesquita_pdf_31282023_2258/138565/B4066C58-F309-42E4-A992-55EB8961211E.PDF'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32751/2273306668.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m#page = pdf_document[page_number]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mdocumento_pdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"O PDF é pesquisável\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Carregar o arquivo PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mpdf_document\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Página do PDF  ATENCAO  (UNICA PAGINA)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mpage_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# Defina o número da página que deseja analisar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tables-detr/lib/python3.10/site-packages/fitz/fitz.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[0m\n\u001b[1;32m   3949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3950\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfrom_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3951\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3952\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"no such file: '{filename}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3953\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3954\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3955\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"'{filename}' is no file\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3956\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mFileDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: no such file: 'pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_16/mesquita_pdf_31282023_2258/138565/B4066C58-F309-42E4-A992-55EB8961211E.PDF'"
     ]
    }
   ],
   "source": [
    "processo = \"pdf_pesquisavel\"\n",
    "\n",
    "erros = {}\n",
    "# 1. Leitura recursiva de diretorios e arquivos a partir de root\n",
    "pdf_info = {}  # Dicionário para armazenar informações sobre PDFs\n",
    "\n",
    "\n",
    "i = 1\n",
    "for root, dirs, files in os.walk(root_doc_analise):\n",
    "    dir_name = os.path.basename(root)\n",
    "    #print(dir_name)\n",
    "    for file in files:\n",
    "        \n",
    "        if file.lower().endswith('.pdf'):\n",
    "            doc2convert = file\n",
    "            document_path_1 = os.path.join(root, file)\n",
    "            pdf_document = fitz.open(document_path_1)\n",
    "            #page_number = 0  # Defina o número da página que deseja analisar\n",
    "            #page = pdf_document[page_number]\n",
    "            documento_pdf = True\n",
    "            status = \"O PDF é pesquisável\"\n",
    "            # Carregar o arquivo PDF\n",
    "            pdf_document = fitz.open(file_path)\n",
    "\n",
    "            # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "            page_number = 0  # Defina o número da página que deseja analisar\n",
    "            page = pdf_document[page_number]\n",
    "\n",
    "            # Definir retângulo de interesse\n",
    "            x0 = 0\n",
    "            y0 = 4\n",
    "            x1 = 600\n",
    "            y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "            # Extrair texto dentro do retângulo\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            \n",
    "            print(text)\n",
    "            \n",
    "        \n",
    "            if text:\n",
    "                page_number = 0\n",
    "                print(page_number)\n",
    "            else:\n",
    "                page_number = 1\n",
    "                print(page_number)\n",
    "            \n",
    "            nf_data_cabecalho = {}\n",
    "            \n",
    "            page = pdf_document[page_number]\n",
    "            x0 = 0\n",
    "            y0 = 0\n",
    "            x1 = 600\n",
    "            y1 = 110\n",
    "\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            nf_data_cabecalho = Extc.extract_fields_cabecalho(text)\n",
    "            #nf_data_cabecalho = extract_fields_cabecalho(text)\n",
    "            \n",
    "            nro_nota = nf_data_cabecalho['numero_nota_fiscal']\n",
    "\n",
    "            # 2. PRESTADOR DE SERVIÇO\n",
    "            # Definir retângulo de interesse\n",
    "            nf_data_prestador = {}\n",
    "            x0 = 0\n",
    "            y0 = 100\n",
    "            x1 = 600\n",
    "            y1 = 236  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "            # Extrair texto dentro do retângulo\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            \n",
    "            nf_data_prestador = Extc.extract_fields_prestador(text)\n",
    "            #nf_data_prestador = extract_fields_prestador(text)\n",
    "            \n",
    "            # 3. TOMADOR DE SERVIÇO\n",
    "            # Definir retângulo de interesse\n",
    "            nf_data_tomador = {}\n",
    "            x0 = 0\n",
    "            y0 = 210\n",
    "            x1 = 600\n",
    "            y1 = 340  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "            # Extrair texto dentro do retângulo\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            \n",
    "            \n",
    "            nf_data_tomador = Extc.extract_fields_tomador(text)\n",
    "            #nf_data_tomador = extract_fields_tomador(text)\n",
    "            \n",
    "            \n",
    "            # 4. DESCRIMINACAO DOS SERVIÇOS\n",
    "            nf_data_servico = {}\n",
    "            nf_data_servico['secao'] = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "            # Definir retângulo de interesse\n",
    "            x0 = 0\n",
    "            y0 = 330\n",
    "            x1 = 600\n",
    "            y1 = 500  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "            # Extrair texto dentro do retângulo\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "            # Remover quebras de linha e rótulo\n",
    "            text = text.replace('\\n', ' ')\n",
    "            label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "            if text.startswith(label):\n",
    "                text = text[len(label):].strip()\n",
    "\n",
    "            # Atribuir texto ao dicionário\n",
    "            nf_data_servico['discriminacao_servicos'] = text\n",
    "            \n",
    "            \n",
    "            # 5. VALOR TOTAL\n",
    "            nf_data_valor_total = {}\n",
    "            nf_data_valor_total['secao'] = \"5. VALOR TOTAL\"\n",
    "\n",
    "            # Definir retângulo de interesse\n",
    "            x0 = 0\n",
    "            y0 = 500\n",
    "            x1 = 600\n",
    "            y1 = 550  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "\n",
    "            # Extrair texto dentro do retângulo\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "            # Usar expressão regular para extrair apenas os caracteres numéricos e pontos decimais\n",
    "            valor_total_match = re.search(r'R\\$ ([\\d,.]+)', text)\n",
    "            if valor_total_match:\n",
    "                valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "                nf_data_valor_total['valor_total_nota'] = float(valor_total_sem_formatacao)\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            # 6. CNAE e Item da Lista de Serviços\n",
    "            nf_data_CNAE = {}\n",
    "            nf_data_CNAE['Secao'] = \"6. CNAE e Item da Lista de Serviços\"\n",
    "\n",
    "            # Definir retângulo de interesse CNAE\n",
    "            x0 = 0\n",
    "            y0 = 530\n",
    "            x1 = 600\n",
    "            y1 = 540  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "\n",
    "            # Extrair texto dentro do retângulo\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "\n",
    "            # Extrair CNAE\n",
    "            nf_data_CNAE_match = re.search(r'CNAE\\s+(.+)', text)\n",
    "            if nf_data_CNAE_match:\n",
    "                # Remove a primeira ocorrência de \"CNAE:\"\n",
    "                nf_data_CNAE_str = re.sub(r'^CNAE - ', '', text, count=1)\n",
    "                # Remover quebras de linha\n",
    "                nf_data_CNAE_str = nf_data_CNAE_str.replace('\\n', ' ')\n",
    "                nf_data_CNAE['cnae'] = nf_data_CNAE_str\n",
    "                \n",
    "            \n",
    "            \n",
    "            # Item da Lista de Serviços    \n",
    "            # Definir retângulo de interesse - Item da Lista de Serviços\n",
    "            x0 = 0\n",
    "            y0 = 545\n",
    "            x1 = 600\n",
    "            y1 = 560  # Ajuste este valor para delimitar a região vertical    \n",
    "            # Extrair texto dentro do retângulo\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))     \n",
    "                \n",
    "            # Extrair Item da Lista de Serviços\n",
    "            nf_item_lista_servicos_match = re.search(r'Item da Lista de Serviços\\s+(.+)', text)\n",
    "            if nf_item_lista_servicos_match:\n",
    "                nf_item_lista_servicos_str = re.sub(r'^Item da Lista de Serviços - ', '', text, count=1) \n",
    "                # Remover quebras de linha\n",
    "                #nf_item_lista_servicos_str = nf_item_lista_servicos_str.replace('\\n \\n', '')\n",
    "                nf_item_lista_servicos_str = nf_item_lista_servicos_str.replace('\\n', ' ')\n",
    "                nf_data_CNAE['item_lista_servicos'] = nf_item_lista_servicos_str\n",
    "                \n",
    "            \n",
    "            # 7. VALORES E IMPOSTOS\n",
    "            # Definir retângulo de interesse\n",
    "            nf_data_valores = {}\n",
    "            \n",
    "            x0 = 0\n",
    "            y0 = 550\n",
    "            x1 = 600\n",
    "            y1 = 650  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "            # Extrair texto dentro do retângulo\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "            # Extrair campos e valores\n",
    "            nf_data_valores = Extc.extract_fields_impostos(text)\n",
    "            #nf_data_valores = extract_fields_impostos(text)\n",
    "            \n",
    "            # 8. DADOS COMPLEMENTARES\n",
    "            nf_data_dados_complementares = {}\n",
    "            nf_data_dados_complementares['secao'] = \"8. DADOS COMPLEMENTARES\"\n",
    "\n",
    "            # Definir retângulo de interesse\n",
    "            x0 = 0\n",
    "            y0 = 650\n",
    "            x1 = 600\n",
    "            y1 = 680  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "            # Extrair texto dentro do retângulo\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            # Remove a primeira ocorrência de \"Observação:\"\n",
    "            text = re.sub(r'^DADOS COMPLEMENTARES', '', text, count=1)\n",
    "            if text == \" \":\n",
    "                text = \"NONE\"\n",
    "                nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "            else:    \n",
    "                # Extrair texto dentro do retângulo\n",
    "                nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "            \n",
    "            \n",
    "            \n",
    "            # 9. OUTRAS INFORMAÇOES / CRITICAS  \n",
    "            # Definir retângulo de interesse\n",
    "            nf_data_outras_informacoes = {}\n",
    "            x0 = 0\n",
    "            y0 = 680\n",
    "            x1 = 600\n",
    "            y1 = 725  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "            # Extrair texto dentro do retângulo\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "            # Extrair campos e valores\n",
    "            nf_data_outras_informacoes = Extc.extract_fields_outras_info(text)\n",
    "            #nf_data_outras_informacoes = extract_fields_outras_info(text)\n",
    "            \n",
    "            \n",
    "            # 10. OBSERVACOES\n",
    "            nf_data_observacao = {}\n",
    "            nf_data_observacao['secao'] = \"10. OBSERVACOES\"\n",
    "            # Definir retângulo de interesse\n",
    "            x0 = 0\n",
    "            y0 = 725\n",
    "            x1 = 600\n",
    "            y1 = 760  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "            # Extrair texto dentro do retângulo\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "            # Remove a primeira ocorrência de \"Observação:\"\n",
    "            text = re.sub(r'^Observação:', '', text, count=1)\n",
    "\n",
    "            # Remover quebras de linha\n",
    "            text = text.replace('\\n', ' ')\n",
    "\n",
    "            # Extrair texto dentro do retângulo\n",
    "            nf_data_observacao['observacao'] = text.strip()\n",
    "    \n",
    "            nome_arquivo = file\n",
    "\n",
    "            print(f\"Erro ao gerar o json: {e}\")\n",
    "                #print(pdf_info) \n",
    "            \n",
    "\n",
    "            pdf_info[nro_nota] = {\n",
    "                    \"dados_NF_PDF\": {\n",
    "                    \"data_cabecalho\": data_cabecalho_final,\n",
    "                    \"data_prestador\": data_prestador_final,\n",
    "                    \"data_tomador\": data_tomador_final,\n",
    "                    \"data_servico\": data_servico_final,\n",
    "                    \"data_valor_total\": data_valor_total_final,\n",
    "                    \"data_CNAE\": data_CNAE_final,\n",
    "                    \"data_valores\": data_valores_final,\n",
    "                    \"data_dados_complementares\": data_dados_complementares_final,\n",
    "                    \"data_outras_informacoes\": data_outras_informacoes_final,\n",
    "                    \"data_observacao\": data_observacao_final,\n",
    "                },\n",
    "                \"diretorio\": dir_name, #os.path.basename(root)\n",
    "                \"nome_arquivo\": nome_arquivo,\n",
    "                \"Batch\": batch_name,\n",
    "                \"modelo\": model,\n",
    "                \"processo\": processo,\n",
    "            }\n",
    "\n",
    "                                            \n",
    "            pdf_document.close()    \n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "            print(\"i: \", i) \n",
    "\n",
    "# Nome do arquivo json\n",
    "nome_formado_json = batch_name + \"_\" + processo + \".json\"\n",
    "\n",
    "json_file_path = os.path.join(json_path, nome_formado_json )\n",
    "\n",
    "\n",
    "# Salvando as informações em um arquivo JSON (novo formato nome arquivo V2)\n",
    "with open(json_file_path, \"w\", encoding='utf-8') as json_file:\n",
    "    json.dump(pdf_info, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"As informações foram salvas em {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desenha Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_image_2work = \"pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas/doria_marinho_0295_carlos_leandro.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trattempl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/dani-boy/extractNF/2_process_pipe_unificado_V0.ipynb Cell 65\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/dani-boy/extractNF/2_process_pipe_unificado_V0.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# To draw everything\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/dani-boy/extractNF/2_process_pipe_unificado_V0.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trattempl\u001b[39m.\u001b[39m draw_box_model(model, boundaries_info, sections_info, frames_info, field_boxes_info)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trattempl' is not defined"
     ]
    }
   ],
   "source": [
    "# To draw everything\n",
    "trattempl. draw_box_model(model, boundaries_info, sections_info, frames_info, field_boxes_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To draw only boundaries and sections:\n",
    "draw_box_model(modelo, boundaries_info, sections_info, draw_frames=False, draw_field_boxes=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To draw only field boxes:\n",
    "draw_box_model(modelo, field_boxes_info=field_boxes_info, draw_boundaries=False, draw_sections=False, draw_frames=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.DataFrame([data_prestador_final])\n",
    "# df3 = pd.DataFrame(rowlist_tomador)\n",
    "# df4 = pd.DataFrame(rowlist_servico)\n",
    "# df5 = pd.DataFrame(rowlist_valor_total)\n",
    "# df6 = pd.DataFrame(rowlist_CNAE)\n",
    "# df7 = pd.DataFrame(rowlist_valores)\n",
    "# df8 = pd.DataFrame(rowlist_dados_complementares)\n",
    "# df9 = pd.DataFrame(rowlist_outras_informacoes)\n",
    "# df10 = pd.DataFrame(rowlist_observacao)\n",
    "# df_processamento = pd.concat([df1, df2 ], axis=1)  \n",
    "# df_processamento = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10], axis=1)  \n",
    "\n",
    "            # new_row_prestador_final = data_prestador_final\n",
    "            # new_row_cabecalho.append(data_cabecalho_final) \n",
    "            \n",
    "            # new_row_tomador = data_tomador_final\n",
    "            # rowlist_tomador.append(new_row_tomador)\n",
    "            \n",
    "            # new_row_servico = data_servico_final\n",
    "            # rowlist_servico.append(new_row_servico)\n",
    "            \n",
    "            # new_row_valor_total = data_valor_total_final\n",
    "            # rowlist_valor_total.append(new_row_valor_total) \n",
    "            \n",
    "            # new_row_CNAE = data_CNAE_final\n",
    "            # rowlist_CNAE.append(new_row_CNAE)   \n",
    "            \n",
    "            # new_row_valores = data_valores_final\n",
    "            # rowlist_valores.append(new_row_valores)\n",
    "            \n",
    "            # new_row_dados_complementares = data_dados_complementares_final\n",
    "            # rowlist_dados_complementares.append(new_row_dados_complementares)   \n",
    "            \n",
    "            # new_row_outras_informacoes = data_outras_informacoes_final\n",
    "            # rows_list.append(new_row_outras_informacoes)\n",
    "            \n",
    "            # new_row_observacao = data_observacao_final\n",
    "            # rows_list.append(new_row_observacao)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nf_data_servico = {}#VERIFICAR\n",
    "# analise_doc_nf = {} #VERIFICAR\n",
    "# file_data = [] #VERIFICAR\n",
    "list_document_pages = []\n",
    "#nro_nota = 0\n",
    "# # TEMP\n",
    "# # Nome do arquivo json\n",
    "# nome_formado_json = batch_name +\".json\"\n",
    "#3. path formado para nome do arquivo json\n",
    "# json_file_path = os.path.join(json_path, nome_formado_json)\n",
    "#root_doc_analise = os.path.join(documentos_extracao_path, batch_name)\n",
    "#print(root_doc_analise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.extrai_pdf_pesquisavel as Extc\n",
    "import modules.cronometro as cron\n",
    "import modules.trata_dicionarios as tratdic\n",
    "import modules.trata_imagem as tratimg\n",
    "import modules.trata_files as tratfiles\n",
    "import modules.trata_texto_extraido as trattext\n",
    "import modules.trata_template as trattemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    # secao = \"1 - CABECALHO\"\n",
    "                    # try:\n",
    "                    #     nro_nota = 0\n",
    "                    #     data_cabecalho = {}\n",
    "                    #     data_cabecalho['secao'] = secao\n",
    "                    #     valor_dict = {}\n",
    "                    #     dados_prefeitura = {}\n",
    "                    #     f_frame_name = \"1_frame_prefeitura_nf\"\n",
    "                    #     # 1. funçao basica de modelo \n",
    "                    #     texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    #     text_splited = texto.split('\\n')\n",
    "                        \n",
    "                    #     valor_dict = extract_prefeitura(model, f_frame_name, text_splited)\n",
    "                    #     if valor_dict:\n",
    "                    #         dados_prefeitura.update(valor_dict)\n",
    "                    #     valor = {}   \n",
    "                    #     f_frame_name = \"1_frame_dados_nf\"\n",
    "                    #     dadinho_dados_nf = {}\n",
    "                    #     # 1. funçao basica de modelo \n",
    "                    #     texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #     text_splited = texto_extraido(texto)\n",
    "                    #     keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "                    #     string_pesquisa = \"Número da Nota:\"\n",
    "                    #     texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "                    #     dadinho_dados_nf['numero_nota_fiscal'] = texto\n",
    "                    #     nro_nota = texto\n",
    "                        \n",
    "                    #     string_pesquisa = \"Competência:\"\n",
    "                    #     texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    #     dadinho_dados_nf['competencia'] = texto\n",
    "                        \n",
    "                    #     string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "                    #     texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    #     dadinho_dados_nf['dt_hr_emissao'] = texto\n",
    "                        \n",
    "                    #     string_pesquisa = \"Código Verificação:\"\n",
    "                    #     texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    #     dadinho_dados_nf['codigo_verificacao'] = texto\n",
    "                        \n",
    "                    #     data_cabecalho.update(dados_prefeitura)\n",
    "                    #     data_cabecalho.update(dadinho_dados_nf)\n",
    "                    # except Exception as e:\n",
    "                    #     erros_cabecalho = {}\n",
    "                    #     err_msg = f\"Erro de processo cabecalho: {e}\"\n",
    "                    #     erros['documento'] = file\n",
    "                    #     erros_cabecalho['secao'] = secao\n",
    "                    #     erros_cabecalho['erro'] = err_msg\n",
    "                    #     erros.update(erros_cabecalho)                \n",
    "                \n",
    "                    # secao = \"2. PRESTADOR DE SERVIÇO\"\n",
    "                    # try:\n",
    "                    #     f_frame_name = \"2_frame_cnpj_prestador\"\n",
    "                    #     data_prestador = {}\n",
    "                    #     prestador_inscricao = {}\n",
    "                    #     data_prestador['secao'] = secao\n",
    "                    #     prestador_cnpj_value = {}\n",
    "                    #     texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #     #text_splited = texto_extraido(texto)\n",
    "                    #     prestador_cnpj_value = extract_fields_prestador_cnpj(texto)\n",
    "                    #     if prestador_cnpj_value:\n",
    "                    #         data_prestador.update(prestador_cnpj_value)\n",
    "                    # except Exception as e:\n",
    "                    #     erros_cnpj_prestador = {}\n",
    "                    #     err_msg = (f\"Erro prestador cnpj: {e}\")\n",
    "                    #     erros_cnpj_prestador['secao'] = secao\n",
    "                    #     erros_cnpj_prestador['erro'] = err_msg\n",
    "                    #     erros.update(erros_cnpj_prestador)       \n",
    "                        \n",
    "                    # try:\n",
    "                    #     f_frame_name = \"2_frame_inscricao_prestador\" \n",
    "                    #     texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #     text_splited = texto_extraido(texto)\n",
    "                    #     keyword_list = ['Inscrição Municipal:', 'Inscrição Estadual:']\n",
    "                    #     string_pesquisa = \"Inscrição Municipal:\"\n",
    "                    #     texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    #     prestador_inscricao['prestador_inscricao'] = texto\n",
    "                        \n",
    "                    #     string_pesquisa = \"Inscrição Estadual:\"\n",
    "                    #     texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    #     prestador_inscricao['inscricao_estadual'] = texto\n",
    "                    #     data_prestador.update(prestador_inscricao)\n",
    "                    # except Exception as e:\n",
    "                    #     erros_inscricao_prestador = {}\n",
    "                    #     err_msg = (f\"Erro de processo inscricao prestador: {e}\")\n",
    "                    #     erros_inscricao_prestador['secao'] = secao\n",
    "                    #     erros_inscricao_prestador['erro'] = err_msg\n",
    "                    #     erros.update(erros_inscricao_prestador)\n",
    "\n",
    "                    # try:\n",
    "                    #     f_frame_name = \"2_frame_dados_prestador\"\n",
    "                    #     prestador_dados_value = {}\n",
    "                        \n",
    "                    #     keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "                    #     string_pesquisa = \"Nome/Razão Social:\"\n",
    "                    #     texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    #     text_splited = texto_extraido(texto)\n",
    "                    #     texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    #     prestador_dados_value['razao_social'] = texto\n",
    "\n",
    "                    #     string_pesquisa = \"Nome de Fantasia:\"\n",
    "                    #     #texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    #     #text_splited = texto_extraido(texto)\n",
    "                    #     texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    #     prestador_dados_value['nome_fantasia'] = texto\n",
    "                        \n",
    "                    #     string_pesquisa = \"Endereço:\"\n",
    "                    #     #texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    #     #text_splited = texto_extraido(texto)\n",
    "                    #     texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    #     prestador_dados_value['endereco'] = texto\n",
    "                        \n",
    "                    #     string_pesquisa = \"E-mail:\"\n",
    "                    #     #texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    #     #text_splited = texto_extraido(texto)\n",
    "                    #     texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    #     prestador_dados_value['email'] = texto\n",
    "                    #     data_prestador.update(prestador_dados_value)\n",
    "                        \n",
    "                    # except Exception as e:\n",
    "                    #     print(f\"Erro prestador dados: {e}\") \n",
    "                        \n",
    "                    \n",
    "                    # secao = \"3. TOMADOR DE SERVIÇO\"\n",
    "                    # try:\n",
    "                    #     data_tomador = {}\n",
    "                    #     tomador_cnpj_value = {}\n",
    "                    #     data_tomador['secao'] = secao\n",
    "                    #     f_frame_name = \"3_frame_cnpj_tomador\"\n",
    "                    #     texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #     text_splited = texto_extraido(texto) \n",
    "                    #     tomador_cnpj_value = extract_fields_tomador_cnpj(texto)\n",
    "                    #     if tomador_cnpj_value:\n",
    "                    #         data_tomador.update(tomador_cnpj_value)                  \n",
    "                    # except Exception as e:\n",
    "                    #     print(f\"Erro tomador cnpj: {e}\")\n",
    "                        \n",
    "                    # f_frame_name = \"3_frame_inscricao_tomador\"    \n",
    "                    # try:\n",
    "                    #     data_tomador_inscricao = {}\n",
    "                    #     keyword_list = ['RG:', 'Inscrição Estadual:']\n",
    "                    #     string_pesquisa = \"RG:\"\n",
    "                    #     texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #     text_splited = texto_extraido(texto)\n",
    "                    #     texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    #     data_tomador_inscricao['rg'] = texto\n",
    "                        \n",
    "                    #     string_pesquisa = \"Inscrição Estadual:\"\n",
    "                    #     #texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #     #text_splited = texto_extraido(texto)\n",
    "                    #     texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    #     data_tomador_inscricao['inscricao_estadual'] = texto\n",
    "                    #     data_tomador.update(data_tomador_inscricao)\n",
    "                    # except Exception as e:\n",
    "                    #     print(f\"Erro tomador inscricao: {e}\")\n",
    "                        \n",
    "                    # f_frame_name = \"3_frame_dados_tomador\"\n",
    "                    # try: \n",
    "                    #     data_tomador_dados = {}   \n",
    "                    #     keyword_list = ['Nome/Razão Social:', 'Endereço:', 'E-mail']\n",
    "                    #     string_pesquisa = \"Nome/Razão Social:\"\n",
    "                    #     texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #     text_splited = texto_extraido(texto)\n",
    "                    #     texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    #     data_tomador_dados['razao_social'] = texto\n",
    "                        \n",
    "                    #     string_pesquisa = \"Endereço:\"\n",
    "                    #     #texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #     #text_splited = texto_extraido(texto)\n",
    "                    #     texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    #     data_tomador_dados['endereco'] = texto\n",
    "                        \n",
    "                    #     string_pesquisa = \"E-mail\"\n",
    "                    #     #texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #     #text_splited = texto_extraido(texto)\n",
    "                    #     texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    #     data_tomador_dados['email'] = texto\n",
    "                        \n",
    "                    #     data_tomador.update(data_tomador_dados)\n",
    "                    # except Exception as e:\n",
    "                    #     print(f\"Erro tomador dados: {e}\") \n",
    "                        \n",
    "                    # secao = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "                    # try:\n",
    "                    #     data_servico = {}\n",
    "                    #     data_servico['secao'] = secao\n",
    "                    #     f_frame_name = \"4_frame_descricao_totais\"\n",
    "                    #     texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    #     text = texto.replace('\\n', ' ')\n",
    "                    #     label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "                    #     if text.startswith(label):\n",
    "                    #         text = text[len(label):].strip()\n",
    "                    #     data_servico['discriminacao_servicos'] = text \n",
    "\n",
    "                    # except Exception as e:\n",
    "                    #     print(f\"Erro descricao servico: {e}\")\n",
    "                        \n",
    "                    # secao = \"5. VALOR TOTAL\"\n",
    "                    # try:\n",
    "                    #     data_valor_total = {}\n",
    "                    #     data_valor_total['secao'] = secao\n",
    "                    #     f_frame_name = \"4_frame_valor_total\"   \n",
    "                    #     text = executa_model_frame(model, secao, f_frame_name)  \n",
    "                    #     valor_total_match = re.search(r'R\\$ ([\\d,.]+)', text)\n",
    "                    #     if valor_total_match:\n",
    "                    #         valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "                    #         data_valor_total['valor_total_nota'] = float(valor_total_sem_formatacao)\n",
    "                    # except Exception as e:\n",
    "                    #     print(f\"Erro valor total: {e}\")\n",
    "\n",
    "                    # secao = \"6. CNAE e Item da Lista de Serviços\"\n",
    "                    # try:\n",
    "                    #     data_CNAE = {}\n",
    "                    #     data_CNAE['secao'] = secao\n",
    "                    #     f_frame_name = \"4_frame_cnae_itens_servico\"   \n",
    "                    #     Texto_extraido = executa_model_frame(model, secao, f_frame_name)\n",
    "                    #     text_splited = Texto_extraido.split('\\n')\n",
    "                    #     # Processando CNAE\n",
    "                    #     cnae_line = [line for line in text_splited if 'CNAE' in line][0]\n",
    "                    #     cnae_number = int(extract_number(cnae_line))\n",
    "                    #     cnae_value = cnae_dict.get(cnae_number, \"Valor não encontrado\")\n",
    "                    #     if cnae_value == 'Valor não encontrado':\n",
    "                    #         cnae_value = processa_cnae_outros(cnae_line)\n",
    "                    #         cnae_value = cnae_value.upper()\n",
    "                    #         data_CNAE['cnae'] = cnae_value\n",
    "                    #     else:\n",
    "                    #         cnae_value = cnae_value.upper()\n",
    "                    #         cnae_value = str(cnae_number) + \" - \" + cnae_value\n",
    "                    #         data_CNAE['cnae'] = cnae_value\n",
    "                    #         data_CNAE['item_lista_servicos'] = item_servico_value\n",
    "                    # except Exception as e:\n",
    "                    #     print(f\"Erro busca cnae: {e}\")    \n",
    "            \n",
    "                    # try:\n",
    "                    #     item_servico_line = [line for line in text_splited if 'Item da Lista de Serviços' in line][0]\n",
    "                    #     item_servico_number = float(extract_number(item_servico_line))\n",
    "                    #     item_servico_value = item_servico_dict.get(item_servico_number, \"Valor não encontrado\")\n",
    "                    #     item_servico_value = item_servico_value.upper()\n",
    "                    #     item_servico_value = str(item_servico_number) + \" - \" + item_servico_value\n",
    "                    #     data_CNAE['item_lista_servicos'] = item_servico_value\n",
    "                    # except Exception as e:\n",
    "                    #     print(f\"Erro busca Itens de servico: {e}\")  \n",
    "\n",
    "                    # secao = \"8. DADOS COMPLEMENTARES\"\n",
    "                    # try:\n",
    "                    #     data_valores = {}\n",
    "                    #     data_valores['secao'] = secao\n",
    "                    #     f_frame_name = \"5_frame_valores_impostos\"   \n",
    "                        \n",
    "                    #     result = extract_fields_box(model, f_frame_name, secao)\n",
    "                    #     if result:\n",
    "                    #         data_valores.update(result)\n",
    "                \n",
    "                    #     # secao: 8 - DADOS COMPLEMENTARES\"\n",
    "                    #     data_dados_complementares = {}\n",
    "                    #     f_frame_name  = \"5_frame_dados_complementares\"\n",
    "                    #     section = \"8. DADOS COMPLEMENTARES\"\n",
    "\n",
    "                    #     data_dados_complementares = extract_dados_comple_obs(model, f_frame_name, section)                                           \n",
    "                                                \n",
    "                                                \n",
    "                    #     # secao: 9 - OUTRAS INFORMAÇOES / CRITICAS\n",
    "                    #     data_outras_informacoes = {}\n",
    "                    #     father_value = \"5_frame_inf_criticas\"\n",
    "                    #     section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "                        \n",
    "                    #     result = extract_fb_outras_inf(model, father_value, section)\n",
    "                    #     if result:\n",
    "                    #         data_outras_informacoes.update(result)                        \n",
    "                                            \n",
    "                    #     # secao: 10. OBSERVACOES\n",
    "                    #     data_observacao = {}\n",
    "                    #     f_father = \"5_frame_observacao\"\n",
    "                    #     section = \"10. OBSERVACOES\"\n",
    "\n",
    "                    #     data_observacao = extract_dados_comple_obs(model, f_father, section)\n",
    "                    # except Exception as e:\n",
    "                    #     print(f\"Erro valores complementares: {e}\")   \n",
    "                    \n",
    "                    # nome_arquivo = file\n",
    "                    # #pdf_info[\"diretorio\"] = os.path.basename(root)\n",
    "                    # try:\n",
    "                    #     pdf_info[nro_nota] = {\n",
    "                    #     \"dados_NF_PDF\": {\n",
    "                    #         \"data_cabecalho\": data_cabecalho,\n",
    "                    #         \"data_prestador\": data_prestador,\n",
    "                    #         \"data_tomador\": data_tomador,\n",
    "                    #         \"data_servico\": data_servico,\n",
    "                    #         \"data_valor_total\": data_valor_total,\n",
    "                    #         \"data_CNAE\": data_CNAE,\n",
    "                    #         \"data_valores\": data_valores,\n",
    "                    #         \"data_dados_complementares\": data_dados_complementares,\n",
    "                    #         \"data_outras_informacoes\": data_outras_informacoes,\n",
    "                    #         \"data_observacao\": data_observacao,\n",
    "                    #     },\n",
    "                    #     \"diretorio\": dir_name, #os.path.basename(root)\n",
    "                    #     \"nome_arquivo\": nome_arquivo,\n",
    "                    #     \"Batch\": batch_name,\n",
    "                    #     \"modelo\": model,\n",
    "                    # }\n",
    "                    # except Exception as e:\n",
    "                    #     print(f\"Erro ao gerar o json: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IMPORTANTE - NRO BATCH PARA TESTE    0 = PDF_PESQUISAVEL | 1 = RASTER_PDF\n",
    "\n",
    "i_test = 1\n",
    "\n",
    "tipo_pdf = []\n",
    "tipo_pdf.append('PDF_PESQUISAVEL')\n",
    "tipo_pdf.append('RASTER_PDF')\n",
    "tipo_pdf[i_test]\n",
    "\n",
    "\n",
    "# Tratamento do Path de ORIGEM DO DOCUMENTOS PARA TESTE QUE SERAO MOVIDOS\n",
    "list_path_test = []\n",
    "list_path_test.append(\"pipeline_extracao_documentos/4_area_testes/pdf_pesquisavel_4_test\")\n",
    "list_path_test.append(\"pipeline_extracao_documentos/4_area_testes/raster_pdf_4_test\")\n",
    "list_path_test[i_test]\n",
    "\n",
    "# Frame para teste\n",
    "i_frame = 0\n",
    "\n",
    "frames_pesquisa = []\n",
    "# Filtrar o DataFrame para incluir apenas linhas onde a coluna \"model\" oriundo de: modelo\n",
    "filtered_frames_info = frames_info[frames_info['model'] == model]\n",
    "for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "    frame_name = row_frame['label']\n",
    "    frames_pesquisa.append(frame_name)\n",
    "\n",
    "# Nome Batch\n",
    "batch_name = \"Batch_\" + str(tipo_pdf[i_test]) + \"_\" + str(i_frame)\n",
    "\n",
    "# Nome do arquivo json\n",
    "nome_formado_json = batch_name +\".json\"\n",
    "\n",
    "# Listagem dos frames de pesquisa\n",
    "i = 0\n",
    "for frame in frames_pesquisa:\n",
    "    print(f'seq ={i:>3} | {frame}')\n",
    "    i += 1\n",
    "    \n",
    "if frames_pesquisa[i_frame]:\n",
    "    print(f'\\n\\nDados do teste: batch_name: {batch_name} | frame: {frames_pesquisa[i_frame]} | model: {model} | tipo_pdf: {tipo_pdf[i_test]}')\n",
    "    \n",
    "    \n",
    "######### PATHS\n",
    "#1. path formado para busca de pdfs recursiva\n",
    "root_doc_analise = os.path.join(documentos_extracao_path, batch_name)\n",
    "\n",
    "#2. path para documentos teste RASTER PDF (ATRIBIDO DA LISTA)\n",
    "path_test_pdf = list_path_test[1]\n",
    "\n",
    "#3. path formado para nome do arquivo json\n",
    "json_file_path = os.path.join(json_path, nome_formado_json)\n",
    "\n",
    "#Listando paths utilizados\n",
    "#print(f'\\nroot_doc_analise: {root_doc_analise}\\npath_test_pdf: {path_test_pdf}\\njson_file_path: {json_file_path}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import modules.trata_dicionarios as tratdic\n",
    "\n",
    "import modules.trata_files as tratfiles\n",
    "import modules.trata_texto_extraido as trattext\n",
    "import modules.trata_template as trattemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_info[nr_nro_nf] = {\n",
    "    \"dados_NF_PDF\": {\n",
    "        \"data_cabecalho\": nf_data_cabecalho,\n",
    "        \"data_prestador\": nf_data_prestador,\n",
    "        \"data_tomador\": nf_data_tomador,\n",
    "        \"data_servico\": nf_data_servico,\n",
    "        \"data_valor_total\": nf_data_valor_total,\n",
    "        \"data_CNAE\": nf_data_CNAE,\n",
    "        \"data_valores\": nf_data_valores,\n",
    "        \"data_dados_complementares\": nf_data_dados_complementares,\n",
    "        \"data_outras_informacoes\": nf_data_outras_informacoes,\n",
    "        \"data_observacao\": nf_data_observacao,\n",
    "    },\n",
    "    \"diretorio\": os.path.basename(root),\n",
    "    \"nome_arquivo\": nome_arquivo,\n",
    "}\n",
    "\n",
    "\n",
    "pdf_document.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    nome_arquivo = \"teste\"\n",
    "\n",
    "    pdf_info[\"diretorio\"] = dir_name\n",
    "\n",
    "    pdf_info[nro_nota] = {\n",
    "        \"dados_NF_PDF\": {\n",
    "            \"data_cabecalho\": data_cabecalho_final,\n",
    "            \"data_prestador\": data_prestador_final,\n",
    "            \"data_tomador\": data_tomador_final,\n",
    "            \"data_servico\": data_servico_final,\n",
    "            \"data_valor_total\": data_valor_total_final,\n",
    "            \"data_CNAE\": data_CNAE_final,\n",
    "            \"data_valores\": data_valores_final,\n",
    "            \"data_dados_complementares\": data_dados_complementares_final,\n",
    "            \"data_outras_informacoes\": data_outras_informacoes_final,\n",
    "            \"data_observacao\": data_observacao_final,\n",
    "        },\n",
    "        \"diretorio\": dir_name, #os.path.basename(root)\n",
    "        \"nome_arquivo\": nome_arquivo,\n",
    "        \"Batch\": batch_name,\n",
    "        \"modelo\": model,\n",
    "        \"pdf_realmente_pequisavel\": pdf_realmente_pequisavel,\n",
    "        \"processo\": processo,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "            \n",
    "\n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pdf_info[nro_nota] = {\n",
    "            \"dados_NF_PDF\": {\n",
    "            \"data_cabecalho\": data_cabecalho_final,\n",
    "            \"data_prestador\": data_prestador_final,\n",
    "            \"data_tomador\": data_tomador_final,\n",
    "            \"data_servico\": data_servico_final,\n",
    "            \"data_valor_total\": data_valor_total_final,\n",
    "            \"data_CNAE\": data_CNAE_final,\n",
    "            \"data_valores\": data_valores_final,\n",
    "            \"data_dados_complementares\": data_dados_complementares_final,\n",
    "            \"data_outras_informacoes\": data_outras_informacoes_final,\n",
    "            \"data_observacao\": data_observacao_final,\n",
    "        },\n",
    "        \"diretorio\": dir_name, #os.path.basename(root)\n",
    "        \"nome_arquivo\": nome_arquivo,\n",
    "        \"Batch\": batch_name,\n",
    "        \"modelo\": model,\n",
    "        \"pdf_realmente_pequisavel\": pdf_realmente_pequisavel,\n",
    "        \"processo\": processo,\n",
    "    }\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao gerar o json: {e}\")\n",
    "            #print(pdf_info) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tables-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
