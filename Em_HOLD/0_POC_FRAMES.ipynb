{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <mark> POC - FRAMES </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Atual notebook com as funçoes para processamento de PDF Pesquisavel e Raster e a criaçao dos Dataframes de forma dependente e unica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import platform\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from urllib import response\n",
    "\n",
    "from outlook_msg import Message\n",
    "import extract_msg\n",
    "import zipfile\n",
    "from pyunpack import Archive\n",
    "import py7zr\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "from fuzzywuzzy import fuzz\n",
    "import PyPDF2\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "import locale\n",
    "import time, copy\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import cv2\n",
    "import fitz  # Módulo PyMuPDF\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import Image, ImageDraw\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar\n",
    "from pdf2image import convert_from_path\n",
    "import matplotlib.pyplot as plt\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "# Modulos da solucao\n",
    "import modules.extrai_pdf_pesquisavel as Extc\n",
    "import modules.cronometro as cron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt_BR.utf8'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Config - E-mail\n",
    "# 1. Caminho do arquivo uma mensagem especifica\n",
    "msg_dir_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails'\n",
    "\n",
    "# 2. Path para arquivos atachados compactados\n",
    "msg_attachment_zip = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/13_attachments'\n",
    "\n",
    "\n",
    "#### Config - messages\n",
    "# 3. Caminho do arquivo uma mensagem especifica\n",
    "msg_outros_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/12_messages'\n",
    "\n",
    "# 4. Path para arquivos recebidos manualmente\n",
    "arquivos_recebidos_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/14_documentos_recebidos'\n",
    "\n",
    "\n",
    "####Config Processamento Pipeline\n",
    "\n",
    "# 5. Path para documentos para extracao\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "# 6. Path para gestao de imagens resized\n",
    "image_resized_path = \"pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas\"\n",
    "\n",
    "\n",
    "# 7. Path para DFs e CSVs exportados\n",
    "export_path = \"pipeline_extracao_documentos/6_geral_administacao/exports\"\n",
    "\n",
    "# 8. Path para lixeira\n",
    "root_garbage_path = \"pipeline_extracao_documentos/0_lixeira\"\n",
    "\n",
    "\n",
    "\n",
    "# 12. poppler path\n",
    "poppler_path = \"/home/dani-boy/miniconda3/envs/tables-detr/bin\"\n",
    "\n",
    "# 13. path para config Tesseract\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "#Modelo atual\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "# definindo localizadcao para pt_BR\n",
    "locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. POC Unique_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1 - Funcoes para e-mail e extracao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch</th>\n",
       "      <th>Data</th>\n",
       "      <th>File</th>\n",
       "      <th>Type</th>\n",
       "      <th>Level</th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Parent_Unique_ID</th>\n",
       "      <th>Hash</th>\n",
       "      <th>File_Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Batch, Data, File, Type, Level, Unique_ID, Parent_Unique_ID, Hash, File_Path]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Geraçao do hash do arquivo\n",
    "def generate_file_hash(file_path):\n",
    "    # Abre o arquivo em modo de leitura de bytes\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # Lê o conteúdo do arquivo\n",
    "        file_data = f.read()\n",
    "        # Utiliza o algoritmo SHA-256 para gerar o hash\n",
    "        file_hash = hashlib.sha256(file_data).hexdigest()\n",
    "    return file_hash\n",
    "\n",
    "# 2. Geraçao do Unique_id do arquivo\n",
    "def generate_unique_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# 3. XXX Busca proximo Batch\n",
    "def busca_proximo_batch():\n",
    "    # Abre o arquivo Excel e lê a coluna 'batch'\n",
    "    df = pd.read_excel(\"pipeline_extracao_documentos/6_geral_administacao/exports/df_documento_recebido.xlsx\", usecols=[\"batch\"])\n",
    "    # Pega o último valor da coluna 'batch'\n",
    "    last_value = df.iloc[-1, 0]\n",
    "    \n",
    "    # Extraí o número do último batch e adiciona 1 para o próximo\n",
    "    last_number = int(last_value.split(\"_\")[1])\n",
    "    next_number = last_number + 1\n",
    "    \n",
    "    # Forma o nome do próximo batch\n",
    "    next_batch = f\"Batch_{next_number}\"\n",
    "    \n",
    "    return next_batch    \n",
    "\n",
    "# 4. Função para verificar e criar a pasta se não existir\n",
    "def check_and_create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        \n",
    "# 5. converte nome do arquivo\n",
    "def conv_filename(title):\n",
    "    \n",
    "    # Divide o título em nome e extensão\n",
    "    name, extension = title.rsplit('.', 1) if '.' in title else (title, \"\")\n",
    "\n",
    "    # Remove acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substiti espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remove quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converte para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    # Adiciona a extensão de volta, se houver\n",
    "    if extension:\n",
    "        filename += '.' + extension.lower()\n",
    "\n",
    "    return filename\n",
    "\n",
    "# 6. converte nome do arquivo retirando extensao\n",
    "def conv_filename_no_ext(title):\n",
    "    \n",
    "    # Divida o título em nome e extensão (mas ignore a extensão)\n",
    "    name = title.rsplit('.', 1)[0] if '.' in title else title\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    return filename         \n",
    "\n",
    "# 7. funçao que MOVE documentos e gera add_log_transaction_entry para df_log_transctions\n",
    "def move_doc_processed_file(batch_name, src_path, tgt_path):\n",
    "    \n",
    "    function = \"move_doc_processed_file\"\n",
    "    source_path = src_path\n",
    "    file = os.path.basename(source_path)\n",
    "    sub_dir = os.path.join(tgt_path, batch_name)\n",
    "    destination_path = os.path.join(sub_dir, file)\n",
    "    document_action = \"move_processed_file\"\n",
    "    transaction_detail = (f'document {file} moved by: {function}')\n",
    "    df_move = pd.DataFrame()\n",
    "    try:\n",
    "        document_unique_id = get_document_id_by_file(batch_name, file)\n",
    "        check_and_create_folder(destination_path)\n",
    "        shutil.move(source_path, destination_path)\n",
    "        sucess = True\n",
    "        move_log = add_log_transaction_entry(document_unique_id, batch_name, file, document_action, src_path, tgt_path, transaction_detail, sucess)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao mover documento: {e}\")\n",
    "        sucess = False\n",
    "    \n",
    "    return move_log    \n",
    "\n",
    "# 8. Função para adicionar um novo registro em df_source\n",
    "def add_source_entry(batch_name, file_path, file, type, level, parent_unique_id):\n",
    "    #unique_id = generate_unique_id(type)\n",
    "    unique_id = generate_unique_id()\n",
    "    time_now = cron.timenow_pt_BR()   \n",
    "    file_hash = generate_file_hash(file_path) \n",
    "    if level == 1:\n",
    "        parent_unique_id = unique_id\n",
    "    data = {\n",
    "        'Batch': batch_name,\n",
    "        'Data': time_now,\n",
    "        'File': file,\n",
    "        'Type': type,\n",
    "        'Level': level,\n",
    "        'Unique_ID': unique_id,\n",
    "        'Parent_Unique_ID': parent_unique_id,\n",
    "        'Hash': file_hash,\n",
    "        'File_Path': file_path\n",
    "    }\n",
    "    return data\n",
    "\n",
    "# 9. Add nova linha para atualizar df_log_transctions\n",
    "def add_log_transaction_entry(document_unique_id,batch_name, file, document_action, src_path, tgt_path, transaction_detail, sucess=True):\n",
    "\n",
    "    data_log = {\n",
    "        'Dt_Time': cron.timenow_pt_BR(),\n",
    "        'Batch': batch_name,\n",
    "        'File' : file,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Action': document_action,\n",
    "        'Sorce': src_path,\n",
    "        'Target': tgt_path,\n",
    "        'Transction_Detail': transaction_detail,\n",
    "        'Sucess': sucess,    \n",
    "    }\n",
    "    \n",
    "        \n",
    "    return data_log\n",
    "\n",
    "\n",
    "# 10. Consulta multiparametros\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# 12. Busca filhos - simples\n",
    "def get_children(batch, file_path):\n",
    "    \n",
    "    file = os.path.basename(file_path)\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    # Buscando todos dos filhos de um documento\n",
    "    return filtrar_df(df_id_relations, Batch=batch, Parent_Unique_ID=document_unique_id)\n",
    "\n",
    "\n",
    "# 13. Busca pai -simples\n",
    "def get_father(batch, file_path):\n",
    "    \n",
    "    file = os.path.basename(file_path)\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    # Buscando todos dos filhos de um documento\n",
    "    return filtrar_df(df_id_relations, Batch=batch, Unique_ID=parent_unique_id)\n",
    "\n",
    "\n",
    "# 14. Pesquiso pai pelo Unique_ID e trago um dict\n",
    "def get_father_data_by_children_file(batch, file):\n",
    "    \n",
    "    src_result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    src_parent_unique_id = src_result['Parent_Unique_ID'].values[0]\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=src_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }\n",
    "\n",
    "\n",
    "# 15. Pesquiso pai pelo Unique_ID (document_parent_unique_id) e cria DICT\n",
    "def get_father_by_unique_id(batch, document_parent_unique_id):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=document_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }\n",
    "    \n",
    "# 16. Pesquiso pai pelo file do filho e cria DICT\n",
    "def get_father_data_by_children_file(batch, file):\n",
    "    \n",
    "    src_result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    src_parent_unique_id = src_result['Parent_Unique_ID'].values[0]\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=src_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }        \n",
    "        \n",
    "\n",
    "# 17. Busca o 'Unique_ID' para definir o Parent_Unique_ID sem considerar 'Level'\n",
    "def get_parent_unique_id(df_id_relations, batch_name, file, type):\n",
    "    try:\n",
    "        parent_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type)]['Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        parent_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return parent_unique_id\n",
    "\n",
    "\n",
    "# 18. funcao para trazer somente o 'Unique_ID'\n",
    "def get_document_unique_id(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)]['Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        document_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# 19. funcao para trazer somente o 'Parent_Unique_ID'\n",
    "def get_document_parent_unique_id(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_parent_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)]['Parent_Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        document_parent_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_parent_unique_id\n",
    "\n",
    "\n",
    "# 20. funçao para trazer toda a row de df_id_relations para o documento\n",
    "def get_document_id_relations(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_id_relations = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)].values[0]\n",
    "    except IndexError:\n",
    "        document_id_relations = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_id_relations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# EXEMPLOS de Pesquisa DFss\n",
    "    # get_document_unique_id(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "    # # Busca somente o 'Parent_Unique_ID'\n",
    "    # get_document_parent_unique_id(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "\n",
    "    # #Busca todos os dados da row do documento encontrado\n",
    "    # document_id_relations = get_document_id_relations(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "    # document_batch = document_id_relations[0]\n",
    "    # document_date = document_id_relations[1]\n",
    "    # document_name = document_id_relations[2]\n",
    "    # document_type = document_id_relations[3]\n",
    "    # document_level = document_id_relations[4]\n",
    "    # document_unique_id = document_id_relations[5]\n",
    "    # document_parent_unique_id = document_id_relations[6]\n",
    "    # document_hash = document_id_relations[7]\n",
    "    # document_path = document_id_relations[8]\n",
    "\n",
    "    # # Insercao de um registro pela func add_source_entry\n",
    "    # file_path = \"pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/SPA 15082023.rar\"\n",
    "\n",
    "    # file = os.path.basename(file_path)\n",
    "\n",
    "    # type = \"compressed_file_attachment\"\n",
    "\n",
    "    # level = 1\n",
    "\n",
    "    # parent_unique_id = ''\n",
    "\n",
    "    # # Adicionando um novo registro (substitua 'batch_name' e 'email' conforme necessário)\n",
    "    # new_entry = add_source_entry(batch_name, file_path, file, type, level, parent_unique_id)\n",
    "\n",
    "    # df_id_relations = df_id_relations.append(new_entry, ignore_index=True)\n",
    "\n",
    "    # df_id_relations\n",
    "\n",
    "\n",
    "# Busca proximo Batch caso nao esteja rodando email\n",
    "batch_name = busca_proximo_batch()\n",
    "\n",
    "# 1. Criaçao do DataFrame para armazenar as relações de Unique_ID e Parent_Unique_ID\n",
    "df_id_relations = pd.DataFrame(columns=['Batch', 'Data' ,'File', 'Type', 'Level', 'Unique_ID', 'Parent_Unique_ID', 'Hash', 'File_Path'])\n",
    "\n",
    "# 2. Criaçao do DataFrame para df_start_pipe:\n",
    "#df_start_pipe = pd.DataFrame(columns=['Batch', 'Data' ,'File', 'Type', 'Level', 'Unique_ID', 'dt_hora', 'de', 'assunto', 'email', 'Hash'])\n",
    "\n",
    "df_id_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementando o Root Unique_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>1.1</b> Funcoes importantes - algumas das quais estao sempre por ai  </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. XXX Analisa nro de paginas\n",
    "def analisa_nro_pages(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    pages = pdf_document.pages() # generator object\n",
    "\n",
    "    page_nro = []\n",
    "    for page in pages:\n",
    "        page_nro.append(page)\n",
    "        \n",
    "    nro_paginas = len(page_nro)    \n",
    "    if nro_paginas > 1:\n",
    "        doc_1_page = False\n",
    "        return doc_1_page, nro_paginas    \n",
    "    else:\n",
    "        doc_1_page = True\n",
    "        return doc_1_page, nro_paginas  \n",
    "    pdf_document.close()\n",
    "    \n",
    " \n",
    "\n",
    "# XXX FUNCAO DE SPLIT\n",
    "def split_documentos(qualquer_df, fase, atividade, status):\n",
    "    \n",
    "    documentos_splitados = []\n",
    "    doc_info = {}\n",
    "    rows_list = []\n",
    "    documentos = []\n",
    "    #output_dir = os.path.join(documentos_scan_path, batch_name)\n",
    "    num_linhas_df = qualquer_df.shape[0]\n",
    "\n",
    "    i = num_linhas_df + 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        message_erro = []\n",
    "        nun_pages = row['pages']\n",
    "        batch_name = row['batch']\n",
    "        original_file_name = row['original_file_name']\n",
    "        folder_name = row['directory']\n",
    "        file_path = row['file_path']\n",
    "        level = row['level']\n",
    "        document_type = row['document_type']\n",
    "        doc_action = row['doc_action']\n",
    "        document_unique_id = idx\n",
    "        new_level = level + 1\n",
    "        \n",
    "        if (doc_action == 'splitar') and (status == 'root_analise'):\n",
    "            if nun_pages > 1:\n",
    "                try:\n",
    "                    pdf = fitz.open(file_path)\n",
    "                    # Número total de páginas no PDF\n",
    "                    total_pages = len(pdf)\n",
    "                except Exception as e:\n",
    "                    print(f\"Nao congui abrir o PDF: {e}\")    \n",
    "\n",
    "                # Nome base para os arquivos de saída\n",
    "                base_name = file_path.split('.')[0]  # Remove a extensão do arquivo\n",
    "                file_to_delete = file_path\n",
    "                # Loop para criar um novo PDF para cada página\n",
    "                for page_num in range(total_pages):\n",
    "                    # Cria um novo objeto PDF\n",
    "                    new_pdf = fitz.open()\n",
    "                    # Adiciona a página atual ao novo PDF\n",
    "                    new_pdf.insert_pdf(pdf, from_page=page_num, to_page=page_num)\n",
    "                    # Nome do novo arquivo PDF\n",
    "                    new_pdf_name = f\"{base_name}_page_{page_num + 1}.pdf\"\n",
    "                    # Salva o novo PDF\n",
    "                    new_pdf.save(new_pdf_name)\n",
    "                    # Fecha o novo PDF\n",
    "                    new_pdf.close()\n",
    "                    rotulo = \"prov_nota_fiscal\"\n",
    "                    acao_sugerida = sugestoes_acao.get(rotulo, \"no_defined_action\")\n",
    "                    acao_executada = \"novo_doc_criado\"\n",
    "                    informations = (f'documento criado a partir do split do documento: {original_file_name}')  \n",
    "                    name_pdf_splited = os.path.basename(new_pdf_name)\n",
    "\n",
    "                    new_row = {\n",
    "                        \"seq\": i,\n",
    "                        \"date_time\": cron.timenow_pt_BR(),\n",
    "                        \"batch\": batch_name,\n",
    "                        \"fase_processo\": fase,\n",
    "                        \"nome_atividade\": atividade,\n",
    "                        \"status_documento\": status,\n",
    "                        \"acao_executada\": acao_executada,\n",
    "                        \"original_file_name\": new_pdf_name,\n",
    "                        \"directory\": folder_name,\n",
    "                        \"one_page\": True,\n",
    "                        \"pages\": 1,\n",
    "                        \"document_type\": rotulo,\n",
    "                        \"doc_action\": acao_sugerida,\n",
    "                        \"level\": level,\n",
    "                        \"document_unique_id\": generate_unique_id(),\n",
    "                        \"parent_document_unique_id\": document_unique_id,\n",
    "                        \"file_hash\": generate_file_hash(file_path),\n",
    "                        \"file_path\": file_path,\n",
    "                        \"informations\": informations,\n",
    "                    }\n",
    "                    rows_list.append(new_row)\n",
    "                    i += 1\n",
    "                qualquer_df.loc[idx, 'status_documento'] = \"NAO_PROCESSAR\" \n",
    "                qualquer_df.loc[idx, 'informations'] = \"Paginas splitada em multiplos documentos\" \n",
    "                qualquer_df.loc[idx, 'date_time'] = cron.timenow_pt_BR() \n",
    "    \n",
    "    total_split = i - 1        \n",
    "    df_split = pd.DataFrame(rows_list)\n",
    "    \n",
    "    \n",
    "    return df_split, rows_list\n",
    "\n",
    "\n",
    "# XXX Usando na criacao da imagem \n",
    "def conv_filename_no_ext(title):\n",
    "    \n",
    "    # Divida o título em nome e extensão (mas ignore a extensão)\n",
    "    name = title.rsplit('.', 1)[0] if '.' in title else title\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    return filename \n",
    "\n",
    "\n",
    "# XXX Funcao ajustada para convertere e resize\n",
    "def convertResize(doc2convert, document_path, image_resized_path):\n",
    "    \n",
    "    \n",
    "    name_image = conv_filename_no_ext(doc2convert)\n",
    "    \n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(name_image)}.jpg')\n",
    "    #print(f'image_resized_name: {image_resized_name}\\n')\n",
    "    # 3. Conversao para imagem\n",
    "    pages = convert_from_path(document_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((2067, 2923))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "        image_2work = resized_pages[0]\n",
    "        \n",
    "    return image_2work, image_resized_name\n",
    "\n",
    "\n",
    "\n",
    "def apagar_zone(documentos_extracao_path):\n",
    "    # Para apagar arquivos PDF:Zone\n",
    "    for root, dirs, files in os.walk(documentos_extracao_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            #print(file)\n",
    "            if \":Zone\" in file:\n",
    "                file_to_delete = file_path\n",
    "                os.remove(file_to_delete)\n",
    "                #print(file, \"termina, pode eliminar\")\n",
    "                \n",
    "                \n",
    "def confirma_pdf_pequisavel(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "    # Definir retângulo de interesse\n",
    "    try:\n",
    "        x0 = 0\n",
    "        y0 = 4\n",
    "        x1 = 600\n",
    "        y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "        # Extrair texto dentro do retângulo\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        if text:\n",
    "            page_number = 0\n",
    "            pdf_pequisavel = True\n",
    "        #print(page_number)\n",
    "        else:\n",
    "            page_number = 1\n",
    "            pdf_pequisavel = False\n",
    "        #print(page_number)\n",
    "    except Exception as e:\n",
    "        msg_error = (f\"Erro ao abrir pagina do PDF: {e}\")\n",
    "        pdf_pequisavel = False\n",
    "        pdf_document.close()   \n",
    "         \n",
    "        return pdf_pequisavel\n",
    "                   \n",
    "\n",
    "#generated_parent_document_unique_id = generate_unique_id()  \n",
    "\n",
    "# Processo de deleçao e atualizacao de documentos\n",
    "#e_deleta_peloamor(df_docs_splitados)\n",
    "\n",
    "#me_atualiza_logo_vai_2(novo_df)\n",
    "\n",
    "# apagar_zone(documentos_extracao_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fase = 'processamento' # 'pre-processamento'\n",
    "atividade = 'pre_process'\n",
    "status = 'root_analise'\n",
    "\n",
    "acao_sugerida = \"splitar\"\n",
    "\n",
    "\n",
    "df_root_pipe = split_documentos(df_root_pipe, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualquer_df = pd.concat([qualquer_df, df_split], ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template e dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nome_arquivo:                              batatinha_quando_nasce.pdf | palavra_chave:              default | rotulo: prov_nota_fiscal     | acao_sugerida: NO_PROCESS                    \n"
     ]
    }
   ],
   "source": [
    "nf_model_path = \"pipeline_extracao_documentos/6_geral_administacao/modelos/frames_nf_v9.xlsx\"\n",
    "\n",
    "# 11. path para datasets CNAE e Itens de Serviço\n",
    "nf_datasets_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria do DF\n",
    "frames_nf_v4_df = pd.read_excel(nf_model_path)\n",
    "\n",
    "# Cria dicionários para armazenar diferentes tipos de elementos do modelo\n",
    "document_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'document'].iloc[0]\n",
    "boundaries_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'boundaries']\n",
    "sections_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'section']\n",
    "frames_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'frame']\n",
    "sframe_fields_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'sframe_field']\n",
    "field_boxes_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'field_box']\n",
    "\n",
    "def define_dados_iniciais(texto_tratado):\n",
    "    \n",
    "    dados_iniciais_nf = {}\n",
    "\n",
    "    prefeitura_encontrada = None\n",
    "    de_para_encontrado = None\n",
    "\n",
    "    # 7. ZZZ Dicionário para mapear Prefeitura com sua sigla\n",
    "    de_para_prefeitura = {\n",
    "        \"PREFEITURA DA CIDADE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA DA CIDADE DE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA MUNICIPAL DE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\": \"PM_SPA\",\n",
    "        \"MUNICIPAL DE SAO PEDRO DA ALDEIA\": \"PM_SPA\",\n",
    "        \"PREFEITURA MUNICIPAL DE MESQUITA\": \"PM_MESQUITA\",\n",
    "        \"PREFEITURA MUNICIPAL DE DE MESQUITA\": \"PM_MESQUITA\",\n",
    "        # ... adicione \n",
    "    }\n",
    "\n",
    "    templates = {\n",
    "        (\"PM_MAGE\", \"30.693.231/0001-99\"): \"MAGE_MAICON\",\n",
    "        (\"PM_MAGE\", \"23.317.112/0001-76\"): \"MAGE_MFF\",   \n",
    "        (\"PM_MAGE\", None): \"MAGE\",\n",
    "        (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", \"47.945.459/0001-21\"): \"SAO_PEDRO_GOAT\",\n",
    "        (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", \"68.687.722/0001-08\"): \"SAO_PEDRO_GM\",\n",
    "        (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", \"34.230.979/0038-06\"): \"SAO_PEDRO_SUPERMIX\",\n",
    "        (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", None): \"SAO_PEDRO\",\n",
    "        (\"Pague agora com o seu Pix\", None): \"NAO_PROCESSAR\",\n",
    "        # ... adicione outras combinações aqui\n",
    "    }\n",
    "\n",
    "    cnpj_encontrado = None\n",
    "    # Verifique cada linha do texto\n",
    "    for linha in texto_tratado:\n",
    "        for pref in de_para_prefeitura.keys():\n",
    "            if pref in linha:\n",
    "                #print(linha)\n",
    "                prefeitura_encontrada = pref\n",
    "                dados_iniciais_nf['prefeitura'] = prefeitura_encontrada\n",
    "                #print(prefeitura_encontrada)\n",
    "    # Saímos do loop, agora vamos verificar qual template usar\n",
    "    if prefeitura_encontrada:\n",
    "        de_para_pm = de_para_prefeitura.get(prefeitura_encontrada)\n",
    "        dados_iniciais_nf['de_para_pm'] = de_para_pm\n",
    "        #print(de_para_pm)\n",
    "        if not de_para_pm:\n",
    "            de_para_pm = de_para_prefeitura.get(prefeitura_encontrada, \"NAO_PROCESSAR\")\n",
    "            dados_iniciais_nf['de_para_pm'] = de_para_pm\n",
    "            #print(de_para_pm)\n",
    "    else:\n",
    "        de_para_pm = \"NAO_PROCESSAR\"\n",
    "        #print(de_para_pm)\n",
    "        \n",
    "        \n",
    "        # Verifique cada linha do texto\n",
    "    for linha in texto_tratado:\n",
    "        for de_para, cnpj in templates.keys():\n",
    "            if cnpj and cnpj in linha:\n",
    "                cnpj_encontrado = cnpj\n",
    "                dados_iniciais_nf['cnpj_encontrado'] = cnpj_encontrado\n",
    "                print('usara template para: ', cnpj_encontrado)\n",
    "                \n",
    "    # Saímos do loop, agora vamos verificar qual template usar\n",
    "    if de_para_pm:\n",
    "        template_usar = templates.get((de_para_pm, cnpj_encontrado))\n",
    "        # print(template_usar)\n",
    "        dados_iniciais_nf['template_usar'] = template_usar\n",
    "        if not template_usar:\n",
    "            template_usar = templates.get((de_para_pm, None), \"TEMPLATE_NAO_ENCONTRADO\")\n",
    "            dados_iniciais_nf['template_usar'] = template_usar \n",
    "    else:\n",
    "        template_usar = \"TEMPLATE_NAO_ENCONTRADO\"\n",
    "        dados_iniciais_nf['template_usar'] = template_usar\n",
    "        \n",
    "    \n",
    "    #Confirmando se template existe em frames    \n",
    "    try:        \n",
    "        f_type = 'frame'\n",
    "        #template_usar = 'SAO_PEDRO_SUPERMIX'\n",
    "        result = filtrar_df(frames_nf_v4_df, type=f_type, de_para_pm=de_para_pm, model=template_usar)\n",
    "        model = result['model'].values[0]\n",
    "        if model:\n",
    "            template_oficial = model\n",
    "            if model == template_usar:\n",
    "                dados_iniciais_nf['model'] = template_oficial\n",
    "            else:    \n",
    "                template_usar = \"necessario cadastrar\"\n",
    "                dados_iniciais_nf['template_usar']\n",
    "                \n",
    "            dados_iniciais_nf['template_usar'] = template_usar\n",
    "        else:\n",
    "            template_usar = \"necessario cadastrar\"\n",
    "            dados_iniciais_nf['template_usar'] = template_usar\n",
    "            dados_iniciais_nf['model'] = template_usar\n",
    "                \n",
    "    except Exception as e:\n",
    "       error_msg = (f\"Erro busca cnae: {e}\") \n",
    "               \n",
    "        \n",
    "    return dados_iniciais_nf  \n",
    "\n",
    "\n",
    "\n",
    "# Dicionário para mapear palavras-chave a rótulos\n",
    "mapeamento_palavras_chave = {\n",
    "    \"relatorio\": \"prov_relatorio\",\n",
    "    \"listagem\": \"prov_listagem\",\n",
    "    \"NF\": \"prov_nota_fiscal\",\n",
    "    \"nf\": \"prov_nota_fiscal\",\n",
    "    \"relatorio\": \"prov_listagem\",\n",
    "    \"sintetico\": \"prov_listagem\",\n",
    "    \"livro\": \"prov_livro_registro\",\n",
    "    \"sintético\": \"prov_listagem\",\n",
    "    \"nota\": \"prov_nota_fiscal\",\n",
    "    \"zip\": \"doc_zip\",\n",
    "    \"rar\": \"doc_rar\",\n",
    "    \"valores\": \"prov_dinheiro\",\n",
    "}\n",
    "\n",
    "# Dicionário mapeando rótulos a ações sugeridas\n",
    "sugestoes_acao = {\n",
    "    \"prov_relatorio\": \"NO_PROCESS\",\n",
    "    \"prov_listagem\": \"NO_PROCESS\",\n",
    "    \"prov_nota_fiscal\": \"NO_PROCESS\",\n",
    "    \"sem_rotulo\": \"MANUAL_REV\",\n",
    "    \"prov_livro_registro\": \"NO_PROCESS\",\n",
    "    \"doc_nao_pdf\": \"verificar\",\n",
    "    \"nao_pdf\": \"NO_PROCESS\",\n",
    "    \"doc_zip\": \"NO_PROCESS\",\n",
    "    \"pdf_mul_paginas\": \"SPLIT\",\n",
    "}\n",
    "\n",
    "\n",
    "def define_rotulo_acao(nome_arquivo):\n",
    "    \n",
    "    for palavra_chave, rotulo in mapeamento_palavras_chave.items():\n",
    "        if palavra_chave.lower() in nome_arquivo.lower():\n",
    "            break\n",
    "    else:\n",
    "        rotulo = 'prov_nota_fiscal' #\"sem_rotulo\"\n",
    "        palavra_chave = 'default'\n",
    "        acao_sugerida = sugestoes_acao.get(rotulo, 'None')\n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "        # palavra_chave = 'None' #\"sem_palavra_chave\"\n",
    "        # acao_sugerida = 'None' #\"sem_acao_sugerida\"\n",
    "        \n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "        #print(f'nome_arquivo: {nome_arquivo} | rotulo: {rotulo}')\n",
    "    if rotulo != 'None': #\"sem_rotulo\"\n",
    "        acao_sugerida = sugestoes_acao.get(rotulo, 'None') # \"Ação não definida\"\n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "    \n",
    "    \n",
    "# 2.Testando\n",
    "nome_arquivo = 'batatinha_quando_nasce.pdf' # 'pre-processamento'\n",
    "\n",
    "palavra_chave, rotulo, acao_sugerida = define_rotulo_acao(nome_arquivo)\n",
    "\n",
    "print(f'nome_arquivo: {nome_arquivo:>55} | palavra_chave: {palavra_chave:>20} | rotulo: {rotulo:20} | acao_sugerida: {acao_sugerida:30}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcoes de OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Funcao de extracao\n",
    "def extract_text_PIL(image, coordinates):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    image_croped = image.crop((x0, y0, x1, y1))\n",
    "    texto_extraido = pytesseract.image_to_string(image_croped, lang='por')\n",
    "    return texto_extraido \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_text_from_coordinates(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    print(x0, y0, x1, y1)\n",
    "    frame_image\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=config).strip()\n",
    "    return extracted_text\n",
    "\n",
    "def extract_text_from_coordinates_2(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    print(x0, y0, x1, y1)\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config_1).strip()\n",
    "    return extracted_text\n",
    "\n",
    "\n",
    "def extract_fields_box(modelo, father_value, section):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    filtered_boxes_info = field_boxes_info[(field_boxes_info['father'] == father_value) & (field_boxes_info['model'] == modelo)]\n",
    "    # Iterate nas informações dos boxes de fields e extraia o texto de cada field\n",
    "    for index_field, row_field in filtered_boxes_info.iterrows():\n",
    "        x0, y0, x1, y1 = row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1']\n",
    "        extracted_text_box = extract_text_from_frame_2(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        #extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        \n",
    "        #print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference'], row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1'] ))\n",
    "        # Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "        value = extracted_text_box.split('\\n')[-1]\n",
    "        # Remova qualquer espaço em branco à esquerda ou à direita\n",
    "        value = value.strip()\n",
    "        if row_field['t_value'] == 'number':\n",
    "            # Formate o valor usando a função format_number\n",
    "            #print(\"vou verificar valor\")\n",
    "            value = format_number2(value)\n",
    "            #print(value)\n",
    "        # Armazene o texto extraído com o rótulo correspondente\n",
    "        label = row_field['label']\n",
    "        data_box_valores[label] = value\n",
    "        \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "# 3. Efetua OCR no documento (area parao do texto da NF)\n",
    "def ocr_RasterPDF(image_name):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = 406\n",
    "    y0 = 0\n",
    "    x1= 1540\n",
    "    y1 = 380\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "    return values, extracted_text_frame \n",
    "\n",
    "\n",
    "# 2. Efetua OCR no documento (area parao do texto da NF)\n",
    "def ocr_RasterPDF_free(image_name, vx0, vy0, vx1, vy1):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = vx0\n",
    "    y0 = vy0\n",
    "    x1= vx1\n",
    "    y1 = vy1\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "    return values, extracted_text_frame \n",
    "\n",
    " \n",
    "def extract_text_from_frame(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=config).strip()\n",
    "    return extracted_text \n",
    "\n",
    "\n",
    "def extract_fb_outras_inf(modelo, father_value, section):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    filtered_boxes_info = field_boxes_info[(field_boxes_info['father'] == father_value) & (field_boxes_info['model'] == model)]\n",
    "    # Iterate nas informações dos boxes de fields e extraia o texto de cada field\n",
    "    for index_field, row_field in filtered_boxes_info.iterrows():\n",
    "        \n",
    "        string_pesquisa = row_field['reference']\n",
    "        x0, y0, x1, y1 = row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        label = row_field['label']\n",
    "        #print(f'extracted_text_box {extracted_text_box}, label {label}')\n",
    "        text = extracted_text_box.replace('\\n', '')\n",
    "        if text.startswith(string_pesquisa):\n",
    "            #print(\"aqui:\", text)\n",
    "            text = text[len(label):].strip()\n",
    "            data_box_valores[label] = text\n",
    "    \n",
    "    return   data_box_valores  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcoes REGEX e ORganizacao TEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao importante - process_line\n",
    "def process_line(value, reference, label):\n",
    "    name_match = re.search(fr'{reference} (.+)', value)\n",
    "    if name_match:\n",
    "        extracted_value = reference + \" \" + name_match.group(1)\n",
    "        return {label: extracted_value}\n",
    "    return None\n",
    "\n",
    "# Função para extrair número da string\n",
    "def extract_number(text):\n",
    "    match = re.search(r'\\b\\d+(\\.\\d+)?\\b', text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# 5. XXX Ajusta texto\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF_Pesquisavel-  NO CABECALHO\n",
    "def texto_extraido_nf(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF RASTER NO CABECALHO\n",
    "def texto_extraido_cabecalho(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    text_splited = [s.replace(\")\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "    \n",
    "\n",
    "\n",
    "def format_number(number_str):\n",
    "    # Check for percentage and handle it\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # You can multiply by 100 here if needed\n",
    "\n",
    "    # Check if the string contains \"R$\" or a comma, indicating the original format\n",
    "    if 'R$' in number_str or ',' in number_str:\n",
    "        # Original format: Remove 'R$', replace dots with nothing, and replace commas with dots\n",
    "        number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    else:\n",
    "        # New format: Extract only the numeric part using regex\n",
    "        number_str = re.findall(r'[\\d\\.]+', number_str)[-1]\n",
    "\n",
    "    return float(number_str)\n",
    "\n",
    "# Funçao de formatacao de numeros\n",
    "def format_number2(number_str):\n",
    "    number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # multiplica por 100 para fields %\n",
    "    return float(number_str)\n",
    "\n",
    "\n",
    "\n",
    "#1. funcao: find_value_after_keyword_out_frame_up\n",
    "def find_value_after_keyword_out_frame_up(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "        if text_list[index + 1] not in default_keyword_list:\n",
    "            return text_list[index + 1]\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            for default_keyword in default_keyword_list:\n",
    "                if default_keyword in text_list:\n",
    "                    # Caso especial para 'Nome/Razão Social:'\n",
    "                    if keyword == 'Nome/Razão Social:':\n",
    "                        return text_list[0]\n",
    "        return None\n",
    "    \n",
    "#2. find_value_after_keyword_out_frame_down  \n",
    "def find_value_after_keyword_out_frame_down(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o índice seguinte está dentro da lista\n",
    "        if index + 1 < len(text_list):\n",
    "            # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            try:\n",
    "                index = text_list.index(default_keyword_list[-1])\n",
    "                return text_list[index - 1]\n",
    "            except ValueError:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "#3. find_value_after_keyword_fuzz\n",
    "def find_value_after_keyword_fuzz(keyword, text_list, default_keyword_list=None, fuzziness_threshold=80):\n",
    "    closest_match = None\n",
    "    closest_match_score = 0\n",
    "    \n",
    "    for i, text in enumerate(text_list):\n",
    "        score = fuzz.ratio(keyword, text)\n",
    "        \n",
    "        if score > closest_match_score:\n",
    "            closest_match_score = score\n",
    "            closest_match = text\n",
    "        \n",
    "        if closest_match_score > fuzziness_threshold:\n",
    "            break\n",
    "\n",
    "    if closest_match_score > fuzziness_threshold:\n",
    "        index = text_list.index(closest_match)\n",
    "        if index + 1 < len(text_list):\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "\n",
    "    \n",
    "def pesquisa_keyword(string_pesquisa, text_splited, keyword_list):\n",
    "\n",
    "    resultado_extraido_fuzz = find_value_after_keyword_fuzz(string_pesquisa, text_splited, keyword_list)\n",
    "\n",
    "    if resultado_extraido_fuzz == None:\n",
    "        resultado_extraido_frame_up = find_value_after_keyword_out_frame_up(string_pesquisa, text_splited, keyword_list)\n",
    "        if resultado_extraido_frame_up == None:\n",
    "            resultado_extraido_frame_down = find_value_after_keyword_out_frame_down(string_pesquisa, text_splited, keyword_list)\n",
    "            resultado_extraido = resultado_extraido_frame_down\n",
    "        else:\n",
    "            resultado_extraido = resultado_extraido_frame_up\n",
    "    else:\n",
    "        resultado_extraido = resultado_extraido_fuzz           \n",
    "    #print(resultado_extraido)\n",
    "    return resultado_extraido\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_fields_tomador_cnpj(text):\n",
    "    nf_data_tomador_cnpj = {}\n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in text:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "                        nf_data_tomador_cnpj['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                        nf_data_tomador_cnpj['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "\n",
    "    \n",
    "    # Extrair Telefone\n",
    "    telefone_match = re.search(r'Telefone:\\s+(.+)', text)\n",
    "    if telefone_match:\n",
    "        telefone_str = telefone_match.group(1)\n",
    "        if telefone_str == 'Inscrição Estadual:':\n",
    "            nf_data_tomador_cnpj['telefone'] = None  # Valor padrão quando não há correspondência\n",
    "        elif telefone_str == '':\n",
    "            nf_data_tomador_cnpj['telefone'] = None  # Valor padrão quando não há correspondência\n",
    "                    \n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['telefone'] = telefone_match.group(1)\n",
    "            \n",
    "    \n",
    "    # Extrair Inscrição Municipal\n",
    "    inscricao_municipal_match = re.search(r'Inscrição Municipal:\\s+(.+)', text)\n",
    "    if inscricao_municipal_match:\n",
    "        inscricao_municipal_str = inscricao_municipal_match.group(1)\n",
    "        if inscricao_municipal_str == \"Telefone:\": \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = inscricao_municipal_str\n",
    "    \n",
    "    insc_municipal_match = re.search(r'INSC:MUNICIPAL:\\s+(.+)', text)\n",
    "    if insc_municipal_match:\n",
    "        insc_municipal_str = insc_municipal_match.group(1)\n",
    "        if insc_municipal_str == \"Telefone:\":\n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = insc_municipal_str\n",
    "    else:\n",
    "        nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "            \n",
    "    \n",
    "    return nf_data_tomador_cnpj \n",
    "\n",
    "\n",
    "\n",
    "def extract_dados_comple_obs(modelo, frame_father, section):\n",
    "    \n",
    "    data_dados_complementares = {}\n",
    "    #frame_label = frame_father\n",
    "    \n",
    "    # 1. Filtrando o frames_info para buscar os dados de corte\n",
    "    filtered_frames_info = frames_info[(frames_info['label'] == frame_father) & (frames_info['model'] == modelo)]\n",
    "\n",
    "    # 2. Filtrando o sframe_fields_info para buscar os dados dos campos que estao nos frames\n",
    "    filtered_sframe_fields_info = sframe_fields_info[(sframe_fields_info['father'] == frame_father) & (sframe_fields_info['model'] == modelo)]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "        \n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        \n",
    "        print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_frame['seq'], row_frame['model'], row_frame['father'], row_frame['label'], row_frame['reference'], row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'] ))\n",
    "        for index_field, row_field in filtered_sframe_fields_info.iterrows():\n",
    "            #print(\"{:<5} {:<10} {:<30} {:<20} {:<20}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference']))\n",
    "            \n",
    "            if frame_father == \"5_frame_dados_complementares\":\n",
    "                nf_data_dados_complementares = {}\n",
    "                nf_data_dados_complementares['section'] = section\n",
    "                \n",
    "                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^DADOS COMPLEMENTARES', '', extracted_text_box, count=1)\n",
    "                if text == '':\n",
    "                    text = None\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text\n",
    "                else:    \n",
    "                    # Extrair texto dentro do retângulo\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "                    \n",
    "                return nf_data_dados_complementares                \n",
    "                \n",
    "            elif frame_father == \"5_frame_observacao\":\n",
    "                nf_data_observacao = {}\n",
    "                nf_data_observacao['section'] = section \n",
    "                                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^Observação:', '', extracted_text_box, count=1)\n",
    "\n",
    "                # Remover quebras de linha\n",
    "                text = text.replace('\\n', ' ')\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                nf_data_observacao['observacao'] = text.strip()\n",
    "                \n",
    "                return nf_data_observacao \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def processa_cnae_outros(text):\n",
    "    nf_data_CNAE_match = re.search(r'CNAE\\s+(.+)', text)\n",
    "    if nf_data_CNAE_match:\n",
    "        try:\n",
    "            # Remove a primeira ocorrência de \"CNAE:\"\n",
    "            nf_data_CNAE_str = re.sub(r'^CNAE - ', '', text, count=1)\n",
    "            # Remover quebras de linha\n",
    "            nf_data_CNAE_str = nf_data_CNAE_str.replace('\\n', ' ')\n",
    "            return nf_data_CNAE_str \n",
    "        except Exception as e:\n",
    "            print(f\"Erro busca cnae: {e}\") \n",
    "        \n",
    "    return None   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_name = \"Batch_20\" #Excepcionalmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_parent_document_unique_id = generate_unique_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importantando df_root_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. XXX SE deseja importar o DF df_analise_pipe\n",
    "\n",
    "df_root_pipe_path = \"df_root_analise.xlsx\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria df_documento_recebido\n",
    "df_root_pipe = pd.read_excel(df_root_pipe_path)\n",
    "\n",
    "# Ajusta o indice\n",
    "df_root_pipe.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.2 Funçao de Analise do Pipeline - gerar <mark> <b>df_root_pipe </b></mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.XXX  Acao 1 - Ler todo o pipeline de documentos recebidos - ESSA E A UNICA FUNCAO QUE ITERA NO DIRETORIO\n",
    "def scan_pipeline_documentos(documentos_extracao_path, batch_name, fase, atividade, status):\n",
    "    \n",
    "    doc_info = {}\n",
    "    resumo = {}\n",
    "    raw_document = []\n",
    "    \n",
    "    output_dir = os.path.join(documentos_extracao_path, batch_name)\n",
    "    i = 1\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        folder_name = os.path.basename(root)\n",
    "        #print(folder_name)\n",
    "        for file in files:\n",
    "            nome_arquivo = file\n",
    "            palavra_chave, rotulo, acao_sugerida = define_rotulo_acao(nome_arquivo)\n",
    "            acao_executada = \"Analise\"\n",
    "            informations = ' '    \n",
    "            file_name = file.lower()    \n",
    "            file_path = os.path.join(root, file)\n",
    "            new_path_name = os.path.join(output_dir, file)\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                doc_one_page, nro_pgs = analisa_nro_pages(file_path)\n",
    "            # if doc_one_page == False:\n",
    "            #             rotulo = 'pdf_mul_paginas'\n",
    "            level = 3\n",
    "            diretorio = os.path.basename(file_path)\n",
    "            if folder_name == batch_name:\n",
    "                folder_name = \"root_dir\"\n",
    "                \n",
    "            #print(f'nome_arquivo: {nome_arquivo:>55} | palavra_chave: {palavra_chave:>20} | rotulo: {rotulo:20} | acao_sugerida: {acao_sugerida:30}')    \n",
    "            \n",
    "            new_row = {\n",
    "                \"seq\": i,\n",
    "                \"date_time\": cron.timenow_pt_BR(),\n",
    "                \"batch\": batch_name,\n",
    "                \"fase_processo\": fase,\n",
    "                \"nome_atividade\": atividade,\n",
    "                \"status_documento\": status,\n",
    "                \"acao_executada\": acao_executada,\n",
    "                \"original_file_name\": file,\n",
    "                \"directory\": folder_name,\n",
    "                \"one_page\": doc_one_page,\n",
    "                \"pages\": nro_pgs,\n",
    "                \"palavra_chave\": palavra_chave,\n",
    "                \"document_tag\": rotulo,\n",
    "                \"action_item\": acao_sugerida,\n",
    "                \"level\": level,\n",
    "                \"document_unique_id\": generate_unique_id(),\n",
    "                \"parent_document_unique_id\": fake_parent_document_unique_id,\n",
    "                \"file_hash\": generate_file_hash(file_path),\n",
    "                \"file_path\": file_path,\n",
    "                \"informations\": informations,\n",
    "            }\n",
    "            raw_document.append(new_row)\n",
    "\n",
    "            \n",
    "            # print(f'seq: {i} | file: {file}'\n",
    "            i += 1\n",
    "    df_trans_pipe = pd.DataFrame(raw_document)\n",
    "      \n",
    "                \n",
    "    return df_trans_pipe, raw_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Crio o DF df_scan_pipe\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'scan_analise'\n",
    "status = 'root_analise'\n",
    "\n",
    "documentos = []\n",
    "\n",
    "#df_trans_pipe = pd.DataFrame()\n",
    "\n",
    "df_root_pipe, documentos = scan_pipeline_documentos(documentos_extracao_path, batch_name, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "df_root_pipe.query('one_page == False & palavra_chave == \"sem_palavra_chave\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. XXX Usando loc para Filtrar Baseado em one_page == False\n",
    "df_pages_2_split = df_root_pipe[df_root_pipe['one_page'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporando o processo de IteraÇao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta e o principio da melhor funcao do mundo - SIM ELA E MAS PRECISA FICAR AQUI?\n",
    "def extracao_pipeline(qualquer_df, fase, atividade, status):\n",
    "    \n",
    "    linhas_analise = []\n",
    "    bloco_1_list = []\n",
    "    bloco_2_list = []\n",
    "    bloco_3_list = []\n",
    "    imagens_list = []  \n",
    "    new_data = [] \n",
    "    \n",
    "    pre_processo = ['nro_nota', 'competencia', 'dt_hr_emissao', 'codigo_verificacao', 'ajustar_nome', 'split_paginas', 'ajustar_imagem', 'buscar_nome_prefeitura', 'enviar_canceladas', 'enviar_listagens']\n",
    "    \n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    \n",
    "    fase_processo_atual = fase\n",
    "    atividade_processo_atual = atividade\n",
    "    status_documento_atual = status\n",
    "    \n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        message_erro = []\n",
    "        \n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        document_unique_id = idx\n",
    "        seq_df = row['seq']\n",
    "        batch_name = row['batch']\n",
    "        fase_processo = row['fase_processo']\n",
    "        nome_atividade = row['nome_atividade']\n",
    "        status_documento = row['status_documento']\n",
    "        original_file_name = row['original_file_name']\n",
    "        file_directory = row['directory']\n",
    "        level = row['level']\n",
    "        d_type = row['level']\n",
    "        document_type = row['document_type']\n",
    "        pdf_pesquisavel = row['pdf_pesquisavel']\n",
    "        one_page_doc = row['one_page']\n",
    "        modelo = row['modelo']\n",
    "        \n",
    "        prefeitura = row['prefeitura']\n",
    "        \n",
    "        parent_document_unique_id = row['parent_document_unique_id']\n",
    "        file_path = row['file_path']\n",
    "        file_hash = row['file_hash']\n",
    "        \n",
    "        # 2. Busca modelo\n",
    "        if (not status_documento == 'NAO_PROCESSAR') or (not document_type == 'outros') or (not one_page_doc == True):\n",
    "            \n",
    "            if atividade_processo_atual == 'extracao_prestador':\n",
    "            #print(f' 1 - seq: {seq_df} | file: {original_file_name} |status_documento: {status_documento} pdf_pesquisavel: {pdf_pesquisavel}\\n')\n",
    "                if status_documento == status_documento_atual:\n",
    "         \n",
    "                    print(f'seq_df: {seq_df} status_documento: {status_documento} | modelo: {modelo:>20} | file: {original_file_name:>40} | prefeitura: {prefeitura:>55} | {pdf_pesquisavel}\\n')\n",
    "                    \n",
    "                    result_list = []\n",
    "                    \n",
    "                    dfcnpj_prestador = {}\n",
    "                    dfincricao_prestador = {}\n",
    "                    dfdados_prestador = {}\n",
    "                        \n",
    "                    # doc2convert = original_file_name\n",
    "                    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "                    modelo = 'SAO_PEDRO_SUPERMIX'\n",
    "                    f_tipo = 'frame'\n",
    "                    \n",
    "                    dfcnpj_prestador, dfincricao_prestador, dfdados_prestador, textoextraido  = processar_dados_dados_documentos(row, original_file_name, file_path, pdf_pesquisavel, section, modelo, f_tipo)\n",
    "                    \n",
    "                    print(f'\\n {dfcnpj_prestador}\\n{dfincricao_prestador}\\n{dfdados_prestador}\\n')\n",
    "                    \n",
    "               \n",
    "      \n",
    "\n",
    "\n",
    "                i += 1\n",
    "              \n",
    "    return textoextraido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe.to_excel('df_root_analise.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se Quiser evoluir no doc Analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. XXX SE deseja importar o DF df_analise_pipe\n",
    "\n",
    "df_analise_pipe_path = \"Em_HOLD/df_mapeamento_e_analise2.xlsx\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria df_documento_recebido\n",
    "df_analise_pipe = pd.read_excel(df_analise_pipe_path)\n",
    "\n",
    "# Ajusta o indice\n",
    "df_analise_pipe.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Crio o DF df_scan_pipe\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'scan_analise'\n",
    "status = 'mapear'\n",
    "\n",
    "df_analise_pipe, documentos = scan_pipeline_documentos(documentos_scan_path, batch_name, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>2.x</b> Processo de mapeamento e criacao do  df_analise_pipe </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. XXX df_analise_pipe e o df oficial de analise do pipeline\n",
    "df_analise_pipe\n",
    "\n",
    "# 3. Numero de linhas do DF\n",
    "num_linhas_df = df_analise_pipe.shape[0]\n",
    "\n",
    "num_linhas_df # variavel para o numero de linhas do DF\n",
    "\n",
    "\n",
    "# 4. XXX Usando loc para Filtrar Baseado em one_page == False\n",
    "df_pages_2_split = df_analise_pipe[df_analise_pipe['one_page'] == False]\n",
    "\n",
    "num_docs_2_split = df_pages_2_split.shape[0]\n",
    "\n",
    "\n",
    "print(f'nro documentos em df_analise_pipe: {num_linhas_df} | nro documentos + 1 pagina: {df_pages_2_split.shape[0]}')\n",
    "    \n",
    "# 5. XXX Definimos o Index dos DFs\n",
    "df_analise_pipe.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "df_pages_2_split.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# 6. XXX Executo a criacao dos documentos split \n",
    "fase = 'pre-processamento'\n",
    "atividade = 'scan_analise'\n",
    "status = 'processar'\n",
    "\n",
    "# df_docs_splitados recebera o DF com os documentos splitados\n",
    "df_docs_splitados = split_documentos(df_pages_2_split, df_analise_pipe, num_linhas_df, fase, atividade, status)\n",
    "\n",
    "\n",
    "\n",
    "# 7. XXX Retiro o indice do DF - Resetando o índice e mantendo o índice original como uma nova coluna\n",
    "df_analise_pipe.reset_index(inplace=True)\n",
    "\n",
    "# 8. XXX Concatenando os DataFrames\n",
    "df_analise_pipe = pd.concat([df_analise_pipe, df_docs_splitados], ignore_index=True)\n",
    "\n",
    "# 9. XXX Volto novamente o indice do DF\n",
    "df_analise_pipe.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "# 10. XXX Salvo em Excel (pode ser feito durante fases)\n",
    "df_analise_pipe.to_excel(\"df_mapeamento_e_analise.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2B. XXX Efetuo a analise do pipeline de documentos e inicio da extracao de dados\n",
    "# analisar_pdf_pesquisavel\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'Reavaliar_PDF_Pesquisavel' #'pesquisar_prefeitura' # pesquisar_prefeitura  pesquisar_modelo    mensagem_status = \"Reavaliar_PDF_Pesquisavel\"\n",
    "status = 'mapear'\n",
    "\n",
    "imagens_list = analise_extracao_pipeline(subset_df_analise_pipe, df_analise_pipe, fase, atividade, status)\n",
    "\n",
    "\n",
    "if imagens_list:\n",
    "    remove_images(imagens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "subset_df_analise_pipe = df_analise_pipe.query('seq == 59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Salvando o DF (IMPORTANTE)\n",
    "df_analise_pipe.to_excel(\"df_mapeamento_e_analise2.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Criando uma nova coluna no DF\n",
    "df_analise_pipe.insert(loc=17, column='s_act', value=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustando dados do DF e efetuando queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise_pipe['pdf_pesquisavel'] = df_analise_pipe.apply(lambda row: row['coluna1'] * 2 if row['coluna2'] > 0 else row['coluna1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizei o DF baseado em condiçao de outra coluna\n",
    "df_analise_pipe['pdf_pesquisavel'] = df_analise_pipe.apply(lambda row: False if row['status_documento'] == \"Template_encontrado\" else row['pdf_pesquisavel'], axis=1)\n",
    "\n",
    "\n",
    "# Podemos chegar de forma mais rapida\n",
    "df_analise_pipe.loc[df_analise_pipe['status_documento'] == \"Template_encontrado\", 'pdf_pesquisavel'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df_analise_pipe.quert('nro_nota')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efetuando pesquisa no DF\n",
    "df_analise_pipe.query('pdf_pesquisavel == False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise_pipe.query('nro_nota == 13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Para Analise de PDF Pesquisavel, uei uma copia do df_analise_pipe\n",
    "subset_df_analise_pipe = df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "subset_df_analise_pipe = df_analise_pipe.query('seq < 21 & seq > 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Criando uma nova coluna no DF\n",
    "df_analise_pipe.insert(loc=13, column='model_frame', value=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>1.</b> Efetuando subset do df para pesquisa </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Usando iloc para Filtrar um Número Específico de Linhas\n",
    "subset_df_analise_pipe = df_analise_pipe.iloc[:10]\n",
    "\n",
    "subset_df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Usando loc para Filtrar Baseado em Condições\n",
    "subset_df = df_scan_pipe.loc[df_scan_pipe['directory'] == 'camaleao']\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert file_hash == file_hash2, \"Os arquivos são diferentes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Usando iloc para Filtrar um Número Específico de Linhas\n",
    "subset_df = df_scan_pipe.iloc[:10]\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "subset_df = df_scan_pipe.query('directory == \"root_dir\" & seq < 10 & seq > 7')\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Filtrando Linhas Baseadas em Valores em uma Lista\n",
    "\n",
    "valores = [11, 16, 30, 41]\n",
    "subset_df = df_scan_pipe[df_scan_pipe['seq'].isin(valores)]\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registro_específico = df_scan_pipe.loc['e1f4b1af-30f3-45d2-85a7-1bb895bd5325']\n",
    "\n",
    "registro_específico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Efetuando Extracao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcoes de extracao\n",
    "def processar_dados_iniciais(row, original_file_name, file_path):\n",
    "    \n",
    "    # lista_texto_extraido = []\n",
    "    nf_dados_doc = {}\n",
    "    pdf_pesquisavel = None\n",
    "    extracted_txt = pesquisa_prefeitura_pdf_pesquisavel(file_path)\n",
    "    \n",
    "    if extracted_txt:\n",
    "        pdf_pesquisavel = True\n",
    "    else:\n",
    "        pdf_pesquisavel = False \n",
    "       \n",
    "        x0 = 110\n",
    "        y0 = 0\n",
    "        x1= 1929\n",
    "        y1 = 786\n",
    "        \n",
    "        image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "        extracted_txt = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "    \n",
    "    nf_dados_doc['file_name'] = original_file_name    \n",
    "    nf_dados_doc['pdf_pesquisavel'] = pdf_pesquisavel \n",
    "    value = {}   \n",
    "    texto_tratado = texto_extraido(extracted_txt)\n",
    "    value = define_dados_iniciais(texto_tratado)\n",
    "    if value:\n",
    "        nf_dados_doc.update(value)\n",
    "   \n",
    "\n",
    "\n",
    "    return nf_dados_doc\n",
    "\n",
    "def pesquisa_prefeitura_pdf_pesquisavel(file_path):    \n",
    "    \n",
    "    # Carregar o arquivo PDF\n",
    "    pdf_document = fitz.open(file_path)\n",
    "\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 4\n",
    "    x1 = 600\n",
    "    y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    \n",
    "    #print(text)\n",
    "    \n",
    "    if text:\n",
    "       page_number = 0\n",
    "       #print(page_number)\n",
    "    else:\n",
    "       page_number = 1\n",
    "       #print(page_number)\n",
    "    \n",
    "    pdf_document.close()\n",
    "   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Funcoes de Regex - cabecalho - documento ped pesquisavel\n",
    "nf_data_servico = {}\n",
    "nf_data_erros = {}\n",
    "nf_lista_erros = []\n",
    "\n",
    "# 0. Pesquisa PDF\n",
    "def is_pdf_searchable(pdf_path):\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        is_searchable = all(page.get_text(\"text\") != \"\" for page in pdf_document)\n",
    "        pdf_document.close()\n",
    "        return is_searchable\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao verificar o PDF: {e}\")\n",
    "        return False\n",
    "\n",
    "# 1. CABECALHO\n",
    "def extract_fields_cabecalho(text):\n",
    "    nf_data_cabecalho = {}\n",
    "    # Extrair Nome da Prefeitura\n",
    "    nome_prefeitura_match = re.search(r'PREFEITURA (.+)', text)\n",
    "    if nome_prefeitura_match:\n",
    "        nome_prefeitura = \"PREFEITURA \" + nome_prefeitura_match.group(1)\n",
    "        nf_data_cabecalho['nome_prefeitura'] = nome_prefeitura\n",
    "    else:\n",
    "        nf_data_cabecalho['nome_prefeitura'] = None    \n",
    "\n",
    "    # Extrair Tipo de NF\n",
    "    tipo_nf_match = re.search(r'NOTA FISCAL (.+)', text)\n",
    "    if tipo_nf_match:\n",
    "        tipo_nf = \"NOTA FISCAL \" + tipo_nf_match.group(1)\n",
    "        nf_data_cabecalho['tipo_nota_fiscal'] = tipo_nf\n",
    "    else:\n",
    "        nf_data_cabecalho['tipo_nota_fiscal'] = None    \n",
    "    \n",
    "    # Extrair Número da Nota\n",
    "    numero_nota_match = re.search(r'Número da Nota:\\s+(\\d+)', text)\n",
    "    if numero_nota_match:\n",
    "        nr_nro_nf = numero_nota_match.group(1)\n",
    "        nf_data_cabecalho['numero_nota_fiscal'] = numero_nota_match.group(1)\n",
    "    else:\n",
    "        nf_data_cabecalho['numero_nota_fiscal'] = None\n",
    "        nf_data_erros['cabecalho'] = \"numero_nota\"\n",
    "        nf_lista_erros.append(nf_data_erros)        \n",
    "\n",
    "    # Extrair Competência\n",
    "    competencia_match = re.search(r'Competência:\\s+(.+)', text)\n",
    "    if competencia_match:\n",
    "        nf_data_cabecalho['competencia'] = competencia_match.group(1)\n",
    "    else:\n",
    "        nf_data_cabecalho['competencia'] = None\n",
    "        nf_data_erros['cabecalho'] = \"competencia\"\n",
    "        nf_lista_erros.append(nf_data_erros)\n",
    "            \n",
    "\n",
    "    # Extrair Data e Hora de Emissão\n",
    "    data_emissao_match = re.search(r'Data e Hora da Emissão:\\s+(.+)', text)\n",
    "    if data_emissao_match:\n",
    "        nf_data_cabecalho['dt_hr_emissao'] = data_emissao_match.group(1)\n",
    "    else:\n",
    "        nf_data_cabecalho['dt_hr_emissao'] = None    \n",
    "        \n",
    "    # Extrair Data e Hora de Emissão\n",
    "    codigo_verificacao_match = re.search(r'Código Verificação:\\s+(.+)', text)\n",
    "    if codigo_verificacao_match:\n",
    "        nf_data_cabecalho['codigo_verificacao'] = codigo_verificacao_match.group(1)\n",
    "    else:\n",
    "        nf_data_cabecalho['codigo_verificacao'] = None\n",
    "        nf_data_erros['cabecalho'] = \"codigo_verificacao\"\n",
    "        nf_lista_erros.append(nf_data_erros)        \n",
    "\n",
    "    return nf_data_cabecalho\n",
    "\n",
    "\n",
    "def processar_cabecalho_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    section = \"1 - CABECALHO\"\n",
    "    frame_type = 'frame'\n",
    "    f_frame_label = \"1_frame_dados_nf\"\n",
    "    data_box_valores = {'secao': section}\n",
    "    nf_data_cabecalho = {}\n",
    "    message_erro = []\n",
    "    nf_data_cabecalho['original_file_name'] = original_file_name\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == frame_type) & (frames_nf_v4_df['label'] == f_frame_label)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        frame_id = row_frame['id']\n",
    "        label = row_frame['label']\n",
    "        x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "        #print(f'doc {\"pdf_pesquisavel\" if pdf_pesquisavel else \"raster_pdf\"}: {label:>30} | id: {frame_id:>3} | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6} ')\n",
    "        \n",
    "        pdf_document = fitz.open(file_path)\n",
    "        # Página do PDF\n",
    "        page_number = 0  # Defina o número da página que deseja analisar\n",
    "        page = pdf_document[page_number]\n",
    "        #x0, y0, x1, y1 = (0, 0, 600, 110)\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        nf_data_cabecalho = extract_fields_cabecalho(text)\n",
    "        # nf_data_cabecalho['original_file_name'] = original_file_name\n",
    "        # nf_data_cabecalho['pdf_pesquisavel'] = pdf_pesquisavel_map\n",
    "        \n",
    "        pdf_document.close()\n",
    "        \n",
    "    return nf_data_cabecalho \n",
    "\n",
    "\n",
    "\n",
    "def processar_cabecalho_raster_pdf(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    section = \"1 - CABECALHO\"\n",
    "    pdf_pesquisavel = False\n",
    "    father = \"1_frame_prefeitura_nf\"\n",
    "    data_extrated_prefeitura = {}\n",
    "    tipo_2 = \"sframe_field\"\n",
    "    \n",
    "    data_extrated_prefeitura['pdf_pesquisavel'] = pdf_pesquisavel_map\n",
    "    data_extrated_prefeitura['original_file_name'] = original_file_name\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['section_json'] == section)]\n",
    "    filtered_sframe_field_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['father'] == father) & (frames_nf_v4_df['type'] == tipo_2)]\n",
    "    image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        label_value = row_frame['label']\n",
    "        reference_value = row_frame['reference']\n",
    "        f_type = row_frame['type']\n",
    "        frame_id = row_frame['id']\n",
    "        if row_frame['label']  == \"1_frame_prefeitura_nf\":\n",
    "            x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "            texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "            #print(texto_extraido)\n",
    "            label_value = row_frame['label']\n",
    "            #print(\"label_value\", label_value)\n",
    "            values = texto_extraido.split('\\n')\n",
    "            for index_sframe, row_sframe in filtered_sframe_field_nf_v4_df.iterrows():\n",
    "                label_value = row_sframe['label']\n",
    "                #print(\"label_value\", label_value)\n",
    "                if label_value == \"nome_prefeitura\":\n",
    "                    reference_value = row_sframe['reference']\n",
    "                    for value in values:\n",
    "                        result = process_line(value, reference_value, label_value)\n",
    "                        if result:\n",
    "                            data_extrated_prefeitura.update(result)\n",
    "                elif label_value == \"secretaria\":\n",
    "                    reference_value = row_sframe['reference']\n",
    "                    for value in values:\n",
    "                        result = process_line(value, reference_value, label_value)\n",
    "                        if result:\n",
    "                            data_extrated_prefeitura.update(result) \n",
    "                elif label_value == \"tipo_nota_fiscal\":\n",
    "                    reference_value = row_sframe['reference']  \n",
    "                    for value in values:\n",
    "                        result = process_line(value, reference_value, label_value)\n",
    "                        if result:\n",
    "                            data_extrated_prefeitura.update(result)\n",
    "        \n",
    "        if row_frame['label']  == \"1_frame_dados_nf\":\n",
    "            x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "            texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "            #print(texto_extraido)\n",
    "            text_splited = texto_extraido_cabecalho(texto_extraido)\n",
    "            keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "            string_pesquisa = \"Número da Nota:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "            data_extrated_prefeitura['numero_nota_fiscal'] = texto\n",
    "\n",
    "            string_pesquisa = \"Competência:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_extrated_prefeitura['competencia'] = texto\n",
    "\n",
    "            string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_extrated_prefeitura['dt_hr_emissao'] = texto\n",
    "\n",
    "            string_pesquisa = \"Código Verificação:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_extrated_prefeitura['codigo_verificacao'] = texto\n",
    "            #print(f'{frame_id:>5}  {f_type:>20} | {label_value:>30} |  ref:{reference_value:>30}  | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6}')\n",
    "\n",
    "    return data_extrated_prefeitura\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_PIL(image, coordinates):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    image_croped = image.crop((x0, y0, x1, y1))\n",
    "    texto_extraido = pytesseract.image_to_string(image_croped, lang='por')\n",
    "    return texto_extraido "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Consulta multiparametros\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"MAGE\"\n",
    "tipo = \"frame\"\n",
    "label = \"2_frame_cnpj_prestador\"\n",
    "section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "pdf_pesquisavel_map = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipo= \"frame\"\n",
    "message_erro = []\n",
    "nf_dados_prestador = {}\n",
    "pdf_pesquisavel_map = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_20/MAGE_PDF_31282023_2254/1004813/NF 55 JML.pdf\"\n",
    "original_file_name = \"NF 55 JML.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_prestador_raster_pdf(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = False\n",
    "    \n",
    "    tipo= \"frame\"\n",
    "    message_erro = []\n",
    "    nf_dados_prestador = {}\n",
    "    model = row['model']\n",
    "    print(model)\n",
    "\n",
    "    process_prestador = ['2_frame_cnpj_prestador', '2_frame_inscricao_prestador', '2_frame_dados_prestador']\n",
    "    image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "    for father in process_prestador:\n",
    "            label = father\n",
    "            if label == \"2_frame_cnpj_prestador\": \n",
    "                coordinates = get_coordinates_filter(model_map, tipo, label, section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                extract_text = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                #text_splited = texto_extraido_nf(extract_text)\n",
    "                if \"CPF/CNPJ:\" in extract_text:\n",
    "                    cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', extract_text)\n",
    "                    if cpf_cnpj_formatado_match:\n",
    "                        texto = cpf_cnpj_formatado_match.group(1)\n",
    "                    \n",
    "                        nf_dados_prestador['cpf_cnpj_com_mascara'] = texto\n",
    "                        nf_dados_prestador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "                else:\n",
    "                    nf_dados_prestador['cpf_cnpj_com_mascara'] = 'None'\n",
    "                    nf_dados_prestador['cpf_cnpj_sem_mascara'] = 'None'           \n",
    "                        \n",
    "                telefone_str = None\n",
    "                #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "                telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', extract_text)\n",
    "                if telefone_match: \n",
    "                    telefone_str = telefone_match.group(1)\n",
    "                    # Remover quebras de linha\n",
    "                    telefone_str = telefone_str.replace('.', '')\n",
    "                    telefone_str = telefone_str.replace('\\n', '')\n",
    "                            \n",
    "                    nf_dados_prestador['telefone'] = telefone_str\n",
    "                else:\n",
    "                    nf_dados_prestador['telefone'] = 'None'\n",
    "                    \n",
    "            # if father_to_process == \"2_frame_cnpj_prestador\":\n",
    "            elif label == '2_frame_inscricao_prestador':\n",
    "                coordinates = get_coordinates_filter(model_map, tipo, label, section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                extract_text = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                \n",
    "                text_splited = texto_extraido_nf(extract_text)\n",
    "                \n",
    "                \n",
    "                # Extrair Inscrição Estadual\n",
    "                inscricao_estadual_match = re.search(r'Inscrição Estadual:\\s+(.+)', extract_text)\n",
    "                if inscricao_estadual_match:\n",
    "                    inscricao_estadual_str = inscricao_estadual_match.group(1)\n",
    "                    if inscricao_estadual_str == 'Nome/Razão Social:':\n",
    "                        nf_inscr_estadual = 'None'\n",
    "                        nf_dados_prestador['inscricao_estadual'] = \"NONE\"\n",
    "                    else:\n",
    "                        nf_inscr_estadual = inscricao_estadual_match.group(1)\n",
    "                        nf_dados_prestador['inscricao_estadual'] = inscricao_estadual_match.group(1) \n",
    "                \n",
    "\n",
    "            elif label == '2_frame_dados_prestador':\n",
    "                dados_prestador = {}\n",
    "                coordinates = get_coordinates_filter(model_map, tipo, label, section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                \n",
    "                extract_text = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                dados_prestador = extract_fields_prestador(extract_text)\n",
    "                \n",
    "                text_splited = texto_extraido_nf(extract_text)\n",
    "                keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "                string_pesquisa = \"Nome/Razão Social:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            \n",
    "                nf_dados_prestador['razao_social'] = texto\n",
    "\n",
    "                string_pesquisa = \"Nome de Fantasia:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                nf_dados_prestador['nome_fantasia'] = texto\n",
    "                \n",
    "                string_pesquisa = \"Endereço:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                nf_dados_prestador['endereco'] = texto\n",
    "                \n",
    "                string_pesquisa = \"E-mail:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                nf_dados_prestador['email'] = texto\n",
    "                \n",
    "    return nf_dados_prestador            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao importante - process_line\n",
    "def process_line(value, reference, label):\n",
    "    name_match = re.search(fr'{reference} (.+)', value)\n",
    "    if name_match:\n",
    "        extracted_value = reference + \" \" + name_match.group(1)\n",
    "        return {label: extracted_value}\n",
    "    return None\n",
    "\n",
    "# Função para extrair número da string\n",
    "def extract_number(text):\n",
    "    match = re.search(r'\\b\\d+(\\.\\d+)?\\b', text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# 5. XXX Ajusta texto\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF_Pesquisavel-  NO CABECALHO\n",
    "def texto_extraido_nf(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF RASTER NO CABECALHO\n",
    "def texto_extraido_cabecalho(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    text_splited = [s.replace(\")\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_prestador_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    frame_type = 'frame'\n",
    "    f_frame_label = \"2_frame_cnpj_prestador\"\n",
    "    data_box_valores = {'secao': section}\n",
    "    nf_data_prestador = {}\n",
    "    message_erro = []\n",
    "    nf_data_prestador['original_file_name'] = original_file_name\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == frame_type) & (frames_nf_v4_df['label'] == f_frame_label)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        frame_id = row_frame['id']\n",
    "        label = row_frame['label']\n",
    "        x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "        #print(f'doc {\"pdf_pesquisavel\" if pdf_pesquisavel else \"raster_pdf\"}: {label:>30} | id: {frame_id:>3} | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6} ')\n",
    "        \n",
    "        pdf_document = fitz.open(file_path)\n",
    "        # Página do PDF\n",
    "        page_number = 0  # Defina o número da página que deseja analisar\n",
    "        page = pdf_document[page_number]\n",
    "        #x0, y0, x1, y1 = (0, 0, 600, 110)\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        nf_data_prestador = extract_fields_prestador(text)\n",
    "        # nf_data_cabecalho['original_file_name'] = original_file_name\n",
    "        # nf_data_cabecalho['pdf_pesquisavel'] = pdf_pesquisavel_map\n",
    "        \n",
    "        pdf_document.close()\n",
    "        \n",
    "    return nf_data_prestador \n",
    "\n",
    "\n",
    "# 2. PRESTADOR DE SERVIÇO\n",
    "def extract_fields_prestador(text): # Função para extrair campos e valores dentro de um retângulo\n",
    "    nf_data_prestador = {}\n",
    "    \n",
    "    nf_data_prestador['secao'] = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    \n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in text:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "                        nf_data_prestador['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                        nf_data_prestador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "\n",
    "    # Extrair Inscrição Municipal\n",
    "    inscricao_municipal_match = re.search(r'Inscrição Municipal:\\s+(.+)', text)\n",
    "    if inscricao_municipal_match:\n",
    "        nf_data_prestador['inscricao_municipal'] = inscricao_municipal_match.group(1)\n",
    "        \n",
    "               \n",
    "    # Extrair Inscrição Estadual\n",
    "    #if \"Inscrição Estadual:\" in text:\n",
    "    \n",
    "    # Extrair Inscrição Estadual\n",
    "    inscricao_estadual_match = re.search(r'Inscrição Estadual:\\s+(.+)', text)\n",
    "    if inscricao_estadual_match:\n",
    "        inscricao_estadual_str = inscricao_estadual_match.group(1)\n",
    "        if inscricao_estadual_str == 'Nome/Razão Social:':\n",
    "            nf_data_prestador['inscricao_estadual'] = \"NONE\"\n",
    "        else:    \n",
    "            nf_data_prestador['inscricao_estadual'] = inscricao_estadual_match.group(1)       \n",
    "        \n",
    "                \n",
    "    \n",
    "\n",
    "    # Extrair Telefone\n",
    "    #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "    telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', text)\n",
    "    if telefone_match: \n",
    "        telefone_str = telefone_match.group(1)\n",
    "        # Remover quebras de linha\n",
    "        telefone_str = telefone_str.replace('.', '')\n",
    "        telefone_str = telefone_str.replace('\\n', '')\n",
    "                \n",
    "        nf_data_prestador['telefone'] = telefone_str\n",
    "    else:\n",
    "        nf_data_prestador['telefone'] = \"NONE\"\n",
    "\n",
    "         \n",
    "                \n",
    "    # Nome/Razão Social:\n",
    "    razao_social_match = re.search(r'Nome/Razão Social:\\s+(.+)', text)\n",
    "    if razao_social_match:\n",
    "        nf_data_prestador['razao_social'] = razao_social_match.group(1)  \n",
    "                \n",
    "    # Nome de Fantasia:\n",
    "    nome_fantasia_match = re.search(r'Nome de Fantasia:\\s+(.+)', text)\n",
    "    if nome_fantasia_match:\n",
    "        nf_data_prestador['nome_fantasia'] = nome_fantasia_match.group(1)                                    \n",
    "                \n",
    "            \n",
    "    # Endereço:\n",
    "    endereco_match = re.search(r'Endereço:\\s+(.+)', text)\n",
    "    if endereco_match:\n",
    "        nf_data_prestador['endereco'] = endereco_match.group(1) \n",
    "    \n",
    "    # E-mail:\n",
    "    email_match = re.search(r'E-mail:\\s+(.+)', text)\n",
    "    if email_match:\n",
    "        nf_data_prestador['email'] = email_match.group(1)  \n",
    "    else:\n",
    "        nf_data_prestador['email'] = \"NONE\"  # Valor padrão quando não há correspondência\n",
    "   \n",
    "        \n",
    "\n",
    "    return nf_data_prestador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. TOMADOR DE SERVIÇO\n",
    "def processar_tomador_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    section = \"3. TOMADOR DE SERVIÇO\"\n",
    "    frame_type = 'frame'\n",
    "    f_frame_label = \"3_frame_cnpj_tomador\"\n",
    "    data_box_valores = {'secao': section}\n",
    "    nf_data_tomador = {}\n",
    "    message_erro = []\n",
    "    nf_data_tomador['original_file_name'] = original_file_name\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == frame_type) & (frames_nf_v4_df['label'] == f_frame_label)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        frame_id = row_frame['id']\n",
    "        label = row_frame['label']\n",
    "        x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "        print(f'model: {model_map} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: {label:>30} | id: {frame_id:>3} | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6} ')\n",
    "        \n",
    "        pdf_document = fitz.open(file_path)\n",
    "        # Página do PDF\n",
    "        page_number = 0  # Defina o número da página que deseja analisar\n",
    "        page = pdf_document[page_number]\n",
    "        #x0, y0, x1, y1 = (0, 0, 600, 110)\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        nf_data_tomador = extract_fields_tomador(text)\n",
    "  \n",
    "        pdf_document.close()\n",
    "        \n",
    "    return nf_data_tomador \n",
    "\n",
    "\n",
    "# 3. TOMADOR DE SERVIÇO\n",
    "def extract_fields_tomador(text):\n",
    "    nf_data_tomador = {}\n",
    "    \n",
    "    \n",
    "    nf_data_tomador['secao'] = \"3. TOMADOR DE SERVIÇO\"\n",
    "    \n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in text:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "                        nf_data_tomador['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                        nf_data_tomador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "\n",
    "        \n",
    "    # Extrair RG    \n",
    "    rg_match = re.search(r'RG:\\s+(.+)', text)   \n",
    "    if rg_match:\n",
    "        rg_str = rg_match.group(1)\n",
    "        if rg_str == 'Telefone:':\n",
    "            nf_data_tomador['rg'] = \"NONE\"  # Valor padrão quando não há correspondência\n",
    "        else:    \n",
    "            nf_data_tomador['rg'] = rg_match.group(1)  \n",
    " \n",
    "        \n",
    "    # Extrair Telefone\n",
    "    telefone_match = re.search(r'Telefone:\\s+(.+)', text)\n",
    "    if telefone_match:\n",
    "        telefone_str = telefone_match.group(1)\n",
    "        if telefone_str == 'Inscrição Estadual:':\n",
    "            nf_data_tomador['telefone'] = \"NONE\"  # Valor padrão quando não há correspondência\n",
    "        else:    \n",
    "            nf_data_tomador['telefone'] = telefone_match.group(1)\n",
    "     \n",
    "\n",
    "    # Extrair Inscrição Municipal\n",
    "    inscricao_municipal_match = re.search(r'Inscrição Municipal:\\s+(.+)', text)\n",
    "    if inscricao_municipal_match:\n",
    "        nf_data_tomador['inscricao_municipal'] = inscricao_municipal_match.group(1)\n",
    "                \n",
    "                \n",
    "                \n",
    "    # Extrair Inscrição Estadual\n",
    "    inscricao_estadual_match = re.search(r'Inscrição Estadual:\\s+(.+)', text)\n",
    "    if inscricao_estadual_match:\n",
    "        inscricao_estadual_str = inscricao_estadual_match.group(1)\n",
    "        if inscricao_estadual_str == 'Nome/Razão Social:':\n",
    "            nf_data_tomador['inscricao_estadual'] = \"NONE\"\n",
    "        else:    \n",
    "            nf_data_tomador['inscricao_estadual'] = inscricao_estadual_match.group(1)   \n",
    "                \n",
    "    \n",
    "    # Nome/Razão Social:\n",
    "    razao_social_match = re.search(r'Nome/Razão Social:\\s+(.+)', text)\n",
    "    if razao_social_match:\n",
    "        nf_data_tomador['razao_social'] = razao_social_match.group(1)                                                \n",
    "                \n",
    "    # Endereço:\n",
    "    endereco_match = re.search(r'Endereço:\\s+(.+)', text)\n",
    "    if endereco_match:\n",
    "        nf_data_tomador['endereco'] = endereco_match.group(1) \n",
    "    \n",
    "    # E-mail:\n",
    "    email_match = re.search(r'E-mail:\\s+(.+)', text)\n",
    "    if email_match:\n",
    "        nf_data_tomador['email'] = email_match.group(1) \n",
    "    else:\n",
    "        nf_data_tomador['email'] = \"NONE\"  # Valor padrão quando não há correspondência    \n",
    "\n",
    "    return nf_data_tomador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. DESCRIMINACAO DOS SERVIÇOS\n",
    "def processar_servicos_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    section = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "    frame_type = 'frame'\n",
    "    f_frame_label = \"4_frame_descricao_totais\"\n",
    "    data_box_valores = {'secao': section}\n",
    "    nf_data_servico = {}\n",
    "    message_erro = []\n",
    "    nf_data_servico['original_file_name'] = original_file_name\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == frame_type) & (frames_nf_v4_df['label'] == f_frame_label)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        frame_id = row_frame['id']\n",
    "        label = row_frame['label']\n",
    "        x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "        print(f'model: {model_map} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: {label:>30} | id: {frame_id:>3} | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6} ')\n",
    "        \n",
    "        pdf_document = fitz.open(file_path)\n",
    "        # Página do PDF\n",
    "        page_number = 0  # Defina o número da página que deseja analisar\n",
    "        page = pdf_document[page_number]\n",
    "        #x0, y0, x1, y1 = (0, 0, 600, 110)\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        \n",
    "        # Remover quebras de linha e rótulo\n",
    "        text = text.replace('\\n', ' ')\n",
    "        label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "        if text.startswith(label):\n",
    "            text = text[len(label):].strip()\n",
    "\n",
    "\n",
    "        try:\n",
    "            discrimanacao_servico = text   \n",
    "        except Exception as e:\n",
    "            msg = (f\"doc: | {e}\")\n",
    "            discrimanacao_servico = \"Descricao nao encontrada\"\n",
    "\n",
    "        # Atribuir texto ao dicionário\n",
    "        nf_data_servico['discriminacao_servicos'] = discrimanacao_servico\n",
    "        \n",
    "        pdf_document.close()\n",
    "        \n",
    "    return nf_data_servico "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates_filter(pdf_pesquisavel_map, model, tipo, label, section):\n",
    "    \n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model, type=tipo, label=label, section_json=section)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [((row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p']) if pdf_pesquisavel_map else (row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1']))]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates_filter_pdf_pesquisavel(model, tipo, label, section):\n",
    "    \n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model, type=tipo, label=label, section_json=section)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [(row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p'])]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POC COM FRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame para teste\n",
    "i_frame = 0\n",
    "\n",
    "frames_processo = []\n",
    "# Filtrar o DataFrame para incluir apenas linhas onde a coluna \"model\" oriundo de: modelo\n",
    "filtered_frames_info = frames_info[frames_info['model'] == model]\n",
    "for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "    frame_name = row_frame['label']\n",
    "    frames_processo.append(frame_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_pesquisavel_map = False\n",
    "section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "model = \"MAGE\"\n",
    "tipo = \"frame\"\n",
    "label = \"2_frame_cnpj_prestador\"\n",
    "\n",
    "get_coordinates_filter(pdf_pesquisavel_map, model, tipo, label, section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: MAGE pdf_pesquisavel: labe:          1_frame_prefeitura_nf || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:               1_frame_dados_nf || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:         2_frame_cnpj_prestador || x0:   0.0 | y0: 100.0 | x1: 600.0 | y1: 236.0 \n",
      "model: MAGE pdf_pesquisavel: labe:    2_frame_inscricao_prestador || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:        2_frame_dados_prestador || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:           3_frame_cnpj_tomador || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:      3_frame_inscricao_tomador || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:          3_frame_dados_tomador || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:       4_frame_descricao_totais || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:            4_frame_valor_total || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:     4_frame_cnae_itens_servico || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:       5_frame_valores_impostos || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:   5_frame_dados_complementares || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:           5_frame_inf_criticas || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:             5_frame_observacao || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n"
     ]
    }
   ],
   "source": [
    "pdf_pesquisavel_map = True\n",
    "for frame in frames_processo:\n",
    "    label = frame\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map, model, tipo, label, section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    print(f'model: {model} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6} ')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: MAGE pdf_pesquisavel: labe:          1_frame_prefeitura_nf || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:               1_frame_dados_nf || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:         2_frame_cnpj_prestador || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:    2_frame_inscricao_prestador || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:        2_frame_dados_prestador || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:           3_frame_cnpj_tomador || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:      3_frame_inscricao_tomador || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:          3_frame_dados_tomador || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:       4_frame_descricao_totais || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:            4_frame_valor_total || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:     4_frame_cnae_itens_servico || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:       5_frame_valores_impostos || x0:   0.0 | y0: 545.0 | x1: 600.0 | y1: 650.0 \n",
      "model: MAGE pdf_pesquisavel: labe:   5_frame_dados_complementares || x0:   0.0 | y0: 650.0 | x1: 600.0 | y1: 680.0 \n",
      "model: MAGE pdf_pesquisavel: labe:           5_frame_inf_criticas || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n",
      "model: MAGE pdf_pesquisavel: labe:             5_frame_observacao || x0:   nan | y0:   nan | x1:   nan | y1:   nan \n"
     ]
    }
   ],
   "source": [
    "pdf_pesquisavel_map = True\n",
    "for frame in frames_processo:\n",
    "    model = \"MAGE\"\n",
    "    label = frame\n",
    "    coordinates = get_coordinates_filter_pdf_pesquisavel(model, tipo, label, section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    print(f'model: {model} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: MAGE pdf_pesquisavel: labe:       4_frame_descricao_totais || x0:   0.0 | y0: 330.0 | x1: 600.0 | y1: 500.0 \n"
     ]
    }
   ],
   "source": [
    "pdf_pesquisavel_map = True\n",
    "section = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "model = \"MAGE\"\n",
    "tipo = \"frame\"\n",
    "label = \"4_frame_descricao_totais\"\n",
    "\n",
    "coordinates = get_coordinates_filter_pdf_pesquisavel(model, tipo, label, section)\n",
    "x0, y0, x1, y1 = coordinates[0]\n",
    "get_coordinates_filter(pdf_pesquisavel_map, model, tipo, label, section)\n",
    "print(f'model: {model} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: MAGE pdf_pesquisavel: labe:       5_frame_valores_impostos || x0:   0.0 | y0: 545.0 | x1: 600.0 | y1: 650.0 \n"
     ]
    }
   ],
   "source": [
    "pdf_pesquisavel_map = True\n",
    "section = \"8. DADOS COMPLEMENTARES\"\n",
    "model = \"MAGE\"\n",
    "tipo = \"frame\"\n",
    "label = \"5_frame_valores_impostos\"\n",
    "\n",
    "coordinates = get_coordinates_filter_pdf_pesquisavel(model, tipo, label, section)\n",
    "x0, y0, x1, y1 = coordinates[0]\n",
    "get_coordinates_filter(pdf_pesquisavel_map, model, tipo, label, section)\n",
    "print(f'model: {model} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame para teste\n",
    "i_frame = 0\n",
    "\n",
    "frames_processo = []\n",
    "# Filtrar o DataFrame para incluir apenas linhas onde a coluna \"model\" oriundo de: modelo\n",
    "filtered_frames_info = frames_info[frames_info['model'] == model]\n",
    "for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "    frame_name = row_frame['label']\n",
    "    frames_processo.append(frame_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Nome Batch\n",
    "batch_name = \"Batch_\" + str(tipo_pdf[i_test]) + \"_\" + str(i_frame)\n",
    "\n",
    "# Nome do arquivo json\n",
    "nome_formado_json = batch_name +\".json\"\n",
    "\n",
    "# Listagem dos frames de pesquisa\n",
    "i = 0\n",
    "for frame in frames_pesquisa:\n",
    "    print(f'seq ={i:>3} | {frame}')\n",
    "    i += 1\n",
    "    \n",
    "if frames_pesquisa[i_frame]:\n",
    "    print(f'\\n\\nDados do teste: batch_name: {batch_name} | frame: {frames_pesquisa[i_frame]} | model: {modelo} | tipo_pdf: {tipo_pdf[i_test]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_prestador_raster_pdf(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    nf_data_servico = {}\n",
    "    \n",
    "    process = ['4_frame_descricao_totais']\n",
    "    \n",
    "    section = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "    pdf_pesquisavel_map = True\n",
    "        \n",
    "    tipo= \"frame\"\n",
    "    message_erro = []\n",
    "    nf_dados_prestador = {}\n",
    "    model = row['model']\n",
    "    print(model)\n",
    "\n",
    "    for father in process_prestador:\n",
    "        label = father\n",
    "        if label == \"4_frame_descricao_totais\": \n",
    "        coordinates = get_coordinates_filter(model_map, tipo, label, section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                extract_text = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                #text_splited = texto_extraido_nf(extract_text)\n",
    "                if \"CPF/CNPJ:\" in extract_text:\n",
    "                    cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', extract_text)\n",
    "                    if cpf_cnpj_formatado_match:\n",
    "                        texto = cpf_cnpj_formatado_match.group(1)\n",
    "                    \n",
    "                        nf_dados_prestador['cpf_cnpj_com_mascara'] = texto\n",
    "                        nf_dados_prestador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "                else:\n",
    "                    nf_dados_prestador['cpf_cnpj_com_mascara'] = 'None'\n",
    "                    nf_dados_prestador['cpf_cnpj_sem_mascara'] = 'None'           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta e o principio da melhor funcao do mundo\n",
    "def extracao_pipeline(qualquer_df, fase, atividade, status):\n",
    "    \n",
    "    doc_info = {}\n",
    "    resumo = {}\n",
    "    row_teste_info = []\n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    func_fase = fase\n",
    "    func_atividade = atividade\n",
    "    func_status = status\n",
    "    lista_dicts = []\n",
    "   \n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        dados_iniciais = {}\n",
    "        # dados_root_pipe[idx] = row.to_dict()\n",
    "        row_info = row.to_dict()\n",
    "        message_erro = []\n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        map_document_unique_id = idx\n",
    "        #dados_root_pipe['document_unique_id'] = map_document_unique_id\n",
    "        map_seq = row['seq']\n",
    "        #dados_root_pipe['seq'] = map_seq\n",
    "        map_batch_name = row['batch']\n",
    "        #dados_root_pipe['batch'] = map_batch_name\n",
    "        map_fase_processo = row['fase_processo']\n",
    "        #dados_root_pipe['fase_processo'] = map_fase_processo\n",
    "        map_nome_atividade = row['nome_atividade']\n",
    "        map_status_documento = row['status_documento']\n",
    "        map_original_file_name = row['original_file_name']\n",
    "        map_directory = row['directory']\n",
    "        map_one_page = row['one_page']\n",
    "        map_palavra_chave = row['palavra_chave']\n",
    "        map_document_tag = row['document_tag']\n",
    "        map_action_item = row['action_item']\n",
    "        map_level = row['level']\n",
    "        file_path = row['file_path']\n",
    "        if (map_status_documento != 'NO_PROCESS'):\n",
    "            #1. Buscar Prefeitura, de/para e modelo\n",
    "            if map_status_documento == 'PREPROCESS_EXTRACT':\n",
    "                \n",
    "                dados_iniciais = processar_dados_iniciais(row, map_original_file_name, file_path)\n",
    "                \n",
    "                prefeitura_map = dados_iniciais['prefeitura']\n",
    "                pdf_pesquisavel_map = dados_iniciais['pdf_pesquisavel']\n",
    "                de_para_map = dados_iniciais['de_para_pm']\n",
    "                model_map = dados_iniciais['model']\n",
    "                #cnpj_map = dados_iniciais['cnpj']\n",
    "                \n",
    "                row_info['document_unique_id'] = map_document_unique_id\n",
    "                row_info['prefeitura'] = prefeitura_map\n",
    "                row_info['pdf_pesquisavel'] = pdf_pesquisavel_map    \n",
    "                row_info['de_para_pm'] = de_para_map\n",
    "                row_info['model'] = model_map\n",
    "                \n",
    "                #row_info['cnpj'] = cnpj_map\n",
    "                data_cabecalho = {}\n",
    "                data_cabecalho['secao'] = \"1 - CABECALHO\"\n",
    "                \n",
    "                if pdf_pesquisavel_map:\n",
    "                   data_cabecalho = processar_cabecalho_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, map_original_file_name, file_path)\n",
    "                else:\n",
    "                    data_cabecalho = processar_cabecalho_raster_pdf(row, pdf_pesquisavel_map, model_map, map_original_file_name, file_path)\n",
    "                \n",
    "                nome_prefeitura_map = data_cabecalho['nome_prefeitura']\n",
    "                tipo_nota_fiscal_map = data_cabecalho['tipo_nota_fiscal']\n",
    "                nro_nota_map = data_cabecalho['numero_nota_fiscal']\n",
    "                competencia_map = data_cabecalho['competencia']\n",
    "                dt_hr_emissao_map = data_cabecalho['dt_hr_emissao']  \n",
    "                codigo_ver_map = data_cabecalho['codigo_verificacao']\n",
    "                \n",
    "                row_info['prefeitura'] = nome_prefeitura_map \n",
    "                row_info['tipo_nota_fiscal'] = tipo_nota_fiscal_map\n",
    "                row_info['numero_nota_fiscal'] = nro_nota_map\n",
    "                row_info['competencia'] = competencia_map\n",
    "                row_info['dt_hr_emissao'] = dt_hr_emissao_map\n",
    "                row_info['codigo_verificacao'] = codigo_ver_map\n",
    "                \n",
    "                erros = []\n",
    "                prestador_lista = []\n",
    "                data_prestador = {}\n",
    "                if pdf_pesquisavel_map:\n",
    "                   data_prestador = processar_prestador_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, map_original_file_name, file_path)\n",
    "                try:   \n",
    "                    row_info['p_cpf_cnpj_com_mascara'] = data_prestador['cpf_cnpj_com_mascara']\n",
    "                    row_info['p_cpf_cnpj_sem_mascara'] = data_prestador['cpf_cnpj_sem_mascara']\n",
    "                    row_info['p_telefone'] = data_prestador['telefone']\n",
    "                    row_info['p_inscricao_municipal'] = data_prestador['inscricao_municipal']\n",
    "                    row_info['p_inscricao_estadual'] =  data_prestador['inscricao_estadual']\n",
    "                    row_info['p_razao_social'] = data_prestador['razao_social']\n",
    "                    row_info['p_nome_fantasia']= data_prestador['nome_fantasia']\n",
    "                    row_info['p_endereco'] = data_prestador['endereco']\n",
    "                    row_info['p_email'] = data_prestador['email']\n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "                    row_info['p_cpf_cnpj_com_mascara'] = \" \"\n",
    "                    row_info['p_cpf_cnpj_sem_mascara'] = \" \"\n",
    "                    row_info['p_telefone'] = \" \"\n",
    "                    row_info['p_inscricao_municipal'] = \" \"\n",
    "                    row_info['p_inscricao_estadual'] = \" \"\n",
    "                    row_info['p_razao_social'] = \" \"\n",
    "                    row_info['p_nome_fantasia'] = \" \"\n",
    "                    row_info['p_endereco'] = \" \"\n",
    "                    row_info['p_email'] = \" \"\n",
    "                    message_erro.append(msg)\n",
    "                    \n",
    "                #row_teste_info.append(data_prestador)\n",
    "                erros = []\n",
    "                tomador_lista = []\n",
    "                data_tomador = {}\n",
    "                \n",
    "                if pdf_pesquisavel_map:\n",
    "                   data_tomador = processar_tomador_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, map_original_file_name, file_path)\n",
    "                try:   \n",
    "                    row_info['t_cpf_cnpj_com_mascara'] = data_tomador['cpf_cnpj_com_mascara']\n",
    "                    row_info['t_cpf_cnpj_sem_mascara'] = data_tomador['cpf_cnpj_sem_mascara']\n",
    "                    row_info['t_rg'] = data_tomador['rg']\n",
    "                    row_info['t_telefone'] = data_tomador['telefone']\n",
    "                    row_info['t_inscricao_municipal'] = data_tomador['inscricao_municipal']\n",
    "                    row_info['t_inscricao_estadual'] =  data_tomador['inscricao_estadual']\n",
    "                    row_info['t_razao_social'] = data_tomador['razao_social']\n",
    "                    row_info['t_endereco'] = data_tomador['endereco']\n",
    "                    row_info['t_email'] = data_tomador['email']\n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "                    row_info['t_cpf_cnpj_com_mascara'] = \" \"\n",
    "                    row_info['t_cpf_cnpj_sem_mascara'] = \" \"\n",
    "                    row_info['t_rg'] = \" \"\n",
    "                    row_info['t_telefone'] = \" \"\n",
    "                    row_info['t_inscricao_municipal'] = \" \"\n",
    "                    row_info['t_inscricao_estadual'] = \" \"\n",
    "                    row_info['t_razao_social'] = \" \"\n",
    "                    row_info['t_endereco'] = \" \"\n",
    "                    row_info['t_email'] = \" \"\n",
    "                    message_erro.append(msg)\n",
    "                \n",
    "                nf_data_servicor = {}    \n",
    "                if pdf_pesquisavel_map:\n",
    "                   nf_data_servicor = processar_servicos_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, map_original_file_name, file_path)\n",
    "                          \n",
    "                try:\n",
    "                    discrimanacao_servico = nf_data_servicor['discriminacao_servicos'] \n",
    "                    row_info['discriminacao_servicos'] = discrimanacao_servico \n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "                    discrimanacao_servico = \"Descricao nao encontrada\"\n",
    "                    row_info['discriminacao_servicos'] = discrimanacao_servico\n",
    "                print(discrimanacao_servico)\n",
    "                #print(f' i: {i}  map_seq: {map_seq:>5} Pesquisa: {pdf_pesquisavel_map}  | file: {map_original_file_name}\\n\\{data_prestador}\\n\\n\\n')\n",
    "                # print(f' 1 - seq: {map_seq:>5} Pesquisa? {pdf_pesquisavel_map} | {cpf_cnpj_com_masc_map:>13} | Pesq: {razao_social_map:>400} | dt_hr_emissao: {endereco_map}')\n",
    "                \n",
    "\n",
    "                                      \n",
    "                               \n",
    "                \n",
    "\n",
    "                #row_teste_info.append(data_cabecalho)        \n",
    "                # \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                lista_dicts.append(row_info)\n",
    "        i += 1\n",
    "\n",
    "    novo_df = pd.DataFrame(lista_dicts)\n",
    "     \n",
    "    return novo_df\n",
    "    #return prestador_lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: MAGE pdf_pesquisavel:           3_frame_cnpj_tomador | id:  27 | x0:    0.0 y0:  210.0 x1:  600.0 y1:  340.0 \n",
      "model: MAGE pdf_pesquisavel:       4_frame_descricao_totais | id:  40 | x0:    0.0 y0:  330.0 x1:  600.0 y1:  500.0 \n",
      "Comissões de vendas em produtos de papelaria \n",
      "Descricao nao encontrada\n",
      "Descricao nao encontrada\n",
      "Descricao nao encontrada\n",
      "Descricao nao encontrada\n",
      "model: MAGE pdf_pesquisavel:           3_frame_cnpj_tomador | id:  27 | x0:    0.0 y0:  210.0 x1:  600.0 y1:  340.0 \n",
      "model: MAGE pdf_pesquisavel:       4_frame_descricao_totais | id:  40 | x0:    0.0 y0:  330.0 x1:  600.0 y1:  500.0 \n",
      "REFERENTE A VENDA DA UNIDADE 401, BLOCO 1 CLIENTE: Carlos Eduardo Pereira Alvaro Santos CPF: 17457292756 DADOS BANCARIOS: RAZAO SOCIAL: J MACHADO NETO IMOVEIS CNPJ: 44884090000104 Banco: 336 - BANCO C6 S.A. Agência: 0001 Conta Corrente: 15098040-0 PIx: 44884090000104\n",
      "model: MAGE pdf_pesquisavel:           3_frame_cnpj_tomador | id:  27 | x0:    0.0 y0:  210.0 x1:  600.0 y1:  340.0 \n",
      "model: MAGE pdf_pesquisavel:       4_frame_descricao_totais | id:  40 | x0:    0.0 y0:  330.0 x1:  600.0 y1:  500.0 \n",
      " financeiro@floc.com.br DISCRIMINAÇÃO DOS SERVIÇOS Serviços Portaria Julho 2023 e Diárias Festa Capela R$ 250,00 \n"
     ]
    }
   ],
   "source": [
    "# 3B. XXX Efetuo a extracao de dados\n",
    "# analisar_pdf_pesquisavel\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'PREPROCESS' \n",
    "status = 'PREPROCESS_EXTRACT'\n",
    "raw_document_list = []\n",
    "dados_iniciais_qr = {}\n",
    "incricao_prest = {}\n",
    "dados_prest = {}\n",
    "\n",
    "#raw_texto = extracao_pipeline(df_analise_pipe, fase, atividade, status)subset_df\n",
    "# df_ini, dados_iniciais_qr = extracao_pipeline(df_root_pipe, fase, atividade, status)\n",
    "df = extracao_pipeline(df_root_pipe, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>date_time</th>\n",
       "      <th>batch</th>\n",
       "      <th>fase_processo</th>\n",
       "      <th>nome_atividade</th>\n",
       "      <th>status_documento</th>\n",
       "      <th>acao_executada</th>\n",
       "      <th>original_file_name</th>\n",
       "      <th>directory</th>\n",
       "      <th>one_page</th>\n",
       "      <th>...</th>\n",
       "      <th>t_cpf_cnpj_com_mascara</th>\n",
       "      <th>t_cpf_cnpj_sem_mascara</th>\n",
       "      <th>t_rg</th>\n",
       "      <th>t_telefone</th>\n",
       "      <th>t_inscricao_municipal</th>\n",
       "      <th>t_inscricao_estadual</th>\n",
       "      <th>t_razao_social</th>\n",
       "      <th>t_endereco</th>\n",
       "      <th>t_email</th>\n",
       "      <th>discriminacao_servicos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Nota Fiscal Eletrônica julho.pdf</td>\n",
       "      <td>1004268</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Comissões de vendas em produtos de papelaria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>1.pdf</td>\n",
       "      <td>11790236</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Descricao nao encontrada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NF 55 JML.pdf</td>\n",
       "      <td>1004813</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Descricao nao encontrada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Digitalização 26 de jul de 2023(1).pdf</td>\n",
       "      <td>11779446</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Descricao nao encontrada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>JULH-2023-NotaFiscal 2.pdf</td>\n",
       "      <td>241538</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Descricao nao encontrada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Nota Fiscal Nº27 VITALE ECO.pdf</td>\n",
       "      <td>11778530</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>REFERENTE A VENDA DA UNIDADE 401, BLOCO 1 CLIE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31</td>\n",
       "      <td>13/09/2023 08:32:22</td>\n",
       "      <td>Batch_20</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NFS.300 07.2023 FLOC TEXTIL.pdf</td>\n",
       "      <td>10597</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>financeiro@floc.com.br DISCRIMINAÇÃO DOS SERV...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq            date_time     batch fase_processo nome_atividade  \\\n",
       "0    7  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "1   14  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "2   18  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "3   22  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "4   23  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "5   24  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "6   31  13/09/2023 08:32:22  Batch_20       analise   scan_analise   \n",
       "\n",
       "     status_documento acao_executada                      original_file_name  \\\n",
       "0  PREPROCESS_EXTRACT        Analise        Nota Fiscal Eletrônica julho.pdf   \n",
       "1  PREPROCESS_EXTRACT        Analise                                   1.pdf   \n",
       "2  PREPROCESS_EXTRACT        Analise                           NF 55 JML.pdf   \n",
       "3  PREPROCESS_EXTRACT        Analise  Digitalização 26 de jul de 2023(1).pdf   \n",
       "4  PREPROCESS_EXTRACT        Analise              JULH-2023-NotaFiscal 2.pdf   \n",
       "5  PREPROCESS_EXTRACT        Analise         Nota Fiscal Nº27 VITALE ECO.pdf   \n",
       "6  PREPROCESS_EXTRACT        Analise         NFS.300 07.2023 FLOC TEXTIL.pdf   \n",
       "\n",
       "   directory  one_page  ...  t_cpf_cnpj_com_mascara t_cpf_cnpj_sem_mascara  \\\n",
       "0    1004268     False  ...                                                  \n",
       "1   11790236      True  ...                                                  \n",
       "2    1004813      True  ...                                                  \n",
       "3   11779446      True  ...                                                  \n",
       "4     241538      True  ...                                                  \n",
       "5   11778530      True  ...                                                  \n",
       "6      10597      True  ...                                                  \n",
       "\n",
       "  t_rg t_telefone  t_inscricao_municipal t_inscricao_estadual t_razao_social  \\\n",
       "0                                                                              \n",
       "1                                                                              \n",
       "2                                                                              \n",
       "3                                                                              \n",
       "4                                                                              \n",
       "5                                                                              \n",
       "6                                                                              \n",
       "\n",
       "  t_endereco  t_email                             discriminacao_servicos  \n",
       "0                          Comissões de vendas em produtos de papelaria   \n",
       "1                                               Descricao nao encontrada  \n",
       "2                                               Descricao nao encontrada  \n",
       "3                                               Descricao nao encontrada  \n",
       "4                                               Descricao nao encontrada  \n",
       "5                      REFERENTE A VENDA DA UNIDADE 401, BLOCO 1 CLIE...  \n",
       "6                       financeiro@floc.com.br DISCRIMINAÇÃO DOS SERV...  \n",
       "\n",
       "[7 rows x 48 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Salvando o DF (IMPORTANTE)\n",
    "df.to_excel(\"df_root_ana_parcial1.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Consulta multiparametros\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields_cnpj(texto_extraido, original_file_name): \n",
    "  \n",
    "    message_erro = []\n",
    "    \n",
    "    data_nf_cnpj_prestador = {}\n",
    "      \n",
    "    print(f'2 extract_fields_cnpj:  {texto_extraido}')\n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in texto_extraido:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', texto_extraido)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "            data_nf_cnpj_prestador['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "            data_nf_cnpj_prestador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "    else:\n",
    "        data_nf_cnpj_prestador['cpf_cnpj_com_mascara'] = None\n",
    "        data_nf_cnpj_prestador['cpf_cnpj_sem_mascara'] = None            \n",
    "            \n",
    "            \n",
    "    telefone_str = None\n",
    "\n",
    "    #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "    telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', texto_extraido)\n",
    "    if telefone_match: \n",
    "        telefone_str = telefone_match.group(1)\n",
    "        # Remover quebras de linha\n",
    "        telefone_str = telefone_str.replace('.', '')\n",
    "        telefone_str = telefone_str.replace('\\n', '')\n",
    "                \n",
    "        data_nf_cnpj_prestador['telefone'] = telefone_str\n",
    "    else:\n",
    "        data_nf_cnpj_prestador['telefone'] = None \n",
    "    \n",
    "    #print(data_nf_cnpj_prestador)    \n",
    "\n",
    "             \n",
    "    return data_nf_cnpj_prestador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 7. VALORES E IMPOSTOS\n",
    "def extract_fields_impostos(text):\n",
    "    nf_data_valores = {}\n",
    "    nf_data_valores['secao'] = \"7. VALORES E IMPOSTOS\"\n",
    "    \n",
    "    # Extrair VALOR SERVIÇOS:\n",
    "    valor_servicos_match = re.search(r'VALOR SERVIÇOS:\\s+(.+)', text)\n",
    "    if valor_servicos_match:\n",
    "        valor_servicos_str = valor_servicos_match.group(1)\n",
    "        valor_servicos_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_servicos_str)\n",
    "        if valor_servicos_sem_formato:\n",
    "            valor_servicos_sem_formatacao = valor_servicos_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_servicos'] = float(valor_servicos_sem_formatacao)\n",
    "        else:\n",
    "            nf_data_valores['valor_servicos'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "  \n",
    "  \n",
    "    # Extrair VALOR DEDUÇÃO:\n",
    "    valor_deducao_match = re.search(r'DEDUÇÃO:\\s+(.+)', text)\n",
    "    if valor_deducao_match:\n",
    "        valor_deducao_str = valor_deducao_match.group(1)\n",
    "        valor_deducao_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_deducao_str)\n",
    "        if valor_deducao_sem_formato:\n",
    "            valor_deducao_sem_formato = valor_deducao_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_deducao'] = float(valor_deducao_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_deducao'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "        \n",
    "        \n",
    "    # Extrair DESC. INCOND:\n",
    "    valor_desc_match = re.search(r'DESC. INCOND:\\s+(.+)', text)\n",
    "    if valor_desc_match:\n",
    "        valor_desc_str = valor_desc_match.group(1)\n",
    "        valor_desc_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_desc_str)\n",
    "        if valor_desc_sem_formato:\n",
    "            valor_desc_sem_formato = valor_desc_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['desc_incond'] = float(valor_desc_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['desc_incond'] = 0.0  # Valor não encontrado ou não está no formato esperado        \n",
    "        \n",
    "\n",
    "    # Extrair BASE DE CÁLCULO:\n",
    "    valor_calculo_match = re.search(r'CÁLCULO:\\s+(.+)', text)\n",
    "    if valor_calculo_match:\n",
    "        valor_calculo_str = valor_calculo_match.group(1)\n",
    "        valor_calculo_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_calculo_str)\n",
    "        if valor_calculo_sem_formato:\n",
    "            valor_calculo_sem_formato = valor_calculo_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['base_calculo'] = float(valor_calculo_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['base_calculo'] = 0.0  # Valor não encontrado ou não está no formato esperado    \n",
    "\n",
    "\n",
    "\n",
    "    # Extrair ALÍQUOTA:\n",
    "    valor_aliquota_match = re.search(r'ALÍQUOTA:\\s+(.+)', text)\n",
    "    if valor_aliquota_match:\n",
    "        valor_aliquota_str = valor_aliquota_match.group(1)\n",
    "        valor_aliquota_sem_formato = re.search(r'([\\d.,]+)%', valor_aliquota_str)  # Ajuste aqui\n",
    "        if valor_aliquota_sem_formato:\n",
    "            valor_aliquota_sem_formato = valor_aliquota_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['aliquota'] = float(valor_aliquota_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['aliquota'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "\n",
    "\n",
    "    # Extrair VALOR ISS:\n",
    "    valor_iss_match = re.search(r'VALOR ISS:\\s+(.+)', text)\n",
    "    if valor_iss_match:\n",
    "        valor_iss_str = valor_iss_match.group(1)\n",
    "        valor_iss_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_iss_str)\n",
    "        if valor_iss_sem_formato:\n",
    "            valor_iss_sem_formato = valor_iss_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_iss'] = float(valor_iss_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_iss'] = 0.0  # Valor não encontrado ou não está no formato esperado \n",
    "\n",
    "    # Extrair VALOR ISS RETIDO:\n",
    "    valor_iss_retido_match = re.search(r'RETIDO:\\s+(.+)', text)\n",
    "    if valor_iss_match:\n",
    "        valor_iss_retido_str = valor_iss_retido_match.group(1)\n",
    "        valor_iss_retido_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_iss_retido_str)\n",
    "        if valor_iss_retido_sem_formato:\n",
    "            valor_iss_retido_sem_formato = valor_iss_retido_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_iss_retido'] = float(valor_iss_retido_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_iss_retido'] = 0.0  # Valor não encontrado ou não está no formato esperado \n",
    "\n",
    "    # Extrair VALOR DESC. COND:\n",
    "    valor_desc_cond_match = re.search(r'DESC. COND:\\s+(.+)', text)\n",
    "    if valor_desc_cond_match:\n",
    "        valor_desc_cond_str = valor_desc_cond_match.group(1)\n",
    "        valor_desc_cond_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_desc_cond_str)\n",
    "        if valor_desc_cond_sem_formato:\n",
    "            valor_desc_cond_sem_formato = valor_desc_cond_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['desc_cond'] = float(valor_desc_cond_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['desc_cond'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "    \n",
    "    # Extrair VALOR PIS:\n",
    "    valor_pis_match = re.search(r'VALOR PIS:\\s+(.+)', text)\n",
    "    if valor_pis_match:\n",
    "        valor_pis_str = valor_pis_match.group(1)\n",
    "        valor_pis_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_pis_str)\n",
    "        if valor_pis_sem_formato:\n",
    "            valor_pis_sem_formato = valor_pis_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_pis'] = float(valor_pis_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_pis'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "    \n",
    "    # Extrair VALOR COFINS:\n",
    "    valor_cofins_match = re.search(r'VALOR COFINS:\\s+(.+)', text)\n",
    "    if valor_cofins_match:\n",
    "        valor_cofins_str = valor_cofins_match.group(1)\n",
    "        valor_cofins_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_cofins_str)\n",
    "        if valor_cofins_sem_formato:\n",
    "            valor_cofins_sem_formato = valor_cofins_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_cofins'] = float(valor_cofins_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_cofins'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "            \n",
    "    # Extrair VALOR IR:\n",
    "    valor_ir_match = re.search(r'VALOR IR:\\s+(.+)', text)\n",
    "    if valor_ir_match:\n",
    "        valor_ir_str = valor_ir_match.group(1)\n",
    "        valor_ir_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_ir_str)\n",
    "        if valor_ir_sem_formato:\n",
    "            valor_ir_sem_formato = valor_ir_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_ir'] = float(valor_ir_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_ir'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "            \n",
    "    # Extrair VALOR INSS:\n",
    "    valor_inss_match = re.search(r'VALOR INSS:\\s+(.+)', text)\n",
    "    if valor_inss_match:\n",
    "        valor_inss_str = valor_inss_match.group(1)\n",
    "        valor_inss_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_inss_str)\n",
    "        if valor_inss_sem_formato:\n",
    "            valor_inss_sem_formato = valor_inss_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_inss'] = float(valor_inss_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_inss'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "            \n",
    "    # Extrair VALOR CSLL:\n",
    "    valor_csll_match = re.search(r'VALOR CSLL:\\s+(.+)', text)\n",
    "    if valor_csll_match:\n",
    "        valor_csll_str = valor_csll_match.group(1)\n",
    "        valor_csll_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_csll_str)\n",
    "        if valor_csll_sem_formato:\n",
    "            valor_csll_sem_formato = valor_csll_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_csll'] = float(valor_csll_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_csll'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "    \n",
    "    # Extrair OUTRAS RETENÇÕES:\n",
    "    outras_retencoes_match = re.search(r'OUTRAS RETENÇÕES:\\s+(.+)', text)\n",
    "    if outras_retencoes_match:\n",
    "        outras_retencoes_str = outras_retencoes_match.group(1)\n",
    "        outras_retencoes_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', outras_retencoes_str)\n",
    "        if outras_retencoes_sem_formato:\n",
    "            outras_retencoes_sem_formato = outras_retencoes_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['outras_retencoes'] = float(outras_retencoes_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['outras_retencoes'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "    \n",
    "    \n",
    "    # Extrair VALOR LÍQUIDO:\n",
    "    valor_liquido_match = re.search(r'VALOR LÍQUIDO:\\s+(.+)', text)\n",
    "    if valor_liquido_match:\n",
    "        valor_liquido_str = valor_liquido_match.group(1)\n",
    "        valor_liquido_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_liquido_str)\n",
    "        if valor_liquido_sem_formato:\n",
    "            valor_liquido_sem_formato = valor_liquido_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_liquido'] = float(valor_liquido_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_liquido'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "        \n",
    "\n",
    "    return nf_data_valores\n",
    "\n",
    "\n",
    "# 9. OUTRAS INFORMAÇOES / CRITICAS\n",
    "def extract_fields_outras_info(text):\n",
    "    nf_data_outras_informacoes = {}\n",
    "    nf_data_outras_informacoes['secao'] = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "    \n",
    "    # Extrair EXIGIBILIDADE ISS:\n",
    "    exigibilidade_iss_match = re.search(r'EXIGIBILIDADE ISS\\s+(.+)', text)\n",
    "    if exigibilidade_iss_match:\n",
    "        exigibilidade_iss_value = exigibilidade_iss_match.group(1).strip()\n",
    "        nf_data_outras_informacoes['exigibilidade_iss'] = exigibilidade_iss_value\n",
    "        \n",
    "    # Extrair REGIME TRIBUTAÇÃO:\n",
    "    regime_tributacao_match = re.search(r'REGIME TRIBUTAÇÃO\\s+(.+)', text)\n",
    "    if regime_tributacao_match:\n",
    "        regime_tributacao_value = regime_tributacao_match.group(1).strip()\n",
    "        nf_data_outras_informacoes['regime_tributacao'] = regime_tributacao_value\n",
    "    \n",
    "    # Extrair SIMPLES NACIONAL:\n",
    "    simples_nacional_match = re.search(r'SIMPLES NACIONAL\\s+(.+)', text)\n",
    "    if simples_nacional_match:\n",
    "        simples_nacional_value = simples_nacional_match.group(1).strip()\n",
    "        nf_data_outras_informacoes['simples_nacional'] = simples_nacional_value\n",
    "        \n",
    "        \n",
    "    # Extrair ISSQN RETIDO:\n",
    "    local_prestacao_servico_match = re.search(r'ISSQN RETIDO\\s+(.+)', text)\n",
    "    if local_prestacao_servico_match:\n",
    "        local_prestacao_servico_value = local_prestacao_servico_match.group(1).strip()\n",
    "        nf_data_outras_informacoes['issqn_retido'] = local_prestacao_servico_value        \n",
    "        \n",
    "    \n",
    "    # Extrair LOCAL PRESTAÇÃO SERVIÇO:\n",
    "    local_prestacao_servico_match = re.search(r'LOCAL\\. PRESTAÇÃO\\s+SERVIÇO\\s+(.+)', text)\n",
    "    if local_prestacao_servico_match:\n",
    "        local_prestacao_servico_value = local_prestacao_servico_match.group(1).strip()\n",
    "        nf_data_outras_informacoes['local_prestacao_servico'] = local_prestacao_servico_value\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Extrair LOCAL INCIDÊNCIA:\n",
    "    local_incidencia_match = re.search(r'LOCAL INCIDÊNCIA\\s+(.+)', text)\n",
    "    if local_incidencia_match:\n",
    "        local_incidencia_value = local_incidencia_match.group(1).strip()\n",
    "        nf_data_outras_informacoes['local_incidencia'] = local_incidencia_value\n",
    "   \n",
    "    \n",
    "    return nf_data_outras_informacoes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tables-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
