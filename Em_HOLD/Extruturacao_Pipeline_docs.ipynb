{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <mark>Novo Processo de Pipeline - 1_Extruturacao_Pipeline_docs.ipynb</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atual notebook com as funçoes para processamento de PDF Pesquisavel e Raster e a criaçao dos Dataframes de forma dependente e unica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1 Extração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1 - Modules e config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import platform\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from urllib import response\n",
    "\n",
    "from outlook_msg import Message\n",
    "import extract_msg\n",
    "import zipfile\n",
    "from pyunpack import Archive\n",
    "import py7zr\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "import locale\n",
    "import time, copy\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import cv2\n",
    "import fitz  # Módulo PyMuPDF\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import Image, ImageDraw\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar\n",
    "from pdf2image import convert_from_path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "# Modulos da solucao\n",
    "import modules.extrai_pdf_pesquisavel as Extc\n",
    "import modules.cronometro as cron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Config - E-mail\n",
    "\n",
    "# 1. Caminho do arquivo uma mensagem especifica\n",
    "msg_dir_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails'\n",
    "\n",
    "# 2. Path para arquivos atachados compactados\n",
    "msg_attachment_zip = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments'\n",
    "\n",
    "# 3. Path para arquivos recebidos manualmente\n",
    "arquivos_recebidos_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments'\n",
    "\n",
    "# 4. Path para documentos para extracao\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 6. Path para gestao de imagens resized\n",
    "image_resized_path = \"pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas\"\n",
    "\n",
    "\n",
    "# 1. Path para DFs e CSVs exportados\n",
    "export_path = \"pipeline_extracao_documentos/6_geral_administacao/exports\"\n",
    "\n",
    "# 2. Path para lixeira\n",
    "root_garbage_path = \"pipeline_extracao_documentos/0_lixeira\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### funcoes para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Funcao flush_documents\n",
    "def flush_documents(i, folder_name, file, file4garbage_path, new_path_file4garbage):\n",
    "    \n",
    "    # IMPORTANTE: Coloque aqui sua lógica para cada arquivo\n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    status_process = \"MOVIDO\"\n",
    "    \n",
    "    return {\n",
    "        'id': i,\n",
    "        'data_eliminacao':time_now, \n",
    "        'diretorio': folder_name,\n",
    "        'arquivo': file,\n",
    "        'path_origem': file4garbage_path,\n",
    "        'path_destino': new_path_file4garbage,\n",
    "        'status': status_process,\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "param_garbage_path = root_garbage_path\n",
    "# 2. FunÇao para mover documentos de um diretorio para a lixeira\n",
    "def garbage_pipe(docs4garbage_path, param_garbage_path=root_garbage_path):\n",
    "    rows_list = []   \n",
    "    i = 1\n",
    "    for root, dirs, files in os.walk(docs4garbage_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            # 1. Defino o path do arquivo que sera mandado para lixeira\n",
    "            file4garbage_path = os.path.join(root, file)\n",
    "            # 2. Defino dsubdiretorio para envio de documentos\n",
    "            garbage_subdir_path = os.path.join(param_garbage_path, folder_name)\n",
    "            \n",
    "            # 3. Crio o subdiretorio em garbage\n",
    "            if not os.path.exists(garbage_subdir_path):\n",
    "                os.makedirs(garbage_subdir_path)\n",
    "            \n",
    "            # 4. Defino o path final do arquivo para a lixeira\n",
    "            new_path_file4garbage = os.path.join(garbage_subdir_path, file)\n",
    "            # 5. Processo a transferencia\n",
    "            try:\n",
    "                shutil.move(file4garbage_path, new_path_file4garbage)\n",
    "                print(f'p{i:>3} | move: {file} | for: {new_path_file4garbage}')\n",
    "                new_row = flush_documents(i, folder_name, file, file4garbage_path, new_path_file4garbage)\n",
    "                rows_list.append(new_row)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao mover documento {file}: {e}\")\n",
    "            folder_to_remove = os.path.join(docs4garbage_path, folder_name)\n",
    "            i += 1 \n",
    "               \n",
    "    os.rmdir(folder_to_remove)\n",
    "    \n",
    "    df_garbage = pd.DataFrame(rows_list)\n",
    "    \n",
    "\n",
    "            \n",
    "            #print(\"1. path da lixeira:\", param_garbage_path)\n",
    "    print(i-1, \"Documentos movidos\")\n",
    "    \n",
    "    \n",
    "    # 2. Tratando nome de carga do df_extracao_files_Batch\n",
    "    df_garbage_file = \"df_garbage_\"\n",
    "    df_filename_wr = df_garbage_file + str(folder_name) + \".xlsx\"\n",
    "    path_to_export_df = os.path.join(export_path, df_filename_wr)\n",
    "    # Salvando o DF para excel\n",
    "    df_garbage.to_excel(path_to_export_df, index=False)\n",
    "    \n",
    "    return df_garbage\n",
    "\n",
    "# 3. Buscar o primeiro documento do diretorio EXTERNO para processar\n",
    "def busca_doc_test(src_dir_path, tipo_file, tgt_dir_path):\n",
    " \n",
    "    extensao = \".\" + str(tipo_file)\n",
    "    # Verifique se o diretório de destino está vazio\n",
    "    if os.listdir(tgt_dir_path):\n",
    "        raise Exception(\"O diretório de destino não está vazio!\")\n",
    "\n",
    "    for roots, directories, documents in os.walk(src_dir_path):\n",
    "        # Filtre os documentos para incluir apenas aqueles com extensão .pdf\n",
    "        doc_files = [doc for doc in documents if doc.lower().endswith(extensao)]\n",
    "        \n",
    "        if doc_files: # Verifique se há algum arquivo PDF no diretório\n",
    "            first_doc_file = doc_files[0] # Obtenha o primeiro arquivo PDF\n",
    "            source_path = os.path.join(roots, first_doc_file) # Construa o caminho completo para o primeiro arquivo PDF\n",
    "            print(f\"Encontrei o primeiro arquivo: {source_path}\")\n",
    "            \n",
    "            # Mova o arquivo para o diretório de destino\n",
    "            shutil.move(source_path, tgt_dir_path)\n",
    "            \n",
    "            break # Saia do loop após encontrar o primeiro arquivo PDF\n",
    "\n",
    "\n",
    "def move_pdf_processed_ok(document_path):\n",
    "    \n",
    "    source_path = document_path\n",
    "    destination_path = os.path.join(f'{nf_processada_path}/{str(doc2convert)}')\n",
    "    shutil.move(source_path, destination_path)\n",
    "\n",
    "# # 3. parametros e paths para processamento\n",
    "# param_garbage_path = root_garbage_path\n",
    "# docs4garbage_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "# df_garbage = garbage_pipe(docs4garbage_path, param_garbage_path=root_garbage_path)\n",
    "# df_garbage  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trazendo o primeiro e-mail para teste\n",
    "sorce_mail = \"pipeline_extracao_documentos/0_arquivos_teste_pipeline/emails\"\n",
    "target_dir_path = \"pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails\"\n",
    "\n",
    "tipo_file = \"msg\"\n",
    "busca_doc_test(sorce_mail, tipo_file, target_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1 - Funcoes para e-mail e extracao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>1.</b> Abrindo o pipeline de documentos do email </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geraçao do hash do arquivo\n",
    "def generate_file_hash(file_path):\n",
    "    # Abre o arquivo em modo de leitura de bytes\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # Lê o conteúdo do arquivo\n",
    "        file_data = f.read()\n",
    "        # Utiliza o algoritmo SHA-256 para gerar o hash\n",
    "        file_hash = hashlib.sha256(file_data).hexdigest()\n",
    "    return file_hash\n",
    "\n",
    "# Geraçao do Unique_id do arquivo\n",
    "def generate_unique_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "\n",
    "# 1. XXX Busca proximo Batch\n",
    "def busca_proximo_batch():\n",
    "    # Abre o arquivo Excel e lê a coluna 'batch'\n",
    "    df = pd.read_excel(\"pipeline_extracao_documentos/6_geral_administacao/exports/df_documento_recebido.xlsx\", usecols=[\"batch\"])\n",
    "    # Pega o último valor da coluna 'batch'\n",
    "    last_value = df.iloc[-1, 0]\n",
    "    \n",
    "    # Extraí o número do último batch e adiciona 1 para o próximo\n",
    "    last_number = int(last_value.split(\"_\")[1])\n",
    "    next_number = last_number + 1\n",
    "    \n",
    "    # Forma o nome do próximo batch\n",
    "    next_batch = f\"Batch_{next_number}\"\n",
    "    \n",
    "    return next_batch    \n",
    "\n",
    "# 2. XXX Criacao do pripeline para processar email\n",
    "def process_email(msg, origem_docs, first_doc_file):\n",
    "    \n",
    "    batch_name = busca_proximo_batch()\n",
    "    origem = origem_docs\n",
    "    msg_raw_sender = msg.sender\n",
    "    parts = msg_raw_sender.rsplit('<', 1)\n",
    "    msg_email_address = parts[1].strip('<>')\n",
    "    msg_sender = parts[0].strip(' ')\n",
    "    msg_subject = msg.subject\n",
    "    msg_body = msg.body\n",
    "    # String original\n",
    "    original_date_str = msg.date\n",
    "    date_email = cron.convert_email_date(original_date_str)\n",
    "    \n",
    "    #status_pro = '...'\n",
    "    return {\n",
    "        'dt_hora': date_email,\n",
    "        'origem': origem,\n",
    "        'de': msg_sender,\n",
    "        'assunto': msg_subject,\n",
    "        'batch': batch_name,\n",
    "        'email': msg_email_address,\n",
    "        'msg_file': first_doc_file,\n",
    "    }, batch_name\n",
    "\n",
    "# 3. FunÇao para buscar emails e attachments\n",
    "def email_pipe():\n",
    "    \n",
    "    locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")\n",
    "    rows_list = []   \n",
    "    for root, dirs, files in os.walk(msg_dir_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        doc_files = [doc for doc in files if doc.lower().endswith(\".msg\")]\n",
    "        if doc_files: # Verifique se há algum arquivo .msg no diretório\n",
    "            first_doc_file = doc_files[0] # Obtenha o primeiro arquivo .msg\n",
    "            email_path = os.path.join(root, first_doc_file) # Construa o caminho completo para o primeiro arquivo .msg\n",
    "            try:\n",
    "                origem_docs = 'email'            \n",
    "                msg = extract_msg.Message(email_path)\n",
    "                new_row, nome_batch = process_email(msg, origem_docs, first_doc_file)\n",
    "                \n",
    "                hash_value = generate_file_hash(email_path)\n",
    "                \n",
    "                rows_list.append(new_row)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao ler email: {e}\") \n",
    "    \n",
    "    \n",
    "    df_mail = pd.DataFrame(rows_list)\n",
    "    # Caminho da pasta onde você quer salvar os anexos\n",
    "    pasta_destino = msg_attachment_zip\n",
    "\n",
    "    # Verifica se a pasta existe; se não, cria ela\n",
    "    if not os.path.exists(pasta_destino):\n",
    "        os.makedirs(pasta_destino)\n",
    "\n",
    "    with open(email_path) as msg_file:\n",
    "        msg = Message(msg_file)\n",
    "    \n",
    "    total_attch = len(msg.attachments)\n",
    "    \n",
    "    i = 0\n",
    "    # Loop para salvar cada anexo\n",
    "    for i in range(total_attch):\n",
    "        attachment = msg.attachments[i]\n",
    "        caminho_completo_anexo = os.path.join(pasta_destino, attachment.filename)\n",
    "\n",
    "        #print(caminho_completo_anexo)\n",
    "        with attachment.open() as attachment_fp, open(caminho_completo_anexo, 'wb') as output_fp:\n",
    "            output_fp.write(attachment_fp.read())\n",
    "\n",
    "    return df_mail, nome_batch\n",
    "\n",
    "# 4. XXX converte nome do arquivo\n",
    "def conv_filename(title):\n",
    "    \n",
    "    # Divide o título em nome e extensão\n",
    "    name, extension = title.rsplit('.', 1) if '.' in title else (title, \"\")\n",
    "\n",
    "    # Remove acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substiti espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remove quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converte para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    # Adiciona a extensão de volta, se houver\n",
    "    if extension:\n",
    "        filename += '.' + extension.lower()\n",
    "\n",
    "    return filename\n",
    "\n",
    "# 5. XXX Extrai documentos compatados\n",
    "def extract_documents(src_compressed_file_path, tgt_directory_path, batch):\n",
    "\n",
    "    # Diretório onde você quer salvar os arquivos extraídos\n",
    "    output_dir = os.path.join(tgt_directory_path, batch)\n",
    "\n",
    "    folder_file_dict = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(src_compressed_file_path):\n",
    "        #print(f'{root}  | {dirs} | {document} | {files}\\n')\n",
    "        \n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            if file.lower().endswith('.zip'):\n",
    "                zip_file = file\n",
    "                zip_file_path = os.path.join(root, file)\n",
    "                # Obtém o nome base do arquivo ZIP para usar como subdiretório\n",
    "                zip_base_name = os.path.splitext(os.path.basename(zip_file_path))[0]\n",
    "                zip_basename = conv_filename(zip_base_name)\n",
    "                \n",
    "                # Cria o subdiretório com base no nome do arquivo ZIP\n",
    "                root_output_dir = os.path.join(output_dir, zip_basename)\n",
    "                \n",
    "                if not os.path.exists(root_output_dir):\n",
    "                    os.makedirs(root_output_dir) # estou criando o diretorio caso nao exista\n",
    "\n",
    "                # Abre o arquivo ZIP\n",
    "                with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                    for member in zip_ref.namelist():\n",
    "                        # Separa o nome da pasta e o nome do arquivo usando barra invertida como delimitador\n",
    "                        parts = member.rsplit('\\\\', 1)\n",
    "                        folder_name = parts[0] if len(parts) > 1 else ''\n",
    "                        #folder_name = conv_filename(folder_temp)\n",
    "                        filename = parts[-1]\n",
    "                        if filename:  # ignora diretórios\n",
    "                            # Adiciona ao dicionário\n",
    "                            #filename = conv_filename(filename)\n",
    "                            # Cria um subdiretório se ele não existir\n",
    "                            sub_dir = os.path.join(root_output_dir, folder_name)\n",
    "                            if not os.path.exists(sub_dir):\n",
    "                                os.makedirs(sub_dir)\n",
    "\n",
    "                            # Salva o arquivo no subdiretório especificado\n",
    "                            source = zip_ref.open(member)\n",
    "                            target_path = os.path.join(sub_dir, filename)\n",
    "                            \n",
    "                            with open(target_path, \"wb\") as target:\n",
    "                                target.write(source.read())\n",
    "                                dir_path = os.path.dirname(filename)\n",
    "                                \n",
    "            elif file.lower().endswith('.rar'):\n",
    "                rar_file = file\n",
    "                rar_file = conv_filename_no_ext(rar_file)\n",
    "                rar_file_path = os.path.join(root, file) \n",
    "                root_output_dir = os.path.join(output_dir, rar_file)\n",
    "                if not os.path.exists(root_output_dir):\n",
    "                    os.makedirs(root_output_dir)\n",
    "                Archive(rar_file_path).extractall(root_output_dir)  \n",
    "                \n",
    "            elif file.lower().endswith('.7z'):\n",
    "                sevenz_file = file\n",
    "                sevenz_file = conv_filename_no_ext(sevenz_file)\n",
    "                sevenz_file_path = os.path.join(root, file)\n",
    "                root_output_dir = os.path.join(output_dir, sevenz_file)                      \n",
    "                                \n",
    "                with py7zr.SevenZipFile(sevenz_file_path, mode='r') as z:\n",
    "                    z.extractall(root_output_dir)\n",
    "                    \n",
    "            elif file.lower().endswith('.pdf'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                new_path_name = os.path.join(output_dir, file)\n",
    "                if not os.path.exists(output_dir):\n",
    "                                os.makedirs(output_dir)\n",
    "                shutil.move(file_path, new_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca proximo Batch caso nao esteja rodando email\n",
    "batch_name = busca_proximo_batch()\n",
    "batch_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Processa e-mail e guarda seus anexos\n",
    "df_mail, batch_name = email_pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt_hora</th>\n",
       "      <th>origem</th>\n",
       "      <th>de</th>\n",
       "      <th>assunto</th>\n",
       "      <th>batch</th>\n",
       "      <th>email</th>\n",
       "      <th>msg_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/08/2023 23:20:56</td>\n",
       "      <td>email</td>\n",
       "      <td>Verlânio Gallindo</td>\n",
       "      <td>Fwd: Notas Magé 2</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>verlanio@gmail.com</td>\n",
       "      <td>Fwd Notas Magé 2.msg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dt_hora origem                 de            assunto     batch  \\\n",
       "0  10/08/2023 23:20:56  email  Verlânio Gallindo  Fwd: Notas Magé 2  Batch_17   \n",
       "\n",
       "                email              msg_file  \n",
       "0  verlanio@gmail.com  Fwd Notas Magé 2.msg  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Efetua a extraçao de documento\n",
    "extract_documents(msg_attachment_zip, documentos_extracao_path, batch_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. XXX Estabelece o pipeline de documentos\n",
    "def abertura_pipeline(index, batch_name, file, documentos_extracao_path, folder_name, file_path):\n",
    "    action_itens = {}\n",
    "    \n",
    "    status_pro = \"para_analise\"\n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'index': index,\n",
    "        'data_processamento': time_now,\n",
    "        'Batch': batch_name,\n",
    "        'diretorio_origem': folder_name,\n",
    "        'nome_arquivo': file,\n",
    "        'diretorio_origem': file_path,\n",
    "        'status': status_pro,\n",
    "    }\n",
    "\n",
    "\n",
    "# 3. funcao para ajustar documentos no pipeline antes de serem processados\n",
    "def pipeline_preprocessamento():\n",
    "    rows_list = []\n",
    "    i = 0\n",
    "    for root, dirs, files in os.walk(documentos_extracao_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                i += 1\n",
    "                new_row = abertura_pipeline(i, batch_name, file, documentos_extracao_path, folder_name, file_path)\n",
    "                rows_list.append(new_row)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao criar nova row: {e}\") \n",
    "            \n",
    "    \n",
    "    df_documento_recebido = pd.DataFrame(rows_list)\n",
    "    \n",
    "    return df_documento_recebido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_documento_recebido = pipeline_preprocessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>data_processamento</th>\n",
       "      <th>Batch</th>\n",
       "      <th>diretorio_origem</th>\n",
       "      <th>nome_arquivo</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Nota Fiscal Eletrônica 1717 - BALL - BVI.PDF</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NF 688 - ConsoR Zadar- Engetecnica (ENZA).pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>1127 fogo em 20-07-23.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NF 689- PMMacae- 8ª Med CO-22-22 TB Contrato 0...</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Nota Fiscal_229_Agosto-Schneider.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>38 EDSON JOSE OLIVEIRA DOS SANTOS.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>12 - Ótica do Cizinho.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>FLEXPRIN.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NFS-e 35.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NFS-e 37.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NFS-e 36 CANCELADA.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NFS-e 38 CANCELADA.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Doria Marinho 0297 Raquel.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Doria Marinho 0298 Marcelo.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Doria Marinho 0295 Carlos Leandro.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Doria Marinho 0300 Vanisa.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Doria Marinho 0296 Vanisa (Cancelada).pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Doria Marinho 0301 Ultrascan.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Listagem de NFS-e - Sintético IM 237881_104871...</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>Doria Marinho 0299 Luciana.pdf</td>\n",
       "      <td>para_analise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index   data_processamento     Batch  \\\n",
       "0       1  06/09/2023 18:45:57  Batch_17   \n",
       "1       2  06/09/2023 18:45:57  Batch_17   \n",
       "2       3  06/09/2023 18:45:57  Batch_17   \n",
       "3       4  06/09/2023 18:45:57  Batch_17   \n",
       "4       5  06/09/2023 18:45:57  Batch_17   \n",
       "5       6  06/09/2023 18:45:57  Batch_17   \n",
       "6       7  06/09/2023 18:45:57  Batch_17   \n",
       "7       8  06/09/2023 18:45:57  Batch_17   \n",
       "8       9  06/09/2023 18:45:57  Batch_17   \n",
       "9      10  06/09/2023 18:45:57  Batch_17   \n",
       "10     11  06/09/2023 18:45:57  Batch_17   \n",
       "11     12  06/09/2023 18:45:57  Batch_17   \n",
       "12     13  06/09/2023 18:45:57  Batch_17   \n",
       "13     14  06/09/2023 18:45:57  Batch_17   \n",
       "14     15  06/09/2023 18:45:57  Batch_17   \n",
       "15     16  06/09/2023 18:45:57  Batch_17   \n",
       "16     17  06/09/2023 18:45:57  Batch_17   \n",
       "17     18  06/09/2023 18:45:57  Batch_17   \n",
       "18     19  06/09/2023 18:45:57  Batch_17   \n",
       "19     20  06/09/2023 18:45:57  Batch_17   \n",
       "\n",
       "                                     diretorio_origem  \\\n",
       "0   pipeline_extracao_documentos/2_documentos_para...   \n",
       "1   pipeline_extracao_documentos/2_documentos_para...   \n",
       "2   pipeline_extracao_documentos/2_documentos_para...   \n",
       "3   pipeline_extracao_documentos/2_documentos_para...   \n",
       "4   pipeline_extracao_documentos/2_documentos_para...   \n",
       "5   pipeline_extracao_documentos/2_documentos_para...   \n",
       "6   pipeline_extracao_documentos/2_documentos_para...   \n",
       "7   pipeline_extracao_documentos/2_documentos_para...   \n",
       "8   pipeline_extracao_documentos/2_documentos_para...   \n",
       "9   pipeline_extracao_documentos/2_documentos_para...   \n",
       "10  pipeline_extracao_documentos/2_documentos_para...   \n",
       "11  pipeline_extracao_documentos/2_documentos_para...   \n",
       "12  pipeline_extracao_documentos/2_documentos_para...   \n",
       "13  pipeline_extracao_documentos/2_documentos_para...   \n",
       "14  pipeline_extracao_documentos/2_documentos_para...   \n",
       "15  pipeline_extracao_documentos/2_documentos_para...   \n",
       "16  pipeline_extracao_documentos/2_documentos_para...   \n",
       "17  pipeline_extracao_documentos/2_documentos_para...   \n",
       "18  pipeline_extracao_documentos/2_documentos_para...   \n",
       "19  pipeline_extracao_documentos/2_documentos_para...   \n",
       "\n",
       "                                         nome_arquivo        status  \n",
       "0        Nota Fiscal Eletrônica 1717 - BALL - BVI.PDF  para_analise  \n",
       "1       NF 688 - ConsoR Zadar- Engetecnica (ENZA).pdf  para_analise  \n",
       "2                           1127 fogo em 20-07-23.pdf  para_analise  \n",
       "3   NF 689- PMMacae- 8ª Med CO-22-22 TB Contrato 0...  para_analise  \n",
       "4                Nota Fiscal_229_Agosto-Schneider.pdf  para_analise  \n",
       "5               38 EDSON JOSE OLIVEIRA DOS SANTOS.pdf  para_analise  \n",
       "6                           12 - Ótica do Cizinho.pdf  para_analise  \n",
       "7                                        FLEXPRIN.pdf  para_analise  \n",
       "8                                        NFS-e 35.pdf  para_analise  \n",
       "9                                        NFS-e 37.pdf  para_analise  \n",
       "10                             NFS-e 36 CANCELADA.pdf  para_analise  \n",
       "11                             NFS-e 38 CANCELADA.pdf  para_analise  \n",
       "12                      Doria Marinho 0297 Raquel.pdf  para_analise  \n",
       "13                     Doria Marinho 0298 Marcelo.pdf  para_analise  \n",
       "14              Doria Marinho 0295 Carlos Leandro.pdf  para_analise  \n",
       "15                      Doria Marinho 0300 Vanisa.pdf  para_analise  \n",
       "16          Doria Marinho 0296 Vanisa (Cancelada).pdf  para_analise  \n",
       "17                   Doria Marinho 0301 Ultrascan.pdf  para_analise  \n",
       "18  Listagem de NFS-e - Sintético IM 237881_104871...  para_analise  \n",
       "19                     Doria Marinho 0299 Luciana.pdf  para_analise  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_documento_recebido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. XXX Analisa nro de paginas\n",
    "def analisa_nro_pages(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    pages = pdf_document.pages() # generator object\n",
    "\n",
    "    page_nro = []\n",
    "    for page in pages:\n",
    "        page_nro.append(page)\n",
    "        \n",
    "    nro_paginas = len(page_nro)    \n",
    "    if nro_paginas > 1:\n",
    "        doc_1_page = False\n",
    "        return doc_1_page, nro_paginas    \n",
    "    else:\n",
    "        doc_1_page = True\n",
    "        return doc_1_page, nro_paginas  \n",
    "    pdf_document.close() \n",
    "    \n",
    "    \n",
    "# 3. XXX Split de paginas\n",
    "def split_pdf_pages(file2split_path):\n",
    "    \n",
    "    try:\n",
    "        pdf = fitz.open(file2split_path)\n",
    "        # Número total de páginas no PDF\n",
    "        total_pages = len(pdf)\n",
    "    except Exception as e:\n",
    "        print(f\"Nao congui abrir o PDF: {e}\")    \n",
    "\n",
    "    # Nome base para os arquivos de saída\n",
    "    base_name = file2split_path.split('.')[0]  # Remove a extensão do arquivo\n",
    "    \n",
    "    file_to_delete = file2split_path\n",
    "\n",
    "    # Loop para criar um novo PDF para cada página\n",
    "    for page_num in range(total_pages):\n",
    "        # Cria um novo objeto PDF\n",
    "        new_pdf = fitz.open()\n",
    "        # Adiciona a página atual ao novo PDF\n",
    "        new_pdf.insert_pdf(pdf, from_page=page_num, to_page=page_num)\n",
    "        # Nome do novo arquivo PDF\n",
    "        new_pdf_name = f\"{base_name}_page_{page_num + 1}.pdf\"\n",
    "        # Salva o novo PDF\n",
    "        new_pdf.save(new_pdf_name)\n",
    "        # Fecha o novo PDF\n",
    "        new_pdf.close()\n",
    "    return new_pdf_name, file_to_delete\n",
    "    # Fecha o PDF original\n",
    "    pdf.close()   \n",
    "    \n",
    "    \n",
    "# 4. XXX Efetua a conversao e o resize pagina - NF (convem pensar noutro modelo para listagem) \n",
    "def convertResizeAnalise_1page(doc2convert, document_path, image_resized_path):\n",
    "    \n",
    "    # 1. construo um novo nome para o documento imagem\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(doc2convert)}.jpg')\n",
    "    \n",
    "    # 2. Conversao para imagem\n",
    "    pages = convert_from_path(document_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    # 3. Para cada pagina faco o resize (apesar de ser somente uma)\n",
    "    resized_pages = []\n",
    "    for page in pages:\n",
    "        resized_page = page.resize((2067, 2923))\n",
    "        resized_pages.append(resized_page)\n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "    return resized_pages[0], image_resized_name \n",
    "\n",
    "\n",
    "# 5. XXX Pesquisa prefeitura no documento (dando as coordenadas) e efetuando o OCR\n",
    "def pequisaTextoDoc(image_name):\n",
    "\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = 0\n",
    "    y0 = 0\n",
    "    x1= 2066\n",
    "    y1 = 2922\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "    \n",
    "    return extracted_text_frame \n",
    "\n",
    "# 6.1. XXX Ajusta texto\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "# 6.2. XXX Ajusta texto sem quebrar o \":\"\n",
    "def texto_extraido_nf(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_df_analise(df_documento_recebido):\n",
    "    linhas_df_analise = []\n",
    "    pre_processo = ['ajustar_nome', 'split_paginas', 'ajustar_imagem', 'buscar_nome_prefeitura', 'enviar_canceladas', 'enviar_listagens']\n",
    "    \n",
    "    for idx, row in df_documento_recebido.iterrows():\n",
    "        file_name = row['nome_arquivo']\n",
    "        file_path = row['diretorio_origem']\n",
    "        #file_path = f\"{row['diretorio_origem']}/{file_name}\"  # Ajustar como necessário\n",
    "\n",
    "        # Chama a função analisa_nro_pages\n",
    "        doc_1_page, nro_paginas = analisa_nro_pages(file_path)\n",
    "\n",
    "        # Chama a função conv_filename\n",
    "        new_name = conv_filename(file_name)\n",
    "\n",
    "        acoes_necessarias = {}\n",
    "\n",
    "        # Se o documento tem mais de uma página\n",
    "        if not doc_1_page:\n",
    "            acoes_necessarias['split_paginas'] = nro_paginas\n",
    "\n",
    "        # Se o nome do arquivo precisa ser ajustado\n",
    "        if new_name != file_name:\n",
    "            acoes_necessarias['ajustar_nome'] = new_name\n",
    "            \n",
    "        if \"cancelada\" in file_name.lower():\n",
    "            acoes_necessarias['enviar_canceladas'] = True\n",
    "            \n",
    "        if \"listagem\" in file_name.lower():\n",
    "            acoes_necessarias['enviar_listagens'] = True\n",
    "                \n",
    "\n",
    "        for action in pre_processo:\n",
    "            if action in acoes_necessarias:\n",
    "                info = acoes_necessarias[action]\n",
    "                seq = pre_processo.index(action) + 1  # Pega a sequência da ação baseada na lista pre_processo\n",
    "                \n",
    "                nova_linha = {\n",
    "                    'index': row['index'],\n",
    "                    'data_processamento': row['data_processamento'],\n",
    "                    'Batch': row['Batch'],\n",
    "                    'nome_arquivo': row['nome_arquivo'],\n",
    "                    'seq': seq,\n",
    "                    'action': action,\n",
    "                    'info_adicional': info,\n",
    "                    'diretorio_origem': row['diretorio_origem'],\n",
    "\n",
    "\n",
    "                }\n",
    "                linhas_df_analise.append(nova_linha)\n",
    "\n",
    "    df_analise = pd.DataFrame(linhas_df_analise)\n",
    "    return df_analise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise = criar_df_analise(df_documento_recebido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>data_processamento</th>\n",
       "      <th>Batch</th>\n",
       "      <th>nome_arquivo</th>\n",
       "      <th>seq</th>\n",
       "      <th>action</th>\n",
       "      <th>info_adicional</th>\n",
       "      <th>diretorio_origem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>Nota Fiscal Eletrônica 1717 - BALL - BVI.PDF</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>nota_fiscal_eletronica_1717___ball___bvi.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>NF 688 - ConsoR Zadar- Engetecnica (ENZA).pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>nf_688___consor_zadar__engetecnica_enza.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1127 fogo em 20-07-23.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>1127_fogo_em_20_07_23.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>NF 689- PMMacae- 8ª Med CO-22-22 TB Contrato 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>nf_689__pmmacae__8a_med_co_22_22_tb_contrato_0...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>Nota Fiscal_229_Agosto-Schneider.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>nota_fiscal_229_agosto_schneider.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>38 EDSON JOSE OLIVEIRA DOS SANTOS.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>38_edson_jose_oliveira_dos_santos.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>12 - Ótica do Cizinho.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>12___otica_do_cizinho.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>FLEXPRIN.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>flexprin.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>NFS-e 35.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>nfs_e_35.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>NFS-e 35.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>split_paginas</td>\n",
       "      <td>2</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>NFS-e 37.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>nfs_e_37.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>NFS-e 37.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>split_paginas</td>\n",
       "      <td>2</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>NFS-e 36 CANCELADA.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>nfs_e_36_cancelada.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>NFS-e 36 CANCELADA.pdf</td>\n",
       "      <td>5</td>\n",
       "      <td>enviar_canceladas</td>\n",
       "      <td>True</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>NFS-e 38 CANCELADA.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>nfs_e_38_cancelada.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>NFS-e 38 CANCELADA.pdf</td>\n",
       "      <td>5</td>\n",
       "      <td>enviar_canceladas</td>\n",
       "      <td>True</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>Doria Marinho 0297 Raquel.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>doria_marinho_0297_raquel.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>Doria Marinho 0298 Marcelo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>doria_marinho_0298_marcelo.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>Doria Marinho 0295 Carlos Leandro.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>doria_marinho_0295_carlos_leandro.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>Doria Marinho 0300 Vanisa.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>doria_marinho_0300_vanisa.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>Doria Marinho 0296 Vanisa (Cancelada).pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>doria_marinho_0296_vanisa_cancelada.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>17</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>Doria Marinho 0296 Vanisa (Cancelada).pdf</td>\n",
       "      <td>5</td>\n",
       "      <td>enviar_canceladas</td>\n",
       "      <td>True</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>18</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>Doria Marinho 0301 Ultrascan.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>doria_marinho_0301_ultrascan.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>19</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>Listagem de NFS-e - Sintético IM 237881_104871...</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>listagem_de_nfs_e___sintetico_im_237881_104871...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>Listagem de NFS-e - Sintético IM 237881_104871...</td>\n",
       "      <td>2</td>\n",
       "      <td>split_paginas</td>\n",
       "      <td>28</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>19</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>Listagem de NFS-e - Sintético IM 237881_104871...</td>\n",
       "      <td>6</td>\n",
       "      <td>enviar_listagens</td>\n",
       "      <td>True</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20</td>\n",
       "      <td>06/09/2023 18:45:57</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>Doria Marinho 0299 Luciana.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>ajustar_nome</td>\n",
       "      <td>doria_marinho_0299_luciana.pdf</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index   data_processamento     Batch  \\\n",
       "0       1  06/09/2023 18:45:57  Batch_17   \n",
       "1       2  06/09/2023 18:45:57  Batch_17   \n",
       "2       3  06/09/2023 18:45:57  Batch_17   \n",
       "3       4  06/09/2023 18:45:57  Batch_17   \n",
       "4       5  06/09/2023 18:45:57  Batch_17   \n",
       "5       6  06/09/2023 18:45:57  Batch_17   \n",
       "6       7  06/09/2023 18:45:57  Batch_17   \n",
       "7       8  06/09/2023 18:45:57  Batch_17   \n",
       "8       9  06/09/2023 18:45:57  Batch_17   \n",
       "9       9  06/09/2023 18:45:57  Batch_17   \n",
       "10     10  06/09/2023 18:45:57  Batch_17   \n",
       "11     10  06/09/2023 18:45:57  Batch_17   \n",
       "12     11  06/09/2023 18:45:57  Batch_17   \n",
       "13     11  06/09/2023 18:45:57  Batch_17   \n",
       "14     12  06/09/2023 18:45:57  Batch_17   \n",
       "15     12  06/09/2023 18:45:57  Batch_17   \n",
       "16     13  06/09/2023 18:45:57  Batch_17   \n",
       "17     14  06/09/2023 18:45:57  Batch_17   \n",
       "18     15  06/09/2023 18:45:57  Batch_17   \n",
       "19     16  06/09/2023 18:45:57  Batch_17   \n",
       "20     17  06/09/2023 18:45:57  Batch_17   \n",
       "21     17  06/09/2023 18:45:57  Batch_17   \n",
       "22     18  06/09/2023 18:45:57  Batch_17   \n",
       "23     19  06/09/2023 18:45:57  Batch_17   \n",
       "24     19  06/09/2023 18:45:57  Batch_17   \n",
       "25     19  06/09/2023 18:45:57  Batch_17   \n",
       "26     20  06/09/2023 18:45:57  Batch_17   \n",
       "\n",
       "                                         nome_arquivo  seq             action  \\\n",
       "0        Nota Fiscal Eletrônica 1717 - BALL - BVI.PDF    1       ajustar_nome   \n",
       "1       NF 688 - ConsoR Zadar- Engetecnica (ENZA).pdf    1       ajustar_nome   \n",
       "2                           1127 fogo em 20-07-23.pdf    1       ajustar_nome   \n",
       "3   NF 689- PMMacae- 8ª Med CO-22-22 TB Contrato 0...    1       ajustar_nome   \n",
       "4                Nota Fiscal_229_Agosto-Schneider.pdf    1       ajustar_nome   \n",
       "5               38 EDSON JOSE OLIVEIRA DOS SANTOS.pdf    1       ajustar_nome   \n",
       "6                           12 - Ótica do Cizinho.pdf    1       ajustar_nome   \n",
       "7                                        FLEXPRIN.pdf    1       ajustar_nome   \n",
       "8                                        NFS-e 35.pdf    1       ajustar_nome   \n",
       "9                                        NFS-e 35.pdf    2      split_paginas   \n",
       "10                                       NFS-e 37.pdf    1       ajustar_nome   \n",
       "11                                       NFS-e 37.pdf    2      split_paginas   \n",
       "12                             NFS-e 36 CANCELADA.pdf    1       ajustar_nome   \n",
       "13                             NFS-e 36 CANCELADA.pdf    5  enviar_canceladas   \n",
       "14                             NFS-e 38 CANCELADA.pdf    1       ajustar_nome   \n",
       "15                             NFS-e 38 CANCELADA.pdf    5  enviar_canceladas   \n",
       "16                      Doria Marinho 0297 Raquel.pdf    1       ajustar_nome   \n",
       "17                     Doria Marinho 0298 Marcelo.pdf    1       ajustar_nome   \n",
       "18              Doria Marinho 0295 Carlos Leandro.pdf    1       ajustar_nome   \n",
       "19                      Doria Marinho 0300 Vanisa.pdf    1       ajustar_nome   \n",
       "20          Doria Marinho 0296 Vanisa (Cancelada).pdf    1       ajustar_nome   \n",
       "21          Doria Marinho 0296 Vanisa (Cancelada).pdf    5  enviar_canceladas   \n",
       "22                   Doria Marinho 0301 Ultrascan.pdf    1       ajustar_nome   \n",
       "23  Listagem de NFS-e - Sintético IM 237881_104871...    1       ajustar_nome   \n",
       "24  Listagem de NFS-e - Sintético IM 237881_104871...    2      split_paginas   \n",
       "25  Listagem de NFS-e - Sintético IM 237881_104871...    6   enviar_listagens   \n",
       "26                     Doria Marinho 0299 Luciana.pdf    1       ajustar_nome   \n",
       "\n",
       "                                       info_adicional  \\\n",
       "0        nota_fiscal_eletronica_1717___ball___bvi.pdf   \n",
       "1         nf_688___consor_zadar__engetecnica_enza.pdf   \n",
       "2                           1127_fogo_em_20_07_23.pdf   \n",
       "3   nf_689__pmmacae__8a_med_co_22_22_tb_contrato_0...   \n",
       "4                nota_fiscal_229_agosto_schneider.pdf   \n",
       "5               38_edson_jose_oliveira_dos_santos.pdf   \n",
       "6                           12___otica_do_cizinho.pdf   \n",
       "7                                        flexprin.pdf   \n",
       "8                                        nfs_e_35.pdf   \n",
       "9                                                   2   \n",
       "10                                       nfs_e_37.pdf   \n",
       "11                                                  2   \n",
       "12                             nfs_e_36_cancelada.pdf   \n",
       "13                                               True   \n",
       "14                             nfs_e_38_cancelada.pdf   \n",
       "15                                               True   \n",
       "16                      doria_marinho_0297_raquel.pdf   \n",
       "17                     doria_marinho_0298_marcelo.pdf   \n",
       "18              doria_marinho_0295_carlos_leandro.pdf   \n",
       "19                      doria_marinho_0300_vanisa.pdf   \n",
       "20            doria_marinho_0296_vanisa_cancelada.pdf   \n",
       "21                                               True   \n",
       "22                   doria_marinho_0301_ultrascan.pdf   \n",
       "23  listagem_de_nfs_e___sintetico_im_237881_104871...   \n",
       "24                                                 28   \n",
       "25                                               True   \n",
       "26                     doria_marinho_0299_luciana.pdf   \n",
       "\n",
       "                                     diretorio_origem  \n",
       "0   pipeline_extracao_documentos/2_documentos_para...  \n",
       "1   pipeline_extracao_documentos/2_documentos_para...  \n",
       "2   pipeline_extracao_documentos/2_documentos_para...  \n",
       "3   pipeline_extracao_documentos/2_documentos_para...  \n",
       "4   pipeline_extracao_documentos/2_documentos_para...  \n",
       "5   pipeline_extracao_documentos/2_documentos_para...  \n",
       "6   pipeline_extracao_documentos/2_documentos_para...  \n",
       "7   pipeline_extracao_documentos/2_documentos_para...  \n",
       "8   pipeline_extracao_documentos/2_documentos_para...  \n",
       "9   pipeline_extracao_documentos/2_documentos_para...  \n",
       "10  pipeline_extracao_documentos/2_documentos_para...  \n",
       "11  pipeline_extracao_documentos/2_documentos_para...  \n",
       "12  pipeline_extracao_documentos/2_documentos_para...  \n",
       "13  pipeline_extracao_documentos/2_documentos_para...  \n",
       "14  pipeline_extracao_documentos/2_documentos_para...  \n",
       "15  pipeline_extracao_documentos/2_documentos_para...  \n",
       "16  pipeline_extracao_documentos/2_documentos_para...  \n",
       "17  pipeline_extracao_documentos/2_documentos_para...  \n",
       "18  pipeline_extracao_documentos/2_documentos_para...  \n",
       "19  pipeline_extracao_documentos/2_documentos_para...  \n",
       "20  pipeline_extracao_documentos/2_documentos_para...  \n",
       "21  pipeline_extracao_documentos/2_documentos_para...  \n",
       "22  pipeline_extracao_documentos/2_documentos_para...  \n",
       "23  pipeline_extracao_documentos/2_documentos_para...  \n",
       "24  pipeline_extracao_documentos/2_documentos_para...  \n",
       "25  pipeline_extracao_documentos/2_documentos_para...  \n",
       "26  pipeline_extracao_documentos/2_documentos_para...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame de análise\n",
    "df_analise = pd.DataFrame(columns=['data_processamento', 'Batch', 'diretorio_origem', 'nome_arquivo', 'unique_ID', 'parent_ID', 'seq', 'action', 'nro_paginas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_processamento</th>\n",
       "      <th>Batch</th>\n",
       "      <th>diretorio_origem</th>\n",
       "      <th>nome_arquivo</th>\n",
       "      <th>unique_ID</th>\n",
       "      <th>parent_ID</th>\n",
       "      <th>seq</th>\n",
       "      <th>action</th>\n",
       "      <th>nro_paginas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [data_processamento, Batch, diretorio_origem, nome_arquivo, unique_ID, parent_ID, seq, action, nro_paginas]\n",
       "Index: []"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ao receber um novo documento\n",
    "new_unique_id = generate_unique_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'843c1013-9c84-4823-9a38-cc1ff7332d74'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame de documentos recebidos\n",
    "df_documento_recebido = pd.DataFrame(columns=['Batch', 'nome_arquivo', 'caminho_arquivo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20823/2890610164.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_documento_recebido = df_documento_recebido.append({'Batch': 'Batch_17', 'nome_arquivo': 'Exemplo.pdf', 'caminho_arquivo': '/caminho/Exemplo.pdf'}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Preenche o df_documento_recebido com informações do novo documento\n",
    "df_documento_recebido = df_documento_recebido.append({'Batch': 'Batch_17', 'nome_arquivo': 'Exemplo.pdf', 'caminho_arquivo': '/caminho/Exemplo.pdf'}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20823/4173418973.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_analise = df_analise.append({\n"
     ]
    }
   ],
   "source": [
    "# Preenche o df_analise com análises iniciais e o unique_ID\n",
    "df_analise = df_analise.append({\n",
    "    'data_processamento': pd.Timestamp.now(),\n",
    "    'Batch': 'Batch_17',\n",
    "    'diretorio_origem': '/caminho/',\n",
    "    'nome_arquivo': 'Exemplo.pdf',\n",
    "    'unique_ID': new_unique_id,\n",
    "    'parent_ID': None,\n",
    "    'seq': 1,\n",
    "    'action': 'recebido',\n",
    "    'nro_paginas': 2\n",
    "}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_processamento</th>\n",
       "      <th>Batch</th>\n",
       "      <th>diretorio_origem</th>\n",
       "      <th>nome_arquivo</th>\n",
       "      <th>unique_ID</th>\n",
       "      <th>parent_ID</th>\n",
       "      <th>seq</th>\n",
       "      <th>action</th>\n",
       "      <th>nro_paginas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-06 19:18:19.157168</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>/caminho/</td>\n",
       "      <td>Exemplo.pdf</td>\n",
       "      <td>843c1013-9c84-4823-9a38-cc1ff7332d74</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>recebido</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          data_processamento     Batch diretorio_origem nome_arquivo  \\\n",
       "0 2023-09-06 19:18:19.157168  Batch_17        /caminho/  Exemplo.pdf   \n",
       "\n",
       "                              unique_ID parent_ID seq    action nro_paginas  \n",
       "0  843c1013-9c84-4823-9a38-cc1ff7332d74      None   1  recebido           2  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_17/fwdnotasfaltantesnosistemadeemisso/Doria Marinho 0295 Carlos Leandro.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path_2 = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_17/fwdnotasfaltantesnosistemadeemisso/Doria Marinho 0295 Carlos Leandro copy.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_path = \"pipeline_extracao_documentos/0_arquivos_teste_pipeline/emails/Notas Magé.msg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_value = generate_file_hash(doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O hash do arquivo é: 53305daf18b4e42f36e27a8b900fae3a57cc033b402ba5d00c183782bc28b189\n"
     ]
    }
   ],
   "source": [
    "print(f\"O hash do arquivo é: {hash_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_value_2 = generate_file_hash(doc_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O hash do arquivo é: 2ec583e65c6cc2db60103ab3a4ebc7384414294bebfd081c6bb498ba9b7d17a5\n"
     ]
    }
   ],
   "source": [
    "print(f\"O hash do arquivo é: {hash_value_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O hash do arquivo é: 3c9b796c1d5cb097d8aa95616ffd08864eebb55635313c4dfe59af9d73982a30\n"
     ]
    }
   ],
   "source": [
    "hash_msg_value = generate_file_hash(msg_path)\n",
    "print(f\"O hash do arquivo é: {hash_msg_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = \"pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/fwdnotasfaltantesnosistemadeemisso.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O hash do arquivo é: ff1653e239202b20e56261e7f2b0fe91cb75009f92a83b2e3f2d4e08f1645ed9\n"
     ]
    }
   ],
   "source": [
    "hash_zip_value = generate_file_hash(zip_path)\n",
    "print(f\"O hash do arquivo é: {hash_zip_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar aqui um df_documento_recebido para teste\n",
    "# df_documento_recebido = ...\n",
    "\n",
    "# Chama a função e cria df_analise\n",
    "# df_analise = criar_df_analise(df_documento_recebido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "idx: 0 | file_name: Nota Fiscal Eletrônica 1717 - BALL - BVI.PDF\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 1 | file_name: NF 688 - ConsoR Zadar- Engetecnica (ENZA).pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 2 | file_name: 1127 fogo em 20-07-23.pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 3 | file_name: NF 689- PMMacae- 8ª Med CO-22-22 TB Contrato 045-22.pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 4 | file_name: Nota Fiscal_229_Agosto-Schneider.pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 5 | file_name: 38 EDSON JOSE OLIVEIRA DOS SANTOS.pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 6 | file_name: 12 - Ótica do Cizinho.pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 7 | file_name: FLEXPRIN.pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 8 | file_name: NFS-e 35.pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 9 | file_name: NFS-e 37.pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 10 | file_name: NFS-e 36 CANCELADA.pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 11 | file_name: NFS-e 38 CANCELADA.pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 12 | file_name: Doria Marinho 0297 Raquel.pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 13 | file_name: Doria Marinho 0298 Marcelo.pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 14 | file_name: Doria Marinho 0295 Carlos Leandro.pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 15 | file_name: Doria Marinho 0300 Vanisa.pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 16 | file_name: Doria Marinho 0296 Vanisa (Cancelada).pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 17 | file_name: Doria Marinho 0301 Ultrascan.pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n",
      "\n",
      "\n",
      "idx: 18 | file_name: Doria Marinho 0299 Luciana.pdf\n",
      "\n",
      "seq: 1 | action: ajustar_nome\n",
      "seq: 2 | action: split_paginas\n",
      "seq: 3 | action: ajustar_imagem\n",
      "seq: 4 | action: buscar_nome_prefeitura\n"
     ]
    }
   ],
   "source": [
    "# Iterar sobre cada linha do df_documento_recebido\n",
    "for idx, row in df_documento_recebido.iterrows():\n",
    "    # Iterar sobre cada ação da lista pre_processo\n",
    "    file_name = row['nome_arquivo']\n",
    "    print(f'\\n\\nidx: {idx} | file_name: {file_name}\\n')\n",
    "    for seq, action in enumerate(pre_processo, start=1):\n",
    "        print(f'seq: {seq} | action: {action}')\n",
    "        nova_linha = {\n",
    "            'index': row['index'],\n",
    "            'data_processamento': row['data_processamento'],\n",
    "            'Batch': row['Batch'],\n",
    "            'diretorio_origem': row['diretorio_origem'],\n",
    "            'nome_arquivo': row['nome_arquivo'],\n",
    "            'seq': seq,\n",
    "            'action': action\n",
    "        }\n",
    "        \n",
    "        linhas_df_analise.append(nova_linha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o novo DataFrame\n",
    "df_analise = pd.DataFrame(linhas_df_analise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def criar_df_analise(df_documento_recebido):\n",
    "#     linhas_df_analise = []\n",
    "    \n",
    "#     for idx, row in df_documento_recebido.iterrows():\n",
    "#         file_name = row['nome_arquivo']\n",
    "#         file_path = row['diretorio_origem']\n",
    "\n",
    "#         # Chama a função analisa_nro_pages\n",
    "#         doc_1_page, nro_paginas = analisa_nro_pages(file_path)\n",
    "\n",
    "#         # Chama a função conv_filename\n",
    "#         new_name = conv_filename(file_name)\n",
    "\n",
    "#         acoes_necessarias = []\n",
    "\n",
    "#         # Se o documento tem mais de uma página\n",
    "#         if not doc_1_page:\n",
    "#             acoes_necessarias.append(('split_paginas', nro_paginas))\n",
    "\n",
    "#         # Se o nome do arquivo precisa ser ajustado\n",
    "#         if new_name != file_name:\n",
    "#             acoes_necessarias.append(('ajustar_nome', new_name))\n",
    "\n",
    "#         for seq, (action, info) in enumerate(acoes_necessarias, start=1):\n",
    "#             nova_linha = {\n",
    "#                 'index': row['index'],\n",
    "#                 'data_processamento': row['data_processamento'],\n",
    "#                 'Batch': row['Batch'],\n",
    "#                 'nome_arquivo': row['nome_arquivo'],\n",
    "#                 'seq': seq,\n",
    "#                 'action': action,\n",
    "#                 'info_adicional': info,  # Para armazenar informações como nro_paginas ou new_name\n",
    "#                 'diretorio_origem': row['diretorio_origem']\n",
    "\n",
    "#             }\n",
    "#             linhas_df_analise.append(nova_linha)\n",
    "\n",
    "#     df_analise = pd.DataFrame(linhas_df_analise)\n",
    "#     return df_analise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processo = ['ajustar_nome', 'split_paginas', 'ajustar_imagem', 'buscar_nome_prefeitura']\n",
    "\n",
    "# Lista vazia para armazenar as linhas do novo DataFrame\n",
    "linhas_df_analise = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_pdf(args):\n",
    "    # Lógica para analisar o PDF e retornar uma lista de ações necessárias\n",
    "    acoes = []\n",
    "    if args['numero_paginas'] > 1:\n",
    "        acoes.append('split_paginas')\n",
    "    if args['nome_precisa_ajuste']:\n",
    "        acoes.append('ajustar_nome')\n",
    "    # Adicionar outras verificações aqui\n",
    "    return acoes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linhas_df_analise = []\n",
    "\n",
    "for idx, row in df_documento_recebido.iterrows():\n",
    "    # Realizar a análise inicial para determinar quais ações são necessárias\n",
    "    acoes_necessarias = analisar_pdf(row)\n",
    "    \n",
    "    for seq, action in enumerate(acoes_necessarias, start=1):\n",
    "        nova_linha = {\n",
    "            'index': row['index'],\n",
    "            'data_processamento': row['data_processamento'],\n",
    "            'Batch': row['Batch'],\n",
    "            'diretorio_origem': row['diretorio_origem'],\n",
    "            'nome_arquivo': row['nome_arquivo'],\n",
    "            'seq': seq,\n",
    "            'action': action\n",
    "        }\n",
    "        linhas_df_analise.append(nova_linha)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ZZZ Estou analisando a quantidade de paginas \n",
    "one_page_doc, paginas = analisa_nro_pages(new_path_name)\n",
    "\n",
    "# 2. ZZZ Faço um If para tomar de decisao do documentos\n",
    "if not one_page_doc:\n",
    "    print(f'documento com varias paginas = {paginas} ser splitado')\n",
    "else:    \n",
    "    print(f'documento apenas {paginas} pagina e nao preciosa ser splitado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. XXX Estabelece o pipeline de documentos\n",
    "def ajusta_pipeline(index, batch_name, file, documentos_extracao_path, folder_name, file_path):\n",
    "    \n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    # 1. Processo de ajuste de nome do documento\n",
    "    new_name = conv_filename(file)\n",
    "    #print(f'file: {file} | new_name: {new_name}')\n",
    "    root_dir_batch = os.path.join(documentos_extracao_path, batch_name)\n",
    "    root_dir_batch_dir = os.path.join(root_dir_batch, folder_name)\n",
    "    new_name_file_path = os.path.join(root_dir_batch_dir, new_name)\n",
    "    if new_name != file:\n",
    "        #print(f'nome mudou | {folder_name} | file: {file} | new_name: {new_name}\\n{file_path}\\n{new_name_file_path}\\n')\n",
    "        status_pro = 'NOME AJUSTADO'\n",
    "        try:\n",
    "            shutil.move(file_path, new_name_file_path)\n",
    "            status_pro = 'NOME AJUSTADO'\n",
    "            new_path_name = new_name_file_path\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao mudar nome do arquivo: {e}\")  \n",
    "    else:\n",
    "        status_pro = 'processado'\n",
    "        #print(f'Nao mudou | {folder_name}  | file: {file} | new_name: {new_name}')\n",
    "        \n",
    "        new_path_name = file_path\n",
    "    \n",
    "    # 2. Verifica nro de paginas\n",
    "    one_page_doc, paginas = analisa_nro_pages(new_path_name)\n",
    "    # 2. ZZZ Faço um If para tomar de decisao do documentos\n",
    "    if not one_page_doc:\n",
    "        try:\n",
    "            print(f'documento com varias paginas = {paginas} ser splitado')\n",
    "            # 3. Processa o split de paginas\n",
    "            new_pdf_path_file, nome_file_deletar = split_pdf_pages(new_path_name)\n",
    "            \n",
    "            # 3.1 elimina documento com varias paginas\n",
    "            if new_pdf_path_file:\n",
    "                os.remove(nome_file_deletar)\n",
    "            \n",
    "            df_split = processa_pipeline()   \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao efetuar split de paginas: {e}\")       \n",
    "    else:    \n",
    "        print(f'documento apenas {paginas} pagina e nao preciosa ser splitado')\n",
    "\n",
    "    return {\n",
    "        'index': index,\n",
    "        'data_processamento': time_now,\n",
    "        'Batch': batch_name,\n",
    "        'diretorio_origem': folder_name,\n",
    "        'nome_arquivo_origem': file,\n",
    "        'nome_arquivo_destino': new_name,\n",
    "        'one_page_doc': one_page_doc,\n",
    "        'numero_paginas': paginas,\n",
    "        'status': status_pro,\n",
    "    }\n",
    "\n",
    "\n",
    "# 3. FunÇao para ajustar documentos no pipeline antes de serem processados\n",
    "def processa_pipeline():\n",
    "    rows_list = []\n",
    "    i = 1\n",
    "    for root, dirs, files in os.walk(documentos_extracao_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                new_row = ajusta_pipeline(i, batch_name, file, documentos_extracao_path, folder_name, file_path)\n",
    "                rows_list.append(new_row)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao criar nova row: {e}\") \n",
    "        i += 1\n",
    "    \n",
    "    df_processa_pipe = pd.DataFrame(rows_list)\n",
    "    \n",
    "    return df_processa_pipe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tables-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
