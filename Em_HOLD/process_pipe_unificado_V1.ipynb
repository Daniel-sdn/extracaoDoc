{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <mark>Processa pipeline models - 2_process_pipe_unificado_V1</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atual notebook com as funçoes para processamento de PDF Pesquisavel e Raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "\n",
    "import platform\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "\n",
    "\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import re\n",
    "import cv2\n",
    "import fitz  # Módulo PyMuPDF\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "import PyPDF2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import locale\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import time, copy\n",
    "\n",
    "\n",
    "from outlook_msg import Message\n",
    "import extract_msg\n",
    "import zipfile\n",
    "from pyunpack import Archive\n",
    "import py7zr\n",
    "\n",
    "\n",
    "from pytz import timezone\n",
    "from urllib import response\n",
    "\n",
    "\n",
    "\n",
    "import modules.extrai_pdf_pesquisavel as Extc\n",
    "import modules.cronometro as cron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt_BR.utf8'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_test = 17\n",
    "\n",
    "### PRESTAR ATENCAO\n",
    "#model = 'mage'\n",
    "#model = 'mesquita'\n",
    "model = 'pedro_aldeia'\n",
    "\n",
    "\n",
    "# Nome Batch\n",
    "batch_name = \"Batch_\" + str(i_test)\n",
    "\n",
    "root_doc_analise = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "image_resized_path = \"pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas\"\n",
    "\n",
    "######### PATHS\n",
    "#1. path formado para busca de pdfs recursiva\n",
    "root_doc_analise = os.path.join(root_doc_analise, batch_name)\n",
    "\n",
    "# 7. path para arquivos json\n",
    "json_path = \"pipeline_extracao_documentos/5_documentos_processados/jsons\"\n",
    "\n",
    "# Nome do arquivo json\n",
    "nome_formado_json = batch_name +\".json\"\n",
    "\n",
    "\n",
    "# 1. path para documentos PDF (omelhor se estiverem dentro de um unico diretorio)\n",
    "root_pdf_path = \"pipeline_extracao_documentos/2_documentos_para_extracao\"\n",
    "\n",
    "# 2. path para documentos PDF que podem estar aguardando para serem processados\n",
    "root_pdf_aguardando_path = \"pipeline_extracao_documentos/3_tratamento_excecoes/pdf_aguardando_processar\"\n",
    "\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao\"\n",
    "\n",
    "# 3. path para documentos PDF externos para serem processados\n",
    "root_external_pdf_path = \"content_from_pdftool/data/data_pdf/NF_para_processamento/NFRJ_PDF_para _ocr\"\n",
    "# 4. path para documentos PDF PESQUISAVEIS externos para serem processados\n",
    "root_external_pdf_pesquisavel_path = \"content_from_pdftool/data/data_pdf/NF_processadas/NFRJ/fwdnotasfiscaisemitidaslmpadalegal\"\n",
    "\n",
    "# 5. path para imagem padrao\n",
    "image_resized_path = 'pipeline_extracao_documentos/6_geral_administacao/images/processadas'\n",
    "\n",
    "# 6. path para log\n",
    "log_path = 'pipeline_extracao_documentos/6_geral_administacao/logs'\n",
    "\n",
    "# 8. path para NFs processadas\n",
    "nf_processada_path = \"pipeline_extracao_documentos/5_documentos_processados\"\n",
    "\n",
    "#### paths de objetos para criacao/gestao (dicionarios/datasets)\n",
    "cnae_dict_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets/MAGE_CNAE_X_ITEM_SERVICO_V1.xlsx\"\n",
    "\n",
    "# VERIFICAR\n",
    "tgt_imagens = \"pipeline_extracao_documentos/6_geral_administacao/images\"\n",
    "\n",
    "# 12. poppler path\n",
    "poppler_path = \"/home/dani-boy/miniconda3/envs/tables-detr/bin\"\n",
    "\n",
    "# 13. path para config Tesseract\n",
    "tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "# definindo localizadcao para pt_BR\n",
    "locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")\n",
    "\n",
    "\n",
    "#### E-mail\n",
    "\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao\"\n",
    "\n",
    "# 1. Caminho do arquivo uma mensagem especifica\n",
    "msg_dir_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails'\n",
    "\n",
    "# 2. Path para arquivos atachados compactados\n",
    "msg_attachment_zip = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments'\n",
    "\n",
    "# 3. Path para documentos atachados:\n",
    "documentos_extracao_path = 'pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "i_test: 13 | model: mage | batch_name: Batch_13 | root_doc_analise: pipeline_extracao_documentos/2_documentos_para_extracao/Batch_13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'\\ni_test: {i_test} | model: {model} | batch_name: {batch_name} | root_doc_analise: {root_doc_analise}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Batch</th>\n",
       "      <th>diretorio_origem</th>\n",
       "      <th>nome_arquivo_origem</th>\n",
       "      <th>nome_arquivo_destino</th>\n",
       "      <th>data_processamento</th>\n",
       "      <th>tipo_pdf</th>\n",
       "      <th>qut_paginas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Batch_12</td>\n",
       "      <td>notas_mage_31082023</td>\n",
       "      <td>NF 689- PMMacae- 8ª Med CO-22-22 TB Contrato 0...</td>\n",
       "      <td>nf_689__pmmacae__8a_med_co_22_22_tb_contrato_0...</td>\n",
       "      <td>01/09/2023 14:22:44</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Batch_12</td>\n",
       "      <td>notas_mage_31082023</td>\n",
       "      <td>nfse 4414.pdf</td>\n",
       "      <td>nfse_4414.pdf</td>\n",
       "      <td>01/09/2023 14:22:44</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Batch_12</td>\n",
       "      <td>notas_mage_31082023</td>\n",
       "      <td>nfse 4409.pdf</td>\n",
       "      <td>nfse_4409.pdf</td>\n",
       "      <td>01/09/2023 14:22:44</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Batch_12</td>\n",
       "      <td>notas_mage_31082023</td>\n",
       "      <td>NF 688 - ConsoR Zadar- Engetecnica (ENZA) (1).pdf</td>\n",
       "      <td>nf_688___consor_zadar__engetecnica_enza_1.pdf</td>\n",
       "      <td>01/09/2023 14:22:44</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Batch_12</td>\n",
       "      <td>notas_mage_31082023</td>\n",
       "      <td>12225 COFIM -.pdf</td>\n",
       "      <td>12225_cofim__.pdf</td>\n",
       "      <td>01/09/2023 14:22:44</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>4</td>\n",
       "      <td>Batch_16</td>\n",
       "      <td>159871</td>\n",
       "      <td>2023 -8.pdf</td>\n",
       "      <td>2023__8.pdf</td>\n",
       "      <td>01/09/2023 15:28:23</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>5</td>\n",
       "      <td>Batch_16</td>\n",
       "      <td>160014</td>\n",
       "      <td>31-07.pdf</td>\n",
       "      <td>31_07.pdf</td>\n",
       "      <td>01/09/2023 15:28:23</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>5</td>\n",
       "      <td>Batch_16</td>\n",
       "      <td>160014</td>\n",
       "      <td>ACFrOgBLgYewSPQAweUd3QJkpDqN5Kp2dFIyNq7d6wJCRY...</td>\n",
       "      <td>acfrogblgyewspqaweud3qjkpdqn5kp2dfiynq7d6wjcry...</td>\n",
       "      <td>01/09/2023 15:28:23</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>6</td>\n",
       "      <td>Batch_16</td>\n",
       "      <td>126623</td>\n",
       "      <td>41C46D8F-73AB-4906-A4C6-C7DC92C05828.PDF</td>\n",
       "      <td>41c46d8f_73ab_4906_a4c6_c7dc92c05828.pdf</td>\n",
       "      <td>01/09/2023 15:28:23</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>7</td>\n",
       "      <td>Batch_16</td>\n",
       "      <td>138565</td>\n",
       "      <td>B4066C58-F309-42E4-A992-55EB8961211E.PDF</td>\n",
       "      <td>b4066c58_f309_42e4_a992_55eb8961211e.pdf</td>\n",
       "      <td>01/09/2023 15:28:23</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index     Batch     diretorio_origem  \\\n",
       "0       2  Batch_12  notas_mage_31082023   \n",
       "1       2  Batch_12  notas_mage_31082023   \n",
       "2       2  Batch_12  notas_mage_31082023   \n",
       "3       2  Batch_12  notas_mage_31082023   \n",
       "4       2  Batch_12  notas_mage_31082023   \n",
       "..    ...       ...                  ...   \n",
       "78      4  Batch_16               159871   \n",
       "79      5  Batch_16               160014   \n",
       "80      5  Batch_16               160014   \n",
       "81      6  Batch_16               126623   \n",
       "82      7  Batch_16               138565   \n",
       "\n",
       "                                  nome_arquivo_origem  \\\n",
       "0   NF 689- PMMacae- 8ª Med CO-22-22 TB Contrato 0...   \n",
       "1                                       nfse 4414.pdf   \n",
       "2                                       nfse 4409.pdf   \n",
       "3   NF 688 - ConsoR Zadar- Engetecnica (ENZA) (1).pdf   \n",
       "4                                   12225 COFIM -.pdf   \n",
       "..                                                ...   \n",
       "78                                        2023 -8.pdf   \n",
       "79                                          31-07.pdf   \n",
       "80  ACFrOgBLgYewSPQAweUd3QJkpDqN5Kp2dFIyNq7d6wJCRY...   \n",
       "81           41C46D8F-73AB-4906-A4C6-C7DC92C05828.PDF   \n",
       "82           B4066C58-F309-42E4-A992-55EB8961211E.PDF   \n",
       "\n",
       "                                 nome_arquivo_destino   data_processamento  \\\n",
       "0   nf_689__pmmacae__8a_med_co_22_22_tb_contrato_0...  01/09/2023 14:22:44   \n",
       "1                                       nfse_4414.pdf  01/09/2023 14:22:44   \n",
       "2                                       nfse_4409.pdf  01/09/2023 14:22:44   \n",
       "3       nf_688___consor_zadar__engetecnica_enza_1.pdf  01/09/2023 14:22:44   \n",
       "4                                   12225_cofim__.pdf  01/09/2023 14:22:44   \n",
       "..                                                ...                  ...   \n",
       "78                                        2023__8.pdf  01/09/2023 15:28:23   \n",
       "79                                          31_07.pdf  01/09/2023 15:28:23   \n",
       "80  acfrogblgyewspqaweud3qjkpdqn5kp2dfiynq7d6wjcry...  01/09/2023 15:28:23   \n",
       "81           41c46d8f_73ab_4906_a4c6_c7dc92c05828.pdf  01/09/2023 15:28:23   \n",
       "82           b4066c58_f309_42e4_a992_55eb8961211e.pdf  01/09/2023 15:28:23   \n",
       "\n",
       "    tipo_pdf  qut_paginas  \n",
       "0      False            1  \n",
       "1       True            1  \n",
       "2       True            1  \n",
       "3      False            1  \n",
       "4       True            1  \n",
       "..       ...          ...  \n",
       "78      True            1  \n",
       "79      True            1  \n",
       "80      True            1  \n",
       "81      True            1  \n",
       "82      True            1  \n",
       "\n",
       "[83 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Tratando nome de carga do df_processamento\n",
    "dataset_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets\"\n",
    "\n",
    "df_processamento_file = \"df_processamento_\"\n",
    "\n",
    "df_processamento_file_read = df_processamento_file + str(i_test - 1) + \".xlsx\"\n",
    "\n",
    "# 2. Tratando nome de carga do df_extracao_files_Batch\n",
    "df_extracao_files_Batch_file = \"df_extracao_files_Batch_\"\n",
    "\n",
    "df_extracao_files_Batch_file_read = df_extracao_files_Batch_file + str(i_test - 1) + \".xlsx\"\n",
    "\n",
    "\n",
    "df_processamento_file_read_path = os.path.join(dataset_path, df_processamento_file_read)\n",
    "\n",
    "df_processamento_file_read_path\n",
    "\n",
    "df_extracao_files_Batch_file_read_path = os.path.join(dataset_path, df_extracao_files_Batch_file_read)\n",
    "\n",
    "df_extracao_files_Batch_file_read_path\n",
    "\n",
    "df_processamento = pd.read_excel(df_processamento_file_read_path) \n",
    "\n",
    "df_processamento\n",
    "\n",
    "df_extracao_files = pd.read_excel(df_extracao_files_Batch_file_read_path) \n",
    "\n",
    "df_extracao_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trata dicionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. path para models\n",
    "nf_model_path = \"pipeline_extracao_documentos/6_geral_administacao/modelos/frames_nf_v9.xlsx\"\n",
    "\n",
    "# 11. path para datasets CNAE e Itens de Serviço\n",
    "nf_datasets_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria do DF\n",
    "frames_nf_v4_df = pd.read_excel(nf_model_path)\n",
    "\n",
    "# Cria dicionários para armazenar diferentes tipos de elementos do modelo\n",
    "document_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'document'].iloc[0]\n",
    "boundaries_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'boundaries']\n",
    "sections_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'section']\n",
    "frames_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'frame']\n",
    "sframe_fields_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'sframe_field']\n",
    "field_boxes_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'field_box']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mage_cnae_x_item_servico_df = pd.read_excel(cnae_dict_path)\n",
    "\n",
    "# Creating a dictionary for CNAE codes and descriptions\n",
    "cnae_dict = dict(zip(mage_cnae_x_item_servico_df['cnae'], mage_cnae_x_item_servico_df['descricao_cnae']))\n",
    "item_servico_dict = dict(zip(mage_cnae_x_item_servico_df['item_servico'], mage_cnae_x_item_servico_df['descricao_item_servico']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_dict_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m model_dictionary\n\u001b[1;32m     18\u001b[0m \u001b[39m# 3. Cria o dict de modelos\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m model_dict \u001b[39m=\u001b[39m create_model_dictionary(model_dict_path) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_dict_path' is not defined"
     ]
    }
   ],
   "source": [
    "# 2. Leitura do arquivo CSV e criação do dicionário modelos\n",
    "def create_model_dictionary(model_dict_path):\n",
    "    model_dictionary = {}\n",
    "    with open(model_dict_path, 'r') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            prefeitura_name = row['prefeitura']\n",
    "            model_name = row['model']\n",
    "\n",
    "            if prefeitura_name not in model_dictionary:\n",
    "                model_dictionary[prefeitura_name] = model_name\n",
    "            \n",
    "            #model_dictionary[prefeitura_name].append(model_name)\n",
    "    \n",
    "    return model_dictionary\n",
    "\n",
    "\n",
    "# 3. Cria o dict de modelos\n",
    "model_dict = create_model_dictionary(model_dict_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mif\u001b[39;00m model_dict[name_prefeitura]:\n\u001b[1;32m      2\u001b[0m     modelo \u001b[39m=\u001b[39m model_dict[name_prefeitura]\n\u001b[1;32m      3\u001b[0m     model \u001b[39m=\u001b[39m model_dict[name_prefeitura]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_dict' is not defined"
     ]
    }
   ],
   "source": [
    "if model_dict[name_prefeitura]:\n",
    "    modelo = model_dict[name_prefeitura]\n",
    "    model = model_dict[name_prefeitura]\n",
    "    message_log = f\"Nome de prefeitura: {name_prefeitura} encontrada para: {doc2convert} modelo a ser utilizado: {modelo}\"\n",
    "    print(message_log)\n",
    "    #logging.info(message_log) \n",
    "else:\n",
    "    modelo = model_padrao\n",
    "    message_log = f'\\nmodelo nao encontrado para prefeitura {name_prefeitura}, sera utilizado o modelo {modelo}'   \n",
    "    print(message_log)\n",
    "    #logging.info(message_log)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### funcoes de arquivo - IMPORTANTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "no such file: 'seuarquivo.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11407/1473825272.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Caminho para o PDF que você quer dividir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minput_pdf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"seuarquivo.pdf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Abre o PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_pdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Número total de páginas no PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtotal_pages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tables-detr/lib/python3.10/site-packages/fitz/fitz.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[0m\n\u001b[1;32m   3949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3950\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfrom_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3951\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3952\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"no such file: '{filename}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3953\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3954\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3955\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"'{filename}' is no file\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3956\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mFileDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: no such file: 'seuarquivo.pdf'"
     ]
    }
   ],
   "source": [
    "## Modelo para criar varias paginas do PDF\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Caminho para o PDF que você quer dividir\n",
    "input_pdf_path = \"seuarquivo.pdf\"\n",
    "\n",
    "# Abre o PDF\n",
    "pdf = fitz.open(input_pdf_path)\n",
    "\n",
    "# Número total de páginas no PDF\n",
    "total_pages = len(pdf)\n",
    "\n",
    "# Nome base para os arquivos de saída\n",
    "base_name = input_pdf_path.split('.')[0]  # Remove a extensão do arquivo\n",
    "\n",
    "# Loop para criar um novo PDF para cada página\n",
    "for page_num in range(total_pages):\n",
    "    # Cria um novo objeto PDF\n",
    "    new_pdf = fitz.open()\n",
    "    # Adiciona a página atual ao novo PDF\n",
    "    new_pdf.insert_pdf(pdf, from_page=page_num, to_page=page_num)\n",
    "    # Nome do novo arquivo PDF\n",
    "    new_pdf_name = f\"{base_name}_page_{page_num + 1}.pdf\"\n",
    "    # Salva o novo PDF\n",
    "    new_pdf.save(new_pdf_name)\n",
    "    # Fecha o novo PDF\n",
    "    new_pdf.close()\n",
    "\n",
    "# Fecha o PDF original\n",
    "pdf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move NF processadas ok\n",
    "def move_pdf_processed_ok(document_path, nf_processada_path, batch_name, doc2convert):\n",
    "    # Determine the destination directory\n",
    "    destination_dir = os.path.join(nf_processada_path, batch_name)\n",
    "\n",
    "    # Check if the destination directory exists; if not, create it\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "\n",
    "    # Determine the destination path including the filename\n",
    "    destination_path = os.path.join(destination_dir, os.path.basename(document_path))\n",
    "\n",
    "    # Move the file from the source path to the destination path\n",
    "    try:\n",
    "        shutil.move(document_path, destination_path)\n",
    "        print(f\"Sucesso ao mover: {document_path} para: {destination_path}\")\n",
    "        return True, destination_path, None  # Success, destination path, no error\n",
    "    except Exception as e:\n",
    "        error_message = f\"Erro ao mover: {document_path} para: {destination_path}: {str(e)}\"\n",
    "        print(error_message)\n",
    "        return False, None, error_message  # Failure, no destination path, error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_pdf_processed_ok(document_path):\n",
    "    \n",
    "    source_path = document_path\n",
    "    destination_path = os.path.join(f'{nf_processada_path}/{str(doc2convert)}')\n",
    "    shutil.move(source_path, destination_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mover PDF para diretorio de processados\n",
    "source_path = document_path\n",
    "destination_path = os.path.join(f'{nf_processada_path}/{str(doc2convert)}')\n",
    "shutil.move(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***DELETAR**** o arquivo de imagem gerado\n",
    "image_path_to_delete = name_image_2work\n",
    "os.remove(image_path_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voltar (mover) PDF para diretorio para ser processado\n",
    "source_path = destination_path\n",
    "\n",
    "destination_path = document_path\n",
    "\n",
    "shutil.move(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processo para iterar estrutura de arquivos, diretorios\n",
    "for roots, directories, documents in os.walk(image_resized_path):\n",
    "    print(f'\\n {documents} {name_image_2work}\\n\\n')\n",
    "    #if documents == :\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Buscar o primeiro documento do diretorio EXTERNO para processar\n",
    "source_directory = root_external_pdf_path\n",
    "destination_directory = root_pdf_path\n",
    "\n",
    "# Verifique se o diretório de destino está vazio\n",
    "if os.listdir(destination_directory):\n",
    "    raise Exception(\"O diretório de destino não está vazio!\")\n",
    "\n",
    "for roots, directories, documents in os.walk(source_directory):\n",
    "    # Filtre os documentos para incluir apenas aqueles com extensão .pdf\n",
    "    pdf_files = [doc for doc in documents if doc.lower().endswith('.pdf')]\n",
    "    \n",
    "    if pdf_files: # Verifique se há algum arquivo PDF no diretório\n",
    "        first_pdf_file = pdf_files[0] # Obtenha o primeiro arquivo PDF\n",
    "        source_path = os.path.join(roots, first_pdf_file) # Construa o caminho completo para o primeiro arquivo PDF\n",
    "        print(f\"Found the first PDF file: {source_path}\")\n",
    "        \n",
    "        # Mova o arquivo para o diretório de destino\n",
    "        shutil.move(source_path, destination_directory)\n",
    "        \n",
    "        break # Saia do loop após encontrar o primeiro arquivo PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Funcoes necessarias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. converte nome do arquivo\n",
    "def conv_filename(title):\n",
    "    \n",
    "    # Divide o título em nome e extensão\n",
    "    name, extension = title.rsplit('.', 1) if '.' in title else (title, \"\")\n",
    "\n",
    "    # Remove acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substiti espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remove quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converte para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    # Adiciona a extensão de volta, se houver\n",
    "    if extension:\n",
    "        filename += '.' + extension.lower()\n",
    "\n",
    "    return filename\n",
    "\n",
    "def conv_filename_no_ext(title):\n",
    "    \n",
    "    # Divida o título em nome e extensão (mas ignore a extensão)\n",
    "    name = title.rsplit('.', 1)[0] if '.' in title else title\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    return filename \n",
    "\n",
    "def confirma_pdf_pequisavel(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 4\n",
    "    x1 = 600\n",
    "    y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    if text:\n",
    "       page_number = 0\n",
    "       pdf_realmente_pequisavel = True\n",
    "       #print(page_number)\n",
    "    else:\n",
    "       page_number = 1\n",
    "       pdf_realmente_pequisavel = False\n",
    "       #print(page_number)\n",
    "    \n",
    "    try:\n",
    "        page = pdf_document[page_number]\n",
    "        x0 = 0\n",
    "        y0 = 0\n",
    "        x1 = 600\n",
    "        y1 = 110\n",
    "\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        nf_data_cabecalho = Extc.extract_fields_cabecalho(text)\n",
    "        \n",
    "        if nf_data_cabecalho:\n",
    "            pdf_realmente_pequisavel = True\n",
    "        else:\n",
    "            pdf_realmente_pequisavel = False\n",
    "            \n",
    "      \n",
    "        return pdf_realmente_pequisavel, page_number\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao abrir pagina do PDF: {e}\")\n",
    "        return pdf_realmente_pequisavel, page_number\n",
    "    pdf_document.close()   \n",
    "\n",
    "def confirma_pdf_pequisavelx(file, file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 4\n",
    "    x1 = 600\n",
    "    y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    if text:\n",
    "       page_number = 0\n",
    "       print(page_number)\n",
    "    else:\n",
    "       page_number = 1\n",
    "       print(page_number)\n",
    "    \n",
    "    try:\n",
    "        page = pdf_document[page_number]\n",
    "        x0 = 0\n",
    "        y0 = 0\n",
    "        x1 = 600\n",
    "        y1 = 110\n",
    "\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        nf_data_cabecalho = Extc.extract_fields_cabecalho(text)\n",
    "        \n",
    "        if nf_data_cabecalho:\n",
    "            pdf_realmente_pequisavel = True\n",
    "        else:\n",
    "            pdf_realmente_pequisavel = False\n",
    "            \n",
    "        pdf_document.close()\n",
    "        \n",
    "        return pdf_realmente_pequisavel\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao abrir pagina do PDF: {e}\")\n",
    "        return pdf_realmente_pequisavel\n",
    "\n",
    "\n",
    "# 2. Pesquisa prefeitura no documento\n",
    "def pequisaModel(image_name):\n",
    "\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = 406\n",
    "    y0 = 0\n",
    "    x1= 1540\n",
    "    y1 = 380\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "\n",
    "    # 5. Interacao para pesquisar prefeitura\n",
    "    for value in values:\n",
    "        nome_prefeitura_match = re.search(r'PREFEITURA (.+)', value)\n",
    "        if nome_prefeitura_match:\n",
    "            nome_prefeitura = \"PREFEITURA \" + nome_prefeitura_match.group(1) \n",
    "            return  nome_prefeitura  \n",
    "    \n",
    "\n",
    "\n",
    "# 3. move NF processadas ok\n",
    "def move_raster_pdf(document_path, raster_pdf_path, batch_name, doc2convert):\n",
    "    # Determine the destination directory\n",
    "    destination_dir = os.path.join(raster_pdf_path, batch_name)\n",
    "\n",
    "    # Check if the destination directory exists; if not, create it\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "\n",
    "    # Determine the destination path including the filename\n",
    "    destination_path = os.path.join(destination_dir, os.path.basename(document_path))\n",
    "\n",
    "    # Move the file from the source path to the destination path\n",
    "    try:\n",
    "        shutil.move(document_path, destination_path)\n",
    "        print(f\"Sucesso ao mover: {document_path} para: {destination_path}\")\n",
    "        return True, destination_path, None  # Success, destination path, no error\n",
    "    except Exception as e:\n",
    "        error_message = f\"Erro ao mover: {document_path} para: {destination_path}: {str(e)}\"\n",
    "        print(error_message)\n",
    "        return False, None, error_message  # Failure, no destination path, error message\n",
    "\n",
    "# 5. Verifica se PDF e pesquisavel ou nao e grava metadados dele\n",
    "def is_pdf_searchable_analise(pdf_path):\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        pages = pdf_document.page_count\n",
    "        is_searchable = all(page.get_text(\"text\") != \"\" for page in pdf_document)\n",
    "        dados_pdf = pdf_document.metadata\n",
    "        pdf_document.close()\n",
    "        return is_searchable, dados_pdf, pages\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao verificar o PDF: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Lendo o email e anexos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>1.</b> Lendo email e anexos </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email: Fwd Notas Magé 2.msg, email_path: pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails/Fwd Notas Magé 2.msg\n",
      "10\n",
      "pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/12 - Ótica do Cizinho.pdf\n",
      "pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/38 EDSON JOSE OLIVEIRA DOS SANTOS.pdf\n",
      "pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/FLEXPRIN.pdf\n",
      "pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/NF 688 - ConsoR Zadar- Engetecnica (ENZA).pdf\n",
      "pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/1127 fogo em 20-07-23.pdf\n",
      "pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/Nota Fiscal Eletrônica 1717 - BALL - BVI.PDF\n",
      "pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/NF 689- PMMacae- 8ª Med CO-22-22 TB Contrato 045-22.pdf\n",
      "pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/Nota Fiscal_229_Agosto-Schneider.pdf\n",
      "pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/fwdresnotasausentesnosistema.zip\n",
      "pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/fwdnotasfaltantesnosistemadeemisso.zip\n"
     ]
    }
   ],
   "source": [
    "# Defina a localização para pt_BR\n",
    "locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")\n",
    "\n",
    "for root, dirs, files in os.walk(msg_dir_path):\n",
    "    #print(f'{root}  | {dirs} | {document} | {files}\\n')\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        if file.lower().endswith('.msg'):\n",
    "            email_message = file\n",
    "            email_path = os.path.join(root, file)\n",
    "            if email_message:\n",
    "                print(f'email: {email_message}, email_path: {email_path}')\n",
    "            else:\n",
    "                print(\"Nao ha email\")\n",
    "                break\n",
    "try:            \n",
    "    msg = extract_msg.Message(email_path)\n",
    "    msg_raw_sender = msg.sender\n",
    "    parts = msg_raw_sender.rsplit('<', 1)\n",
    "    msg_email_address = parts[1].strip('<>')\n",
    "    msg_sender = parts[0].strip(' ')\n",
    "    msg_subject = msg.subject\n",
    "    msg_body = msg.body\n",
    "    # String original\n",
    "    original_date_str = msg.date\n",
    "    date_email = cron.convert_email_date(original_date_str)\n",
    "        \n",
    "    # Caminho da pasta onde você quer salvar os anexos\n",
    "    pasta_destino = msg_attachment_zip\n",
    "\n",
    "    # Verifica se a pasta existe; se não, cria ela\n",
    "    if not os.path.exists(pasta_destino):\n",
    "        os.makedirs(pasta_destino)\n",
    "\n",
    "    with open(email_path) as msg_file:\n",
    "        msg = Message(msg_file)\n",
    "    \n",
    "    total_attch = len(msg.attachments)\n",
    "    \n",
    "    print(total_attch)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro ao ler email: {e}\")\n",
    "    \n",
    "\n",
    "arquivos_zip = []\n",
    "arquivos = []\n",
    "i = 0\n",
    "# Loop para salvar cada anexo\n",
    "for i in range(total_attch):\n",
    "    attachment = msg.attachments[i]\n",
    "    caminho_completo_anexo = os.path.join(pasta_destino, attachment.filename)\n",
    "    if file.lower().endswith('.zip') or file.lower().endswith('.rar') or file.lower().endswith('.7z'):\n",
    "        arquivos_zip.append(attachment.filename)\n",
    "    else:\n",
    "        arquivos.append(attachment.filename)\n",
    "      \n",
    "    print(caminho_completo_anexo)\n",
    "    with attachment.open() as attachment_fp, open(caminho_completo_anexo, 'wb') as output_fp:\n",
    "        output_fp.write(attachment_fp.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 Salvando os attachments do e-mail </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>2.</b> Extraindo compressed files </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório onde você quer salvar os arquivos extraídos\n",
    "output_dir = os.path.join(documentos_extracao_path, batch_name)\n",
    "\n",
    "folder_file_dict = {}\n",
    "\n",
    "for root, dirs, files in os.walk(msg_attachment_zip):\n",
    "    #print(f'{root}  | {dirs} | {document} | {files}\\n')\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        if file.lower().endswith('.zip'):\n",
    "            zip_file = file\n",
    "            zip_file_path = os.path.join(root, file)\n",
    "            # Obtém o nome base do arquivo ZIP para usar como subdiretório\n",
    "            zip_base_name = os.path.splitext(os.path.basename(zip_file_path))[0]\n",
    "            zip_basename = conv_filename(zip_base_name)\n",
    "            \n",
    "            # Cria o subdiretório com base no nome do arquivo ZIP\n",
    "            root_output_dir = os.path.join(output_dir, zip_basename)\n",
    "            \n",
    "            if not os.path.exists(root_output_dir):\n",
    "                os.makedirs(root_output_dir) # estou criando o diretorio caso nao exista\n",
    "\n",
    "            # Abre o arquivo ZIP\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                for member in zip_ref.namelist():\n",
    "                    # Separa o nome da pasta e o nome do arquivo usando barra invertida como delimitador\n",
    "                    parts = member.rsplit('\\\\', 1)\n",
    "                    folder_name = parts[0] if len(parts) > 1 else ''\n",
    "                    #folder_name = conv_filename(folder_temp)\n",
    "                    filename = parts[-1]\n",
    "                    if filename:  # ignora diretórios\n",
    "                        # Adiciona ao dicionário\n",
    "                        #filename = conv_filename(filename)\n",
    "                        # Cria um subdiretório se ele não existir\n",
    "                        sub_dir = os.path.join(root_output_dir, folder_name)\n",
    "                        if not os.path.exists(sub_dir):\n",
    "                            os.makedirs(sub_dir)\n",
    "\n",
    "                        # Salva o arquivo no subdiretório especificado\n",
    "                        source = zip_ref.open(member)\n",
    "                        target_path = os.path.join(sub_dir, filename)\n",
    "                        \n",
    "                        with open(target_path, \"wb\") as target:\n",
    "                            target.write(source.read())\n",
    "                            dir_path = os.path.dirname(filename)\n",
    "                            \n",
    "        elif file.lower().endswith('.rar'):\n",
    "            rar_file = file\n",
    "            rar_file = conv_filename_no_ext(rar_file)\n",
    "            rar_file_path = os.path.join(root, file) \n",
    "            root_output_dir = os.path.join(output_dir, rar_file)\n",
    "            if not os.path.exists(root_output_dir):\n",
    "                os.makedirs(root_output_dir)\n",
    "            Archive(rar_file_path).extractall(root_output_dir)  \n",
    "            \n",
    "        elif file.lower().endswith('.7z'):\n",
    "            sevenz_file = file\n",
    "            sevenz_file = conv_filename_no_ext(sevenz_file)\n",
    "            sevenz_file_path = os.path.join(root, file)\n",
    "            root_output_dir = os.path.join(output_dir, sevenz_file)                      \n",
    "                            \n",
    "            with py7zr.SevenZipFile(sevenz_file_path, mode='r') as z:\n",
    "                z.extractall(root_output_dir)\n",
    "                \n",
    "        elif file.lower().endswith('.pdf'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            new_path_name = os.path.join(output_dir, file)\n",
    "            if not os.path.exists(output_dir):\n",
    "                            os.makedirs(output_dir)\n",
    "            shutil.move(file_path, new_path_name)\n",
    "            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para armazenar as linhas\n",
    "rows_list = []\n",
    "\n",
    "# Função recursiva para adicionar linha\n",
    "def add_row_recursively(rows_list, \n",
    "                        index, \n",
    "                        Batch, \n",
    "                        diretorio_ori, \n",
    "                        arquivo_origem, \n",
    "                        arquivo_destino, \n",
    "                        data_hora, \n",
    "                        tipo_doc_pdf,\n",
    "                        trully_searcheable, \n",
    "                        qtd_paginas,\n",
    "                        amostra_texto\n",
    "                        ):\n",
    "    if index == 0:\n",
    "        return rows_list\n",
    "    else:\n",
    "        new_row = {\n",
    "                    'index': index,\n",
    "                    'Batch': Batch,\n",
    "                    'diretorio_origem': diretorio_ori,\n",
    "                    'nome_arquivo_origem': arquivo_origem,\n",
    "                    'nome_arquivo_destino': arquivo_destino,\n",
    "                    'data_processamento': data_hora,\n",
    "                    'tipo_pdf': tipo_doc_pdf,\n",
    "                    'trully_search': trully_searcheable,\n",
    "                    'qut_paginas': qtd_paginas,\n",
    "                    'amostra_text': amostra_texto\n",
    "                    }\n",
    "        rows_list.append(new_row)\n",
    "        \n",
    "        return add_row_recursively(rows_list, index-1, Batch, diretorio_ori, arquivo_origem, arquivo_destino, data_hora, tipo_doc_pdf, qtd_paginas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>3.</b> Ajustando o filename e criando o dicionario </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ajustando o filename e criando o dicionario\n",
    "folder_file_dict = {}\n",
    "rows_list = []\n",
    "text_cabecalho = []\n",
    "output_dir = os.path.join(documentos_extracao_path, batch_name)\n",
    "i = 0\n",
    "for root, dirs, files in os.walk(\"pipeline_extracao_documentos/2_documentos_para_extracao\"):\n",
    "    folder_name = os.path.basename(root)\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        #pesquisavel, metadados, paginas = is_pdf_searchable_analise(file_path)\n",
    "        \n",
    "        pdf_realmente_pesquisavel = confirma_pdf_pequisavel(file_path)\n",
    "\n",
    "        new_name = conv_filename(file)\n",
    "        new_path_name = os.path.join(root, new_name)\n",
    "        #print(f'\\nfile: {file} | new_name: {new_name} ')\n",
    "        shutil.move(file_path, new_path_name)\n",
    "        folder_file_dict.setdefault(folder_name, []).append(new_name)\n",
    "        time_now = cron.timenow_pt_BR()\n",
    "        new_row = {\n",
    "                    'index': i,\n",
    "                    'Batch': batch_name,\n",
    "                    'diretorio_origem': folder_name,\n",
    "                    'nome_arquivo_origem': file,\n",
    "                    'nome_arquivo_destino': new_name,\n",
    "                    'data_processamento': time_now,\n",
    "                    'tipo_pdf': \"pesquisavel\",\n",
    "                    'trully_search':pdf_realmente_pesquisavel,\n",
    "                    'qut_paginas': paginas,\n",
    "                    'amostra_text': \"texto_cabe\"\n",
    "                    \n",
    "                    }\n",
    "        rows_list.append(new_row)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files_v2 = pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Batch</th>\n",
       "      <th>diretorio_origem</th>\n",
       "      <th>nome_arquivo_origem</th>\n",
       "      <th>nome_arquivo_destino</th>\n",
       "      <th>data_processamento</th>\n",
       "      <th>tipo_pdf</th>\n",
       "      <th>trully_search</th>\n",
       "      <th>qut_paginas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1002479</td>\n",
       "      <td>nota_fiscal_eletronica_190.pdf</td>\n",
       "      <td>nota_fiscal_eletronica_190.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1002479</td>\n",
       "      <td>nota_fiscal_eletronica_188.pdf</td>\n",
       "      <td>nota_fiscal_eletronica_188.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1002479</td>\n",
       "      <td>nota_fiscal_eletronica_189.pdf</td>\n",
       "      <td>nota_fiscal_eletronica_189.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1001924</td>\n",
       "      <td>listagem_de_nfs_e___sintetico_gf_pereira.pdf</td>\n",
       "      <td>listagem_de_nfs_e___sintetico_gf_pereira.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1004403</td>\n",
       "      <td>nf_11_igreja_evangelica_batista_eterna_vida.pdf</td>\n",
       "      <td>nf_11_igreja_evangelica_batista_eterna_vida.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1004463</td>\n",
       "      <td>nota_fiscal_coco_legal_julho_2023.pdf</td>\n",
       "      <td>nota_fiscal_coco_legal_julho_2023.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1004268</td>\n",
       "      <td>nota_fiscal_eletronica_julho.pdf</td>\n",
       "      <td>nota_fiscal_eletronica_julho.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1004268</td>\n",
       "      <td>relatorio_de_listagem.pdf</td>\n",
       "      <td>relatorio_de_listagem.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1005516</td>\n",
       "      <td>listagem_de_nfs_e__clinica_anna_stofoles.pdf</td>\n",
       "      <td>listagem_de_nfs_e__clinica_anna_stofoles.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>11789914</td>\n",
       "      <td>nf_12_renilda.pdf</td>\n",
       "      <td>nf_12_renilda.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>11779370</td>\n",
       "      <td>emissao_24_07.pdf</td>\n",
       "      <td>emissao_24_07.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>7481</td>\n",
       "      <td>listagem_de_nfs_e___sintetico.pdf</td>\n",
       "      <td>listagem_de_nfs_e___sintetico.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>242207</td>\n",
       "      <td>listagem_de_nfs_e___sintetico.pdf</td>\n",
       "      <td>listagem_de_nfs_e___sintetico.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>11790236</td>\n",
       "      <td>1.pdf</td>\n",
       "      <td>1.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1005075</td>\n",
       "      <td>nf_29_gri.pdf</td>\n",
       "      <td>nf_29_gri.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1004847</td>\n",
       "      <td>nf_19_condominio_edificio_mendes_campos.pdf</td>\n",
       "      <td>nf_19_condominio_edificio_mendes_campos.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1004847</td>\n",
       "      <td>nf_20_edificio_mendes.pdf</td>\n",
       "      <td>nf_20_edificio_mendes.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1004813</td>\n",
       "      <td>nf_55_jml.pdf</td>\n",
       "      <td>nf_55_jml.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>240746</td>\n",
       "      <td>nf_jogos_lot_surui_07_2023.pdf</td>\n",
       "      <td>nf_jogos_lot_surui_07_2023.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>240746</td>\n",
       "      <td>nf_nao_jogos_lot_surui_07_2023.pdf</td>\n",
       "      <td>nf_nao_jogos_lot_surui_07_2023.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>240274</td>\n",
       "      <td>listagem_de_nfs_e___sintetico_rx_face.pdf</td>\n",
       "      <td>listagem_de_nfs_e___sintetico_rx_face.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>11779446</td>\n",
       "      <td>digitalizacao_26_de_jul_de_20231.pdf</td>\n",
       "      <td>digitalizacao_26_de_jul_de_20231.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>241538</td>\n",
       "      <td>julh_2023_notafiscal_2.pdf</td>\n",
       "      <td>julh_2023_notafiscal_2.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>11778530</td>\n",
       "      <td>nota_fiscal_no27_vitale_eco.pdf</td>\n",
       "      <td>nota_fiscal_no27_vitale_eco.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>21</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>11778530</td>\n",
       "      <td>nota_fiscal_no26_origem.pdf</td>\n",
       "      <td>nota_fiscal_no26_origem.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>22</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1001204</td>\n",
       "      <td>nota_magno_150.pdf</td>\n",
       "      <td>nota_magno_150.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>22</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1001204</td>\n",
       "      <td>nota_magno_151.pdf</td>\n",
       "      <td>nota_magno_151.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>11779609</td>\n",
       "      <td>listagem_de_nfs_e___sintetico_clin_daiane_rodr...</td>\n",
       "      <td>listagem_de_nfs_e___sintetico_clin_daiane_rodr...</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>24</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>1001255</td>\n",
       "      <td>listagem_de_nfs_e___sintetico_odonto_ja.pdf</td>\n",
       "      <td>listagem_de_nfs_e___sintetico_odonto_ja.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>25</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>10597</td>\n",
       "      <td>nfs299_072023_floc.pdf</td>\n",
       "      <td>nfs299_072023_floc.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>25</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>10597</td>\n",
       "      <td>nfs300_072023_floc_textil.pdf</td>\n",
       "      <td>nfs300_072023_floc_textil.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>28</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>115964</td>\n",
       "      <td>Livro de Registro do ISSQN.pdf</td>\n",
       "      <td>livro_de_registro_do_issqn.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>29</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>159871</td>\n",
       "      <td>2023 -5.pdf</td>\n",
       "      <td>2023__5.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>29</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>159871</td>\n",
       "      <td>2023 -7.pdf</td>\n",
       "      <td>2023__7.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>29</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>159871</td>\n",
       "      <td>2023 -4.pdf</td>\n",
       "      <td>2023__4.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>29</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>159871</td>\n",
       "      <td>2023 -6.pdf</td>\n",
       "      <td>2023__6.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>29</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>159871</td>\n",
       "      <td>2023 -3.pdf</td>\n",
       "      <td>2023__3.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>29</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>159871</td>\n",
       "      <td>2023 -8.pdf</td>\n",
       "      <td>2023__8.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>30</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>160014</td>\n",
       "      <td>31-07.pdf</td>\n",
       "      <td>31_07.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>30</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>160014</td>\n",
       "      <td>ACFrOgBLgYewSPQAweUd3QJkpDqN5Kp2dFIyNq7d6wJCRY...</td>\n",
       "      <td>acfrogblgyewspqaweud3qjkpdqn5kp2dfiynq7d6wjcry...</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>31</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>126623</td>\n",
       "      <td>41C46D8F-73AB-4906-A4C6-C7DC92C05828.PDF</td>\n",
       "      <td>41c46d8f_73ab_4906_a4c6_c7dc92c05828.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>32</td>\n",
       "      <td>Batch_17</td>\n",
       "      <td>138565</td>\n",
       "      <td>B4066C58-F309-42E4-A992-55EB8961211E.PDF</td>\n",
       "      <td>b4066c58_f309_42e4_a992_55eb8961211e.pdf</td>\n",
       "      <td>01/09/2023 18:44:46</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index     Batch diretorio_origem  \\\n",
       "0       3  Batch_17          1002479   \n",
       "1       3  Batch_17          1002479   \n",
       "2       3  Batch_17          1002479   \n",
       "3       4  Batch_17          1001924   \n",
       "4       5  Batch_17          1004403   \n",
       "5       6  Batch_17          1004463   \n",
       "6       7  Batch_17          1004268   \n",
       "7       7  Batch_17          1004268   \n",
       "8       8  Batch_17          1005516   \n",
       "9       9  Batch_17         11789914   \n",
       "10     10  Batch_17         11779370   \n",
       "11     11  Batch_17             7481   \n",
       "12     12  Batch_17           242207   \n",
       "13     13  Batch_17         11790236   \n",
       "14     14  Batch_17          1005075   \n",
       "15     15  Batch_17          1004847   \n",
       "16     15  Batch_17          1004847   \n",
       "17     16  Batch_17          1004813   \n",
       "18     17  Batch_17           240746   \n",
       "19     17  Batch_17           240746   \n",
       "20     18  Batch_17           240274   \n",
       "21     19  Batch_17         11779446   \n",
       "22     20  Batch_17           241538   \n",
       "23     21  Batch_17         11778530   \n",
       "24     21  Batch_17         11778530   \n",
       "25     22  Batch_17          1001204   \n",
       "26     22  Batch_17          1001204   \n",
       "27     23  Batch_17         11779609   \n",
       "28     24  Batch_17          1001255   \n",
       "29     25  Batch_17            10597   \n",
       "30     25  Batch_17            10597   \n",
       "31     28  Batch_17           115964   \n",
       "32     29  Batch_17           159871   \n",
       "33     29  Batch_17           159871   \n",
       "34     29  Batch_17           159871   \n",
       "35     29  Batch_17           159871   \n",
       "36     29  Batch_17           159871   \n",
       "37     29  Batch_17           159871   \n",
       "38     30  Batch_17           160014   \n",
       "39     30  Batch_17           160014   \n",
       "40     31  Batch_17           126623   \n",
       "41     32  Batch_17           138565   \n",
       "\n",
       "                                  nome_arquivo_origem  \\\n",
       "0                      nota_fiscal_eletronica_190.pdf   \n",
       "1                      nota_fiscal_eletronica_188.pdf   \n",
       "2                      nota_fiscal_eletronica_189.pdf   \n",
       "3        listagem_de_nfs_e___sintetico_gf_pereira.pdf   \n",
       "4     nf_11_igreja_evangelica_batista_eterna_vida.pdf   \n",
       "5               nota_fiscal_coco_legal_julho_2023.pdf   \n",
       "6                    nota_fiscal_eletronica_julho.pdf   \n",
       "7                           relatorio_de_listagem.pdf   \n",
       "8        listagem_de_nfs_e__clinica_anna_stofoles.pdf   \n",
       "9                                   nf_12_renilda.pdf   \n",
       "10                                  emissao_24_07.pdf   \n",
       "11                  listagem_de_nfs_e___sintetico.pdf   \n",
       "12                  listagem_de_nfs_e___sintetico.pdf   \n",
       "13                                              1.pdf   \n",
       "14                                      nf_29_gri.pdf   \n",
       "15        nf_19_condominio_edificio_mendes_campos.pdf   \n",
       "16                          nf_20_edificio_mendes.pdf   \n",
       "17                                      nf_55_jml.pdf   \n",
       "18                     nf_jogos_lot_surui_07_2023.pdf   \n",
       "19                 nf_nao_jogos_lot_surui_07_2023.pdf   \n",
       "20          listagem_de_nfs_e___sintetico_rx_face.pdf   \n",
       "21               digitalizacao_26_de_jul_de_20231.pdf   \n",
       "22                         julh_2023_notafiscal_2.pdf   \n",
       "23                    nota_fiscal_no27_vitale_eco.pdf   \n",
       "24                        nota_fiscal_no26_origem.pdf   \n",
       "25                                 nota_magno_150.pdf   \n",
       "26                                 nota_magno_151.pdf   \n",
       "27  listagem_de_nfs_e___sintetico_clin_daiane_rodr...   \n",
       "28        listagem_de_nfs_e___sintetico_odonto_ja.pdf   \n",
       "29                             nfs299_072023_floc.pdf   \n",
       "30                      nfs300_072023_floc_textil.pdf   \n",
       "31                     Livro de Registro do ISSQN.pdf   \n",
       "32                                        2023 -5.pdf   \n",
       "33                                        2023 -7.pdf   \n",
       "34                                        2023 -4.pdf   \n",
       "35                                        2023 -6.pdf   \n",
       "36                                        2023 -3.pdf   \n",
       "37                                        2023 -8.pdf   \n",
       "38                                          31-07.pdf   \n",
       "39  ACFrOgBLgYewSPQAweUd3QJkpDqN5Kp2dFIyNq7d6wJCRY...   \n",
       "40           41C46D8F-73AB-4906-A4C6-C7DC92C05828.PDF   \n",
       "41           B4066C58-F309-42E4-A992-55EB8961211E.PDF   \n",
       "\n",
       "                                 nome_arquivo_destino   data_processamento  \\\n",
       "0                      nota_fiscal_eletronica_190.pdf  01/09/2023 18:44:46   \n",
       "1                      nota_fiscal_eletronica_188.pdf  01/09/2023 18:44:46   \n",
       "2                      nota_fiscal_eletronica_189.pdf  01/09/2023 18:44:46   \n",
       "3        listagem_de_nfs_e___sintetico_gf_pereira.pdf  01/09/2023 18:44:46   \n",
       "4     nf_11_igreja_evangelica_batista_eterna_vida.pdf  01/09/2023 18:44:46   \n",
       "5               nota_fiscal_coco_legal_julho_2023.pdf  01/09/2023 18:44:46   \n",
       "6                    nota_fiscal_eletronica_julho.pdf  01/09/2023 18:44:46   \n",
       "7                           relatorio_de_listagem.pdf  01/09/2023 18:44:46   \n",
       "8        listagem_de_nfs_e__clinica_anna_stofoles.pdf  01/09/2023 18:44:46   \n",
       "9                                   nf_12_renilda.pdf  01/09/2023 18:44:46   \n",
       "10                                  emissao_24_07.pdf  01/09/2023 18:44:46   \n",
       "11                  listagem_de_nfs_e___sintetico.pdf  01/09/2023 18:44:46   \n",
       "12                  listagem_de_nfs_e___sintetico.pdf  01/09/2023 18:44:46   \n",
       "13                                              1.pdf  01/09/2023 18:44:46   \n",
       "14                                      nf_29_gri.pdf  01/09/2023 18:44:46   \n",
       "15        nf_19_condominio_edificio_mendes_campos.pdf  01/09/2023 18:44:46   \n",
       "16                          nf_20_edificio_mendes.pdf  01/09/2023 18:44:46   \n",
       "17                                      nf_55_jml.pdf  01/09/2023 18:44:46   \n",
       "18                     nf_jogos_lot_surui_07_2023.pdf  01/09/2023 18:44:46   \n",
       "19                 nf_nao_jogos_lot_surui_07_2023.pdf  01/09/2023 18:44:46   \n",
       "20          listagem_de_nfs_e___sintetico_rx_face.pdf  01/09/2023 18:44:46   \n",
       "21               digitalizacao_26_de_jul_de_20231.pdf  01/09/2023 18:44:46   \n",
       "22                         julh_2023_notafiscal_2.pdf  01/09/2023 18:44:46   \n",
       "23                    nota_fiscal_no27_vitale_eco.pdf  01/09/2023 18:44:46   \n",
       "24                        nota_fiscal_no26_origem.pdf  01/09/2023 18:44:46   \n",
       "25                                 nota_magno_150.pdf  01/09/2023 18:44:46   \n",
       "26                                 nota_magno_151.pdf  01/09/2023 18:44:46   \n",
       "27  listagem_de_nfs_e___sintetico_clin_daiane_rodr...  01/09/2023 18:44:46   \n",
       "28        listagem_de_nfs_e___sintetico_odonto_ja.pdf  01/09/2023 18:44:46   \n",
       "29                             nfs299_072023_floc.pdf  01/09/2023 18:44:46   \n",
       "30                      nfs300_072023_floc_textil.pdf  01/09/2023 18:44:46   \n",
       "31                     livro_de_registro_do_issqn.pdf  01/09/2023 18:44:46   \n",
       "32                                        2023__5.pdf  01/09/2023 18:44:46   \n",
       "33                                        2023__7.pdf  01/09/2023 18:44:46   \n",
       "34                                        2023__4.pdf  01/09/2023 18:44:46   \n",
       "35                                        2023__6.pdf  01/09/2023 18:44:46   \n",
       "36                                        2023__3.pdf  01/09/2023 18:44:46   \n",
       "37                                        2023__8.pdf  01/09/2023 18:44:46   \n",
       "38                                          31_07.pdf  01/09/2023 18:44:46   \n",
       "39  acfrogblgyewspqaweud3qjkpdqn5kp2dfiynq7d6wjcry...  01/09/2023 18:44:46   \n",
       "40           41c46d8f_73ab_4906_a4c6_c7dc92c05828.pdf  01/09/2023 18:44:46   \n",
       "41           b4066c58_f309_42e4_a992_55eb8961211e.pdf  01/09/2023 18:44:46   \n",
       "\n",
       "    tipo_pdf  trully_search  qut_paginas  \n",
       "0       True           True            1  \n",
       "1       True           True            1  \n",
       "2       True           True            1  \n",
       "3       True           True            1  \n",
       "4       True           True            1  \n",
       "5       True           True            1  \n",
       "6       True           True            1  \n",
       "7       True           True            1  \n",
       "8       True           True            1  \n",
       "9       True           True            1  \n",
       "10      True           True            1  \n",
       "11      True           True            1  \n",
       "12      True           True            1  \n",
       "13      True          False            1  \n",
       "14      True           True            1  \n",
       "15      True           True            1  \n",
       "16      True           True            1  \n",
       "17      True          False            1  \n",
       "18      True           True            1  \n",
       "19      True           True            1  \n",
       "20      True           True            1  \n",
       "21      True          False            1  \n",
       "22      True          False            1  \n",
       "23      True           True            1  \n",
       "24      True           True            1  \n",
       "25      True           True            1  \n",
       "26      True           True            1  \n",
       "27      True           True            1  \n",
       "28      True           True            1  \n",
       "29      True           True            1  \n",
       "30      True           True            1  \n",
       "31      True           True            1  \n",
       "32      True           True            1  \n",
       "33      True           True            1  \n",
       "34      True           True            1  \n",
       "35      True           True            1  \n",
       "36      True           True            1  \n",
       "37      True           True            1  \n",
       "38      True           True            1  \n",
       "39      True           True            1  \n",
       "40      True           True            1  \n",
       "41      True           True            1  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_files_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9286/2817542823.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_extracao_files = df_extracao_files.append(df_files, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Anexando o novo DataFrame ao original\n",
    "df_extracao_files = df_extracao_files.append(df_files, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Batch</th>\n",
       "      <th>diretorio_origem</th>\n",
       "      <th>nome_arquivo_origem</th>\n",
       "      <th>nome_arquivo_destino</th>\n",
       "      <th>data_processamento</th>\n",
       "      <th>tipo_pdf</th>\n",
       "      <th>qut_paginas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Batch_12</td>\n",
       "      <td>notas_mage_31082023</td>\n",
       "      <td>NF 689- PMMacae- 8ª Med CO-22-22 TB Contrato 0...</td>\n",
       "      <td>nf_689__pmmacae__8a_med_co_22_22_tb_contrato_0...</td>\n",
       "      <td>01/09/2023 14:22:44</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Batch_12</td>\n",
       "      <td>notas_mage_31082023</td>\n",
       "      <td>nfse 4414.pdf</td>\n",
       "      <td>nfse_4414.pdf</td>\n",
       "      <td>01/09/2023 14:22:44</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Batch_12</td>\n",
       "      <td>notas_mage_31082023</td>\n",
       "      <td>nfse 4409.pdf</td>\n",
       "      <td>nfse_4409.pdf</td>\n",
       "      <td>01/09/2023 14:22:44</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Batch_12</td>\n",
       "      <td>notas_mage_31082023</td>\n",
       "      <td>NF 688 - ConsoR Zadar- Engetecnica (ENZA) (1).pdf</td>\n",
       "      <td>nf_688___consor_zadar__engetecnica_enza_1.pdf</td>\n",
       "      <td>01/09/2023 14:22:44</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Batch_12</td>\n",
       "      <td>notas_mage_31082023</td>\n",
       "      <td>12225 COFIM -.pdf</td>\n",
       "      <td>12225_cofim__.pdf</td>\n",
       "      <td>01/09/2023 14:22:44</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>4</td>\n",
       "      <td>Batch_16</td>\n",
       "      <td>159871</td>\n",
       "      <td>2023 -8.pdf</td>\n",
       "      <td>2023__8.pdf</td>\n",
       "      <td>01/09/2023 15:28:23</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>5</td>\n",
       "      <td>Batch_16</td>\n",
       "      <td>160014</td>\n",
       "      <td>31-07.pdf</td>\n",
       "      <td>31_07.pdf</td>\n",
       "      <td>01/09/2023 15:28:23</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>5</td>\n",
       "      <td>Batch_16</td>\n",
       "      <td>160014</td>\n",
       "      <td>ACFrOgBLgYewSPQAweUd3QJkpDqN5Kp2dFIyNq7d6wJCRY...</td>\n",
       "      <td>acfrogblgyewspqaweud3qjkpdqn5kp2dfiynq7d6wjcry...</td>\n",
       "      <td>01/09/2023 15:28:23</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>6</td>\n",
       "      <td>Batch_16</td>\n",
       "      <td>126623</td>\n",
       "      <td>41C46D8F-73AB-4906-A4C6-C7DC92C05828.PDF</td>\n",
       "      <td>41c46d8f_73ab_4906_a4c6_c7dc92c05828.pdf</td>\n",
       "      <td>01/09/2023 15:28:23</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>7</td>\n",
       "      <td>Batch_16</td>\n",
       "      <td>138565</td>\n",
       "      <td>B4066C58-F309-42E4-A992-55EB8961211E.PDF</td>\n",
       "      <td>b4066c58_f309_42e4_a992_55eb8961211e.pdf</td>\n",
       "      <td>01/09/2023 15:28:23</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index     Batch     diretorio_origem  \\\n",
       "0       2  Batch_12  notas_mage_31082023   \n",
       "1       2  Batch_12  notas_mage_31082023   \n",
       "2       2  Batch_12  notas_mage_31082023   \n",
       "3       2  Batch_12  notas_mage_31082023   \n",
       "4       2  Batch_12  notas_mage_31082023   \n",
       "..    ...       ...                  ...   \n",
       "78      4  Batch_16               159871   \n",
       "79      5  Batch_16               160014   \n",
       "80      5  Batch_16               160014   \n",
       "81      6  Batch_16               126623   \n",
       "82      7  Batch_16               138565   \n",
       "\n",
       "                                  nome_arquivo_origem  \\\n",
       "0   NF 689- PMMacae- 8ª Med CO-22-22 TB Contrato 0...   \n",
       "1                                       nfse 4414.pdf   \n",
       "2                                       nfse 4409.pdf   \n",
       "3   NF 688 - ConsoR Zadar- Engetecnica (ENZA) (1).pdf   \n",
       "4                                   12225 COFIM -.pdf   \n",
       "..                                                ...   \n",
       "78                                        2023 -8.pdf   \n",
       "79                                          31-07.pdf   \n",
       "80  ACFrOgBLgYewSPQAweUd3QJkpDqN5Kp2dFIyNq7d6wJCRY...   \n",
       "81           41C46D8F-73AB-4906-A4C6-C7DC92C05828.PDF   \n",
       "82           B4066C58-F309-42E4-A992-55EB8961211E.PDF   \n",
       "\n",
       "                                 nome_arquivo_destino   data_processamento  \\\n",
       "0   nf_689__pmmacae__8a_med_co_22_22_tb_contrato_0...  01/09/2023 14:22:44   \n",
       "1                                       nfse_4414.pdf  01/09/2023 14:22:44   \n",
       "2                                       nfse_4409.pdf  01/09/2023 14:22:44   \n",
       "3       nf_688___consor_zadar__engetecnica_enza_1.pdf  01/09/2023 14:22:44   \n",
       "4                                   12225_cofim__.pdf  01/09/2023 14:22:44   \n",
       "..                                                ...                  ...   \n",
       "78                                        2023__8.pdf  01/09/2023 15:28:23   \n",
       "79                                          31_07.pdf  01/09/2023 15:28:23   \n",
       "80  acfrogblgyewspqaweud3qjkpdqn5kp2dfiynq7d6wjcry...  01/09/2023 15:28:23   \n",
       "81           41c46d8f_73ab_4906_a4c6_c7dc92c05828.pdf  01/09/2023 15:28:23   \n",
       "82           b4066c58_f309_42e4_a992_55eb8961211e.pdf  01/09/2023 15:28:23   \n",
       "\n",
       "    tipo_pdf  qut_paginas  \n",
       "0      False            1  \n",
       "1       True            1  \n",
       "2       True            1  \n",
       "3      False            1  \n",
       "4       True            1  \n",
       "..       ...          ...  \n",
       "78      True            1  \n",
       "79      True            1  \n",
       "80      True            1  \n",
       "81      True            1  \n",
       "82      True            1  \n",
       "\n",
       "[83 rows x 8 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extracao_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'115964': ['livro_de_registro_do_issqn.pdf'],\n",
       " '159871': ['2023__5.pdf',\n",
       "  '2023__7.pdf',\n",
       "  '2023__4.pdf',\n",
       "  '2023__6.pdf',\n",
       "  '2023__3.pdf',\n",
       "  '2023__8.pdf'],\n",
       " '160014': ['31_07.pdf',\n",
       "  'acfrogblgyewspqaweud3qjkpdqn5kp2dfiynq7d6wjcrymgxkby0xaq7m2xyrrh8asjkxsfk1z9f4bsqat1di5gppkc3ahrhnhavaawbjuamkpiluuxpydd2ovrxzk.pdf'],\n",
       " '126623': ['41c46d8f_73ab_4906_a4c6_c7dc92c05828.pdf'],\n",
       " '138565': ['b4066c58_f309_42e4_a992_55eb8961211e.pdf']}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_file_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dicionarios e Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho de folder_names: 5\n",
      "Tamanho de file_counts: 5\n",
      "Tamanho de file_names: 5\n"
     ]
    }
   ],
   "source": [
    "folder_names = []\n",
    "file_counts = []\n",
    "file_names = []\n",
    "# Iterar sobre o dicionário para coletar informações\n",
    "for folder, files in folder_file_dict.items():\n",
    "    folder_names.append(folder)\n",
    "    file_counts.append(len(files))\n",
    "    file_names.append(files)\n",
    "# folder_file_dict é algo como {'pasta1': 'arquivo1.pdf', 'pasta2': 'arquivo2.pdf'}\n",
    "folder_names = list(folder_file_dict.keys())\n",
    "file_names = list(folder_file_dict.values())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 6, 2, 1, 1]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {} \n",
    "# Criar o DataFrame do batch\n",
    "df_batch = pd.DataFrame({\n",
    "    \"Dt_hora\": [date_email],\n",
    "    \"Assunto\": [msg_subject],\n",
    "    \"Arquivos_zip\": [arquivos_zip],\n",
    "    \"Quantidade de Documentos\": [file_counts],\n",
    "    \"De\": [msg_sender],\n",
    "    \"batch\": [batch_name],\n",
    "    \"email\": [msg_email_address],\n",
    "\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9286/1808731608.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_processamento = df_processamento.append(df_batch, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Anexando o novo DataFrame ao original\n",
    "df_processamento = df_processamento.append(df_batch, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dt_hora</th>\n",
       "      <th>Assunto</th>\n",
       "      <th>Quantidade de Documentos</th>\n",
       "      <th>De</th>\n",
       "      <th>batch</th>\n",
       "      <th>email</th>\n",
       "      <th>Arquivos_zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17/08/2023 16:11:50</td>\n",
       "      <td>Notas Magé</td>\n",
       "      <td>[115]</td>\n",
       "      <td>Bruna Maciel Adame</td>\n",
       "      <td>Batch_1</td>\n",
       "      <td>bruna@modernizacaopublica.com.br</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17/08/2023 15:49:21</td>\n",
       "      <td>Notas Sao Pedro da Aldeia</td>\n",
       "      <td>[28]</td>\n",
       "      <td>Bruna Maciel Adame</td>\n",
       "      <td>Batch_2</td>\n",
       "      <td>bruna@modernizacaopublica.com.br</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17/08/2023 11:56:51</td>\n",
       "      <td>Notas São Pedro da Aldeia</td>\n",
       "      <td>[10]</td>\n",
       "      <td>Bruna Maciel Adame</td>\n",
       "      <td>Batch_3</td>\n",
       "      <td>bruna@modernizacaopublica.com.br</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17/08/2023 12:01:16</td>\n",
       "      <td>Notas Mesquita</td>\n",
       "      <td>[2]</td>\n",
       "      <td>Bruna Maciel Adame</td>\n",
       "      <td>na</td>\n",
       "      <td>bruna@modernizacaopublica.com.br</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/08/2023 11:25:42</td>\n",
       "      <td>Fwd: Notas Magé</td>\n",
       "      <td>[9]</td>\n",
       "      <td>Verlânio Gallindo</td>\n",
       "      <td>Batch_5</td>\n",
       "      <td>verlanio@gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-10-08 23:20:56</td>\n",
       "      <td>Fwd: Notas Magé 2</td>\n",
       "      <td>[11]</td>\n",
       "      <td>Verlânio Gallindo</td>\n",
       "      <td>Batch_6</td>\n",
       "      <td>verlanio@gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10/08/2023 23:20:56</td>\n",
       "      <td>Fwd: Notas Magé 2</td>\n",
       "      <td>[75]</td>\n",
       "      <td>Verlânio Gallindo</td>\n",
       "      <td>Batch_6_Atualizado</td>\n",
       "      <td>verlanio@gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10/08/2023 17:54:14</td>\n",
       "      <td>Fwd: Notas Magé 1</td>\n",
       "      <td>[21, 8, 18, 3]</td>\n",
       "      <td>Verlânio Gallindo</td>\n",
       "      <td>Batch_7</td>\n",
       "      <td>verlanio@gmail.com</td>\n",
       "      <td>['07-2023 (1).7z', 'fwdrenotafiscal.zip', 'fwd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12/08/2023 11:25:42</td>\n",
       "      <td>Fwd: Notas Magé</td>\n",
       "      <td>[9]</td>\n",
       "      <td>Verlânio Gallindo</td>\n",
       "      <td>Batch_8</td>\n",
       "      <td>verlanio@gmail.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31/08/2023 22:27:59</td>\n",
       "      <td>Notas Magé</td>\n",
       "      <td>[2, 17]</td>\n",
       "      <td>Bruna Maciel Adame</td>\n",
       "      <td>Batch_12</td>\n",
       "      <td>bruna@modernizacaopublica.com.br</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>31/08/2023 22:27:59</td>\n",
       "      <td>Notas Magé</td>\n",
       "      <td>[17]</td>\n",
       "      <td>Bruna Maciel Adame</td>\n",
       "      <td>Batch_12</td>\n",
       "      <td>bruna@modernizacaopublica.com.br</td>\n",
       "      <td>['notas mage 31082023.zip']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31/08/2023 22:31:10</td>\n",
       "      <td>Nota SPA</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Bruna Maciel Adame</td>\n",
       "      <td>Batch_13</td>\n",
       "      <td>bruna@modernizacaopublica.com.br</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31/08/2023 23:00:27</td>\n",
       "      <td>Notas Magé 2</td>\n",
       "      <td>[3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, ...</td>\n",
       "      <td>Bruna Maciel Adame</td>\n",
       "      <td>Batch_14</td>\n",
       "      <td>bruna@modernizacaopublica.com.br</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>31/08/2023 23:00:54</td>\n",
       "      <td>Notas SPA 2</td>\n",
       "      <td>[1, 1, 1, 3, 2, 1, 8, 2, 1, 1, 1, 1]</td>\n",
       "      <td>Bruna Maciel Adame</td>\n",
       "      <td>Batch_15</td>\n",
       "      <td>bruna@modernizacaopublica.com.br</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>31/08/2023 23:01:14</td>\n",
       "      <td>Nota mesquita</td>\n",
       "      <td>[1, 6, 2, 1, 1]</td>\n",
       "      <td>Bruna Maciel Adame</td>\n",
       "      <td>Batch_16</td>\n",
       "      <td>bruna@modernizacaopublica.com.br</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dt_hora                    Assunto  \\\n",
       "0   17/08/2023 16:11:50                 Notas Magé   \n",
       "1   17/08/2023 15:49:21  Notas Sao Pedro da Aldeia   \n",
       "2   17/08/2023 11:56:51  Notas São Pedro da Aldeia   \n",
       "3   17/08/2023 12:01:16             Notas Mesquita   \n",
       "4   12/08/2023 11:25:42            Fwd: Notas Magé   \n",
       "5   2023-10-08 23:20:56          Fwd: Notas Magé 2   \n",
       "6   10/08/2023 23:20:56          Fwd: Notas Magé 2   \n",
       "7   10/08/2023 17:54:14          Fwd: Notas Magé 1   \n",
       "8   12/08/2023 11:25:42            Fwd: Notas Magé   \n",
       "9   31/08/2023 22:27:59                 Notas Magé   \n",
       "10  31/08/2023 22:27:59                 Notas Magé   \n",
       "11  31/08/2023 22:31:10                   Nota SPA   \n",
       "12  31/08/2023 23:00:27               Notas Magé 2   \n",
       "13  31/08/2023 23:00:54                Notas SPA 2   \n",
       "14  31/08/2023 23:01:14              Nota mesquita   \n",
       "\n",
       "                             Quantidade de Documentos                  De  \\\n",
       "0                                               [115]  Bruna Maciel Adame   \n",
       "1                                                [28]  Bruna Maciel Adame   \n",
       "2                                                [10]  Bruna Maciel Adame   \n",
       "3                                                 [2]  Bruna Maciel Adame   \n",
       "4                                                 [9]   Verlânio Gallindo   \n",
       "5                                                [11]   Verlânio Gallindo   \n",
       "6                                                [75]   Verlânio Gallindo   \n",
       "7                                      [21, 8, 18, 3]   Verlânio Gallindo   \n",
       "8                                                 [9]   Verlânio Gallindo   \n",
       "9                                             [2, 17]  Bruna Maciel Adame   \n",
       "10                                               [17]  Bruna Maciel Adame   \n",
       "11                                                [1]  Bruna Maciel Adame   \n",
       "12  [3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, ...  Bruna Maciel Adame   \n",
       "13               [1, 1, 1, 3, 2, 1, 8, 2, 1, 1, 1, 1]  Bruna Maciel Adame   \n",
       "14                                    [1, 6, 2, 1, 1]  Bruna Maciel Adame   \n",
       "\n",
       "                 batch                             email  \\\n",
       "0              Batch_1  bruna@modernizacaopublica.com.br   \n",
       "1              Batch_2  bruna@modernizacaopublica.com.br   \n",
       "2              Batch_3  bruna@modernizacaopublica.com.br   \n",
       "3                   na  bruna@modernizacaopublica.com.br   \n",
       "4              Batch_5                verlanio@gmail.com   \n",
       "5              Batch_6                verlanio@gmail.com   \n",
       "6   Batch_6_Atualizado                verlanio@gmail.com   \n",
       "7              Batch_7                verlanio@gmail.com   \n",
       "8              Batch_8                verlanio@gmail.com   \n",
       "9             Batch_12  bruna@modernizacaopublica.com.br   \n",
       "10            Batch_12  bruna@modernizacaopublica.com.br   \n",
       "11            Batch_13  bruna@modernizacaopublica.com.br   \n",
       "12            Batch_14  bruna@modernizacaopublica.com.br   \n",
       "13            Batch_15  bruna@modernizacaopublica.com.br   \n",
       "14            Batch_16  bruna@modernizacaopublica.com.br   \n",
       "\n",
       "                                         Arquivos_zip  \n",
       "0                                                 NaN  \n",
       "1                                                 NaN  \n",
       "2                                                 NaN  \n",
       "3                                                 NaN  \n",
       "4                                                 NaN  \n",
       "5                                                 NaN  \n",
       "6                                                 NaN  \n",
       "7   ['07-2023 (1).7z', 'fwdrenotafiscal.zip', 'fwd...  \n",
       "8                                                  []  \n",
       "9                                                  []  \n",
       "10                        ['notas mage 31082023.zip']  \n",
       "11                                                 []  \n",
       "12                                                 []  \n",
       "13                                                 []  \n",
       "14                                                 []  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tratando nome de carga do df_processamento\n",
    "dataset_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets\"\n",
    "\n",
    "df_processamento_file = \"df_processamento_\"\n",
    "\n",
    "df_processamento_file_write = df_processamento_file + str(i_test) + \".xlsx\"\n",
    "\n",
    "# 2. Tratando nome de carga do df_extracao_files_Batch\n",
    "df_extracao_files_Batch_file = \"df_extracao_files_Batch_\"\n",
    "\n",
    "df_extracao_files_Batch_file_write = df_extracao_files_Batch_file + str(i_test) + \".xlsx\"\n",
    "\n",
    "\n",
    "df_processamento_file_write_path = os.path.join(dataset_path, df_processamento_file_write)\n",
    "\n",
    "df_processamento_file_write_path\n",
    "\n",
    "df_extracao_files_Batch_file_write_path = os.path.join(dataset_path, df_extracao_files_Batch_file_write)\n",
    "\n",
    "df_extracao_files_Batch_file_write_path\n",
    "\n",
    "# Salvando o DF para excel\n",
    "df_extracao_files.to_excel(df_extracao_files_Batch_file_write_path, index=False)\n",
    "\n",
    "# Salvando o DF para excel\n",
    "df_processamento.to_excel(df_processamento_file_write_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funcoes modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Funcao de conversao e resize do documento\n",
    "def convertResize(doc2convert, document_path, image_resized_path):\n",
    "    \n",
    "    \"\"\"# 1. remocao do sufixo .pdf\n",
    "    if doc2convert.split(\".\")[1].islower():\n",
    "        nameImage= doc2convert.removesuffix(\".pdf\")\n",
    "    else:\n",
    "        nameImage= doc2convert.removesuffix(\".PDF\")\"\"\"\n",
    "    \n",
    "    # 2. construo um novo nome para o documento imagem\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(doc2convert)}.jpg')\n",
    "    \n",
    "    # 3. Conversao para imagem\n",
    "    pages = convert_from_path(document_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((2067, 2923))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "    return resized_pages[0], image_resized_name\n",
    "\n",
    "\n",
    "def convertResizeAnalise_1page(doc2convert, document_path, image_resized_path):\n",
    "    \n",
    "    \"\"\"# 1. remocao do sufixo .pdf\n",
    "    if doc2convert.split(\".\")[1].islower():\n",
    "        nameImage= doc2convert.removesuffix(\".pdf\")\n",
    "    else:\n",
    "        nameImage= doc2convert.removesuffix(\".PDF\")\"\"\"\n",
    "    \n",
    "    # 2. construo um novo nome para o documento imagem\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(doc2convert)}.jpg')\n",
    "    \n",
    "    # 3. Conversao para imagem\n",
    "    pages = convert_from_path(document_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    resized_pages = []\n",
    "    for page in pages:\n",
    "        resized_page = page.resize((2067, 2923))\n",
    "        resized_pages.append(resized_page)\n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "    return resized_pages[0], image_resized_name\n",
    "\n",
    "\n",
    "# Funcao de conversao e resize do documento\n",
    "def convertResize_analise(nome_documento, document_path, image_resized_path):\n",
    "    \n",
    "    \"\"\"# 1. remocao do sufixo .pdf\n",
    "    if doc2convert.split(\".\")[1].islower():\n",
    "        nameImage= doc2convert.removesuffix(\".pdf\")\n",
    "    else:\n",
    "        nameImage= doc2convert.removesuffix(\".PDF\")\"\"\"\n",
    "    \n",
    "    # 2. construo um novo nome para o documento imagem\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(nome_documento)}.jpg')\n",
    "    \n",
    "    # 3. Conversao para imagem\n",
    "    pages = convert_from_path(document_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((2067, 2923))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "    return resized_pages[0], image_resized_name\n",
    "\n",
    "\n",
    "\n",
    "# Trata Ocr\n",
    "def extract_fields_box(modelo, father_value, section):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    filtered_boxes_info = field_boxes_info[(field_boxes_info['father'] == father_value) & (field_boxes_info['model'] == modelo)]\n",
    "    # Iterate nas informações dos boxes de fields e extraia o texto de cada field\n",
    "    for index_field, row_field in filtered_boxes_info.iterrows():\n",
    "        x0, y0, x1, y1 = row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        #print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference'], row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1'] ))\n",
    "        # Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "        value = extracted_text_box.split('\\n')[-1]\n",
    "        # Remova qualquer espaço em branco à esquerda ou à direita\n",
    "        value = value.strip()\n",
    "        if row_field['t_value'] == 'number':\n",
    "            # Formate o valor usando a função format_number\n",
    "            #print(\"vou verificar valor\")\n",
    "            value = format_number2(value)\n",
    "            #print(value)\n",
    "        # Armazene o texto extraído com o rótulo correspondente\n",
    "        label = row_field['label']\n",
    "        data_box_valores[label] = value\n",
    "        \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "# 3. Efetua OCR no documento (area parao do texto da NF)\n",
    "def ocr_RasterPDF(image_name):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = 406\n",
    "    y0 = 0\n",
    "    x1= 1540\n",
    "    y1 = 380\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "    return values, extracted_text_frame \n",
    "\n",
    "\n",
    "\n",
    "# 2. Efetua OCR no documento (area parao do texto da NF)\n",
    "def ocr_RasterPDF_free(image_name, vx0, vy0, vx1, vy1):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = vx0\n",
    "    y0 = vy0\n",
    "    x1= vx1\n",
    "    y1 = vy1\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "    return values, extracted_text_frame \n",
    "\n",
    "\n",
    "# 1. Interacao para pesquisar prefeitura\n",
    "def pesquisa_texto(texto):\n",
    "    nome_prefeitura_match = re.search(r'PREFEITURA (.+)', texto)\n",
    "    if nome_prefeitura_match:\n",
    "        is_prefeitura = \"PREFEITURA \" + nome_prefeitura_match.group(1)\n",
    "        \n",
    "        return  is_prefeitura\n",
    "    else:\n",
    "        raise ValueError(\"Nao consegui pesquisar\")\n",
    "\n",
    "    \n",
    "    \n",
    "# 2. Extracao OCR\n",
    "def extract_text_from_coordinates(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=config).strip()\n",
    "    return extracted_text\n",
    "\n",
    "# 1. funçao basica de modelo \n",
    "def executa_model_frame(model, secao, father_name):\n",
    "\n",
    "    data_dados_frame = {}\n",
    "    \n",
    "    tipo = \"frame\"\n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model) & (frames_nf_v4_df['label'] == f_frame_name) & (frames_nf_v4_df['type'] == tipo)]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        extracted_text_frame = extract_text_from_coordinates(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        \n",
    "        frame_seq = row_frame['seq']\n",
    "        frame_model = row_frame['model']\n",
    "        frame_label = row_frame['label']\n",
    "        frame_type = row_frame['type']\n",
    "        frame_section = row_frame['section_json']\n",
    "        frame_reference = row_frame['reference']\n",
    "        frame_father = row_frame['father']\n",
    "        frame_id = row_frame['id']\n",
    "        #print(f'\\fid: {frame_id:>3} | seq: {frame_seq:>3} | model: {frame_model:>8} | type: {frame_type:>15} | Father: {frame_father} label: {frame_label:>30} | section: {frame_section:>20} {frame_reference:>30}')\n",
    "        \n",
    "    return extracted_text_frame\n",
    "\n",
    "\n",
    "\n",
    "# Sao iguais \n",
    "def extract_text_from_frame(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=config).strip()\n",
    "    return extracted_text \n",
    "\n",
    "\n",
    "\n",
    "# Trata texto extraido\n",
    "# Função para extrair número da string\n",
    "def extract_number(text):\n",
    "    match = re.search(r'\\b\\d+(\\.\\d+)?\\b', text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "# Funcao importante - process_line\n",
    "def process_line(value, reference, label):\n",
    "    name_match = re.search(fr'{reference} (.+)', value)\n",
    "    if name_match:\n",
    "        extracted_value = reference + \" \" + name_match.group(1)\n",
    "        return {label: extracted_value}\n",
    "    return None\n",
    "\n",
    "\n",
    "def format_number(number_str):\n",
    "    # Check for percentage and handle it\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # You can multiply by 100 here if needed\n",
    "\n",
    "    # Check if the string contains \"R$\" or a comma, indicating the original format\n",
    "    if 'R$' in number_str or ',' in number_str:\n",
    "        # Original format: Remove 'R$', replace dots with nothing, and replace commas with dots\n",
    "        number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    else:\n",
    "        # New format: Extract only the numeric part using regex\n",
    "        number_str = re.findall(r'[\\d\\.]+', number_str)[-1]\n",
    "\n",
    "    return float(number_str)\n",
    "\n",
    "# Funçao de formatacao de numeros\n",
    "def format_number2(number_str):\n",
    "    number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # multiplica por 100 para fields %\n",
    "    return float(number_str)\n",
    "\n",
    "\n",
    "\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1. funcao: find_value_after_keyword_out_frame_up\n",
    "def find_value_after_keyword_out_frame_up(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "        if text_list[index + 1] not in default_keyword_list:\n",
    "            return text_list[index + 1]\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            for default_keyword in default_keyword_list:\n",
    "                if default_keyword in text_list:\n",
    "                    # Caso especial para 'Nome/Razão Social:'\n",
    "                    if keyword == 'Nome/Razão Social:':\n",
    "                        return text_list[0]\n",
    "        return None\n",
    "    \n",
    "#2. find_value_after_keyword_out_frame_down  \n",
    "def find_value_after_keyword_out_frame_down(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o índice seguinte está dentro da lista\n",
    "        if index + 1 < len(text_list):\n",
    "            # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            try:\n",
    "                index = text_list.index(default_keyword_list[-1])\n",
    "                return text_list[index - 1]\n",
    "            except ValueError:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "#3. find_value_after_keyword_fuzz\n",
    "def find_value_after_keyword_fuzz(keyword, text_list, default_keyword_list=None, fuzziness_threshold=80):\n",
    "    closest_match = None\n",
    "    closest_match_score = 0\n",
    "    \n",
    "    for i, text in enumerate(text_list):\n",
    "        score = fuzz.ratio(keyword, text)\n",
    "        \n",
    "        if score > closest_match_score:\n",
    "            closest_match_score = score\n",
    "            closest_match = text\n",
    "        \n",
    "        if closest_match_score > fuzziness_threshold:\n",
    "            break\n",
    "\n",
    "    if closest_match_score > fuzziness_threshold:\n",
    "        index = text_list.index(closest_match)\n",
    "        if index + 1 < len(text_list):\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "\n",
    "    \n",
    "def pesquisa_keyword(string_pesquisa, text_splited, keyword_list):\n",
    "\n",
    "    resultado_extraido_fuzz = find_value_after_keyword_fuzz(string_pesquisa, text_splited, keyword_list)\n",
    "\n",
    "    if resultado_extraido_fuzz == None:\n",
    "        resultado_extraido_frame_up = find_value_after_keyword_out_frame_up(string_pesquisa, text_splited, keyword_list)\n",
    "        if resultado_extraido_frame_up == None:\n",
    "            resultado_extraido_frame_down = find_value_after_keyword_out_frame_down(string_pesquisa, text_splited, keyword_list)\n",
    "            resultado_extraido = resultado_extraido_frame_down\n",
    "        else:\n",
    "            resultado_extraido = resultado_extraido_frame_up\n",
    "    else:\n",
    "        resultado_extraido = resultado_extraido_fuzz           \n",
    "    #print(resultado_extraido)\n",
    "    return resultado_extraido\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cabecalho_prefeitura():\n",
    "    valor_dict = {}\n",
    "    dados_prefeitura = {}\n",
    "    f_frame_name = \"1_frame_prefeitura_nf\"\n",
    "    # 1. funçao basica de modelo \n",
    "    texto = executa_model_frame(model, secao, f_frame_name)\n",
    "    text_splited = texto.split('\\n')\n",
    "    \n",
    "    valor_dict = extract_prefeitura(model, f_frame_name, text_splited)\n",
    "    if valor_dict:\n",
    "        dados_prefeitura.update(valor_dict)\n",
    "        \n",
    "        \n",
    "    return dados_prefeitura \n",
    "                \n",
    "def cabecalho_dados():\n",
    "\n",
    "    valor = {}   \n",
    "    f_frame_name = \"1_frame_dados_nf\"\n",
    "    \n",
    "    dadinho_dados_nf = {}\n",
    "    \n",
    "    # 1. funçao basica de modelo \n",
    "    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "    text_splited = texto_extraido(texto)\n",
    "    keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "    string_pesquisa = \"Número da Nota:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "    dadinho_dados_nf['numero_nota_fiscal'] = texto\n",
    "\n",
    "\n",
    "    string_pesquisa = \"Competência:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['competencia'] = texto\n",
    "    \n",
    "    \n",
    "    string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['dt_hr_emissao'] = texto\n",
    "    \n",
    "    \n",
    "    string_pesquisa = \"Código Verificação:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['codigo_verificacao'] = texto\n",
    "    \n",
    "    return dadinho_dados_nf   \n",
    "\n",
    "\n",
    "def extract_fields_prestador_cnpj(text): # Função para extrair campos e valores dentro de um retângulo\n",
    "    \n",
    "    \n",
    "    nf_data_prestador_cnpj = {}\n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in text:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "                        nf_data_prestador_cnpj['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                        nf_data_prestador_cnpj['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "\n",
    "\n",
    "    # Extrair Telefone\n",
    "    telefone_str = None\n",
    "    \n",
    "    #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "    telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', text)\n",
    "    if telefone_match: \n",
    "        telefone_str = telefone_match.group(1)\n",
    "        # Remover quebras de linha\n",
    "        telefone_str = telefone_str.replace('.', '')\n",
    "        telefone_str = telefone_str.replace('\\n', '')\n",
    "                \n",
    "        nf_data_prestador_cnpj['telefone'] = telefone_str\n",
    "    else:\n",
    "        nf_data_prestador_cnpj['telefone'] = None   \n",
    "    \n",
    "    \n",
    "    return nf_data_prestador_cnpj \n",
    "\n",
    "\n",
    "def extract_fields_tomador_cnpj(text):\n",
    "    nf_data_tomador_cnpj = {}\n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in text:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "                        nf_data_tomador_cnpj['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                        nf_data_tomador_cnpj['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "\n",
    "    \n",
    "    # Extrair Telefone\n",
    "    telefone_match = re.search(r'Telefone:\\s+(.+)', text)\n",
    "    if telefone_match:\n",
    "        telefone_str = telefone_match.group(1)\n",
    "        if telefone_str == 'Inscrição Estadual:':\n",
    "            nf_data_tomador_cnpj['telefone'] = None  # Valor padrão quando não há correspondência\n",
    "        elif telefone_str == '':\n",
    "            nf_data_tomador_cnpj['telefone'] = None  # Valor padrão quando não há correspondência\n",
    "                    \n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['telefone'] = telefone_match.group(1)\n",
    "            \n",
    "    \n",
    "    # Extrair Inscrição Municipal\n",
    "    inscricao_municipal_match = re.search(r'Inscrição Municipal:\\s+(.+)', text)\n",
    "    if inscricao_municipal_match:\n",
    "        inscricao_municipal_str = inscricao_municipal_match.group(1)\n",
    "        if inscricao_municipal_str == \"Telefone:\": \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = inscricao_municipal_str\n",
    "    \n",
    "    insc_municipal_match = re.search(r'INSC:MUNICIPAL:\\s+(.+)', text)\n",
    "    if insc_municipal_match:\n",
    "        insc_municipal_str = insc_municipal_match.group(1)\n",
    "        if insc_municipal_str == \"Telefone:\":\n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = insc_municipal_str\n",
    "    else:\n",
    "        nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "            \n",
    "    \n",
    "    return nf_data_tomador_cnpj \n",
    "\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "def extract_dados_comple_obs(modelo, frame_father, section):\n",
    "    \n",
    "    data_dados_complementares = {}\n",
    "    #frame_label = frame_father\n",
    "    \n",
    "    # 1. Filtrando o frames_info para buscar os dados de corte\n",
    "    filtered_frames_info = frames_info[(frames_info['label'] == frame_father) & (frames_info['model'] == modelo)]\n",
    "\n",
    "    # 2. Filtrando o sframe_fields_info para buscar os dados dos campos que estao nos frames\n",
    "    filtered_sframe_fields_info = sframe_fields_info[(sframe_fields_info['father'] == frame_father) & (sframe_fields_info['model'] == modelo)]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "        \n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        \n",
    "        print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_frame['seq'], row_frame['model'], row_frame['father'], row_frame['label'], row_frame['reference'], row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'] ))\n",
    "        for index_field, row_field in filtered_sframe_fields_info.iterrows():\n",
    "            #print(\"{:<5} {:<10} {:<30} {:<20} {:<20}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference']))\n",
    "            \n",
    "            if frame_father == \"5_frame_dados_complementares\":\n",
    "                nf_data_dados_complementares = {}\n",
    "                nf_data_dados_complementares['section'] = section\n",
    "                \n",
    "                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^DADOS COMPLEMENTARES', '', extracted_text_box, count=1)\n",
    "                if text == '':\n",
    "                    text = None\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text\n",
    "                else:    \n",
    "                    # Extrair texto dentro do retângulo\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "                    \n",
    "                return nf_data_dados_complementares                \n",
    "                \n",
    "            elif frame_father == \"5_frame_observacao\":\n",
    "                nf_data_observacao = {}\n",
    "                nf_data_observacao['section'] = section \n",
    "                                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^Observação:', '', extracted_text_box, count=1)\n",
    "\n",
    "                # Remover quebras de linha\n",
    "                text = text.replace('\\n', ' ')\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                nf_data_observacao['observacao'] = text.strip()\n",
    "                \n",
    "                return nf_data_observacao \n",
    "            \n",
    "            \n",
    "secao = \"1 - CABECALHO\"\n",
    "f_frame_name = \"1_frame_prefeitura_nf\"\n",
    "\n",
    "\n",
    "#4. Extrai prefeitura\n",
    "def extract_prefeitura(model, father, values):\n",
    "    \n",
    "    tipo = \"sframe_field\"\n",
    "    data_extrated_prefeitura = {}\n",
    "    #print(tipo)\n",
    "\n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model) & (frames_nf_v4_df['father'] == father) & (frames_nf_v4_df['type'] == tipo)]\n",
    "\n",
    "    for index_sframe, row_sframe in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        label_value = row_sframe['label']\n",
    "        \n",
    "        #print(\"label_value\", label_value)\n",
    "        \n",
    "        if label_value == \"nome_prefeitura\":\n",
    "            reference_value = row_sframe['reference']\n",
    "            for value in values:\n",
    "                result = process_line(value, reference_value, label_value)\n",
    "                if result:\n",
    "                    data_extrated_prefeitura.update(result)\n",
    "        elif label_value == \"secretaria\":\n",
    "            reference_value = row_sframe['reference']\n",
    "            for value in values:\n",
    "                result = process_line(value, reference_value, label_value)\n",
    "                if result:\n",
    "                    data_extrated_prefeitura.update(result) \n",
    "        elif label_value == \"tipo_nota_fiscal\":\n",
    "            reference_value = row_sframe['reference']  \n",
    "            for value in values:\n",
    "                result = process_line(value, reference_value, label_value)\n",
    "                if result:\n",
    "                    data_extrated_prefeitura.update(result)\n",
    "                    \n",
    "    return data_extrated_prefeitura\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def processa_cnae_outros(text):\n",
    "    nf_data_CNAE_match = re.search(r'CNAE\\s+(.+)', text)\n",
    "    if nf_data_CNAE_match:\n",
    "        try:\n",
    "            # Remove a primeira ocorrência de \"CNAE:\"\n",
    "            nf_data_CNAE_str = re.sub(r'^CNAE - ', '', text, count=1)\n",
    "            # Remover quebras de linha\n",
    "            nf_data_CNAE_str = nf_data_CNAE_str.replace('\\n', ' ')\n",
    "            return nf_data_CNAE_str \n",
    "        except Exception as e:\n",
    "            print(f\"Erro busca cnae: {e}\") \n",
    "        \n",
    "    return None   \n",
    "\n",
    "\n",
    "\n",
    "def extract_fb_outras_inf(modelo, father_value, section):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    filtered_boxes_info = field_boxes_info[(field_boxes_info['father'] == father_value) & (field_boxes_info['model'] == model)]\n",
    "    # Iterate nas informações dos boxes de fields e extraia o texto de cada field\n",
    "    for index_field, row_field in filtered_boxes_info.iterrows():\n",
    "        \n",
    "        string_pesquisa = row_field['reference']\n",
    "        x0, y0, x1, y1 = row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        label = row_field['label']\n",
    "        #print(f'extracted_text_box {extracted_text_box}, label {label}')\n",
    "        text = extracted_text_box.replace('\\n', '')\n",
    "        if text.startswith(string_pesquisa):\n",
    "            #print(\"aqui:\", text)\n",
    "            text = text[len(label):].strip()\n",
    "            data_box_valores[label] = text\n",
    "    \n",
    "    return   data_box_valores      \n",
    "                 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## | REALMENTE O PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processa_raster_pdf(image_2work):\n",
    "    \n",
    "    secao = \"1 - CABECALHO\"\n",
    "    try:\n",
    "        nro_nota = 0\n",
    "        nd_data_cabecalho = {}\n",
    "        nd_data_cabecalho['secao'] = secao\n",
    "        valor_dict = {}\n",
    "        dados_prefeitura = {}\n",
    "        f_frame_name = \"1_frame_prefeitura_nf\"\n",
    "        # 1. funçao basica de modelo \n",
    "        texto = executa_model_frame(model, secao, f_frame_name)\n",
    "        text_splited = texto.split('\\n')\n",
    "        \n",
    "        valor_dict = extract_prefeitura(model, f_frame_name, text_splited)\n",
    "        if valor_dict:\n",
    "            dados_prefeitura.update(valor_dict)\n",
    "        valor = {}   \n",
    "        f_frame_name = \"1_frame_dados_nf\"\n",
    "        dadinho_dados_nf = {}\n",
    "        # 1. funçao basica de modelo \n",
    "        texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        text_splited = texto_extraido(texto)\n",
    "        keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "        string_pesquisa = \"Número da Nota:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "        dadinho_dados_nf['numero_nota_fiscal'] = texto\n",
    "        nro_nota = texto\n",
    "        \n",
    "        string_pesquisa = \"Competência:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        dadinho_dados_nf['competencia'] = texto\n",
    "        \n",
    "        string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        dadinho_dados_nf['dt_hr_emissao'] = texto\n",
    "        \n",
    "        string_pesquisa = \"Código Verificação:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        dadinho_dados_nf['codigo_verificacao'] = texto\n",
    "        \n",
    "        nd_data_cabecalho.update(dados_prefeitura)\n",
    "        nd_data_cabecalho.update(dadinho_dados_nf)\n",
    "    except Exception as e:\n",
    "        # erros_cabecalho = {}\n",
    "        err_msg = f\"Erro de processo cabecalho: {e}\"\n",
    "        print(err_msg)\n",
    "        # erros['documento'] = file\n",
    "        # erros_cabecalho['secao'] = secao\n",
    "        # erros_cabecalho['erro'] = err_msg\n",
    "        # erros.update(erros_cabecalho)                \n",
    "    \n",
    "    secao = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    try:\n",
    "        f_frame_name = \"2_frame_cnpj_prestador\"\n",
    "        nd_data_prestador = {}\n",
    "        prestador_inscricao = {}\n",
    "        nd_data_prestador['secao'] = secao\n",
    "        prestador_cnpj_value = {}\n",
    "        texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        #text_splited = texto_extraido(texto)\n",
    "        prestador_cnpj_value = extract_fields_prestador_cnpj(texto)\n",
    "        if prestador_cnpj_value:\n",
    "            nd_data_prestador.update(prestador_cnpj_value)\n",
    "    except Exception as e:\n",
    "        # erros_cnpj_prestador = {}\n",
    "        err_msg = (f\"Erro prestador cnpj: {e}\")\n",
    "        print(err_msg)\n",
    "        \n",
    "        # erros_cnpj_prestador['secao'] = secao\n",
    "        # erros_cnpj_prestador['erro'] = err_msg\n",
    "        # erros.update(erros_cnpj_prestador)       \n",
    "        \n",
    "    try:\n",
    "        f_frame_name = \"2_frame_inscricao_prestador\" \n",
    "        texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        text_splited = texto_extraido(texto)\n",
    "        keyword_list = ['Inscrição Municipal:', 'Inscrição Estadual:']\n",
    "        string_pesquisa = \"Inscrição Municipal:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        prestador_inscricao['prestador_inscricao'] = texto\n",
    "        \n",
    "        string_pesquisa = \"Inscrição Estadual:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        prestador_inscricao['inscricao_estadual'] = texto\n",
    "        nd_data_prestador.update(prestador_inscricao)\n",
    "    except Exception as e:\n",
    "        # erros_inscricao_prestador = {}\n",
    "        err_msg = (f\"Erro de processo inscricao prestador: {e}\")\n",
    "        print(err_msg)\n",
    "        # erros_inscricao_prestador['secao'] = secao\n",
    "        # erros_inscricao_prestador['erro'] = err_msg\n",
    "        # erros.update(erros_inscricao_prestador)\n",
    "\n",
    "    try:\n",
    "        f_frame_name = \"2_frame_dados_prestador\"\n",
    "        prestador_dados_value = {}\n",
    "        \n",
    "        keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "        string_pesquisa = \"Nome/Razão Social:\"\n",
    "        texto = executa_model_frame(model, secao, f_frame_name)\n",
    "        text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        prestador_dados_value['razao_social'] = texto\n",
    "\n",
    "        string_pesquisa = \"Nome de Fantasia:\"\n",
    "        #texto = executa_model_frame(model, secao, f_frame_name)\n",
    "        #text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        prestador_dados_value['nome_fantasia'] = texto\n",
    "        \n",
    "        string_pesquisa = \"Endereço:\"\n",
    "        #texto = executa_model_frame(model, secao, f_frame_name)\n",
    "        #text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        prestador_dados_value['endereco'] = texto\n",
    "        \n",
    "        string_pesquisa = \"E-mail:\"\n",
    "        #texto = executa_model_frame(model, secao, f_frame_name)\n",
    "        #text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        prestador_dados_value['email'] = texto\n",
    "        nd_data_prestador.update(prestador_dados_value)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro prestador dados: {e}\") \n",
    "        \n",
    "    \n",
    "    secao = \"3. TOMADOR DE SERVIÇO\"\n",
    "    try:\n",
    "        nd_data_tomador = {}\n",
    "        tomador_cnpj_value = {}\n",
    "        nd_data_tomador['secao'] = secao\n",
    "        f_frame_name = \"3_frame_cnpj_tomador\"\n",
    "        texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        text_splited = texto_extraido(texto) \n",
    "        tomador_cnpj_value = extract_fields_tomador_cnpj(texto)\n",
    "        if tomador_cnpj_value:\n",
    "            nd_data_tomador.update(tomador_cnpj_value)                  \n",
    "    except Exception as e:\n",
    "        print(f\"Erro tomador cnpj: {e}\")\n",
    "        \n",
    "    f_frame_name = \"3_frame_inscricao_tomador\"    \n",
    "    try:\n",
    "        data_tomador_inscricao = {}\n",
    "        keyword_list = ['RG:', 'Inscrição Estadual:']\n",
    "        string_pesquisa = \"RG:\"\n",
    "        texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        data_tomador_inscricao['rg'] = texto\n",
    "        \n",
    "        string_pesquisa = \"Inscrição Estadual:\"\n",
    "        #texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        #text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        data_tomador_inscricao['inscricao_estadual'] = texto\n",
    "        nd_data_tomador.update(data_tomador_inscricao)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro tomador inscricao: {e}\")\n",
    "        \n",
    "    f_frame_name = \"3_frame_dados_tomador\"\n",
    "    try: \n",
    "        data_tomador_dados = {}   \n",
    "        keyword_list = ['Nome/Razão Social:', 'Endereço:', 'E-mail']\n",
    "        string_pesquisa = \"Nome/Razão Social:\"\n",
    "        texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        data_tomador_dados['razao_social'] = texto\n",
    "        \n",
    "        string_pesquisa = \"Endereço:\"\n",
    "        #texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        #text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        data_tomador_dados['endereco'] = texto\n",
    "        \n",
    "        string_pesquisa = \"E-mail\"\n",
    "        #texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "        #text_splited = texto_extraido(texto)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        data_tomador_dados['email'] = texto\n",
    "        \n",
    "        nd_data_tomador.update(data_tomador_dados)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro tomador dados: {e}\") \n",
    "        \n",
    "    secao = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "    try:\n",
    "        nd_data_servico = {}\n",
    "        nd_data_servico['secao'] = secao\n",
    "        f_frame_name = \"4_frame_descricao_totais\"\n",
    "        texto = executa_model_frame(model, secao, f_frame_name)\n",
    "        text = texto.replace('\\n', ' ')\n",
    "        label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "        if text.startswith(label):\n",
    "            text = text[len(label):].strip()\n",
    "        nd_data_servico['discriminacao_servicos'] = text \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro descricao servico: {e}\")\n",
    "            \n",
    "    secao = \"5. VALOR TOTAL\"\n",
    "    try:\n",
    "        nd_data_valor_total = {}\n",
    "        data_valor_total['secao'] = secao\n",
    "        f_frame_name = \"4_frame_valor_total\"   \n",
    "        text = executa_model_frame(model, secao, f_frame_name)  \n",
    "        valor_total_match = re.search(r'R\\$ ([\\d,.]+)', text)\n",
    "        if valor_total_match:\n",
    "            valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "            nd_data_valor_total['valor_total_nota'] = float(valor_total_sem_formatacao)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro valor total: {e}\")\n",
    "\n",
    "    secao = \"6. CNAE e Item da Lista de Serviços\"\n",
    "    try:\n",
    "        nd_data_CNAE = {}\n",
    "        nd_data_CNAE['secao'] = secao\n",
    "        f_frame_name = \"4_frame_cnae_itens_servico\"   \n",
    "        Texto_extraido = executa_model_frame(model, secao, f_frame_name)\n",
    "        text_splited = Texto_extraido.split('\\n')\n",
    "        # Processando CNAE\n",
    "        cnae_line = [line for line in text_splited if 'CNAE' in line][0]\n",
    "        cnae_number = int(extract_number(cnae_line))\n",
    "        cnae_value = cnae_dict.get(cnae_number, \"Valor não encontrado\")\n",
    "        if cnae_value == 'Valor não encontrado':\n",
    "            cnae_value = processa_cnae_outros(cnae_line)\n",
    "            cnae_value = cnae_value.upper()\n",
    "            nd_data_CNAE['cnae'] = cnae_value\n",
    "        else:\n",
    "            cnae_value = cnae_value.upper()\n",
    "            cnae_value = str(cnae_number) + \" - \" + cnae_value\n",
    "            nd_data_CNAE['cnae'] = cnae_value\n",
    "            nd_data_CNAE['item_lista_servicos'] = item_servico_value\n",
    "    except Exception as e:\n",
    "        print(f\"Erro busca cnae: {e}\")    \n",
    "\n",
    "    try:\n",
    "        item_servico_line = [line for line in text_splited if 'Item da Lista de Serviços' in line][0]\n",
    "        item_servico_number = float(extract_number(item_servico_line))\n",
    "        item_servico_value = item_servico_dict.get(item_servico_number, \"Valor não encontrado\")\n",
    "        item_servico_value = item_servico_value.upper()\n",
    "        item_servico_value = str(item_servico_number) + \" - \" + item_servico_value\n",
    "        nd_data_CNAE['item_lista_servicos'] = item_servico_value\n",
    "    except Exception as e:\n",
    "        print(f\"Erro busca Itens de servico: {e}\")  \n",
    "\n",
    "    secao = \"8. DADOS COMPLEMENTARES\"\n",
    "    try:\n",
    "        nd_data_valores = {}\n",
    "        nd_data_valores['secao'] = secao\n",
    "        f_frame_name = \"5_frame_valores_impostos\"   \n",
    "        \n",
    "        result = extract_fields_box(model, f_frame_name, secao)\n",
    "        if result:\n",
    "            nd_data_valores.update(result)\n",
    "\n",
    "        # secao: 8 - DADOS COMPLEMENTARES\"\n",
    "        nd_data_dados_complementares = {}\n",
    "        f_frame_name  = \"5_frame_dados_complementares\"\n",
    "        section = \"8. DADOS COMPLEMENTARES\"\n",
    "\n",
    "        nd_data_dados_complementares = extract_dados_comple_obs(model, f_frame_name, section)                                           \n",
    "                                \n",
    "                                \n",
    "        # secao: 9 - OUTRAS INFORMAÇOES / CRITICAS\n",
    "        nd_data_outras_informacoes = {}\n",
    "        father_value = \"5_frame_inf_criticas\"\n",
    "        section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "        \n",
    "        result = extract_fb_outras_inf(model, father_value, section)\n",
    "        if result:\n",
    "            nd_data_outras_informacoes.update(result)                        \n",
    "                            \n",
    "        # secao: 10. OBSERVACOES\n",
    "        nd_data_observacao = {}\n",
    "        f_father = \"5_frame_observacao\"\n",
    "        section = \"10. OBSERVACOES\"\n",
    "\n",
    "        nd_data_observacao = extract_dados_comple_obs(model, f_father, section)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro valores complementares:\")  \n",
    "        \n",
    "    \n",
    "    return nro_nota, nd_data_cabecalho, nd_data_prestador, nd_data_tomador, nd_data_servico, nd_data_valor_total, nd_data_CNAE, nd_data_valores, nd_data_dados_complementares, nd_data_outras_informacoes, nd_data_observacao      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processa_pdf_pesquisavel(file_path):\n",
    "    \n",
    "   \n",
    "    status = \"O PDF é pesquisável\"\n",
    "    # Carregar o arquivo PDF\n",
    "    pdf_document = fitz.open(file_path)\n",
    "\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 4\n",
    "    x1 = 600\n",
    "    y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    \n",
    "    print(text)\n",
    "    \n",
    "    if text:\n",
    "       page_number = 0\n",
    "       print(page_number)\n",
    "    else:\n",
    "       page_number = 1\n",
    "       print(page_number)\n",
    "    \n",
    "    nf_data_cabecalho = {}\n",
    "    \n",
    "    page = pdf_document[page_number]\n",
    "    x0 = 0\n",
    "    y0 = 0\n",
    "    x1 = 600\n",
    "    y1 = 110\n",
    "\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    nf_data_cabecalho = Extc.extract_fields_cabecalho(text)\n",
    "    #nf_data_cabecalho = extract_fields_cabecalho(text)\n",
    "    \n",
    "    nro_nota = nf_data_cabecalho['numero_nota_fiscal']\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2. PRESTADOR DE SERVIÇO\n",
    "    # Definir retângulo de interesse\n",
    "    nf_data_prestador = {}\n",
    "    x0 = 0\n",
    "    y0 = 100\n",
    "    x1 = 600\n",
    "    y1 = 236  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    \n",
    "    nf_data_prestador = Extc.extract_fields_prestador(text)\n",
    "    #nf_data_prestador = extract_fields_prestador(text)\n",
    "    \n",
    "    # 3. TOMADOR DE SERVIÇO\n",
    "    # Definir retângulo de interesse\n",
    "    nf_data_tomador = {}\n",
    "    x0 = 0\n",
    "    y0 = 210\n",
    "    x1 = 600\n",
    "    y1 = 340  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    \n",
    "    \n",
    "    nf_data_tomador = Extc.extract_fields_tomador(text)\n",
    "    #nf_data_tomador = extract_fields_tomador(text)\n",
    "    \n",
    "    \n",
    "    # 4. DESCRIMINACAO DOS SERVIÇOS\n",
    "    nf_data_servico = {}\n",
    "    nf_data_servico['secao'] = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 330\n",
    "    x1 = 600\n",
    "    y1 = 500  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "    # Remover quebras de linha e rótulo\n",
    "    text = text.replace('\\n', ' ')\n",
    "    label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "    if text.startswith(label):\n",
    "        text = text[len(label):].strip()\n",
    "\n",
    "    # Atribuir texto ao dicionário\n",
    "    nf_data_servico['discriminacao_servicos'] = text\n",
    "    \n",
    "    \n",
    "    # 5. VALOR TOTAL\n",
    "    nf_data_valor_total = {}\n",
    "    nf_data_valor_total['secao'] = \"5. VALOR TOTAL\"\n",
    "\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 500\n",
    "    x1 = 600\n",
    "    y1 = 550  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "    # Usar expressão regular para extrair apenas os caracteres numéricos e pontos decimais\n",
    "    valor_total_match = re.search(r'R\\$ ([\\d,.]+)', text)\n",
    "    if valor_total_match:\n",
    "        valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "        nf_data_valor_total['valor_total_nota'] = float(valor_total_sem_formatacao)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    # 6. CNAE e Item da Lista de Serviços\n",
    "    nf_data_CNAE = {}\n",
    "    nf_data_CNAE['Secao'] = \"6. CNAE e Item da Lista de Serviços\"\n",
    "\n",
    "    # Definir retângulo de interesse CNAE\n",
    "    x0 = 0\n",
    "    y0 = 530\n",
    "    x1 = 600\n",
    "    y1 = 540  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "\n",
    "    # Extrair CNAE\n",
    "    nf_data_CNAE_match = re.search(r'CNAE\\s+(.+)', text)\n",
    "    if nf_data_CNAE_match:\n",
    "        # Remove a primeira ocorrência de \"CNAE:\"\n",
    "        nf_data_CNAE_str = re.sub(r'^CNAE - ', '', text, count=1)\n",
    "        # Remover quebras de linha\n",
    "        nf_data_CNAE_str = nf_data_CNAE_str.replace('\\n', ' ')\n",
    "        nf_data_CNAE['cnae'] = nf_data_CNAE_str\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Item da Lista de Serviços    \n",
    "    # Definir retângulo de interesse - Item da Lista de Serviços\n",
    "    x0 = 0\n",
    "    y0 = 545\n",
    "    x1 = 600\n",
    "    y1 = 560  # Ajuste este valor para delimitar a região vertical    \n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))     \n",
    "        \n",
    "    # Extrair Item da Lista de Serviços\n",
    "    nf_item_lista_servicos_match = re.search(r'Item da Lista de Serviços\\s+(.+)', text)\n",
    "    if nf_item_lista_servicos_match:\n",
    "        nf_item_lista_servicos_str = re.sub(r'^Item da Lista de Serviços - ', '', text, count=1) \n",
    "        # Remover quebras de linha\n",
    "        #nf_item_lista_servicos_str = nf_item_lista_servicos_str.replace('\\n \\n', '')\n",
    "        nf_item_lista_servicos_str = nf_item_lista_servicos_str.replace('\\n', ' ')\n",
    "        nf_data_CNAE['item_lista_servicos'] = nf_item_lista_servicos_str\n",
    "        \n",
    "    \n",
    "    # 7. VALORES E IMPOSTOS\n",
    "    # Definir retângulo de interesse\n",
    "    nf_data_valores = {}\n",
    "    \n",
    "    x0 = 0\n",
    "    y0 = 550\n",
    "    x1 = 600\n",
    "    y1 = 650  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "    # Extrair campos e valores\n",
    "    nf_data_valores = Extc.extract_fields_impostos(text)\n",
    "    #nf_data_valores = extract_fields_impostos(text)\n",
    "    \n",
    "    # 8. DADOS COMPLEMENTARES\n",
    "    nf_data_dados_complementares = {}\n",
    "    nf_data_dados_complementares['secao'] = \"8. DADOS COMPLEMENTARES\"\n",
    "\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 650\n",
    "    x1 = 600\n",
    "    y1 = 680  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    # Remove a primeira ocorrência de \"Observação:\"\n",
    "    text = re.sub(r'^DADOS COMPLEMENTARES', '', text, count=1)\n",
    "    if text == \" \":\n",
    "        text = \"NONE\"\n",
    "        nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "    else:    \n",
    "        # Extrair texto dentro do retângulo\n",
    "        nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 9. OUTRAS INFORMAÇOES / CRITICAS  \n",
    "    # Definir retângulo de interesse\n",
    "    nf_data_outras_informacoes = {}\n",
    "    x0 = 0\n",
    "    y0 = 680\n",
    "    x1 = 600\n",
    "    y1 = 725  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "    # Extrair campos e valores\n",
    "    nf_data_outras_informacoes = Extc.extract_fields_outras_info(text)\n",
    "    #nf_data_outras_informacoes = extract_fields_outras_info(text)\n",
    "    \n",
    "    \n",
    "    # 10. OBSERVACOES\n",
    "    nf_data_observacao = {}\n",
    "    nf_data_observacao['secao'] = \"10. OBSERVACOES\"\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 725\n",
    "    x1 = 600\n",
    "    y1 = 760  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "    # Remove a primeira ocorrência de \"Observação:\"\n",
    "    text = re.sub(r'^Observação:', '', text, count=1)\n",
    "\n",
    "    # Remover quebras de linha\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    nf_data_observacao['observacao'] = text.strip()\n",
    "    \n",
    "    # try:\n",
    "    #     nr_nro_nf = nro_nota\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Nao encontrado nro da NF: {e}\")       \n",
    "    \n",
    "\n",
    "    return nro_nota, nf_data_cabecalho, nf_data_prestador, nf_data_tomador, nf_data_servico, nf_data_valor_total, nf_data_CNAE, nf_data_valores, nf_data_dados_complementares, nf_data_outras_informacoes, nf_data_observacao       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Processo RASTER PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "As informações foram salvas em pipeline_extracao_documentos/5_documentos_processados/jsons/Batch_17_Raster_PDF.json\n"
     ]
    }
   ],
   "source": [
    "erros = {}\n",
    "\n",
    "# 1. Leitura recursiva de diretorios e arquivos a partir de root\n",
    "pdf_info = {}  # Dicionário para armazenar informações sobre PDFs\n",
    "\n",
    "nf_data_servico = {}#VERIFICAR\n",
    "analise_doc_nf = {} #VERIFICAR\n",
    "file_data = [] #VERIFICAR\n",
    "\n",
    "list_document_pages = []\n",
    "#nro_nota = 0\n",
    "# TEMP\n",
    "# Nome do arquivo json\n",
    "nome_formado_json = batch_name +\".json\"\n",
    "#3. path formado para nome do arquivo json\n",
    "json_file_path = os.path.join(json_path, nome_formado_json)\n",
    "#root_doc_analise = os.path.join(documentos_extracao_path, batch_name)\n",
    "#print(root_doc_analise)\n",
    "i = 1\n",
    "for root, dirs, files in os.walk(root_doc_analise):\n",
    "    dir_name = os.path.basename(root)\n",
    "    #print(dir_name)\n",
    "    for file in files:\n",
    "        \n",
    "        if file.lower().endswith('.pdf'):\n",
    "            doc2convert = file\n",
    "            document_path_1 = os.path.join(root, file)\n",
    "            pdf_document = fitz.open(document_path_1)\n",
    "            #page_number = 0  # Defina o número da página que deseja analisar\n",
    "            #page = pdf_document[page_number]\n",
    "            \n",
    "            documento_pdf = True\n",
    "            pesquisavel, metadados, paginas = is_pdf_searchable_analise(document_path_1)\n",
    "            \n",
    "            \n",
    "            pdf_realmente_pequisavel = confirma_pdf_pequisavel(document_path_1)\n",
    "            \n",
    "            if not pdf_realmente_pequisavel:\n",
    "                print(f'\\nTeste nro: {i} | doc: {file} | pdf?: {documento_pdf} | pesquisavel?: {pesquisavel} | paginas: {paginas}\\n')\n",
    "                processo = \"PDF_Raster\"\n",
    "                image_2work, name_image_2work = convertResizeAnalise_1page(file, document_path_1, image_resized_path)    \n",
    "                \n",
    "                \n",
    "                secao = \"1 - CABECALHO\"\n",
    "                try:\n",
    "                    nro_nota = 0\n",
    "                    data_cabecalho = {}\n",
    "                    data_cabecalho['secao'] = secao\n",
    "                    valor_dict = {}\n",
    "                    dados_prefeitura = {}\n",
    "                    f_frame_name = \"1_frame_prefeitura_nf\"\n",
    "                    # 1. funçao basica de modelo \n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    text_splited = texto.split('\\n')\n",
    "                    \n",
    "                    valor_dict = extract_prefeitura(model, f_frame_name, text_splited)\n",
    "                    if valor_dict:\n",
    "                        dados_prefeitura.update(valor_dict)\n",
    "                    valor = {}   \n",
    "                    f_frame_name = \"1_frame_dados_nf\"\n",
    "                    dadinho_dados_nf = {}\n",
    "                    # 1. funçao basica de modelo \n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    text_splited = texto_extraido(texto)\n",
    "                    keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "                    string_pesquisa = \"Número da Nota:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "                    dadinho_dados_nf['numero_nota_fiscal'] = texto\n",
    "                    nro_nota = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Competência:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    dadinho_dados_nf['competencia'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    dadinho_dados_nf['dt_hr_emissao'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Código Verificação:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    dadinho_dados_nf['codigo_verificacao'] = texto\n",
    "                    \n",
    "                    data_cabecalho.update(dados_prefeitura)\n",
    "                    data_cabecalho.update(dadinho_dados_nf)\n",
    "                except Exception as e:\n",
    "                    erros_cabecalho = {}\n",
    "                    err_msg = f\"Erro de processo cabecalho: {e}\"\n",
    "                    erros['documento'] = file\n",
    "                    erros_cabecalho['secao'] = secao\n",
    "                    erros_cabecalho['erro'] = err_msg\n",
    "                    erros.update(erros_cabecalho)                \n",
    "               \n",
    "                \n",
    "                \n",
    "                \n",
    "                secao = \"2. PRESTADOR DE SERVIÇO\"\n",
    "                try:\n",
    "                    f_frame_name = \"2_frame_cnpj_prestador\"\n",
    "                    data_prestador = {}\n",
    "                    prestador_inscricao = {}\n",
    "                    data_prestador['secao'] = secao\n",
    "                    prestador_cnpj_value = {}\n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #text_splited = texto_extraido(texto)\n",
    "                    prestador_cnpj_value = extract_fields_prestador_cnpj(texto)\n",
    "                    if prestador_cnpj_value:\n",
    "                        data_prestador.update(prestador_cnpj_value)\n",
    "                except Exception as e:\n",
    "                    erros_cnpj_prestador = {}\n",
    "                    err_msg = (f\"Erro prestador cnpj: {e}\")\n",
    "                    erros_cnpj_prestador['secao'] = secao\n",
    "                    erros_cnpj_prestador['erro'] = err_msg\n",
    "                    erros.update(erros_cnpj_prestador)       \n",
    "                    \n",
    "                try:\n",
    "                    f_frame_name = \"2_frame_inscricao_prestador\" \n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    text_splited = texto_extraido(texto)\n",
    "                    keyword_list = ['Inscrição Municipal:', 'Inscrição Estadual:']\n",
    "                    string_pesquisa = \"Inscrição Municipal:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    prestador_inscricao['prestador_inscricao'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Inscrição Estadual:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    prestador_inscricao['inscricao_estadual'] = texto\n",
    "                    data_prestador.update(prestador_inscricao)\n",
    "                except Exception as e:\n",
    "                    erros_inscricao_prestador = {}\n",
    "                    err_msg = (f\"Erro de processo inscricao prestador: {e}\")\n",
    "                    erros_inscricao_prestador['secao'] = secao\n",
    "                    erros_inscricao_prestador['erro'] = err_msg\n",
    "                    erros.update(erros_inscricao_prestador)\n",
    "\n",
    "                try:\n",
    "                    f_frame_name = \"2_frame_dados_prestador\"\n",
    "                    prestador_dados_value = {}\n",
    "                    \n",
    "                    keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "                    string_pesquisa = \"Nome/Razão Social:\"\n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    prestador_dados_value['razao_social'] = texto\n",
    "\n",
    "                    string_pesquisa = \"Nome de Fantasia:\"\n",
    "                    #texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    #text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    prestador_dados_value['nome_fantasia'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Endereço:\"\n",
    "                    #texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    #text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    prestador_dados_value['endereco'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"E-mail:\"\n",
    "                    #texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    #text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    prestador_dados_value['email'] = texto\n",
    "                    data_prestador.update(prestador_dados_value)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Erro prestador dados: {e}\") \n",
    "                    \n",
    "                \n",
    "                secao = \"3. TOMADOR DE SERVIÇO\"\n",
    "                try:\n",
    "                    data_tomador = {}\n",
    "                    tomador_cnpj_value = {}\n",
    "                    data_tomador['secao'] = secao\n",
    "                    f_frame_name = \"3_frame_cnpj_tomador\"\n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    text_splited = texto_extraido(texto) \n",
    "                    tomador_cnpj_value = extract_fields_tomador_cnpj(texto)\n",
    "                    if tomador_cnpj_value:\n",
    "                        data_tomador.update(tomador_cnpj_value)                  \n",
    "                except Exception as e:\n",
    "                    print(f\"Erro tomador cnpj: {e}\")\n",
    "                    \n",
    "                f_frame_name = \"3_frame_inscricao_tomador\"    \n",
    "                try:\n",
    "                    data_tomador_inscricao = {}\n",
    "                    keyword_list = ['RG:', 'Inscrição Estadual:']\n",
    "                    string_pesquisa = \"RG:\"\n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    data_tomador_inscricao['rg'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Inscrição Estadual:\"\n",
    "                    #texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    data_tomador_inscricao['inscricao_estadual'] = texto\n",
    "                    data_tomador.update(data_tomador_inscricao)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro tomador inscricao: {e}\")\n",
    "                    \n",
    "                f_frame_name = \"3_frame_dados_tomador\"\n",
    "                try: \n",
    "                    data_tomador_dados = {}   \n",
    "                    keyword_list = ['Nome/Razão Social:', 'Endereço:', 'E-mail']\n",
    "                    string_pesquisa = \"Nome/Razão Social:\"\n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    data_tomador_dados['razao_social'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Endereço:\"\n",
    "                    #texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    data_tomador_dados['endereco'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"E-mail\"\n",
    "                    #texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "                    #text_splited = texto_extraido(texto)\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    data_tomador_dados['email'] = texto\n",
    "                    \n",
    "                    data_tomador.update(data_tomador_dados)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro tomador dados: {e}\") \n",
    "                    \n",
    "                secao = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "                try:\n",
    "                    data_servico = {}\n",
    "                    data_servico['secao'] = secao\n",
    "                    f_frame_name = \"4_frame_descricao_totais\"\n",
    "                    texto = executa_model_frame(model, secao, f_frame_name)\n",
    "                    text = texto.replace('\\n', ' ')\n",
    "                    label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "                    if text.startswith(label):\n",
    "                        text = text[len(label):].strip()\n",
    "                    data_servico['discriminacao_servicos'] = text \n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro descricao servico: {e}\")\n",
    "                     \n",
    "                secao = \"5. VALOR TOTAL\"\n",
    "                try:\n",
    "                    data_valor_total = {}\n",
    "                    data_valor_total['secao'] = secao\n",
    "                    f_frame_name = \"4_frame_valor_total\"   \n",
    "                    text = executa_model_frame(model, secao, f_frame_name)  \n",
    "                    valor_total_match = re.search(r'R\\$ ([\\d,.]+)', text)\n",
    "                    if valor_total_match:\n",
    "                        valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "                        data_valor_total['valor_total_nota'] = float(valor_total_sem_formatacao)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro valor total: {e}\")\n",
    "\n",
    "                secao = \"6. CNAE e Item da Lista de Serviços\"\n",
    "                try:\n",
    "                    data_CNAE = {}\n",
    "                    data_CNAE['secao'] = secao\n",
    "                    f_frame_name = \"4_frame_cnae_itens_servico\"   \n",
    "                    Texto_extraido = executa_model_frame(model, secao, f_frame_name)\n",
    "                    text_splited = Texto_extraido.split('\\n')\n",
    "                    # Processando CNAE\n",
    "                    cnae_line = [line for line in text_splited if 'CNAE' in line][0]\n",
    "                    cnae_number = int(extract_number(cnae_line))\n",
    "                    cnae_value = cnae_dict.get(cnae_number, \"Valor não encontrado\")\n",
    "                    if cnae_value == 'Valor não encontrado':\n",
    "                        cnae_value = processa_cnae_outros(cnae_line)\n",
    "                        cnae_value = cnae_value.upper()\n",
    "                        data_CNAE['cnae'] = cnae_value\n",
    "                    else:\n",
    "                        cnae_value = cnae_value.upper()\n",
    "                        cnae_value = str(cnae_number) + \" - \" + cnae_value\n",
    "                        data_CNAE['cnae'] = cnae_value\n",
    "                        data_CNAE['item_lista_servicos'] = item_servico_value\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro busca cnae: {e}\")    \n",
    "          \n",
    "                try:\n",
    "                    item_servico_line = [line for line in text_splited if 'Item da Lista de Serviços' in line][0]\n",
    "                    item_servico_number = float(extract_number(item_servico_line))\n",
    "                    item_servico_value = item_servico_dict.get(item_servico_number, \"Valor não encontrado\")\n",
    "                    item_servico_value = item_servico_value.upper()\n",
    "                    item_servico_value = str(item_servico_number) + \" - \" + item_servico_value\n",
    "                    data_CNAE['item_lista_servicos'] = item_servico_value\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro busca Itens de servico: {e}\")  \n",
    "\n",
    "                secao = \"8. DADOS COMPLEMENTARES\"\n",
    "                try:\n",
    "                    data_valores = {}\n",
    "                    data_valores['secao'] = secao\n",
    "                    f_frame_name = \"5_frame_valores_impostos\"   \n",
    "                    \n",
    "                    result = extract_fields_box(model, f_frame_name, secao)\n",
    "                    if result:\n",
    "                        data_valores.update(result)\n",
    "            \n",
    "                    # secao: 8 - DADOS COMPLEMENTARES\"\n",
    "                    data_dados_complementares = {}\n",
    "                    f_frame_name  = \"5_frame_dados_complementares\"\n",
    "                    section = \"8. DADOS COMPLEMENTARES\"\n",
    "\n",
    "                    data_dados_complementares = extract_dados_comple_obs(model, f_frame_name, section)                                           \n",
    "                                            \n",
    "                                            \n",
    "                    # secao: 9 - OUTRAS INFORMAÇOES / CRITICAS\n",
    "                    data_outras_informacoes = {}\n",
    "                    father_value = \"5_frame_inf_criticas\"\n",
    "                    section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "                    \n",
    "                    result = extract_fb_outras_inf(model, father_value, section)\n",
    "                    if result:\n",
    "                        data_outras_informacoes.update(result)                        \n",
    "                                        \n",
    "                    # secao: 10. OBSERVACOES\n",
    "                    data_observacao = {}\n",
    "                    f_father = \"5_frame_observacao\"\n",
    "                    section = \"10. OBSERVACOES\"\n",
    "\n",
    "                    data_observacao = extract_dados_comple_obs(model, f_father, section)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro valores complementares: {e}\")   \n",
    "                \n",
    "                nome_arquivo = file\n",
    "                #pdf_info[\"diretorio\"] = os.path.basename(root)\n",
    "                try:\n",
    "                    pdf_info[nro_nota] = {\n",
    "                    \"dados_NF_PDF\": {\n",
    "                        \"data_cabecalho\": data_cabecalho,\n",
    "                        \"data_prestador\": data_prestador,\n",
    "                        \"data_tomador\": data_tomador,\n",
    "                        \"data_servico\": data_servico,\n",
    "                        \"data_valor_total\": data_valor_total,\n",
    "                        \"data_CNAE\": data_CNAE,\n",
    "                        \"data_valores\": data_valores,\n",
    "                        \"data_dados_complementares\": data_dados_complementares,\n",
    "                        \"data_outras_informacoes\": data_outras_informacoes,\n",
    "                        \"data_observacao\": data_observacao,\n",
    "                    },\n",
    "                    \"diretorio\": dir_name, #os.path.basename(root)\n",
    "                    \"nome_arquivo\": nome_arquivo,\n",
    "                    \"Batch\": batch_name,\n",
    "                    \"modelo\": model,\n",
    "                    \"processo\": processo,\n",
    "                }\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao gerar o json: {e}\")\n",
    "                pdf_document.close()\n",
    "                \n",
    "                #print(pdf_info)\n",
    "                #if paginas == 1:\n",
    "                if paginas > 1000:\n",
    "                    if i == 1000: #Define quantidade de tratamento de documentos raster PDF\n",
    "                        break\n",
    "            i +=1 \n",
    "processo = \"Raster_PDF\"\n",
    "                \n",
    "# Nome do arquivo json\n",
    "nome_formado_json = batch_name + \"_\" + processo + \".json\"\n",
    "\n",
    "json_file_path = os.path.join(json_path, nome_formado_json )\n",
    "\n",
    "\n",
    "# Salvando as informações em um arquivo JSON (novo formato nome arquivo V2)\n",
    "with open(json_file_path, \"w\", encoding='utf-8') as json_file:\n",
    "    json.dump(pdf_info, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"As informações foram salvas em {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Processo PDF Pesquisavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_doc_analise = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "batch_name = \"Batch_17\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "diretorio: Batch_17 nro: 1 | doc: nf_689__pmmacae__8a_med_co_22_22_tb_contrato_045_22.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "pdf_realmente_pequisavel:  (False, 1)\n",
      "\n",
      "PDF Pesquisavel diretorio: Batch_17 nro: 1 | doc: nf_689__pmmacae__8a_med_co_22_22_tb_contrato_045_22.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "\n",
      "1\n",
      "Erro ao gerar dict nf: page not in document\n",
      "Erro ao gerar o json: name 'nro_nota' is not defined\n",
      "i:  2\n",
      "\n",
      "diretorio: Batch_17 nro: 2 | doc: 12___otica_do_cizinho.pdf | pdf?: True | pesquisavel?: True | paginas: 1\n",
      "\n",
      "pdf_realmente_pequisavel:  (True, 0)\n",
      "\n",
      "PDF Pesquisavel diretorio: Batch_17 nro: 2 | doc: 12___otica_do_cizinho.pdf | pdf?: True | pesquisavel?: True | paginas: 1\n",
      "\n",
      "27/07/2023, 15:36\n",
      "Nota Fiscal de Serviços Eletrônica (NFSe)\n",
      "PREFEITURA MUNICIPAL DE MAGE\n",
      "SECRETARIA MUNICIPAL DA FAZENDA\n",
      "NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e\n",
      "Número da Nota:\n",
      "12\n",
      "Competência:\n",
      "Julho/2023\n",
      "Data e Hora da Emissão:\n",
      "27/07/2023 15:31:00\n",
      "Código Verificação:\n",
      "7EA6BC158\n",
      "PRESTADOR DE SERVIÇOS\n",
      "CPF/CNPJ:\n",
      " 49.583.070/0001-09\n",
      "Inscrição Municipal:\n",
      " 1007329\n",
      "Telefone:\n",
      " 2126301653..\n",
      "Inscrição Estadual:\n",
      " \n",
      "Nome/Razão Social:\n",
      "MACHADO & REBECHI GESTÃO MÉDICA LTDA\n",
      "Nome de Fantasia:\n",
      "CLINICA VIVACE\n",
      "\n",
      "0\n",
      "i:  3\n",
      "\n",
      "diretorio: Batch_17 nro: 3 | doc: 1127_fogo_em_20_07_23.pdf | pdf?: True | pesquisavel?: True | paginas: 1\n",
      "\n",
      "pdf_realmente_pequisavel:  (True, 0)\n",
      "\n",
      "PDF Pesquisavel diretorio: Batch_17 nro: 3 | doc: 1127_fogo_em_20_07_23.pdf | pdf?: True | pesquisavel?: True | paginas: 1\n",
      "\n",
      "31/07/2023 10:38\n",
      "Nota Fiscal de Serviços Eletrônica (NFSe)\n",
      "PREFEITURA MUNICIPAL DE MAGE\n",
      "SECRETARIA MUNICIPAL DA FAZENDA\n",
      "NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e\n",
      "Número da Nota:\n",
      "1127\n",
      "Competência:\n",
      "Julho/2023\n",
      "Data e Hora da Emissão:\n",
      "31/07/2023 10:31:00\n",
      "Código Verificação:\n",
      "885232F35\n",
      "PRESTADOR DE SERVIÇOS\n",
      "CPF/CNPJ:\n",
      " 03.726.851/0001-04\n",
      "Inscrição Municipal:\n",
      " 7980\n",
      "Telefone:\n",
      " 2125607986..\n",
      "Inscrição Estadual:\n",
      " \n",
      "Nome/Razão Social:\n",
      "DESMAQ SERVICOS E DESMONTES LTDA\n",
      "Nome de Fantasia:\n",
      "Endereço:\n",
      "\n",
      "0\n",
      "i:  4\n",
      "\n",
      "diretorio: Batch_17 nro: 4 | doc: nota_fiscal_eletronica_1717___ball___bvi.pdf | pdf?: True | pesquisavel?: True | paginas: 1\n",
      "\n",
      "pdf_realmente_pequisavel:  (True, 0)\n",
      "\n",
      "PDF Pesquisavel diretorio: Batch_17 nro: 4 | doc: nota_fiscal_eletronica_1717___ball___bvi.pdf | pdf?: True | pesquisavel?: True | paginas: 1\n",
      "\n",
      "25/07/2023, 17:08\n",
      "Nota Fiscal de Serviços Eletrônica (NFSe)\n",
      "PREFEITURA MUNICIPAL DE MAGE\n",
      "SECRETARIA MUNICIPAL DA FAZENDA\n",
      "NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e\n",
      "Número da Nota:\n",
      "1717\n",
      "Competência:\n",
      "Julho/2023\n",
      "Data e Hora da Emissão:\n",
      "25/07/2023 17:00:00\n",
      "Código Verificação:\n",
      "BC82B2B7F\n",
      "PRESTADOR DE SERVIÇOS\n",
      "CPF/CNPJ:\n",
      " 08.085.952/0001-67\n",
      "Inscrição Municipal:\n",
      "22002\n",
      "Telefone:\n",
      " 2122687853..\n",
      "Inscrição Estadual:\n",
      "78.140.968\n",
      "Nome/Razão Social:\n",
      "INMASP INDÚSTRIA E COMÉRCIO DE EQUIP. CONTRA INCÊNDIO LTDA\n",
      "Nome de Fantasia:\n",
      "Endereço:\n",
      "\n",
      "0\n",
      "i:  5\n",
      "\n",
      "diretorio: Batch_17 nro: 5 | doc: 38_edson_jose_oliveira_dos_santos.pdf | pdf?: True | pesquisavel?: True | paginas: 1\n",
      "\n",
      "pdf_realmente_pequisavel:  (True, 0)\n",
      "\n",
      "PDF Pesquisavel diretorio: Batch_17 nro: 5 | doc: 38_edson_jose_oliveira_dos_santos.pdf | pdf?: True | pesquisavel?: True | paginas: 1\n",
      "\n",
      "PREFEITURA MUNICIPAL DE MAGE\n",
      "SECRETARIA MUNICIPAL DA FAZENDA\n",
      "NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e\n",
      "Número da Nota:\n",
      "38\n",
      "Competência:\n",
      "Julho/2023\n",
      "Data e Hora da Emissão:\n",
      "25/07/2023 13:33:00\n",
      "Código Verificação:\n",
      "0897AB668\n",
      "PRESTADOR DE SERVIÇOS\n",
      "CPF/CNPJ:\n",
      " 48.693.039/0001-68\n",
      "Inscrição Municipal:\n",
      "1007096\n",
      "Telefone:\n",
      " 2126301653..\n",
      "Inscrição Estadual:\n",
      "12.681.275\n",
      "Nome/Razão Social:\n",
      "VIVACE ESTÚDIO DE FISIOTERAPIA E SAÚDE LTDA\n",
      "Nome de Fantasia:\n",
      "VIVACE\n",
      "\n",
      "0\n",
      "i:  6\n",
      "\n",
      "diretorio: Batch_17 nro: 6 | doc: nf_688___consor_zadar__engetecnica_enza.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "pdf_realmente_pequisavel:  (False, 1)\n",
      "\n",
      "PDF Pesquisavel diretorio: Batch_17 nro: 6 | doc: nf_688___consor_zadar__engetecnica_enza.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "\n",
      "1\n",
      "Erro ao gerar dict nf: page not in document\n",
      "i:  7\n",
      "\n",
      "diretorio: Batch_17 nro: 7 | doc: flexprin.pdf | pdf?: True | pesquisavel?: True | paginas: 1\n",
      "\n",
      "pdf_realmente_pequisavel:  (True, 0)\n",
      "\n",
      "PDF Pesquisavel diretorio: Batch_17 nro: 7 | doc: flexprin.pdf | pdf?: True | pesquisavel?: True | paginas: 1\n",
      "\n",
      "24/07/2023, 15:50\n",
      "Nota Fiscal de Serviços Eletrônica (NFSe)\n",
      "PREFEITURA MUNICIPAL DE MAGE\n",
      "SECRETARIA MUNICIPAL DA FAZENDA\n",
      "NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e\n",
      "Número da Nota:\n",
      "1018\n",
      "Competência:\n",
      "Julho/2023\n",
      "Data e Hora da Emissão:\n",
      "24/07/2023 15:49:00\n",
      "Código Verificação:\n",
      "C0EF5CD50\n",
      "PRESTADOR DE SERVIÇOS\n",
      "CPF/CNPJ:\n",
      " 04.543.932/0001-31\n",
      "Inscrição Municipal:\n",
      "1004370\n",
      "Telefone:\n",
      " 2125096200..\n",
      "Inscrição Estadual:\n",
      "77702555\n",
      "Nome/Razão Social:\n",
      "FLEXPRIN INDUSTRIA COMERCIO E SERVIÇOS MARITIMOS LTDA EPP\n",
      "Nome de Fantasia:\n",
      "S.O.V. SALVATAGEM\n",
      "\n",
      "0\n",
      "i:  8\n",
      "\n",
      "diretorio: Batch_17 nro: 8 | doc: nota_fiscal_229_agosto_schneider.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "pdf_realmente_pequisavel:  (False, 1)\n",
      "\n",
      "PDF Pesquisavel diretorio: Batch_17 nro: 8 | doc: nota_fiscal_229_agosto_schneider.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "\n",
      "1\n",
      "Erro ao gerar dict nf: page not in document\n",
      "i:  9\n",
      "\n",
      "diretorio: fwdresnotasausentesnosistema nro: 9 | doc: nfs_e_37.pdf | pdf?: True | pesquisavel?: True | paginas: 2\n",
      "\n",
      "pdf_realmente_pequisavel:  (True, 0)\n",
      "\n",
      "PDF Pesquisavel diretorio: fwdresnotasausentesnosistema nro: 9 | doc: nfs_e_37.pdf | pdf?: True | pesquisavel?: True | paginas: 2\n",
      "\n",
      "PREFEITURA MUNICIPAL DE MAGE\n",
      "SECRETARIA MUNICIPAL DA FAZENDA\n",
      "NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e\n",
      "Número da Nota:\n",
      "37\n",
      "Competência:\n",
      "Julho/2023\n",
      "Data e Hora da Emissão:\n",
      "27/07/2023 10:12:00\n",
      "Código Verificação:\n",
      "8156E38E6\n",
      "PRESTADOR DE SERVIÇOS\n",
      "CPF/CNPJ:\n",
      "  26.127.269/0001-27\n",
      "Inscrição Municipal:\n",
      " 1004802\n",
      "Telefone:\n",
      "  2126324343..\n",
      "Inscrição Estadual:\n",
      " 12.001.894\n",
      "Nome/Razão Social:\n",
      " SPARTA SERVIÇOS ESPECIALIZADOS LTDA\n",
      "Nome de Fantasia:\n",
      "SPARTA SERVICOS ESPECIALIZADOS\n",
      "\n",
      "0\n",
      "i:  10\n",
      "\n",
      "diretorio: fwdresnotasausentesnosistema nro: 10 | doc: nfs_e_35.pdf | pdf?: True | pesquisavel?: True | paginas: 2\n",
      "\n",
      "pdf_realmente_pequisavel:  (True, 0)\n",
      "\n",
      "PDF Pesquisavel diretorio: fwdresnotasausentesnosistema nro: 10 | doc: nfs_e_35.pdf | pdf?: True | pesquisavel?: True | paginas: 2\n",
      "\n",
      "PREFEITURA MUNICIPAL DE MAGE\n",
      "SECRETARIA MUNICIPAL DA FAZENDA\n",
      "NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e\n",
      "Número da Nota:\n",
      "35\n",
      "Competência:\n",
      "Julho/2023\n",
      "Data e Hora da Emissão:\n",
      "27/07/2023 10:05:00\n",
      "Código Verificação:\n",
      "162953D40\n",
      "PRESTADOR DE SERVIÇOS\n",
      "CPF/CNPJ:\n",
      "  26.127.269/0001-27\n",
      "Inscrição Municipal:\n",
      " 1004802\n",
      "Telefone:\n",
      "  2126324343..\n",
      "Inscrição Estadual:\n",
      " 12.001.894\n",
      "Nome/Razão Social:\n",
      " SPARTA SERVIÇOS ESPECIALIZADOS LTDA\n",
      "Nome de Fantasia:\n",
      "SPARTA SERVICOS ESPECIALIZADOS\n",
      "\n",
      "0\n",
      "i:  11\n",
      "\n",
      "diretorio: fwdresnotasausentesnosistema nro: 11 | doc: nfs_e_38_cancelada.pdf | pdf?: True | pesquisavel?: True | paginas: 1\n",
      "\n",
      "pdf_realmente_pequisavel:  (True, 0)\n",
      "\n",
      "PDF Pesquisavel diretorio: fwdresnotasausentesnosistema nro: 11 | doc: nfs_e_38_cancelada.pdf | pdf?: True | pesquisavel?: True | paginas: 1\n",
      "\n",
      "02/08/2023, 11:01\n",
      "Nota Fiscal de Serviços Eletrônica (NFSe)\n",
      "PREFEITURA MUNICIPAL DE MAGE\n",
      "SECRETARIA MUNICIPAL DA FAZENDA\n",
      "NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e\n",
      "Número da Nota:\n",
      "38\n",
      "Competência:\n",
      "Julho/2023\n",
      "Data e Hora da Emissão:\n",
      "28/07/2023 13:30:00\n",
      "Código Verif icação:\n",
      "C21DA678D\n",
      "PRESTADOR DE SERVIÇOS\n",
      "CPF/CNPJ:\n",
      "  26.127.269/0001-27\n",
      "Inscrição Municipal:\n",
      " 1004802\n",
      "Telef one:\n",
      "  2126324343..\n",
      "Inscrição Estadual:\n",
      " 12.001.894\n",
      "Nome/Razão Social:\n",
      " SPARTA SERVIÇOS ESPECIALIZADOS LTDA\n",
      "Nome de Fantasia:\n",
      "SPARTA SERVICOS ESPECIALIZADOS\n",
      "\n",
      "0\n",
      "i:  12\n",
      "\n",
      "diretorio: fwdresnotasausentesnosistema nro: 12 | doc: nfs_e_36_cancelada.pdf | pdf?: True | pesquisavel?: True | paginas: 1\n",
      "\n",
      "pdf_realmente_pequisavel:  (True, 0)\n",
      "\n",
      "PDF Pesquisavel diretorio: fwdresnotasausentesnosistema nro: 12 | doc: nfs_e_36_cancelada.pdf | pdf?: True | pesquisavel?: True | paginas: 1\n",
      "\n",
      "28/07/2023, 13:35\n",
      "Nota Fiscal de Serviços Eletrônica (NFSe)\n",
      "PREFEITURA MUNICIPAL DE MAGE\n",
      "SECRETARIA MUNICIPAL DA FAZENDA\n",
      "NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e\n",
      "Número da Nota:\n",
      "36\n",
      "Competência:\n",
      "Julho/2023\n",
      "Data e Hora da Emissão:\n",
      "27/07/2023 10:09:00\n",
      "Código Verif icação:\n",
      "48EDBE79E\n",
      "PRESTADOR DE SERVIÇOS\n",
      "CPF/CNPJ:\n",
      "  26.127.269/0001-27\n",
      "Inscrição Municipal:\n",
      " 1004802\n",
      "Telef one:\n",
      "  2126324343..\n",
      "Inscrição Estadual:\n",
      " 12.001.894\n",
      "Nome/Razão Social:\n",
      " SPARTA SERVIÇOS ESPECIALIZADOS LTDA\n",
      "Nome de Fantasia:\n",
      "SPARTA SERVICOS ESPECIALIZADOS\n",
      "\n",
      "0\n",
      "i:  13\n",
      "\n",
      "diretorio: fwdnotasfaltantesnosistemadeemisso nro: 13 | doc: doria_marinho_0301_ultrascan.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "pdf_realmente_pequisavel:  (False, 1)\n",
      "\n",
      "PDF Pesquisavel diretorio: fwdnotasfaltantesnosistemadeemisso nro: 13 | doc: doria_marinho_0301_ultrascan.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "\n",
      "1\n",
      "Erro ao gerar dict nf: page not in document\n",
      "i:  14\n",
      "\n",
      "diretorio: fwdnotasfaltantesnosistemadeemisso nro: 14 | doc: doria_marinho_0300_vanisa.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "pdf_realmente_pequisavel:  (False, 1)\n",
      "\n",
      "PDF Pesquisavel diretorio: fwdnotasfaltantesnosistemadeemisso nro: 14 | doc: doria_marinho_0300_vanisa.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "\n",
      "1\n",
      "Erro ao gerar dict nf: page not in document\n",
      "i:  15\n",
      "\n",
      "diretorio: fwdnotasfaltantesnosistemadeemisso nro: 15 | doc: doria_marinho_0295_carlos_leandro.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "pdf_realmente_pequisavel:  (False, 1)\n",
      "\n",
      "PDF Pesquisavel diretorio: fwdnotasfaltantesnosistemadeemisso nro: 15 | doc: doria_marinho_0295_carlos_leandro.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "\n",
      "1\n",
      "Erro ao gerar dict nf: page not in document\n",
      "i:  16\n",
      "\n",
      "diretorio: fwdnotasfaltantesnosistemadeemisso nro: 16 | doc: doria_marinho_0299_luciana.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "pdf_realmente_pequisavel:  (False, 1)\n",
      "\n",
      "PDF Pesquisavel diretorio: fwdnotasfaltantesnosistemadeemisso nro: 16 | doc: doria_marinho_0299_luciana.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "\n",
      "1\n",
      "Erro ao gerar dict nf: page not in document\n",
      "i:  17\n",
      "\n",
      "diretorio: fwdnotasfaltantesnosistemadeemisso nro: 17 | doc: doria_marinho_0296_vanisa_cancelada.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "pdf_realmente_pequisavel:  (False, 1)\n",
      "\n",
      "PDF Pesquisavel diretorio: fwdnotasfaltantesnosistemadeemisso nro: 17 | doc: doria_marinho_0296_vanisa_cancelada.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "\n",
      "1\n",
      "Erro ao gerar dict nf: page not in document\n",
      "i:  18\n",
      "\n",
      "diretorio: fwdnotasfaltantesnosistemadeemisso nro: 18 | doc: doria_marinho_0297_raquel.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "pdf_realmente_pequisavel:  (False, 1)\n",
      "\n",
      "PDF Pesquisavel diretorio: fwdnotasfaltantesnosistemadeemisso nro: 18 | doc: doria_marinho_0297_raquel.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "\n",
      "1\n",
      "Erro ao gerar dict nf: page not in document\n",
      "i:  19\n",
      "\n",
      "diretorio: fwdnotasfaltantesnosistemadeemisso nro: 19 | doc: doria_marinho_0298_marcelo.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "Erro ao abrir pagina do PDF: page not in document\n",
      "pdf_realmente_pequisavel:  (False, 1)\n",
      "\n",
      "PDF Pesquisavel diretorio: fwdnotasfaltantesnosistemadeemisso nro: 19 | doc: doria_marinho_0298_marcelo.pdf | pdf?: True | pesquisavel?: False | paginas: 1\n",
      "\n",
      "\n",
      "1\n",
      "Erro ao gerar dict nf: page not in document\n",
      "i:  20\n",
      "As informações foram salvas em pipeline_extracao_documentos/5_documentos_processados/jsons/Batch_17_PDF_Pesquisavel.json\n"
     ]
    }
   ],
   "source": [
    "erros = {}\n",
    "# 1. Leitura recursiva de diretorios e arquivos a partir de root\n",
    "pdf_info = {}  # Dicionário para armazenar informações sobre PDFs\n",
    "\n",
    "\n",
    "data_cabecalho_final = {}\n",
    "data_prestador_final = {}\n",
    "data_tomador_final = {}\n",
    "data_servico_final = {}\n",
    "data_valor_total_final = {}\n",
    "data_CNAE_final = {}\n",
    "data_valores_final = {}\n",
    "data_dados_complementares_final = {}\n",
    "data_outras_informacoes_final = {}\n",
    "data_observacao_final = {}\n",
    "\n",
    "result_1 = {}\n",
    "result_2 = {}\n",
    "result_3 = {}\n",
    "result_4 = {}\n",
    "result_5 = {}\n",
    "result_6 = {}\n",
    "result_7 = {}\n",
    "result_8 = {}\n",
    "result_9 = {}\n",
    "result_10 = {}\n",
    "\n",
    "i = 1\n",
    "for root, dirs, files in os.walk(root_doc_analise):\n",
    "    dir_name = os.path.basename(root)\n",
    "    #print(dir_name)\n",
    "    for file in files:\n",
    "        \n",
    "        if file.lower().endswith('.pdf'):\n",
    "            doc2convert = file\n",
    "            document_path_1 = os.path.join(root, file)\n",
    "            pdf_document = fitz.open(document_path_1)\n",
    "            #page_number = 0  # Defina o número da página que deseja analisar\n",
    "            #page = pdf_document[page_number]\n",
    "\n",
    "            \n",
    "            documento_pdf = True\n",
    "            pesquisavel, metadados, paginas = is_pdf_searchable_analise(document_path_1)\n",
    "            print(f'\\ndiretorio: {dir_name} nro: {i} | doc: {file} | pdf?: {documento_pdf} | pesquisavel?: {pesquisavel} | paginas: {paginas}\\n')\n",
    "            \n",
    "            pdf_realmente_pequisavel = confirma_pdf_pequisavel(document_path_1)\n",
    "            \n",
    "            print(\"pdf_realmente_pequisavel: \", pdf_realmente_pequisavel)\n",
    "            \n",
    "            if not pdf_realmente_pequisavel:\n",
    "                print(f'\\nRaster PDF diretorio: {dir_name} nro: {i} | doc: {file} | pdf?: {documento_pdf} | pesquisavel?: {pesquisavel} | paginas: {paginas}\\n')\n",
    "               \n",
    " \n",
    "            else:\n",
    "                print(f'\\nPDF Pesquisavel diretorio: {dir_name} nro: {i} | doc: {file} | pdf?: {documento_pdf} | pesquisavel?: {pesquisavel} | paginas: {paginas}\\n')\n",
    "                try:\n",
    "                    processo = \"PDF_Pesquisavel\"\n",
    "\n",
    "\n",
    "                    nro_nota, result_1, result_2, result_3, result_4, result_5, result_6, result_7, result_8, result_9, result_10 = processa_pdf_pesquisavel(document_path_1)\n",
    "                    \n",
    "                    if result_1:\n",
    "                        data_cabecalho_final.update(result_1)\n",
    "                    if result_2:\n",
    "                        data_prestador_final.update(result_2)   \n",
    "                    if result_3:\n",
    "                        data_tomador_final.update(result_3)\n",
    "                    if result_4:\n",
    "                        data_servico_final.update(result_4)\n",
    "                    if result_5:\n",
    "                        data_valor_total_final.update(result_5)\n",
    "                    if result_6:\n",
    "                        data_CNAE_final.update(result_6)\n",
    "                    if result_7:\n",
    "                        data_valores_final.update(result_7)\n",
    "                    if result_8:\n",
    "                        data_dados_complementares_final.update(result_8)                                                                   \n",
    "                    if result_9:\n",
    "                        data_outras_informacoes_final.update(result_9)   \n",
    "                    if result_10:\n",
    "                        data_observacao_final.update(result_10)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao gerar dict nf: {e}\")         \n",
    "                \n",
    "            nome_arquivo = file\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                pdf_info[nro_nota] = {\n",
    "                        \"dados_NF_PDF\": {\n",
    "                        \"data_cabecalho\": data_cabecalho_final,\n",
    "                        \"data_prestador\": data_prestador_final,\n",
    "                        \"data_tomador\": data_tomador_final,\n",
    "                        \"data_servico\": data_servico_final,\n",
    "                        \"data_valor_total\": data_valor_total_final,\n",
    "                        \"data_CNAE\": data_CNAE_final,\n",
    "                        \"data_valores\": data_valores_final,\n",
    "                        \"data_dados_complementares\": data_dados_complementares_final,\n",
    "                        \"data_outras_informacoes\": data_outras_informacoes_final,\n",
    "                        \"data_observacao\": data_observacao_final,\n",
    "                    },\n",
    "                    \"diretorio\": dir_name, #os.path.basename(root)\n",
    "                    \"nome_arquivo\": nome_arquivo,\n",
    "                    \"Batch\": batch_name,\n",
    "                    \"modelo\": model,\n",
    "                    \"pdf_realmente_pequisavel\": pdf_realmente_pequisavel,\n",
    "                    \"processo\": processo,\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao gerar o json: {e}\")\n",
    "                        #print(pdf_info) \n",
    "                    \n",
    "                                                 \n",
    "            pdf_document.close()    \n",
    "                    \n",
    "            i += 1\n",
    "            \n",
    "            print(\"i: \", i) \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "                \n",
    "\n",
    "# Nome do arquivo json\n",
    "nome_formado_json = batch_name + \"_\" + processo + \".json\"\n",
    "\n",
    "json_file_path = os.path.join(json_path, nome_formado_json )\n",
    "\n",
    "\n",
    "# Salvando as informações em um arquivo JSON (novo formato nome arquivo V2)\n",
    "with open(json_file_path, \"w\", encoding='utf-8') as json_file:\n",
    "    json.dump(pdf_info, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"As informações foram salvas em {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desenha Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_image_2work = \"pipeline_extracao_documentos/6_geral_administacao/images/processadas/Doria Marinho 0297 Raquel.pdf.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To draw everything\n",
    "trattempl. draw_box_model(model, boundaries_info, sections_info, frames_info, field_boxes_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To draw only boundaries and sections:\n",
    "draw_box_model(modelo, boundaries_info, sections_info, draw_frames=False, draw_field_boxes=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To draw only field boxes:\n",
    "draw_box_model(modelo, field_boxes_info=field_boxes_info, draw_boundaries=False, draw_sections=False, draw_frames=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nf_data_servico = {}#VERIFICAR\n",
    "# analise_doc_nf = {} #VERIFICAR\n",
    "# file_data = [] #VERIFICAR\n",
    "list_document_pages = []\n",
    "#nro_nota = 0\n",
    "# # TEMP\n",
    "# # Nome do arquivo json\n",
    "# nome_formado_json = batch_name +\".json\"\n",
    "#3. path formado para nome do arquivo json\n",
    "# json_file_path = os.path.join(json_path, nome_formado_json)\n",
    "#root_doc_analise = os.path.join(documentos_extracao_path, batch_name)\n",
    "#print(root_doc_analise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.extrai_pdf_pesquisavel as Extc\n",
    "import modules.cronometro as cron\n",
    "import modules.trata_dicionarios as tratdic\n",
    "import modules.trata_imagem as tratimg\n",
    "import modules.trata_files as tratfiles\n",
    "import modules.trata_texto_extraido as trattext\n",
    "import modules.trata_template as trattemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IMPORTANTE - NRO BATCH PARA TESTE    0 = PDF_PESQUISAVEL | 1 = RASTER_PDF\n",
    "\n",
    "i_test = 1\n",
    "\n",
    "tipo_pdf = []\n",
    "tipo_pdf.append('PDF_PESQUISAVEL')\n",
    "tipo_pdf.append('RASTER_PDF')\n",
    "tipo_pdf[i_test]\n",
    "\n",
    "\n",
    "# Tratamento do Path de ORIGEM DO DOCUMENTOS PARA TESTE QUE SERAO MOVIDOS\n",
    "list_path_test = []\n",
    "list_path_test.append(\"pipeline_extracao_documentos/4_area_testes/pdf_pesquisavel_4_test\")\n",
    "list_path_test.append(\"pipeline_extracao_documentos/4_area_testes/raster_pdf_4_test\")\n",
    "list_path_test[i_test]\n",
    "\n",
    "# Frame para teste\n",
    "i_frame = 0\n",
    "\n",
    "frames_pesquisa = []\n",
    "# Filtrar o DataFrame para incluir apenas linhas onde a coluna \"model\" oriundo de: modelo\n",
    "filtered_frames_info = frames_info[frames_info['model'] == model]\n",
    "for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "    frame_name = row_frame['label']\n",
    "    frames_pesquisa.append(frame_name)\n",
    "\n",
    "# Nome Batch\n",
    "batch_name = \"Batch_\" + str(tipo_pdf[i_test]) + \"_\" + str(i_frame)\n",
    "\n",
    "# Nome do arquivo json\n",
    "nome_formado_json = batch_name +\".json\"\n",
    "\n",
    "# Listagem dos frames de pesquisa\n",
    "i = 0\n",
    "for frame in frames_pesquisa:\n",
    "    print(f'seq ={i:>3} | {frame}')\n",
    "    i += 1\n",
    "    \n",
    "if frames_pesquisa[i_frame]:\n",
    "    print(f'\\n\\nDados do teste: batch_name: {batch_name} | frame: {frames_pesquisa[i_frame]} | model: {model} | tipo_pdf: {tipo_pdf[i_test]}')\n",
    "    \n",
    "    \n",
    "######### PATHS\n",
    "#1. path formado para busca de pdfs recursiva\n",
    "root_doc_analise = os.path.join(documentos_extracao_path, batch_name)\n",
    "\n",
    "#2. path para documentos teste RASTER PDF (ATRIBIDO DA LISTA)\n",
    "path_test_pdf = list_path_test[1]\n",
    "\n",
    "#3. path formado para nome do arquivo json\n",
    "json_file_path = os.path.join(json_path, nome_formado_json)\n",
    "\n",
    "#Listando paths utilizados\n",
    "#print(f'\\nroot_doc_analise: {root_doc_analise}\\npath_test_pdf: {path_test_pdf}\\njson_file_path: {json_file_path}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import modules.trata_dicionarios as tratdic\n",
    "\n",
    "import modules.trata_files as tratfiles\n",
    "import modules.trata_texto_extraido as trattext\n",
    "import modules.trata_template as trattemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trata files\n",
    "# 3. Ajusta o filename tirando caracteres especiais \n",
    "def conv_filename(title):\n",
    "    \n",
    "    # Divida o título em nome e extensão\n",
    "    name, extension = title.rsplit('.', 1) if '.' in title else (title, \"\")\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    # Adicione a extensão de volta, se houver\n",
    "    if extension:\n",
    "        filename += '.' + extension.lower()\n",
    "\n",
    "    return filename\n",
    "\n",
    "# 4. Ajusta o filename tirando caracteres especiais e a\n",
    "def conv_filename_no_ext(title):\n",
    "    # Divida o título em nome e extensão (mas ignore a extensão)\n",
    "    name = title.rsplit('.', 1)[0] if '.' in title else title\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    return filename  \n",
    "\n",
    "\n",
    "# move NF processadas ok\n",
    "def move_raster_pdf(document_path, raster_pdf_path, batch_name, doc2convert):\n",
    "    # Determine the destination directory\n",
    "    destination_dir = os.path.join(raster_pdf_path, batch_name)\n",
    "\n",
    "    # Check if the destination directory exists; if not, create it\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "\n",
    "    # Determine the destination path including the filename\n",
    "    destination_path = os.path.join(destination_dir, os.path.basename(document_path))\n",
    "\n",
    "    # Move the file from the source path to the destination path\n",
    "    try:\n",
    "        shutil.move(document_path, destination_path)\n",
    "        print(f\"Sucesso ao mover: {document_path} para: {destination_path}\")\n",
    "        return True, destination_path, None  # Success, destination path, no error\n",
    "    except Exception as e:\n",
    "        error_message = f\"Erro ao mover: {document_path} para: {destination_path}: {str(e)}\"\n",
    "        print(error_message)\n",
    "        return False, None, error_message  # Failure, no destination path, error message  \n",
    "    \n",
    "    \n",
    "    # 5. Verifica se PDF e pesquisavel ou nao e grava metadados dele\n",
    "def is_pdf_searchable_analise(pdf_path):\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        pages = pdf_document.page_count\n",
    "        is_searchable = all(page.get_text(\"text\") != \"\" for page in pdf_document)\n",
    "        dados_pdf = pdf_document.metadata\n",
    "        pdf_document.close()\n",
    "        return is_searchable, dados_pdf, pages\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao verificar o PDF: {e}\")\n",
    "        return False    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tables-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
