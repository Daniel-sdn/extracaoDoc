{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POC Unique_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Atual notebook com as funçoes para processamento de PDF Pesquisavel e Raster e a criaçao dos Dataframes de forma dependente e unica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt_BR.utf8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import platform\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from urllib import response\n",
    "\n",
    "from outlook_msg import Message\n",
    "import extract_msg\n",
    "import zipfile\n",
    "from pyunpack import Archive\n",
    "import py7zr\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "from fuzzywuzzy import fuzz\n",
    "import PyPDF2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "import locale\n",
    "import time, copy\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import cv2\n",
    "import fitz  # Módulo PyMuPDF\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import Image, ImageDraw\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar\n",
    "from pdf2image import convert_from_path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "# Modulos da solucao\n",
    "import modules.extrai_pdf_pesquisavel as Extc\n",
    "import modules.cronometro as cron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Config - E-mail\n",
    "# 1. Caminho do arquivo uma mensagem especifica\n",
    "msg_dir_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails'\n",
    "\n",
    "# 2. Path para arquivos atachados compactados\n",
    "msg_attachment_zip = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/13_attachments'\n",
    "\n",
    "\n",
    "#### Config - messages\n",
    "# 3. Caminho do arquivo uma mensagem especifica\n",
    "msg_outros_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/12_messages'\n",
    "\n",
    "# 4. Path para arquivos recebidos manualmente\n",
    "arquivos_recebidos_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/14_documentos_recebidos'\n",
    "\n",
    "\n",
    "####Config Processamento Pipeline\n",
    "\n",
    "# 5. Path para documentos para extracao\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "# 6. Path para gestao de imagens resized\n",
    "image_resized_path = \"pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas\"\n",
    "\n",
    "\n",
    "# 7. Path para DFs e CSVs exportados\n",
    "export_path = \"pipeline_extracao_documentos/6_geral_administacao/exports\"\n",
    "\n",
    "# 8. Path para lixeira\n",
    "root_garbage_path = \"pipeline_extracao_documentos/0_lixeira\"\n",
    "\n",
    "\n",
    "\n",
    "# 12. poppler path\n",
    "poppler_path = \"/home/dani-boy/miniconda3/envs/tables-detr/bin\"\n",
    "\n",
    "# 13. path para config Tesseract\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "#Modelo atual\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "# definindo localizadcao para pt_BR\n",
    "locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. POC Unique_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1 - Funcoes para e-mail e extracao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch</th>\n",
       "      <th>Data</th>\n",
       "      <th>File</th>\n",
       "      <th>Type</th>\n",
       "      <th>Level</th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Parent_Unique_ID</th>\n",
       "      <th>Hash</th>\n",
       "      <th>File_Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Batch, Data, File, Type, Level, Unique_ID, Parent_Unique_ID, Hash, File_Path]\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Geraçao do hash do arquivo\n",
    "def generate_file_hash(file_path):\n",
    "    # Abre o arquivo em modo de leitura de bytes\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # Lê o conteúdo do arquivo\n",
    "        file_data = f.read()\n",
    "        # Utiliza o algoritmo SHA-256 para gerar o hash\n",
    "        file_hash = hashlib.sha256(file_data).hexdigest()\n",
    "    return file_hash\n",
    "\n",
    "# 2. Geraçao do Unique_id do arquivo\n",
    "def generate_unique_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# 3. XXX Busca proximo Batch\n",
    "def busca_proximo_batch():\n",
    "    # Abre o arquivo Excel e lê a coluna 'batch'\n",
    "    df = pd.read_excel(\"pipeline_extracao_documentos/6_geral_administacao/exports/df_documento_recebido.xlsx\", usecols=[\"batch\"])\n",
    "    # Pega o último valor da coluna 'batch'\n",
    "    last_value = df.iloc[-1, 0]\n",
    "    \n",
    "    # Extraí o número do último batch e adiciona 1 para o próximo\n",
    "    last_number = int(last_value.split(\"_\")[1])\n",
    "    next_number = last_number + 1\n",
    "    \n",
    "    # Forma o nome do próximo batch\n",
    "    next_batch = f\"Batch_{next_number}\"\n",
    "    \n",
    "    return next_batch    \n",
    "\n",
    "# 4. Função para verificar e criar a pasta se não existir\n",
    "def check_and_create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        \n",
    "# 5. converte nome do arquivo\n",
    "def conv_filename(title):\n",
    "    \n",
    "    # Divide o título em nome e extensão\n",
    "    name, extension = title.rsplit('.', 1) if '.' in title else (title, \"\")\n",
    "\n",
    "    # Remove acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substiti espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remove quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converte para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    # Adiciona a extensão de volta, se houver\n",
    "    if extension:\n",
    "        filename += '.' + extension.lower()\n",
    "\n",
    "    return filename\n",
    "\n",
    "# 6. converte nome do arquivo retirando extensao\n",
    "def conv_filename_no_ext(title):\n",
    "    \n",
    "    # Divida o título em nome e extensão (mas ignore a extensão)\n",
    "    name = title.rsplit('.', 1)[0] if '.' in title else title\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    return filename         \n",
    "\n",
    "# 7. funçao que MOVE documentos e gera add_log_transaction_entry para df_log_transctions\n",
    "def move_doc_processed_file(batch_name, src_path, tgt_path):\n",
    "    \n",
    "    function = \"move_doc_processed_file\"\n",
    "    source_path = src_path\n",
    "    file = os.path.basename(source_path)\n",
    "    sub_dir = os.path.join(tgt_path, batch_name)\n",
    "    destination_path = os.path.join(sub_dir, file)\n",
    "    document_action = \"move_processed_file\"\n",
    "    transaction_detail = (f'document {file} moved by: {function}')\n",
    "    df_move = pd.DataFrame()\n",
    "    try:\n",
    "        document_unique_id = get_document_id_by_file(batch_name, file)\n",
    "        check_and_create_folder(destination_path)\n",
    "        shutil.move(source_path, destination_path)\n",
    "        sucess = True\n",
    "        move_log = add_log_transaction_entry(document_unique_id, batch_name, file, document_action, src_path, tgt_path, transaction_detail, sucess)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao mover documento: {e}\")\n",
    "        sucess = False\n",
    "    \n",
    "    return move_log    \n",
    "\n",
    "# 8. Função para adicionar um novo registro em df_source\n",
    "def add_source_entry(batch_name, file_path, file, type, level, parent_unique_id):\n",
    "    #unique_id = generate_unique_id(type)\n",
    "    unique_id = generate_unique_id()\n",
    "    time_now = cron.timenow_pt_BR()   \n",
    "    file_hash = generate_file_hash(file_path) \n",
    "    if level == 1:\n",
    "        parent_unique_id = unique_id\n",
    "    data = {\n",
    "        'Batch': batch_name,\n",
    "        'Data': time_now,\n",
    "        'File': file,\n",
    "        'Type': type,\n",
    "        'Level': level,\n",
    "        'Unique_ID': unique_id,\n",
    "        'Parent_Unique_ID': parent_unique_id,\n",
    "        'Hash': file_hash,\n",
    "        'File_Path': file_path\n",
    "    }\n",
    "    return data\n",
    "\n",
    "# 9. Add nova linha para atualizar df_log_transctions\n",
    "def add_log_transaction_entry(document_unique_id,batch_name, file, document_action, src_path, tgt_path, transaction_detail, sucess=True):\n",
    "\n",
    "    data_log = {\n",
    "        'Dt_Time': cron.timenow_pt_BR(),\n",
    "        'Batch': batch_name,\n",
    "        'File' : file,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Action': document_action,\n",
    "        'Sorce': src_path,\n",
    "        'Target': tgt_path,\n",
    "        'Transction_Detail': transaction_detail,\n",
    "        'Sucess': sucess,    \n",
    "    }\n",
    "    \n",
    "        \n",
    "    return data_log\n",
    "\n",
    "\n",
    "# 10. Consulta multiparametros\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# 12. Busca filhos - simples\n",
    "def get_children(batch, file_path):\n",
    "    \n",
    "    file = os.path.basename(file_path)\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    # Buscando todos dos filhos de um documento\n",
    "    return filtrar_df(df_id_relations, Batch=batch, Parent_Unique_ID=document_unique_id)\n",
    "\n",
    "\n",
    "# 13. Busca pai -simples\n",
    "def get_father(batch, file_path):\n",
    "    \n",
    "    file = os.path.basename(file_path)\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    # Buscando todos dos filhos de um documento\n",
    "    return filtrar_df(df_id_relations, Batch=batch, Unique_ID=parent_unique_id)\n",
    "\n",
    "\n",
    "# 14. Pesquiso pai pelo Unique_ID e trago um dict\n",
    "def get_father_data_by_children_file(batch, file):\n",
    "    \n",
    "    src_result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    src_parent_unique_id = src_result['Parent_Unique_ID'].values[0]\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=src_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }\n",
    "\n",
    "\n",
    "# 15. Pesquiso pai pelo Unique_ID (document_parent_unique_id) e cria DICT\n",
    "def get_father_by_unique_id(batch, document_parent_unique_id):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=document_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }\n",
    "    \n",
    "# 16. Pesquiso pai pelo file do filho e cria DICT\n",
    "def get_father_data_by_children_file(batch, file):\n",
    "    \n",
    "    src_result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    src_parent_unique_id = src_result['Parent_Unique_ID'].values[0]\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=src_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }        \n",
    "        \n",
    "\n",
    "# 17. Busca o 'Unique_ID' para definir o Parent_Unique_ID sem considerar 'Level'\n",
    "def get_parent_unique_id(df_id_relations, batch_name, file, type):\n",
    "    try:\n",
    "        parent_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type)]['Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        parent_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return parent_unique_id\n",
    "\n",
    "\n",
    "# 18. funcao para trazer somente o 'Unique_ID'\n",
    "def get_document_unique_id(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)]['Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        document_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# 19. funcao para trazer somente o 'Parent_Unique_ID'\n",
    "def get_document_parent_unique_id(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_parent_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)]['Parent_Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        document_parent_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_parent_unique_id\n",
    "\n",
    "\n",
    "# 20. funçao para trazer toda a row de df_id_relations para o documento\n",
    "def get_document_id_relations(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_id_relations = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)].values[0]\n",
    "    except IndexError:\n",
    "        document_id_relations = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_id_relations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# EXEMPLOS de Pesquisa DFss\n",
    "    # get_document_unique_id(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "    # # Busca somente o 'Parent_Unique_ID'\n",
    "    # get_document_parent_unique_id(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "\n",
    "    # #Busca todos os dados da row do documento encontrado\n",
    "    # document_id_relations = get_document_id_relations(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "    # document_batch = document_id_relations[0]\n",
    "    # document_date = document_id_relations[1]\n",
    "    # document_name = document_id_relations[2]\n",
    "    # document_type = document_id_relations[3]\n",
    "    # document_level = document_id_relations[4]\n",
    "    # document_unique_id = document_id_relations[5]\n",
    "    # document_parent_unique_id = document_id_relations[6]\n",
    "    # document_hash = document_id_relations[7]\n",
    "    # document_path = document_id_relations[8]\n",
    "\n",
    "    # # Insercao de um registro pela func add_source_entry\n",
    "    # file_path = \"pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/SPA 15082023.rar\"\n",
    "\n",
    "    # file = os.path.basename(file_path)\n",
    "\n",
    "    # type = \"compressed_file_attachment\"\n",
    "\n",
    "    # level = 1\n",
    "\n",
    "    # parent_unique_id = ''\n",
    "\n",
    "    # # Adicionando um novo registro (substitua 'batch_name' e 'email' conforme necessário)\n",
    "    # new_entry = add_source_entry(batch_name, file_path, file, type, level, parent_unique_id)\n",
    "\n",
    "    # df_id_relations = df_id_relations.append(new_entry, ignore_index=True)\n",
    "\n",
    "    # df_id_relations\n",
    "\n",
    "\n",
    "# Busca proximo Batch caso nao esteja rodando email\n",
    "batch_name = busca_proximo_batch()\n",
    "\n",
    "# 1. Criaçao do DataFrame para armazenar as relações de Unique_ID e Parent_Unique_ID\n",
    "df_id_relations = pd.DataFrame(columns=['Batch', 'Data' ,'File', 'Type', 'Level', 'Unique_ID', 'Parent_Unique_ID', 'Hash', 'File_Path'])\n",
    "\n",
    "# 2. Criaçao do DataFrame para df_start_pipe:\n",
    "#df_start_pipe = pd.DataFrame(columns=['Batch', 'Data' ,'File', 'Type', 'Level', 'Unique_ID', 'dt_hora', 'de', 'assunto', 'email', 'Hash'])\n",
    "\n",
    "df_id_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executar a demanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1A - XXX Função para processar o e-mail individualmente: df_mail\n",
    "def process_start_input(df_id_relations, msg, msg_type, batch_name, file_path, first_doc_file):\n",
    "    \n",
    "    file = first_doc_file\n",
    "    type = msg_type\n",
    "    if type != \"email\":\n",
    "        msg_de = msg['de']\n",
    "        msg_assunto = msg['assunto']\n",
    "        msg_email = msg['email']\n",
    "        msg_type = msg['msg_type']\n",
    "        orig_dt_time = msg['dt_hora']\n",
    "        print(f'\\norig_dt_time: {orig_dt_time}')\n",
    "        msg_date_time = cron.convert_email_date(orig_dt_time)\n",
    "    else:\n",
    "        msg_de = msg.sender.split('<')[0].strip()\n",
    "        msg_assunto = msg.subject\n",
    "        msg_email = msg.sender.split('<')[-1].strip('<>')\n",
    "        msg_type = type\n",
    "        original_date_str = msg.date\n",
    "        msg_date_time = cron.convert_email_date(original_date_str)\n",
    "        #print(f'original_date_str: {original_date_str} | msg_date_time: {msg_date_time}')\n",
    "        \n",
    "    level = 1\n",
    "    parent_unique_id = ''\n",
    "    new_entry = add_source_entry(batch_name, file_path, file, type, level, parent_unique_id)\n",
    "    df = pd.DataFrame()\n",
    "    df_trans_relation = pd.DataFrame()\n",
    "    df = df.append(new_entry, ignore_index=True)\n",
    "    df_trans_relation = df\n",
    "    #print(df_trans_relation)\n",
    "    \n",
    "    # Busca valores processados de new_entry\n",
    "    unique_id = new_entry['Unique_ID']\n",
    "    file_hash = new_entry['Hash']\n",
    "    data_process = new_entry['Data']\n",
    "    \n",
    "    return {\n",
    "        'Batch': batch_name,\n",
    "        'Data' : data_process,\n",
    "        'File': first_doc_file,\n",
    "        'Type': type,\n",
    "        'Level': level,\n",
    "        'Unique_Id': unique_id,\n",
    "        'dt_hora': msg_date_time,\n",
    "        'de': msg_de,\n",
    "        'assunto': msg_assunto,\n",
    "        'email': msg_email,\n",
    "        'Hash': file_hash\n",
    "    }, new_entry, df_trans_relation \n",
    "\n",
    "# 1B - Função principal para processar e-mails\n",
    "def start_pipe_email(df_id_relations, msg_dir_path, msg_attachment_zip):\n",
    "    locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")\n",
    "    rows_list = []\n",
    "    \n",
    "    check_and_create_folder(msg_attachment_zip)\n",
    "    \n",
    "    for root, dirs, files in os.walk(msg_dir_path):\n",
    "        doc_files = [doc for doc in files if doc.lower().endswith(\".msg\")]\n",
    "        if doc_files:\n",
    "            first_doc_file = doc_files[0]\n",
    "            file_path = os.path.join(root, first_doc_file)\n",
    "            msg_type = \"email\"\n",
    "            msg = extract_msg.Message(file_path)\n",
    "            new_row, new_entry, df_trans_relation = process_start_input(df_id_relations, msg, msg_type, batch_name, file_path, first_doc_file)\n",
    "            rows_list.append(new_row)\n",
    "            # try:\n",
    "\n",
    "                \n",
    "            # except Exception as e:\n",
    "            #     print(f\"Erro ao ler email: {e}\")\n",
    "    \n",
    "    \n",
    "    df_start_pipe_email = pd.DataFrame(rows_list)\n",
    "    \n",
    "    pasta_destino = msg_attachment_zip\n",
    "    \n",
    "    check_and_create_folder(pasta_destino)\n",
    "    \n",
    "    with open(file_path) as msg_file:\n",
    "        msg = Message(msg_file)\n",
    "    \n",
    "    total_attch = len(msg.attachments)\n",
    "    \n",
    "    i = 0\n",
    "    # Loop para salvar cada anexo\n",
    "    for i in range(total_attch):\n",
    "        attachment = msg.attachments[i]\n",
    "        caminho_completo_anexo = os.path.join(pasta_destino, attachment.filename)\n",
    "\n",
    "        #print(caminho_completo_anexo)\n",
    "        with attachment.open() as attachment_fp, open(caminho_completo_anexo, 'wb') as output_fp:\n",
    "            output_fp.write(attachment_fp.read())            \n",
    "    \n",
    "    return df_start_pipe_email, new_entry, df_trans_relation\n",
    "\n",
    "# 1C - XXX Funcao para iniciar o pipeline atraves de outros documentos de entrada\n",
    "def start_pipe_outros(df_id_relations, msg_outros, msg_outros_path, arquivos_recebidos_path):\n",
    "    locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")\n",
    "    rows_list = []\n",
    "    \n",
    "\n",
    "    for root, dirs, files in os.walk(msg_outros_path):\n",
    "        doc_outros_files = [doc for doc in files]\n",
    "        if doc_outros_files:\n",
    "            first_doc_outros_file = doc_outros_files[0]\n",
    "            file_path = os.path.join(root, first_doc_outros_file)\n",
    "            msg = msg_outros\n",
    "            msg_type = msg['msg_type']\n",
    "            first_doc_file = first_doc_outros_file \n",
    "            new_row, new_entry, df_trans_relation = process_start_input(df_id_relations, msg, msg_type, batch_name, file_path, first_doc_file)\n",
    "            rows_list.append(new_row)\n",
    "\n",
    "    \n",
    "    df_start_pipe_outros = pd.DataFrame(rows_list)\n",
    "                \n",
    "    return df_start_pipe_outros, new_entry, df_trans_relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando registro de mensagem outras\n",
    "msg_type = \"mensagem_whats\"\n",
    "\n",
    "msg_outros = {\n",
    "       'de': 'Daniel Nascimento',\n",
    "       'dt_hora': 'qui, 10 ago 2023 23:20:56 -0300',\n",
    "       'assunto': 'Teste rar',\n",
    "       'email':'danielsdn0725@gmail.com',\n",
    "       'msg_type': msg_type \n",
    "}\n",
    "msg_outros\n",
    "\n",
    "\n",
    "# 1C.1 - Iniciando o Pipeline para outras mensagens\n",
    "df_start_pipe_outros, new_entry, df_trans_relation = start_pipe_outros(df_id_relations, msg_outros, msg_outros_path, arquivos_recebidos_path)\n",
    "try:\n",
    "    if not df_start_pipe.empty:\n",
    "        df_start_pipe = df_start_pipe.append(df_start_pipe_outros, ignore_index=True)\n",
    "        \n",
    "        print(df_start_pipe)\n",
    "except Exception as e:\n",
    "        msg = (f\"Erro ao buscar o df_start_pipe: {e}\") \n",
    "        df_start_pipe = df_start_pipe_outros \n",
    "\n",
    "# Atualizando: df_id_relations\n",
    "df_id_relations = df_id_relations.append(df_trans_relation, ignore_index=True)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1B.1 - Criando registros para documento email\n",
    "df_start_pipe_email, new_entry, df_trans_relation = start_pipe_email(df_id_relations, msg_dir_path, msg_attachment_zip)\n",
    "try:\n",
    "    if not df_start_pipe.empty:\n",
    "        df_start_pipe = df_start_pipe.append(df_start_pipe_email, ignore_index=True)\n",
    "        \n",
    "        print(df_start_pipe)\n",
    "except Exception as e:\n",
    "        msg = (f\"Erro ao buscar o df_start_pipe: {e}\") \n",
    "        # 1B.1 - Iniciando o Pipeline para e-mail\n",
    "        df_start_pipe = df_start_pipe_email\n",
    "        \n",
    "# Atualizando: df_id_relations\n",
    "df_id_relations = df_id_relations.append(df_trans_relation, ignore_index=True)\n",
    "df_id_relations \n",
    "\n",
    "df_start_pipe             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2A - XXX Trata Attachments: df_attachment\n",
    "def process_files(df_id_relations, i, parent_entry, batch_name, file, type, level, file_path):\n",
    "    \n",
    "    file_parent_name = parent_entry['File']\n",
    "    file_parent_type = parent_entry['Type']\n",
    "    \n",
    "    parent_unique_id = get_parent_unique_id(df_id_relations, batch_name, file_parent_name, file_parent_type)\n",
    "    #print(parent_unique_id)\n",
    "    \n",
    "    data = add_source_entry(batch_name, file_path, file, type, level, parent_unique_id)\n",
    "    #df_trans_relation = df_id_relations.append(new_entry, ignore_index=True)\n",
    "    \n",
    "   # Busca valores processados de new_entry\n",
    "    unique_id = data['Unique_ID']\n",
    "    file_hash = data['Hash']\n",
    "    data_process = data['Data']\n",
    "    print(f'i: {i} | file: {file} | unique_id: {unique_id} ')\n",
    "    document_action = \"transfered_to_pipeline\"\n",
    "    sucess = True\n",
    "    transaction_detail=\" \"\n",
    "    src_path=\"\"\n",
    "   \n",
    "    data_log = add_log_transaction_entry(unique_id, batch_name, file, document_action, src_path, file_path, transaction_detail, sucess)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'index': i, \n",
    "        'Batch': batch_name,\n",
    "        'Data' : data_process,\n",
    "        'File': file,\n",
    "        'Type': type,\n",
    "        'Level': level,\n",
    "        'Unique_Id': unique_id,\n",
    "        'Parent_Unique_ID': parent_unique_id,\n",
    "        'Hash': file_hash,\n",
    "        'File_Path': file_path\n",
    "    }, data, data_log \n",
    "   \n",
    "      \n",
    "# 2B - Função principal para processar e-mails\n",
    "def attachment_pipe(df_id_relations, parent_entry, msg_attachment_path):\n",
    "\n",
    "    folder_file_dict = {}\n",
    "    rows_list = []\n",
    "    data_list = []\n",
    "    log_list = []\n",
    "    attachment_list = []\n",
    "    arquivos_zip = []\n",
    "    arquivos = []\n",
    "\n",
    "    i = 1\n",
    "    for root, dirs, files in os.walk(msg_attachment_path):\n",
    "        #print(f'{root}  | {dirs} | {document} | {files}\\n')\n",
    "        for file in files:\n",
    "            level = 2\n",
    "            file_path = os.path.join(root, file)\n",
    "            if file.lower().endswith('.zip') or file.lower().endswith('.rar') or file.lower().endswith('.7z'):\n",
    "                type = 'compressed_file_attachment'\n",
    "                arquivos_zip.append(file)\n",
    "            else:\n",
    "                type = 'document_file_attachment'\n",
    "                arquivos.append(file)\n",
    "            \n",
    "            \n",
    "            new_row, data, data_log  = process_files(df_id_relations, i, parent_entry, batch_name, file, type, level, file_path)\n",
    "            rows_list.append(new_row)\n",
    "            data_list.append(data) \n",
    "            log_list .append(data_log)\n",
    "            #print(f'\\ni = {i}')   \n",
    "        \n",
    "            i += 1            \n",
    "\n",
    "    df_attachment = pd.DataFrame(rows_list)\n",
    "    df_trans_relation = pd.DataFrame(data_list)        \n",
    "    df_log_tran = pd.DataFrame(log_list)\n",
    "\n",
    "    return df_attachment, df_trans_relation, df_log_tran, attachment_list, rows_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2C - Ajusto parent_entry para separar as instancias\n",
    "parent_entry = new_entry\n",
    "\n",
    "if new_entry['Type'] == \"email\":\n",
    "    print(\"Estamos no caminho\")\n",
    "    msg_attachment_path = msg_attachment_zip\n",
    "else:\n",
    "    print(\"Outra mensagem\")\n",
    "    msg_attachment_path = arquivos_recebidos_path  \n",
    "\n",
    "\n",
    "msg_attachment_path    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executo func attachment_pipe\n",
    "df_attachment, df_trans_relation, attachment_list, rows_list = attachment_pipe(df_id_relations, parent_entry, msg_attachment_path)\n",
    "df_attachment\n",
    "\n",
    "\n",
    "\n",
    "# 2D - Atualizo: df_id_relations\n",
    "df_id_relations = df_id_relations.append(df_trans_relation, ignore_index=True)\n",
    "df_id_relations\n",
    "\n",
    "\n",
    "# Salvando o DF para excel\n",
    "df_id_relations.to_excel('df_id_relations.xlsx', index=False)\n",
    "df_start_pipe.to_excel('df_start_pipe.xlsx', index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. IMPORTANTE - Funcao de Extracao ZIP\n",
    "def extract_doc_zip(trg_documento_path, documentos_extracao_path, batch_name):\n",
    "    \n",
    "    output_dir = os.path.join(documentos_extracao_path, batch_name)\n",
    "    \n",
    "    zip_info = {}\n",
    "    arquivos_zip = []\n",
    "    i = 1\n",
    "    for root, dirs, files in os.walk(trg_documento_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            level = 2\n",
    "            if file.lower().endswith('.zip'):\n",
    "                zip_file = file\n",
    "                zip_file_path = os.path.join(root, file)\n",
    "                file_type = 'compressed_file_attachment'\n",
    "                print(f'\\n1. Antes de buscar  | {batch_name} | file: {file} | file_type: {file_type} | level: {level}')\n",
    "                l_parent_unique_id = get_document_unique_id(df_id_relations, batch_name, file, file_type, level)\n",
    "                if l_parent_unique_id:\n",
    "                    zip_base_name = os.path.splitext(os.path.basename(zip_file_path))[0]\n",
    "                    # Cria o subdiretório com base no nome do arquivo ZIP\n",
    "                    root_output_dir = os.path.join(output_dir, zip_base_name)\n",
    "                    check_and_create_folder(root_output_dir)\n",
    "                    # Abre o arquivo ZIP\n",
    "                    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                        for member in zip_ref.namelist():\n",
    "                            # Separa o nome da pasta e o nome do arquivo usando barra invertida como delimitador\n",
    "                            parts = member.rsplit('\\\\', 1)\n",
    "                            folder_name = parts[0] if len(parts) > 1 else ''\n",
    "                            #folder_name = conv_filename(folder_temp)\n",
    "                            filename = parts[-1]\n",
    "                            if filename:  \n",
    "                                # Cria um subdiretório se ele não existir\n",
    "                                sub_dir = os.path.join(root_output_dir, folder_name)\n",
    "                                sub_dir_path = os.path.normpath(sub_dir)\n",
    "                                #print(f'sub_dir:  {sub_dir} | folder_name: {folder_name}')\n",
    "                                check_and_create_folder(sub_dir)\n",
    "                                # Salva o arquivo no subdiretório especificado\n",
    "                                source = zip_ref.open(member)\n",
    "                                target_path = os.path.join(sub_dir, filename)\n",
    "                                # dir_path = arquivo['diretorio']\n",
    "                                diretorio = os.path.basename(sub_dir_path)\n",
    "                                # Verifica se a chave já existe no dicionário, senão, cria uma nova chave com uma lista como valor\n",
    "                                if diretorio not in zip_info:\n",
    "                                    zip_info[diretorio] = {\"data\": []}\n",
    "                                zip_info[diretorio][\"data\"].append({\n",
    "                                    \"nome\": os.path.basename(filename),\n",
    "                                    \"l_parent_unique_id\": l_parent_unique_id,\n",
    "                                    \"caminho\": target_path,\n",
    "                                })\n",
    "                                with open(target_path, \"wb\") as target:\n",
    "                                    target.write(source.read())\n",
    "                                    \n",
    "    arquivos_zip.append(zip_info)\n",
    "    return arquivos_zip \n",
    "\n",
    "\n",
    "# 2. IMPORTANTE, Extract RAR\n",
    "def extract_doc_RAR(trg_documento_path, documentos_extracao_path, batch_name):\n",
    "\n",
    "    output_dir = os.path.join(documentos_extracao_path, batch_name)\n",
    "    # Inicialize um dicionário vazio antes de entrar no loop\n",
    "    rar_info = {}\n",
    "    arquivos_rar = []\n",
    "\n",
    "    i = 1\n",
    "    for root, dirs, files in os.walk(msg_attachment_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            level = 2\n",
    "            if file.lower().endswith('.rar'):\n",
    "                file_type = 'compressed_file_attachment'\n",
    "                rar_file = file\n",
    "                l_parent_unique_id = get_document_unique_id(df_id_relations, batch_name, file, file_type, level)\n",
    "                if l_parent_unique_id:\n",
    "                    rar_file = conv_filename_no_ext(rar_file)\n",
    "                    rar_file_path = os.path.join(root, file)\n",
    "                    root_output_dir = os.path.join(output_dir, rar_file)\n",
    "                    check_and_create_folder(root_output_dir)\n",
    "                    Archive(rar_file_path).extractall(root_output_dir)\n",
    "                    # Criar um dicionário para armazenar informações sobre os arquivos extraídos\n",
    "                    rar_info = {rar_file: {\"data\": []}}\n",
    "                    # Listar todos os arquivos no diretório de saída\n",
    "                    for subdir, _, files in os.walk(root_output_dir):\n",
    "                        for filename in files:\n",
    "                            # Obter o caminho completo do arquivo\n",
    "                            full_path = os.path.join(subdir, filename)\n",
    "                            # Obter o nome do diretório\n",
    "                            diretorio = os.path.relpath(subdir, root_output_dir)\n",
    "                            # Adicionar informações sobre o arquivo ao dicionário\n",
    "                            rar_info[rar_file][\"data\"].append({\n",
    "                                \"nome\": filename,\n",
    "                                \"caminho\": full_path,\n",
    "                                \"l_parent_unique_id\": l_parent_unique_id,\n",
    "                                \"diretorio\": diretorio,\n",
    "                            })\n",
    "    arquivos_rar.append(rar_info)\n",
    "    return arquivos_rar \n",
    "\n",
    "\n",
    "# 3. IMPORTANTE, Extract SEVENZ\n",
    "def extract_doc_SEVENZ(trg_documento_path, documentos_extracao_path, batch_name):\n",
    "\n",
    "    output_dir = os.path.join(documentos_extracao_path, batch_name)\n",
    "\n",
    "    # Inicialize um dicionário vazio antes de entrar no loop\n",
    "    sevenz_info = {}\n",
    "    arquivos_7z = []\n",
    "\n",
    "    i = 1\n",
    "    for root, dirs, files in os.walk(msg_attachment_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            level = 2\n",
    "            if file.lower().endswith('.7z'):\n",
    "                file_type = 'compressed_file_attachment'\n",
    "                sevenz_file = file\n",
    "                l_parent_unique_id = get_document_unique_id(df_id_relations, batch_name, file, file_type, level)\n",
    "                if l_parent_unique_id:\n",
    "                    sevenz_file = conv_filename_no_ext(sevenz_file)\n",
    "                    sevenz_file_path = os.path.join(root, file)\n",
    "                    root_output_dir = os.path.join(output_dir, sevenz_file)\n",
    "                    diretorio = os.path.basename(root_output_dir)\n",
    "                    #check_and_create_folder(root_output_dir)\n",
    "                    sevenz_info = {sevenz_file: {\"data\": []}}                      \n",
    "                    with py7zr.SevenZipFile(sevenz_file_path, mode='r') as z:\n",
    "                        z.extractall(root_output_dir)\n",
    "                    #print(f'sevenz_info: {sevenz_info} | diretorio: {diretorio}\\nroot_output_dir: {root_output_dir}')          \n",
    "                    for subdir, _, files in os.walk(root_output_dir):\n",
    "                        for filename in files:\n",
    "                            print(filename)\n",
    "                            # Obter o caminho completo do arquivo\n",
    "                            full_path = os.path.join(subdir, filename)\n",
    "                            # Obter o nome do diretório\n",
    "                            diretorio = os.path.relpath(subdir, root_output_dir)\n",
    "                            # Adicionar informações sobre o arquivo ao dicionário\n",
    "                            sevenz_info[sevenz_file][\"data\"].append({\n",
    "                                \"nome\": filename,\n",
    "                                \"caminho\": full_path,\n",
    "                                \"l_parent_unique_id\": l_parent_unique_id,\n",
    "                                \"diretorio\": diretorio,\n",
    "                            })\n",
    "                i += 1\n",
    "    arquivos_7z.append(sevenz_info)\n",
    "    return arquivos_7z\n",
    "\n",
    "\n",
    "# 4. IMPORTANTE, Extrac/Move PDF\n",
    "def extr_move_doc_PDF(trg_documento_path, documentos_extracao_path, batch_name):\n",
    "\n",
    "    output_dir = os.path.join(documentos_extracao_path, batch_name)\n",
    "    # Inicialize um dicionário vazio antes de entrar no loop\n",
    "    PDF_info = {}\n",
    "    arquivos_pdf = []\n",
    "    i = 1\n",
    "    for root, dirs, files in os.walk(msg_attachment_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            level = 2\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                file_type = 'document_file'\n",
    "                pdf_file = file\n",
    "                file_path = os.path.join(root, file)\n",
    "                new_path_name = os.path.join(output_dir, file)\n",
    "                diretorio = os.path.basename(file_path)\n",
    "                path_relativo = file_path.replace(msg_attachment_path, \"\").lstrip(\"/\")\n",
    "                path_relativo_base = os.path.basename(path_relativo)\n",
    "                path_relativo_dir = path_relativo.replace(path_relativo_base, \"\").lstrip(\"/\")\n",
    "                #check_and_create_folder(output_dir)\n",
    "                try:\n",
    "                    if path_relativo_dir:\n",
    "                        diretorio = os.path.relpath(path_relativo_dir)\n",
    "                        new_base_path = os.path.join(output_dir, diretorio)\n",
    "                        new_path_name = os.path.join(new_base_path, file)\n",
    "                        #print(f'\\nTRUE: diretorio composto: {diretorio}\\n | path_relativo_dir: {path_relativo_dir} | new_base_path_file: {new_base_path}\\nnew_path_name: {new_path_name}\\n')\n",
    "                    else:\n",
    "                        diretorio = \"root\"\n",
    "                        #shutil.move(file_path, new_path_name)\n",
    "                        #print(f'\\nFALSE: diretorio: {diretorio} | file_path: {file_path}\\nnew_path_name: {new_path_name}\\n')\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao definir paths: {e}\") \n",
    "                if diretorio not in PDF_info:\n",
    "                    PDF_info[diretorio] = {\"data\": []}\n",
    "                    \n",
    "                PDF_info[diretorio][\"data\"].append({\n",
    "                    \"nome\": pdf_file,\n",
    "                    #\"l_parent_unique_id\": l_parent_unique_id,\n",
    "                    \"caminho\": new_path_name,\n",
    "                })\n",
    "    arquivos_pdf.append(PDF_info) \n",
    "    return arquivos_pdf                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE, Chamando a funcao para extracao de doc ZIP\n",
    "extracted_docs = extract_doc_zip(msg_attachment_path, documentos_extracao_path, batch_name) \n",
    "\n",
    "extracted_docs = extract_doc_RAR(msg_attachment_path, documentos_extracao_path, batch_name)\n",
    "\n",
    "extracted_docs = extract_doc_SEVENZ(msg_attachment_path, documentos_extracao_path, batch_name)\n",
    "\n",
    "extracted_docs = extr_move_doc_PDF(msg_attachment_path, documentos_extracao_path, batch_name)\n",
    "\n",
    "\n",
    "# Chama funçao para atualizar DFs\n",
    "if extracted_docs:\n",
    "    df_extract, df_trans_relation, df_tran_log, rows_list = extract_pipe(df_id_relations, extracted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3A - XXX Trata documentos extraidos: df_extract (Unider Construction)\n",
    "def process_extract_files(df_id_relations, i, l_parent_unique_id, batch_name, file, diretorio, type, level, file_path):\n",
    "    \n",
    "\n",
    "    #parent_unique_id = get_parent_unique_id(df_id_relations, batch_name, file_parent_name, file_parent_type)\n",
    "    #print(parent_unique_id)\n",
    "    parent_unique_id = l_parent_unique_id\n",
    "    data = add_source_entry(batch_name, file_path, file, type, level, parent_unique_id)\n",
    "    #df_trans_relation = df_id_relations.append(new_entry, ignore_index=True)\n",
    "    \n",
    "   # Busca valores processados de new_entry\n",
    "    unique_id = data['Unique_ID']\n",
    "    file_hash = data['Hash']\n",
    "    data_process = data['Data']\n",
    "    print(f'i: {i} | file: {file} | unique_id: {unique_id} ')\n",
    "    \n",
    "    document_action = \"process_topipeline\"\n",
    "    sucess=True\n",
    "    transaction_detail=\" \"\n",
    "    src_path=\" \"\n",
    "   \n",
    "    data_log = add_log_transaction_entry(unique_id, batch_name, file, document_action, src_path, file_path, transaction_detail, sucess)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'index': i, \n",
    "        'Batch': batch_name,\n",
    "        'Data' : data_process,\n",
    "        'Directory': diretorio,\n",
    "        'File': file,\n",
    "        'Type': type,\n",
    "        'Level': level,\n",
    "        'Unique_Id': unique_id,\n",
    "        'Parent_Unique_ID': parent_unique_id,\n",
    "        'Hash': file_hash,\n",
    "        'File_Path': file_path\n",
    "    }, data, data_log\n",
    "       \n",
    "# 3B - Função principal para processar e-mails\n",
    "def extract_pipe(df_id_relations, arquivos_zip):\n",
    "    \n",
    "    data_list = []\n",
    "    rows_list = []\n",
    "    log_list = []\n",
    "    i = 1\n",
    "    for zip_info in arquivos_zip:\n",
    "        for diretorio, dados in zip_info.items():\n",
    "            print(f'Diretório (ZIP): {diretorio}\\n')\n",
    "            for arquivo in dados['data']:\n",
    "                # print(f'    Arquivo: {arquivo[\"nome\"]}')\n",
    "                # print(f'    {arquivo[\"caminho\"]}\\n\\n')\n",
    "                file_path = arquivo[\"caminho\"]\n",
    "                file = arquivo[\"nome\"]\n",
    "                l_parent_unique_id = arquivo[\"l_parent_unique_id\"]\n",
    "                \n",
    "                level = 3\n",
    "                type = 'document_file'\n",
    "                new_row, data, data_log = process_extract_files(df_id_relations, i, l_parent_unique_id, batch_name, file, diretorio, type, level, file_path)\n",
    "                rows_list.append(new_row)\n",
    "                data_list.append(data)\n",
    "                log_list.append(data_log) \n",
    "                #print(f'\\ni = {i}')   \n",
    "        \n",
    "                i += 1            \n",
    "    df_tran_log = pd.DataFrame(log_list)\n",
    "    df_extract = pd.DataFrame(rows_list)\n",
    "    df_trans_relation = pd.DataFrame(data_list)        \n",
    "\n",
    "\n",
    "    return df_extract, df_trans_relation, df_tran_log, rows_list\n",
    "\n",
    "\n",
    "# Teremos que olhar isto para o 3\n",
    "    file = os.path.basename(file_path)\n",
    "\n",
    "    if (document_type == \"compressed_file_attachment\") and document_level == 2:\n",
    "        print(f'E documento e um attachment ou ZIP, RAR ou 7Z: {document_file}')\n",
    "    elif (document_type == \"document_file_attachment\") and document_level == 2:\n",
    "        if document_file.lower().endswith('.pdf'):\n",
    "            get_father(batch, file_path)\n",
    "            print(f'Documento é um attachment de: , embora seja um PDF: {document_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPPORTANTE _ OFICIAL df_log_transaction Tratamento dos DFss de log \n",
    "df_log_transaction = pd.DataFrame()\n",
    "df_log_tran = pd.DataFrame()\n",
    "df_row = pd.DataFrame()\n",
    "\n",
    "df_log_transaction = df_log_transaction.append(df_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importante para consultas\n",
    "src_path = \"pipeline_extracao_documentos/1_emails_documentos_recebidos/14_documentos_recebidos/fwdnotasemitidasem31072023.zip\"\n",
    "file = os.path.basename(file_path)\n",
    "document_unique_id = get_document_id_by_file(batch_name, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importante para geraçao de novo MOVE\n",
    "src_path = \"pipeline_extracao_documentos/1_emails_documentos_recebidos/14_documentos_recebidos/fwdnotasemitidasem31072023.zip\"\n",
    "tgt_path = \"pipeline_extracao_documentos/5_documentos_processados\"\n",
    "\n",
    "\n",
    "df_row, sucesso = move_doc_processed_file(batch_name, src_path,  tgt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. POC Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Modulos, configs, dicts e funcoes desta parte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.extrai_pdf_pesquisavel as Extc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGS DESTA PARTE\n",
    "# 1. Configuraçoes de pastas\n",
    "documentos_scan_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_model_path = \"pipeline_extracao_documentos/6_geral_administacao/modelos/frames_nf_v9.xlsx\"\n",
    "\n",
    "# 11. path para datasets CNAE e Itens de Serviço\n",
    "nf_datasets_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria do DF\n",
    "frames_nf_v4_df = pd.read_excel(nf_model_path)\n",
    "\n",
    "# Cria dicionários para armazenar diferentes tipos de elementos do modelo\n",
    "document_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'document'].iloc[0]\n",
    "boundaries_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'boundaries']\n",
    "sections_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'section']\n",
    "frames_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'frame']\n",
    "sframe_fields_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'sframe_field']\n",
    "field_boxes_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'field_box']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_nf_v4_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>1.</b> Consistencia de Modelos Prefeitura x CNPJ Prestador </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pesquisa_dict_prefeitura_modelo(texto):\n",
    "\n",
    "    # 7. ZZZ Dicionário para mapear Prefeitura e/ou CNPJ para um template específico\n",
    "    templates = {\n",
    "        (\"PREFEITURA DA CIDADE MAGE\", \"30.693.231/0001-99\"): \"MAGE_MAICON\",\n",
    "        (\"PREFEITURA DA CIDADE MAGE\", \"23.317.112/0001-76\"): \"MAGE_MFF\",\n",
    "        (\"PREFEITURA DA CIDADE MAGE\", None): \"MAGE\",\n",
    "        (\"PREFEITURA MUNICIPAL DE MAGE\", None): \"MAGE\",\n",
    "        (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", \"47.945.459/0001-21\"): \"SAO_PEDRO_GOAT\",\n",
    "        (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", \"68.687.722/0001-08\"): \"SAO_PEDRO_GM\",\n",
    "        (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", \"34.230.979/0038-06\"): \"SAO_PEDRO_SUPERMIX\",\n",
    "        (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", None): \"SAO_PEDRO\",\n",
    "        (\"Pague agora com o seu Pix\", None): \"NAO_PROCESSAR\",\n",
    "        # ... adicione outras combinações aqui\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 8. ZZZ Consistencia para buscar modelo\n",
    "    prefeitura_encontrada = None\n",
    "    cnpj_encontrado = None\n",
    "\n",
    "    # Verifique cada linha do texto\n",
    "    for linha in texto:\n",
    "        for pref, cnpj in templates.keys():\n",
    "            if pref in linha:\n",
    "                #print(linha)\n",
    "                prefeitura_encontrada = pref\n",
    "            if cnpj and cnpj in linha:\n",
    "                cnpj_encontrado = cnpj\n",
    "\n",
    "    # Saímos do loop, agora vamos verificar qual template usar\n",
    "    if prefeitura_encontrada:\n",
    "        template_usar = templates.get((prefeitura_encontrada, cnpj_encontrado))\n",
    "        if not template_usar:\n",
    "            template_usar = templates.get((prefeitura_encontrada, None), \"NAO_PROCESSAR\")\n",
    "    else:\n",
    "        template_usar = \"NAO_PROCESSAR\"\n",
    "\n",
    "    return template_usar, prefeitura_encontrada, cnpj_encontrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><mark> <b>1.</b> Funcoes importantes </mark></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCOES DE TRATAMENTO QUALIFICADAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Usando na criacao da imagem \n",
    "def conv_filename_no_ext(title):\n",
    "    \n",
    "    # Divida o título em nome e extensão (mas ignore a extensão)\n",
    "    name = title.rsplit('.', 1)[0] if '.' in title else title\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    return filename \n",
    "\n",
    "\n",
    "# XXX Funcao ajustada para convertere e resize\n",
    "def convertResize(doc2convert, document_path, image_resized_path):\n",
    "    \n",
    "    \n",
    "    name_image = conv_filename_no_ext(doc2convert)\n",
    "    \n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(name_image)}.jpg')\n",
    "    #print(f'image_resized_name: {image_resized_name}\\n')\n",
    "    # 3. Conversao para imagem\n",
    "    pages = convert_from_path(document_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((2067, 2923))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "        image_2work = resized_pages[0]\n",
    "        \n",
    "    return image_2work, image_resized_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcoes de OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "poppler_path = \"/home/dani-boy/miniconda3/envs/tables-detr/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Trata Ocr\n",
    "def extract_fields_box(modelo, father_value, section):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    filtered_boxes_info = field_boxes_info[(field_boxes_info['father'] == father_value) & (field_boxes_info['model'] == modelo)]\n",
    "    # Iterate nas informações dos boxes de fields e extraia o texto de cada field\n",
    "    for index_field, row_field in filtered_boxes_info.iterrows():\n",
    "        x0, y0, x1, y1 = row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1']\n",
    "        extracted_text_box = extract_text_from_frame_2(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        #extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        \n",
    "        #print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference'], row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1'] ))\n",
    "        # Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "        value = extracted_text_box.split('\\n')[-1]\n",
    "        # Remova qualquer espaço em branco à esquerda ou à direita\n",
    "        value = value.strip()\n",
    "        if row_field['t_value'] == 'number':\n",
    "            # Formate o valor usando a função format_number\n",
    "            #print(\"vou verificar valor\")\n",
    "            value = format_number2(value)\n",
    "            #print(value)\n",
    "        # Armazene o texto extraído com o rótulo correspondente\n",
    "        label = row_field['label']\n",
    "        data_box_valores[label] = value\n",
    "        \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "# 3. Efetua OCR no documento (area parao do texto da NF)\n",
    "def ocr_RasterPDF(image_name):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = 406\n",
    "    y0 = 0\n",
    "    x1= 1540\n",
    "    y1 = 380\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "    return values, extracted_text_frame \n",
    "\n",
    "\n",
    "# 2. Efetua OCR no documento (area parao do texto da NF)\n",
    "def ocr_RasterPDF_free(image_name, vx0, vy0, vx1, vy1):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = vx0\n",
    "    y0 = vy0\n",
    "    x1= vx1\n",
    "    y1 = vy1\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "    return values, extracted_text_frame \n",
    "\n",
    " \n",
    "def extract_text_from_frame(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=config).strip()\n",
    "    return extracted_text \n",
    "\n",
    "\n",
    "def extract_fb_outras_inf(modelo, father_value, section):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    filtered_boxes_info = field_boxes_info[(field_boxes_info['father'] == father_value) & (field_boxes_info['model'] == model)]\n",
    "    # Iterate nas informações dos boxes de fields e extraia o texto de cada field\n",
    "    for index_field, row_field in filtered_boxes_info.iterrows():\n",
    "        \n",
    "        string_pesquisa = row_field['reference']\n",
    "        x0, y0, x1, y1 = row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        label = row_field['label']\n",
    "        #print(f'extracted_text_box {extracted_text_box}, label {label}')\n",
    "        text = extracted_text_box.replace('\\n', '')\n",
    "        if text.startswith(string_pesquisa):\n",
    "            #print(\"aqui:\", text)\n",
    "            text = text[len(label):].strip()\n",
    "            data_box_valores[label] = text\n",
    "    \n",
    "    return   data_box_valores  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcoes REGEX e ORganizacao TEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair número da string\n",
    "def extract_number(text):\n",
    "    match = re.search(r'\\b\\d+(\\.\\d+)?\\b', text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "# Funcao importante - process_line\n",
    "def process_line(value, reference, label):\n",
    "    name_match = re.search(fr'{reference} (.+)', value)\n",
    "    if name_match:\n",
    "        extracted_value = reference + \" \" + name_match.group(1)\n",
    "        return {label: extracted_value}\n",
    "    return None\n",
    "\n",
    "\n",
    "def format_number(number_str):\n",
    "    # Check for percentage and handle it\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # You can multiply by 100 here if needed\n",
    "\n",
    "    # Check if the string contains \"R$\" or a comma, indicating the original format\n",
    "    if 'R$' in number_str or ',' in number_str:\n",
    "        # Original format: Remove 'R$', replace dots with nothing, and replace commas with dots\n",
    "        number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    else:\n",
    "        # New format: Extract only the numeric part using regex\n",
    "        number_str = re.findall(r'[\\d\\.]+', number_str)[-1]\n",
    "\n",
    "    return float(number_str)\n",
    "\n",
    "# Funçao de formatacao de numeros\n",
    "def format_number2(number_str):\n",
    "    number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # multiplica por 100 para fields %\n",
    "    return float(number_str)\n",
    "\n",
    "\n",
    "\n",
    "#1. funcao: find_value_after_keyword_out_frame_up\n",
    "def find_value_after_keyword_out_frame_up(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "        if text_list[index + 1] not in default_keyword_list:\n",
    "            return text_list[index + 1]\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            for default_keyword in default_keyword_list:\n",
    "                if default_keyword in text_list:\n",
    "                    # Caso especial para 'Nome/Razão Social:'\n",
    "                    if keyword == 'Nome/Razão Social:':\n",
    "                        return text_list[0]\n",
    "        return None\n",
    "    \n",
    "#2. find_value_after_keyword_out_frame_down  \n",
    "def find_value_after_keyword_out_frame_down(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o índice seguinte está dentro da lista\n",
    "        if index + 1 < len(text_list):\n",
    "            # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            try:\n",
    "                index = text_list.index(default_keyword_list[-1])\n",
    "                return text_list[index - 1]\n",
    "            except ValueError:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "#3. find_value_after_keyword_fuzz\n",
    "def find_value_after_keyword_fuzz(keyword, text_list, default_keyword_list=None, fuzziness_threshold=80):\n",
    "    closest_match = None\n",
    "    closest_match_score = 0\n",
    "    \n",
    "    for i, text in enumerate(text_list):\n",
    "        score = fuzz.ratio(keyword, text)\n",
    "        \n",
    "        if score > closest_match_score:\n",
    "            closest_match_score = score\n",
    "            closest_match = text\n",
    "        \n",
    "        if closest_match_score > fuzziness_threshold:\n",
    "            break\n",
    "\n",
    "    if closest_match_score > fuzziness_threshold:\n",
    "        index = text_list.index(closest_match)\n",
    "        if index + 1 < len(text_list):\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "\n",
    "    \n",
    "def pesquisa_keyword(string_pesquisa, text_splited, keyword_list):\n",
    "\n",
    "    resultado_extraido_fuzz = find_value_after_keyword_fuzz(string_pesquisa, text_splited, keyword_list)\n",
    "\n",
    "    if resultado_extraido_fuzz == None:\n",
    "        resultado_extraido_frame_up = find_value_after_keyword_out_frame_up(string_pesquisa, text_splited, keyword_list)\n",
    "        if resultado_extraido_frame_up == None:\n",
    "            resultado_extraido_frame_down = find_value_after_keyword_out_frame_down(string_pesquisa, text_splited, keyword_list)\n",
    "            resultado_extraido = resultado_extraido_frame_down\n",
    "        else:\n",
    "            resultado_extraido = resultado_extraido_frame_up\n",
    "    else:\n",
    "        resultado_extraido = resultado_extraido_fuzz           \n",
    "    #print(resultado_extraido)\n",
    "    return resultado_extraido\n",
    "\n",
    "\n",
    "\n",
    "def cabecalho_prefeitura():\n",
    "    valor_dict = {}\n",
    "    dados_prefeitura = {}\n",
    "    f_frame_name = \"1_frame_prefeitura_nf\"\n",
    "    # 1. funçao basica de modelo \n",
    "    texto = executa_model_frame(model, secao, f_frame_name)\n",
    "    text_splited = texto.split('\\n')\n",
    "    \n",
    "    valor_dict = extract_prefeitura(model, f_frame_name, text_splited)\n",
    "    if valor_dict:\n",
    "        dados_prefeitura.update(valor_dict)\n",
    "        \n",
    "        \n",
    "    return dados_prefeitura \n",
    "                \n",
    "def cabecalho_dados():\n",
    "\n",
    "    valor = {}   \n",
    "    f_frame_name = \"1_frame_dados_nf\"\n",
    "    \n",
    "    dadinho_dados_nf = {}\n",
    "    \n",
    "    # 1. funçao basica de modelo \n",
    "    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "    text_splited = texto_extraido(texto)\n",
    "    keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "    string_pesquisa = \"Número da Nota:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "    dadinho_dados_nf['numero_nota_fiscal'] = texto\n",
    "\n",
    "\n",
    "    string_pesquisa = \"Competência:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['competencia'] = texto\n",
    "    \n",
    "    \n",
    "    string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['dt_hr_emissao'] = texto\n",
    "    \n",
    "    \n",
    "    string_pesquisa = \"Código Verificação:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['codigo_verificacao'] = texto\n",
    "    \n",
    "    return dadinho_dados_nf   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_fields_tomador_cnpj(text):\n",
    "    nf_data_tomador_cnpj = {}\n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in text:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "                        nf_data_tomador_cnpj['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                        nf_data_tomador_cnpj['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "\n",
    "    \n",
    "    # Extrair Telefone\n",
    "    telefone_match = re.search(r'Telefone:\\s+(.+)', text)\n",
    "    if telefone_match:\n",
    "        telefone_str = telefone_match.group(1)\n",
    "        if telefone_str == 'Inscrição Estadual:':\n",
    "            nf_data_tomador_cnpj['telefone'] = None  # Valor padrão quando não há correspondência\n",
    "        elif telefone_str == '':\n",
    "            nf_data_tomador_cnpj['telefone'] = None  # Valor padrão quando não há correspondência\n",
    "                    \n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['telefone'] = telefone_match.group(1)\n",
    "            \n",
    "    \n",
    "    # Extrair Inscrição Municipal\n",
    "    inscricao_municipal_match = re.search(r'Inscrição Municipal:\\s+(.+)', text)\n",
    "    if inscricao_municipal_match:\n",
    "        inscricao_municipal_str = inscricao_municipal_match.group(1)\n",
    "        if inscricao_municipal_str == \"Telefone:\": \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = inscricao_municipal_str\n",
    "    \n",
    "    insc_municipal_match = re.search(r'INSC:MUNICIPAL:\\s+(.+)', text)\n",
    "    if insc_municipal_match:\n",
    "        insc_municipal_str = insc_municipal_match.group(1)\n",
    "        if insc_municipal_str == \"Telefone:\":\n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = insc_municipal_str\n",
    "    else:\n",
    "        nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "            \n",
    "    \n",
    "    return nf_data_tomador_cnpj \n",
    "\n",
    "\n",
    "\n",
    "def extract_dados_comple_obs(modelo, frame_father, section):\n",
    "    \n",
    "    data_dados_complementares = {}\n",
    "    #frame_label = frame_father\n",
    "    \n",
    "    # 1. Filtrando o frames_info para buscar os dados de corte\n",
    "    filtered_frames_info = frames_info[(frames_info['label'] == frame_father) & (frames_info['model'] == modelo)]\n",
    "\n",
    "    # 2. Filtrando o sframe_fields_info para buscar os dados dos campos que estao nos frames\n",
    "    filtered_sframe_fields_info = sframe_fields_info[(sframe_fields_info['father'] == frame_father) & (sframe_fields_info['model'] == modelo)]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "        \n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        \n",
    "        print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_frame['seq'], row_frame['model'], row_frame['father'], row_frame['label'], row_frame['reference'], row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'] ))\n",
    "        for index_field, row_field in filtered_sframe_fields_info.iterrows():\n",
    "            #print(\"{:<5} {:<10} {:<30} {:<20} {:<20}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference']))\n",
    "            \n",
    "            if frame_father == \"5_frame_dados_complementares\":\n",
    "                nf_data_dados_complementares = {}\n",
    "                nf_data_dados_complementares['section'] = section\n",
    "                \n",
    "                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^DADOS COMPLEMENTARES', '', extracted_text_box, count=1)\n",
    "                if text == '':\n",
    "                    text = None\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text\n",
    "                else:    \n",
    "                    # Extrair texto dentro do retângulo\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "                    \n",
    "                return nf_data_dados_complementares                \n",
    "                \n",
    "            elif frame_father == \"5_frame_observacao\":\n",
    "                nf_data_observacao = {}\n",
    "                nf_data_observacao['section'] = section \n",
    "                                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^Observação:', '', extracted_text_box, count=1)\n",
    "\n",
    "                # Remover quebras de linha\n",
    "                text = text.replace('\\n', ' ')\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                nf_data_observacao['observacao'] = text.strip()\n",
    "                \n",
    "                return nf_data_observacao \n",
    "            \n",
    "\n",
    "#4. Extrai prefeitura\n",
    "def extract_prefeitura(model, father, values):\n",
    "    \n",
    "    tipo = \"sframe_field\"\n",
    "    data_extrated_prefeitura = {}\n",
    "    #print(tipo)\n",
    "\n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model) & (frames_nf_v4_df['father'] == father) & (frames_nf_v4_df['type'] == tipo)]\n",
    "\n",
    "    for index_sframe, row_sframe in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        label_value = row_sframe['label']\n",
    "        \n",
    "        #print(\"label_value\", label_value)\n",
    "        \n",
    "        if label_value == \"nome_prefeitura\":\n",
    "            reference_value = row_sframe['reference']\n",
    "            for value in values:\n",
    "                result = process_line(value, reference_value, label_value)\n",
    "                if result:\n",
    "                    data_extrated_prefeitura.update(result)\n",
    "        elif label_value == \"secretaria\":\n",
    "            reference_value = row_sframe['reference']\n",
    "            for value in values:\n",
    "                result = process_line(value, reference_value, label_value)\n",
    "                if result:\n",
    "                    data_extrated_prefeitura.update(result) \n",
    "        elif label_value == \"tipo_nota_fiscal\":\n",
    "            reference_value = row_sframe['reference']  \n",
    "            for value in values:\n",
    "                result = process_line(value, reference_value, label_value)\n",
    "                if result:\n",
    "                    data_extrated_prefeitura.update(result)\n",
    "                    \n",
    "    return data_extrated_prefeitura\n",
    "\n",
    "\n",
    "def processa_cnae_outros(text):\n",
    "    nf_data_CNAE_match = re.search(r'CNAE\\s+(.+)', text)\n",
    "    if nf_data_CNAE_match:\n",
    "        try:\n",
    "            # Remove a primeira ocorrência de \"CNAE:\"\n",
    "            nf_data_CNAE_str = re.sub(r'^CNAE - ', '', text, count=1)\n",
    "            # Remover quebras de linha\n",
    "            nf_data_CNAE_str = nf_data_CNAE_str.replace('\\n', ' ')\n",
    "            return nf_data_CNAE_str \n",
    "        except Exception as e:\n",
    "            print(f\"Erro busca cnae: {e}\") \n",
    "        \n",
    "    return None   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcoes de Tratamento do Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Avaliar Funçao basica de modelo \n",
    "def pesquisa_model_frame_coordenadas(model_src, tipo, tipo_processo_pdf):\n",
    "\n",
    "    data_dados_model = {}\n",
    "    dados_modelo = []\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_src) & (frames_nf_v4_df['type'] == tipo)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        frame_model = row_frame['model']\n",
    "        frame_prefeitura = row_frame['prefeitura']\n",
    "        frame_label = row_frame['label']\n",
    "        frame_section_json = row_frame['section_json']\n",
    "        frame_cnpj = row_frame['cnpj']\n",
    "        \n",
    "        if frame_model:\n",
    "            modelo = True\n",
    "        \n",
    "        if tipo_processo_pdf == 'pdf_pesquisavel':\n",
    "            try:\n",
    "                # data for PDF Pesquisavel\n",
    "                x0, y0, x1, y1 = row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']\n",
    "                \n",
    "                data_dados_model = {\n",
    "                    \"prefeitura\": frame_prefeitura,\n",
    "                    \"model\": frame_model,\n",
    "                    \"label\": frame_label,\n",
    "                    \"section_json\": frame_section_json,\n",
    "                    \"cnpj\": frame_cnpj,\n",
    "                    \"x0\": x0,\n",
    "                    \"y0\": y0,\n",
    "                    \"x1\": x1,\n",
    "                    \"y1\": y1,\n",
    "                }\n",
    "                dados_modelo.append(data_dados_model)   \n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao carregar coordenadas do frame: {e}\")\n",
    "                x0, y0, x1, y1 = 0, 0, 0, 0 \n",
    "                \n",
    "\n",
    "        elif tipo_processo_pdf == 'raster_pdf':\n",
    "            try:\n",
    "                # data for Raster_PDF\n",
    "                x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "                \n",
    "                data_dados_model = {\n",
    "                    \"prefeitura\": frame_prefeitura,\n",
    "                    \"model\": frame_model,\n",
    "                    \"label\": frame_label,\n",
    "                    \"section_json\": frame_section_json,\n",
    "                    \"cnpj\": frame_cnpj,\n",
    "                    \"x0\": x0,\n",
    "                    \"y0\": y0,\n",
    "                    \"x1\": x1,\n",
    "                    \"y1\": y1,\n",
    "                }\n",
    "                dados_modelo.append(data_dados_model)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao carregar coordenadas do frame: {e}\")\n",
    "                x0, y0, x1, y1 = 0, 0, 0, 0 \n",
    "                  \n",
    "  \n",
    "    return dados_modelo, modelo\n",
    "\n",
    "\n",
    "# 1. funçao basica de modelo \n",
    "def pesquisa_model_frame(model_src, tipo):\n",
    "\n",
    "    data_dados_model = {}\n",
    "    dados_modelo = []\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_src) & (frames_nf_v4_df['type'] == tipo)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        frame_model = row_frame['model']\n",
    "        # frame_prefeitura = row_frame['prefeitura']\n",
    "        # frame_label = row_frame['label']\n",
    "        # frame_section_json = row_frame['section_json']\n",
    "        # frame_cnpj = row_frame['cnpj']\n",
    "        \n",
    "        # if frame_model:\n",
    "        #     modelo_frame = True\n",
    "        # else:    \n",
    "        #     modelo_frame = False\n",
    "\n",
    "                  \n",
    "  \n",
    "    return frame_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> Funçoes de Extracao Qualificadas </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "def remove_images(imagens_list):\n",
    "    for image in imagens_list:\n",
    "        try:\n",
    "            os.remove(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Nao congui deletar: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX funçao basica de modelo \n",
    "def pesquisa_frame_coordenadas_pdf_pesquisa(model_src, tipo, section, label):\n",
    "\n",
    "    tipo = tipo\n",
    "    data_dados_model = {}\n",
    "    dados_modelo = []\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_src) & (frames_nf_v4_df['type'] == tipo) & (frames_nf_v4_df['section_json'] == section) & (frames_nf_v4_df['label'] == label)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        frame_model = row_frame['model']\n",
    "        frame_label = row_frame['label']\n",
    " \n",
    "        # data for PDF Pesquisavel\n",
    "        x0, y0, x1, y1 = row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']\n",
    " \n",
    "    return x0, y0, x1, y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>2.</b> Funcoes de tratamento de texto para cabecalho </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_PIL(image, coordinates):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    image_croped = image.crop((x0, y0, x1, y1))\n",
    "    texto_extraido = pytesseract.image_to_string(image_croped, lang='por')\n",
    "    return texto_extraido \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# XXX Funcao de extracao\n",
    "def extract_text_from_coordinates(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    print(x0, y0, x1, y1)\n",
    "    frame_image\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=config).strip()\n",
    "    return extracted_text\n",
    "\n",
    "def extract_text_from_coordinates_2(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    print(x0, y0, x1, y1)\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config_1).strip()\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3B. XXX Efetuo a extracao de dados\n",
    "# analisar_pdf_pesquisavel\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'extracao_prestador' #'pesquisar_prefeitura' # pesquisar_prefeitura  pesquisar_modelo    mensagem_status = \"Reavaliar_PDF_Pesquisavel\"\n",
    "status = 'OK'\n",
    "\n",
    "#raw_texto = extracao_pipeline(df_analise_pipe, fase, atividade, status)\n",
    "raw_texto = extracao_pipeline(subset_df_analise_pipe, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX funçao IMPORTANTE basica de modelo \n",
    "def executa_model_frame(model, secao, f_frame_name, f_tipo, image_2work):\n",
    "\n",
    "    data_dados_frame = {}\n",
    "    \n",
    "    print(f'3 em executa model_frame - antes do for {model} se: {secao} |f_frame_name: {f_frame_name} | f_tipo: {f_tipo}')\n",
    "    \n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model) & (frames_nf_v4_df['label'] == f_frame_name) & (frames_nf_v4_df['type'] == f_tipo)]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        texto_extraido_pil = extract_text_PIL(image_2work, (x0, y0, x1, y1)) # PONTO IMPORTANTE\n",
    "        print(f'3 - executa: {model} | {secao} | {f_frame_name}  x0: {x0} y0: {y0} x1: {x1} y1: {y1}')\n",
    "        \n",
    "        frame_seq = row_frame['seq']\n",
    "        frame_model = row_frame['model']\n",
    "        frame_label = row_frame['label']\n",
    "        frame_type = row_frame['type']\n",
    "        frame_section = row_frame['section_json']\n",
    "        frame_reference = row_frame['reference']\n",
    "        frame_father = row_frame['father']\n",
    "        frame_id = row_frame['id']\n",
    "        #print(f'\\fid: {frame_id:>3} | seq: {frame_seq:>3} | model: {frame_model:>8} | type: {frame_type:>15} | Father: {frame_father} label: {frame_label:>30} | section: {frame_section:>20} {frame_reference:>30}')\n",
    "        \n",
    "    return texto_extraido_pil\n",
    "  \n",
    "  \n",
    "  \n",
    "def extract_fields_cabecalho_raster_pdf(row, file_path, image_2work): \n",
    "  \n",
    "    message_erro = []\n",
    "  \n",
    "  \n",
    "    model_src = row['modelo']\n",
    "    original_file_name = row['original_file_name'] \n",
    "    \n",
    "    model = row['modelo']   \n",
    "    secao = \"1 - CABECALHO\"\n",
    "    section = \"1 - CABECALHO\"\n",
    "    f_frame_name = \"1_frame_dados_nf\"\n",
    "    f_tipo = 'frame'\n",
    "        \n",
    "    raw_texto_pil = executa_model_frame(model, secao, f_frame_name, f_tipo, image_2work) # executa_model_frame\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        text_splited = texto_extraido_cabecalho(raw_texto_pil)\n",
    "        keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "        string_pesquisa = \"Número da Nota:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "        nro_nota_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Competência:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        competencia_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        dt_hr_emissao_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Código Verificação:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        codigo_verificacao_frame = texto\n",
    "        \n",
    "    except Exception as e:\n",
    "        # erros_cabecalho = {}\n",
    "        msg = (f\"doc: {original_file_name} | {e}\")\n",
    "        nro_nota_frame = \" \"\n",
    "        competencia_frame = \" \"\n",
    "        dt_hr_emissao_frame = \" \"\n",
    "        codigo_verificacao_frame = \" \"\n",
    "        message_erro.append(msg)\n",
    "        \n",
    "        \n",
    "    return nro_nota_frame, competencia_frame, dt_hr_emissao_frame, codigo_verificacao_frame, message_erro, section, raw_texto_pil \n",
    "\n",
    "def extract_fields_cabecalho_pdf_pesquisavel(row, file_path):\n",
    "    \n",
    "    message_erro = [] \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "    \n",
    "    \n",
    "    model_src = row['modelo']\n",
    "    original_file_name = row['original_file_name']\n",
    "    tipo = \"frame\"\n",
    "    section = '1 - CABECALHO'\n",
    "    label = '1_frame_prefeitura_nf'\n",
    "\n",
    "    x0, y0, x1, y1 = pesquisa_frame_coordenadas_pdf_pesquisa(model_src, tipo, section, label)\n",
    "\n",
    "    try:\n",
    "        texto = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        \n",
    "        text_splited = texto_extraido_nf(texto)\n",
    "        keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "        string_pesquisa = \"Número da Nota:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "        nro_nota_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Competência:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        competencia_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        dt_hr_emissao_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Código Verificação:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        codigo_verificacao_frame = texto\n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} | {e}\")\n",
    "        nro_nota_frame = \" \"\n",
    "        competencia_frame = \" \"\n",
    "        dt_hr_emissao_frame = \" \"\n",
    "        codigo_verificacao_frame = \" \"\n",
    "        message_erro.append(msg)\n",
    "            \n",
    "    pdf_document.close()\n",
    "    #print(row)\n",
    "    return nro_nota_frame, competencia_frame, dt_hr_emissao_frame, codigo_verificacao_frame, message_erro, section\n",
    "\n",
    "\n",
    "# 5. XXX Ajusta texto\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF_Pesquisavel-  NO CABECALHO\n",
    "def texto_extraido_nf(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF RASTER NO CABECALHO\n",
    "def texto_extraido_cabecalho(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    text_splited = [s.replace(\")\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>2.</b> Teste de valores de Texto para Cabecalho </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def corrigir_erro_ocr(texto):\n",
    "    \n",
    "    # Mapeamento de correções comuns\n",
    "    correcoes = {\n",
    "        \"B\": \"8\",\n",
    "        \"I\": \"1\",\n",
    "        \"l\": \"1\", # l minúsculo pode ser confundido com 1\n",
    "        \"S\": \"5\",\n",
    "        # Adicione mais correções conforme necessário\n",
    "    }\n",
    "\n",
    "    # Aplicar as correções\n",
    "    for errado, correto in correcoes.items():\n",
    "        texto = texto.replace(errado, correto)\n",
    "    \n",
    "    # Correções específicas para padrões identificados\n",
    "    texto = re.sub(r'0O0', '000', texto) # Substitui '0O0' por '000'\n",
    "    \n",
    "    # Se o texto tem 10 caracteres, tentamos remover cada caracter para chegar a uma string de 9 caracteres\n",
    "    if len(texto) == 10:\n",
    "        for i in range(10):\n",
    "            texto_temp = texto[:i] + texto[i+1:]\n",
    "            if len(texto_temp) == 9:\n",
    "                texto = texto_temp\n",
    "                break\n",
    "    \n",
    "    # Certificando que o texto tem o comprimento correto\n",
    "    if len(texto) != 9:\n",
    "        return \"Erro: comprimento incorreto\"\n",
    "    \n",
    "    return texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testando a função\n",
    "print(corrigir_erro_ocr(\"7C0O0AE077\")) # Deveria corrigir para \"7C000AE07\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"7C0O0AE07\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrigir_erro_ocr(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrigir_erro_ocr(texto):\n",
    "   \n",
    "    if len(texto) != 9:\n",
    "        msg = (\"Erro: comprimento incorreto\")\n",
    "\n",
    "    # Mapeamento de correções comuns\n",
    "    correcoes = {\n",
    "        \"B\": \"8\",\n",
    "        \"O\": \"0\",\n",
    "        \"I\": \"1\",\n",
    "        \"l\": \"1\", # l minúsculo pode ser confundido com 1\n",
    "        \"S\": \"5\",\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Aplicar as correções\n",
    "    for errado, correto in correcoes.items():\n",
    "        texto = texto.replace(errado, correto)\n",
    "    \n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codigo_verificacao_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = corrigir_erro_ocr(codigo_verificacao_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codigo_verificacao_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(codigo_verificacao_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corrigir_erro_ocr(\"7CO00AE077\")) # Deveria corrigir para \"7C000AE077\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splited = texto_extraido_cabecalho(raw_texto)\n",
    "text_splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "string_pesquisa = \"Número da Nota:\"\n",
    "texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "nro_nota_frame = texto\n",
    "\n",
    "nro_nota_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_pesquisa = \"Competência:\"\n",
    "texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "competencia_frame = texto\n",
    "competencia_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "dt_hr_emissao_frame = texto\n",
    "dt_hr_emissao_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_pesquisa = \"Código Verificação:\"\n",
    "texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "codigo_verificacao_frame = texto\n",
    "codigo_verificacao_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> Analise de Extracao de dados no Pipeline </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. XXX Analisa nro de paginas\n",
    "def analisa_nro_pages(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    pages = pdf_document.pages() # generator object\n",
    "\n",
    "    page_nro = []\n",
    "    for page in pages:\n",
    "        page_nro.append(page)\n",
    "        \n",
    "    nro_paginas = len(page_nro)    \n",
    "    if nro_paginas > 1:\n",
    "        doc_1_page = False\n",
    "        return doc_1_page, nro_paginas    \n",
    "    else:\n",
    "        doc_1_page = True\n",
    "        return doc_1_page, nro_paginas  \n",
    "    pdf_document.close()\n",
    "    \n",
    " \n",
    "\n",
    "# XXX FUNCAO DE SPLIT\n",
    "def split_documentos(qualquer_df, outro_tgt_df, num_linhas_df, fase, atividade, status):\n",
    "    \n",
    "    documentos_splitados = []\n",
    "    doc_info = {}\n",
    "    rows_list = []\n",
    "    documentos = []\n",
    "    #output_dir = os.path.join(documentos_scan_path, batch_name)\n",
    "\n",
    "    fase_processo = fase\n",
    "    atividade_processo = atividade\n",
    "    status_documento = status\n",
    "    \n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    \n",
    "    i = num_linhas_df + 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        message_erro = []\n",
    "        batch_name = row['batch']\n",
    "        original_file_name = row['original_file_name']\n",
    "        folder_name = row['directory']\n",
    "        file_path = row['file_path']\n",
    "        level = row['level']\n",
    "        d_type = row['type']\n",
    "        document_type = row['document_type']\n",
    "        # document_unique_id = row['document_unique_id']\n",
    "        new_level = level + 1\n",
    "        try:\n",
    "            pdf = fitz.open(file_path)\n",
    "            # Número total de páginas no PDF\n",
    "            total_pages = len(pdf)\n",
    "        except Exception as e:\n",
    "            print(f\"Nao congui abrir o PDF: {e}\")    \n",
    "\n",
    "        # Nome base para os arquivos de saída\n",
    "        base_name = file_path.split('.')[0]  # Remove a extensão do arquivo\n",
    "        \n",
    "        file_to_delete = file_path\n",
    "\n",
    "        # Loop para criar um novo PDF para cada página\n",
    "        for page_num in range(total_pages):\n",
    "            # Cria um novo objeto PDF\n",
    "            new_pdf = fitz.open()\n",
    "            # Adiciona a página atual ao novo PDF\n",
    "            new_pdf.insert_pdf(pdf, from_page=page_num, to_page=page_num)\n",
    "            # Nome do novo arquivo PDF\n",
    "            new_pdf_name = f\"{base_name}_page_{page_num + 1}.pdf\"\n",
    "            # Salva o novo PDF\n",
    "            new_pdf.save(new_pdf_name)\n",
    "            # Fecha o novo PDF\n",
    "            new_pdf.close()\n",
    "            status_documento = 'splitado'\n",
    "            name_pdf_splited = os.path.basename(new_pdf_name)\n",
    "            \n",
    "            nova_linha = {\n",
    "                'seq': i,\n",
    "                'date_time': time_now,\n",
    "                'batch': batch_name,\n",
    "                'fase_processo': fase_processo,\n",
    "                'nome_atividade': atividade_processo,\n",
    "                'status_documento': status_documento,\n",
    "                'document_unique_id': generate_unique_id(),\n",
    "                'original_file_name': name_pdf_splited,\n",
    "                'directory': folder_name,\n",
    "                'one_page': True,\n",
    "                'pages': 1,                    \n",
    "                'level': new_level,\n",
    "                'type': d_type,\n",
    "                'document_type': document_type,\n",
    "                'parent_document_unique_id': idx,\n",
    "                'file_path': new_pdf_name,\n",
    "                'file_hash': generate_file_hash(new_pdf_name),\n",
    "            }\n",
    "            rows_list.append(nova_linha)\n",
    "            i += 1\n",
    "        outro_tgt_df.loc[idx, 'status_documento'] = \"NAO_PROCESSAR\"    \n",
    "            \n",
    "\n",
    "    total_split = i - 1\n",
    "                      \n",
    "      \n",
    "    df_split = pd.DataFrame(rows_list)\n",
    "    \n",
    "    return df_split\n",
    "\n",
    "\n",
    "# 1.XXX  Acao 1 - Ler todo o pipeline de documentos recebidos\n",
    "def scan_pipeline_documentos(documentos_scan_path, batch_name, fase, atividade, status):\n",
    "\n",
    "    doc_info = {}\n",
    "    resumo = {}\n",
    "    documentos = []\n",
    "    output_dir = os.path.join(documentos_scan_path, batch_name)\n",
    "\n",
    "    fase_processo = fase\n",
    "    atividade_processo = atividade\n",
    "    status_documento = status\n",
    "    \n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    \n",
    "    #print(f'1. Fase atual: {fase_atual} | Nome atividade: {nome_atividade} | Status documento: {status_documento}')\n",
    "    \n",
    "    # Just for DEV\n",
    "    parent_document_unique_id = generated_parent_document_unique_id\n",
    "\n",
    "    i = 1\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        folder_name = os.path.basename(root)\n",
    "        #print(folder_name)\n",
    "        for file in files:\n",
    "            level = 3\n",
    "            file_path = os.path.join(root, file)\n",
    "            new_path_name = os.path.join(output_dir, file)\n",
    "            diretorio = os.path.basename(file_path)\n",
    "            if folder_name == batch_name:\n",
    "                folder_name = \"root_dir\"\n",
    "            doc_one_page, nro_pgs = analisa_nro_pages(file_path)\n",
    "            #print(f'2. doc_one_page: {doc_one_page} | nro_pgs: {nro_pgs}') \n",
    "            type = 'document_file'\n",
    "            document_type = \"provavel_NFSe\"\n",
    "            #print(f'\\nfile: {file} | diretorio: {folder_name}\\n{file_path} ')\n",
    "            if folder_name not in doc_info:\n",
    "                doc_info[folder_name] = {\"data\": []}\n",
    "            doc_info[folder_name][\"data\"].append({\n",
    "                \"seq\": i,\n",
    "                \"date_time\": time_now,\n",
    "                \"batch\": batch_name,\n",
    "                \"fase_processo\": fase_processo,\n",
    "                \"nome_atividade\": atividade_processo,\n",
    "                \"original_file_name\": file,\n",
    "                \"status_documento\": status_documento,\n",
    "                \"one_page\": doc_one_page,\n",
    "                \"pages\": nro_pgs,\n",
    "                \"directory\": folder_name,\n",
    "                \"file_path\": file_path,\n",
    "                \"level\": level,\n",
    "                \"type\": type,\n",
    "                \"document_type\": document_type,\n",
    "                \"document_unique_id\": generate_unique_id(),\n",
    "                \"parent_document_unique_id\": parent_document_unique_id,\n",
    "                \"file_hash\": generate_file_hash(file_path)\n",
    "            })\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    \n",
    "    documentos.append(doc_info)\n",
    "    total_scan = i - 1\n",
    "    #print(\"Documentos scaneados: \", total_scan)\n",
    "    \n",
    "    # Criaçao do df_scan_pipe\n",
    "    document_row = {}\n",
    "    rows_list = []\n",
    "    for documento in documentos:\n",
    "        #print(documento)\n",
    "        for diretorio, dados in documento.items():\n",
    "            #print(f'Diretório: {diretorio}\\n')\n",
    "            for arquivo in dados['data']:\n",
    "                nova_linha = {\n",
    "                    'seq': arquivo['seq'],\n",
    "                    'date_time': arquivo['date_time'],\n",
    "                    'batch': arquivo['batch'],\n",
    "                    'fase_processo': arquivo['fase_processo'],\n",
    "                    'nome_atividade': arquivo['nome_atividade'],\n",
    "                    'status_documento': arquivo['status_documento'],\n",
    "                    'document_unique_id': arquivo[\"document_unique_id\"],\n",
    "                    'original_file_name': arquivo['original_file_name'],\n",
    "                    'directory': arquivo['directory'],\n",
    "                    'one_page': arquivo['one_page'],\n",
    "                    'pages': arquivo['pages'],\n",
    "                    'pdf_pesquisavel': None,\n",
    "                    'prefeitura': None,\n",
    "                    'cnpj': None,\n",
    "                    'modelo': None,\n",
    "                    'level': arquivo['level'],\n",
    "                    'type': arquivo['type'],\n",
    "                    'document_type': arquivo['document_type'],\n",
    "                    'parent_document_unique_id': arquivo['parent_document_unique_id'],\n",
    "                    'file_path': arquivo[\"file_path\"],\n",
    "                    'file_hash': arquivo[\"file_hash\"],\n",
    "                }\n",
    "                rows_list.append(nova_linha)\n",
    "    \n",
    "    df_trans_pipe = pd.DataFrame(rows_list)            \n",
    "    \n",
    "    return df_trans_pipe, documentos\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#generated_parent_document_unique_id = generate_unique_id()  \n",
    "\n",
    "# Processo de deleçao e atualizacao de documentos\n",
    "#e_deleta_peloamor(df_docs_splitados)\n",
    "\n",
    "#me_atualiza_logo_vai_2(novo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. XXX SE deseja importar o DF df_analise_pipe\n",
    "\n",
    "df_analise_pipe_path = \"df_mapeamento_e_analise0.xlsx\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria df_documento_recebido\n",
    "df_analise_pipe = pd.read_excel(df_analise_pipe_path)\n",
    "\n",
    "# Ajusta o indice\n",
    "df_analise_pipe.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>2.x</b> Processo de mapeamento e criacao do  df_analise_pipe </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Crio o DF df_scan_pipe\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'scan_analise'\n",
    "status = 'mapear'\n",
    "\n",
    "df_analise_pipe, documentos = scan_pipeline_documentos(documentos_scan_path, batch_name, fase, atividade, status)\n",
    "\n",
    "\n",
    "# 2. XXX df_analise_pipe e o df oficial de analise do pipeline\n",
    "df_analise_pipe\n",
    "\n",
    "# 3. Numero de linhas do DF\n",
    "num_linhas_df = df_analise_pipe.shape[0]\n",
    "\n",
    "num_linhas_df # variavel para o numero de linhas do DF\n",
    "\n",
    "\n",
    "# 4. XXX Usando loc para Filtrar Baseado em one_page == False\n",
    "df_pages_2_split = df_analise_pipe[df_analise_pipe['one_page'] == False]\n",
    "\n",
    "num_docs_2_split = df_pages_2_split.shape[0]\n",
    "\n",
    "\n",
    "print(f'nro documentos em df_analise_pipe: {num_linhas_df} | nro documentos + 1 pagina: {df_pages_2_split.shape[0]}')\n",
    "    \n",
    "# 5. XXX Definimos o Index dos DFs\n",
    "df_analise_pipe.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "df_pages_2_split.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# 6. XXX Executo a criacao dos documentos split \n",
    "fase = 'pre-processamento'\n",
    "atividade = 'scan_analise'\n",
    "status = 'processar'\n",
    "\n",
    "# df_docs_splitados recebera o DF com os documentos splitados\n",
    "df_docs_splitados = split_documentos(df_pages_2_split, df_analise_pipe, num_linhas_df, fase, atividade, status)\n",
    "\n",
    "\n",
    "\n",
    "# 7. XXX Retiro o indice do DF - Resetando o índice e mantendo o índice original como uma nova coluna\n",
    "df_analise_pipe.reset_index(inplace=True)\n",
    "\n",
    "# 8. XXX Concatenando os DataFrames\n",
    "df_analise_pipe = pd.concat([df_analise_pipe, df_docs_splitados], ignore_index=True)\n",
    "\n",
    "# 9. XXX Volto novamente o indice do DF\n",
    "df_analise_pipe.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "# 10. XXX Salvo em Excel (pode ser feito durante fases)\n",
    "df_analise_pipe.to_excel(\"df_mapeamento_e_analise.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><mark> <b>2.</b> Analise e extracao de dados no pipeline </mark></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta e o principio da melhor funcao do mundo\n",
    "def analise_extracao_pipeline(qualquer_df, outro_tgt_df, fase, atividade, status):\n",
    "    \n",
    "    linhas_analise = []\n",
    "    bloco_1_list = []\n",
    "    bloco_2_list = []\n",
    "    bloco_3_list = []\n",
    "    imagens_list = []   \n",
    "    \n",
    "    pre_processo = ['ajustar_nome', 'split_paginas', 'ajustar_imagem', 'buscar_nome_prefeitura', 'enviar_canceladas', 'enviar_listagens']\n",
    "    \n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    \n",
    "    fase_processo_atual = fase\n",
    "    atividade_processo_atual = atividade\n",
    "    status_documento_atual = status\n",
    "    \n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        message_erro = []\n",
    "        \n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        document_unique_id = idx\n",
    "        seq_df = row['seq']\n",
    "        batch_name = row['batch']\n",
    "        fase_processo = row['fase_processo']\n",
    "        nome_atividade = row['nome_atividade']\n",
    "        status_documento = row['status_documento']\n",
    "        original_file_name = row['original_file_name']\n",
    "        file_directory = row['directory']\n",
    "        level = row['level']\n",
    "        d_type = row['level']\n",
    "        document_type = row['document_type']\n",
    "        pdf_pesquisavel = row['pdf_pesquisavel']\n",
    "        one_page_doc = row['one_page']\n",
    "        \n",
    "        prefeitura = row['prefeitura']\n",
    "        \n",
    "        parent_document_unique_id = row['parent_document_unique_id']\n",
    "        file_path = row['file_path']\n",
    "        file_hash = row['file_hash']\n",
    "        \n",
    "        # 2. Busca modelo\n",
    "        if (not status_documento == 'NAO_PROCESSAR') or (not document_type == 'outros') or (one_page_doc == True):\n",
    "            \n",
    "            # 1. Analisa se PDF e pesquisavel\n",
    "            if atividade_processo_atual == 'analisar_pdf_pesquisavel':\n",
    "                \n",
    "                pdf_realmente_pequisavel, page_number = confirma_pdf_pequisavel3(file_path)\n",
    "                #if pdf_realmente_pequisavel:\n",
    "                    \n",
    "                qualquer_df.loc[idx, 'date_time'] = time_now\n",
    "                outro_tgt_df.loc[idx, 'date_time'] = time_now \n",
    "                \n",
    "                #print(f'seq: {seq_df} | pdf_pesquisavel: {pdf_realmente_pequisavel} | file: {original_file_name}')\n",
    "                qualquer_df.loc[idx, 'pdf_pesquisavel'] = pdf_realmente_pequisavel\n",
    "                outro_tgt_df.loc[idx, 'pdf_pesquisavel'] = pdf_realmente_pequisavel\n",
    "                \n",
    "                    \n",
    "            # 2. Busca prefeitura\n",
    "            elif atividade_processo_atual == 'pesquisar_prefeitura':\n",
    "                \n",
    "                if pdf_pesquisavel == True:\n",
    "                    text = pesquisa_prefeitura_pdf_pesquisavel(file_path)\n",
    "                    texto = texto_extraido_nf(text)\n",
    "                    \n",
    "                    template_usar, prefeitura_encontrada, cnpj_encontrado = pesquisa_dict_prefeitura_modelo(texto)\n",
    "                    \n",
    "                    if template_usar != 'NAO_PROCESSAR':\n",
    "                        \n",
    "                        model_src = template_usar\n",
    "                        tipo = \"document\"\n",
    "                        frame_model_pesquisado = pesquisa_model_frame(model_src, tipo)\n",
    "                        model_frame = True\n",
    "                        \n",
    "                        outro_tgt_df.loc[idx, 'date_time'] = time_now\n",
    "                        qualquer_df.loc[idx, 'date_time'] = time_now \n",
    "                        \n",
    "                        qualquer_df.loc[idx, 'prefeitura'] = prefeitura_encontrada\n",
    "                        outro_tgt_df.loc[idx, 'prefeitura'] = prefeitura_encontrada\n",
    "                        \n",
    "                        qualquer_df.loc[idx, 'modelo'] = frame_model_pesquisado\n",
    "                        outro_tgt_df.loc[idx, 'modelo'] = frame_model_pesquisado\n",
    "                        \n",
    "                        qualquer_df.loc[idx, 'model_frame'] = model_frame\n",
    "                        outro_tgt_df.loc[idx, 'model_frame'] = model_frame\n",
    "                        \n",
    "                        qualquer_df.loc[idx, 'cnpj'] = cnpj_encontrado\n",
    "                        outro_tgt_df.loc[idx, 'cnpj'] = cnpj_encontrado\n",
    "                        \n",
    "                        #print(f'template_usar: {template_usar} | frame_model_pesquisado{frame_model_pesquisado} | file_name: {original_file_name} ')\n",
    "                        \n",
    "                    else:\n",
    "                        #print(f'\\ntemplate_usar: {template_usar}  | file_name: {original_file_name}\\n') \n",
    "                        model_frame = False\n",
    "                        mensagem_status = \"Reavaliar_PDF_Pesquisavel\"\n",
    "                        \n",
    "                        qualquer_df.loc[idx, 'modelo'] = \"Nao Econtrato\"\n",
    "                        outro_tgt_df.loc[idx, 'modelo'] = \"Nao Econtrato\"\n",
    "                        \n",
    "                        qualquer_df.loc[idx, 'model_frame'] = model_frame\n",
    "                        outro_tgt_df.loc[idx, 'model_frame'] = model_frame\n",
    "                        \n",
    "                        \n",
    "                        qualquer_df.loc[idx, 'status_documento'] = mensagem_status\n",
    "                        outro_tgt_df.loc[idx, 'status_documento'] = mensagem_status\n",
    "                        \n",
    "                        #print(f'Nao ha template para este documento: {template_usar} model_frame: {model_frame} ')   \n",
    "\n",
    "                        \n",
    "                    # if model_frame:\n",
    "                        \n",
    "            elif atividade_processo_atual == 'Reavaliar_PDF_Pesquisavel':\n",
    "                \n",
    "                if status_documento == 'Reavaliar_PDF_Pesquisavel':\n",
    "                \n",
    "                    image_on_the_fly, image_resized_name = convertResizeAnalise_1page(original_file_name, file_path, image_resized_path)\n",
    "                    extracted_text_frame = pequisaTextoDoc(image_on_the_fly)\n",
    "                    texto = texto_extraido_nf(extracted_text_frame)\n",
    "                    \n",
    "                    template_usar, prefeitura_encontrada, cnpj_encontrado = pesquisa_dict_prefeitura_modelo(texto)\n",
    "                    imagens_list.append(image_resized_name)\n",
    "                    \n",
    "                    if template_usar != 'NAO_PROCESSAR':\n",
    "                        \n",
    "                        model_src = template_usar\n",
    "                        tipo = \"document\"\n",
    "                        frame_model_pesquisado = pesquisa_model_frame(model_src, tipo)\n",
    "                        model_frame = True\n",
    "                        mensagem_status = \"Template_encontrado\"\n",
    "                        \n",
    "                        outro_tgt_df.loc[idx, 'date_time'] = time_now\n",
    "                        qualquer_df.loc[idx, 'date_time'] = time_now \n",
    "                        \n",
    "                        qualquer_df.loc[idx, 'prefeitura'] = prefeitura_encontrada\n",
    "                        outro_tgt_df.loc[idx, 'prefeitura'] = prefeitura_encontrada\n",
    "                        \n",
    "                        qualquer_df.loc[idx, 'modelo'] = frame_model_pesquisado\n",
    "                        outro_tgt_df.loc[idx, 'modelo'] = frame_model_pesquisado\n",
    "                        \n",
    "                        qualquer_df.loc[idx, 'model_frame'] = model_frame\n",
    "                        outro_tgt_df.loc[idx, 'model_frame'] = model_frame\n",
    "                        \n",
    "                        qualquer_df.loc[idx, 'cnpj'] = cnpj_encontrado\n",
    "                        outro_tgt_df.loc[idx, 'cnpj'] = cnpj_encontrado\n",
    "                        \n",
    "                        qualquer_df.loc[idx, 'status_documento'] = mensagem_status\n",
    "                        outro_tgt_df.loc[idx, 'status_documento'] = mensagem_status\n",
    "                        \n",
    "                        qualquer_df.loc[idx, 'pdf_pesquisavel'] = False\n",
    "                        outro_tgt_df.loc[idx, 'pdf_pesquisavel'] = False\n",
    "                        \n",
    "                        #print(f'template_usar: {template_usar} | frame_model_pesquisado{frame_model_pesquisado} | file_name: {original_file_name} ')\n",
    "                        \n",
    "                    else:\n",
    "                        #print(f'\\ntemplate_usar: {template_usar}  | file_name: {original_file_name}\\n') \n",
    "                        model_frame = False\n",
    "                        mensagem_status = \"Reavaliar_Tipo_Documento\"\n",
    "                        \n",
    "                        qualquer_df.loc[idx, 'modelo'] = \"Nao Econtrato\"\n",
    "                        outro_tgt_df.loc[idx, 'modelo'] = \"Nao Econtrato\"\n",
    "                        \n",
    "                        qualquer_df.loc[idx, 'model_frame'] = model_frame\n",
    "                        outro_tgt_df.loc[idx, 'model_frame'] = model_frame\n",
    "                        \n",
    "                        \n",
    "                        qualquer_df.loc[idx, 'status_documento'] = mensagem_status\n",
    "                        outro_tgt_df.loc[idx, 'status_documento'] = mensagem_status\n",
    "                    \n",
    "                    #print(f'seq: {seq_df} | file: {original_file_name} | atividade: {atividade_processo_atual}\\n{template_usar} | {prefeitura_encontrada}\\n\\n')\n",
    "                    \n",
    "                # elif pdf_pesquisavel == False:    \n",
    "                #     image_on_the_fly, image_resized_name = convertResizeAnalise_1page(original_file_name, file_path, image_resized_path)\n",
    "                #     imagens_list.append(image_resized_name)\n",
    "                #     nome_prefeitura = pequisaModel(image_on_the_fly)\n",
    "                # else:\n",
    "                #     print(f' valor pdf_pesquisavel: {pdf_pesquisavel} | file: {original_file_name} - sugiro executar a atividade: analisar_pdf_pesquisavel')    \n",
    "                    \n",
    "            \n",
    "                # if nome_prefeitura:\n",
    "                #     print(f'\\n\\nSUCESSO: seq: {seq_df} | file: {original_file_name}   | nome_prefeitura: {nome_prefeitura} | status_documento: {status_documento}\\n\\n')\n",
    "                #     new_document_type = \"provavel_NFSe\"\n",
    "                #     new_status_documento = \"PROCESSAR\"\n",
    "                    \n",
    "                    # outro_tgt_df.loc[idx, 'date_time'] = time_now\n",
    "                    # qualquer_df.loc[idx, 'date_time'] = time_now  \n",
    "                    \n",
    "                    # qualquer_df.loc[idx, 'prefeitura'] = nome_prefeitura\n",
    "                    # outro_tgt_df.loc[idx, 'prefeitura'] = nome_prefeitura\n",
    "                \n",
    "                \n",
    "                \n",
    "                # outro_tgt_df.loc[idx, 'document_type'] = new_document_type\n",
    "                # outro_tgt_df.loc[idx, 'status_documento'] = new_status_documento\n",
    "                \n",
    "                # else:\n",
    "                #     print(f'\\nNAO ACHEI: seq: {seq_df} | file: {original_file_name}   | nome_prefeitura: {nome_prefeitura} | status_documento: {status_documento}')\n",
    "                #     new_document_type = \"outros\"\n",
    "                #     new_status_documento = \"NAO_PROCESSAR\"\n",
    "                \n",
    "                # outro_tgt_df.loc[idx, 'date_time'] = time_now    \n",
    "                # outro_tgt_df.loc[idx, 'document_type'] = new_document_type\n",
    "                # outro_tgt_df.loc[idx, 'status_documento'] = new_status_documento  \n",
    "                \n",
    "                # qualquer_df.loc[idx, 'date_time'] = time_now \n",
    "                # qualquer_df.loc[idx, 'document_type'] = new_document_type\n",
    "                # qualquer_df.loc[idx, 'status_documento'] = new_status_documento\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "     \n",
    "        i += 1\n",
    "    total_map = i - 1\n",
    "    print()\n",
    "    print(\"Documentos mapeados: \", total_map)\n",
    "    \n",
    "    return imagens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2B. XXX Efetuo a analise do pipeline de documentos e inicio da extracao de dados\n",
    "# analisar_pdf_pesquisavel\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'Reavaliar_PDF_Pesquisavel' #'pesquisar_prefeitura' # pesquisar_prefeitura  pesquisar_modelo    mensagem_status = \"Reavaliar_PDF_Pesquisavel\"\n",
    "status = 'mapear'\n",
    "\n",
    "imagens_list = analise_extracao_pipeline(subset_df_analise_pipe, df_analise_pipe, fase, atividade, status)\n",
    "\n",
    "\n",
    "if imagens_list:\n",
    "    remove_images(imagens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields_inscricao_prestador_raster_pdf(row, file_path, image_2work): \n",
    "    \n",
    "    model_src = row['modelo']\n",
    "    original_file_name = row['original_file_name'] \n",
    "    model = row['modelo']   \n",
    "   \n",
    "    secao = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    section = secao\n",
    "     \n",
    "    try:\n",
    "        f_frame_name = \"2_frame_inscricao_prestador\" \n",
    "        print(f'1 no try da insciao :  {model} se: {secao} |f_frame_name: {f_frame_name} | f_tipo: {f_tipo}')\n",
    "        raw_texto_pil = executa_model_frame(model, secao, f_frame_name, f_tipo, image_2work) \n",
    "        text_splited = texto_extraido(raw_texto_pil)\n",
    "        keyword_list = ['Inscrição Municipal:', 'Inscrição Estadual:']\n",
    "        string_pesquisa = \"Inscrição Municipal:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        inscricao_municipal_prestador_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Inscrição Estadual:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        inscricao_estadual_prestador_frame = texto\n",
    "      \n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} | {e}\")\n",
    "        inscricao_municipal_prestador_frame = \" \"\n",
    "        inscricao_estadual_prestador_frame = \" \"\n",
    "        raw_texto_pil = ' '\n",
    "        \n",
    "    return inscricao_municipal_prestador_frame, inscricao_estadual_prestador_frame, msg, raw_texto_pil   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields_prestador_raster_pdf(row, file_path, image_2work): \n",
    "  \n",
    "    message_erro = []\n",
    "  \n",
    "  \n",
    "    model_src = row['modelo']\n",
    "    original_file_name = row['original_file_name'] \n",
    "    model = row['modelo']   \n",
    "   \n",
    "    secao = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    section = secao\n",
    "    try:\n",
    "        f_frame_name = \"2_frame_cnpj_prestador\"\n",
    "        \n",
    "        raw_texto_pil = executa_model_frame(model, secao, f_frame_name, f_tipo, image_2work) \n",
    "        \n",
    "        # Extrair CPF/CNPJ com máscara 1\n",
    "        if \"CPF/CNPJ:\" in raw_texto_pil:\n",
    "            cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', raw_texto_pil)\n",
    "            if cpf_cnpj_formatado_match:\n",
    "                cpf_cnpj_com_mascara_prestador = cpf_cnpj_formatado_match.group(1)\n",
    "                cpf_cnpj_sem_mascara_prestador = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "        else:\n",
    "            cpf_cnpj_com_mascara_prestador = None\n",
    "            cpf_cnpj_sem_mascara_prestador = None            \n",
    "            \n",
    "            \n",
    "        telefone_str = None\n",
    "    \n",
    "        #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "        telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', raw_texto_pil)\n",
    "        if telefone_match: \n",
    "            telefone_str = telefone_match.group(1)\n",
    "            # Remover quebras de linha\n",
    "            telefone_str = telefone_str.replace('.', '')\n",
    "            telefone_str = telefone_str.replace('\\n', '')\n",
    "                    \n",
    "            telefone_prestatador_frame = telefone_str\n",
    "        else:\n",
    "            telefone_prestatador_frame = None \n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} | {e}\")\n",
    "        cpf_cnpj_com_mascara_prestador = None\n",
    "        cpf_cnpj_sem_mascara_prestador = None\n",
    "        telefone_prestatador_frame = None  \n",
    "             \n",
    "        \n",
    "    try:\n",
    "        f_frame_name = \"2_frame_inscricao_prestador\" \n",
    "        \n",
    "        raw_texto_pil = executa_model_frame(model, secao, f_frame_name, f_tipo, image_2work) \n",
    "        text_splited = texto_extraido(raw_texto_pil)\n",
    "        keyword_list = ['Inscrição Municipal:', 'Inscrição Estadual:']\n",
    "        string_pesquisa = \"Inscrição Municipal:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        inscricao_municipal_prestador_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Inscrição Estadual:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        inscricao_estadual_prestador_frame = texto\n",
    "      \n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} | {e}\")\n",
    "        inscricao_municipal_prestador_frame = \" \"\n",
    "        inscricao_estadual_prestador_frame = \" \"\n",
    "\n",
    "  \n",
    "    try:\n",
    "        f_frame_name = \"2_frame_dados_prestador\"\n",
    "        f_tipo = 'frame'\n",
    "        \n",
    "        \n",
    "        raw_texto_pil = executa_model_frame(model, secao, f_frame_name, f_tipo, image_2work)\n",
    "        \n",
    "        keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "        \n",
    "        string_pesquisa = \"Nome/Razão Social:\"\n",
    "        text_splited = texto_extraido(raw_texto_pil)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        razao_social_prestador_frame = texto\n",
    "\n",
    "        string_pesquisa = \"Nome de Fantasia:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        nome_fantasia_prestador_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Endereço:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        endereco_prestador_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"E-mail:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        email_prestador_frame = texto\n",
    "        \n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} | {e}\")\n",
    "        razao_social_prestador_frame = \" \"\n",
    "        nome_fantasia_prestador_frame = \" \"\n",
    "        endereco_prestador_frame = \" \"\n",
    "        email_prestador_frame = \" \"\n",
    "        \n",
    "    return cpf_cnpj_com_mascara_prestador, cpf_cnpj_sem_mascara_prestador, telefone_prestatador_frame, inscricao_municipal_prestador_frame, inscricao_estadual_prestador_frame, razao_social_prestador_frame, nome_fantasia_prestador_frame, endereco_prestador_frame, email_prestador_frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields_cnpj_prestador_raster_pdf(row, file_path, image_2work): \n",
    "  \n",
    "    message_erro = []\n",
    "  \n",
    "  \n",
    "    model_src = row['modelo']\n",
    "    original_file_name = row['original_file_name'] \n",
    "    model = row['modelo']  \n",
    "    \n",
    "    f_tipo = 'frame' \n",
    "   \n",
    "    secao = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    section = secao\n",
    "    print(f'1- antes do try {model} se: {secao} | f_tipo: {f_tipo}')\n",
    "    \n",
    "    f_frame_name = \"2_frame_cnpj_prestador\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    raw_texto_pil = executa_model_frame(model, secao, f_frame_name, f_tipo, image_2work) \n",
    "    \n",
    "    print(f'2 dentro do try {model} se: {secao} |f_frame_name: {f_frame_name} | f_tipo: {f_tipo} | raw_texto_pil: {raw_texto_pil}')\n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in raw_texto_pil:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', raw_texto_pil)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "            cpf_cnpj_com_mascara_prestador = cpf_cnpj_formatado_match.group(1)\n",
    "            cpf_cnpj_sem_mascara_prestador = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "    else:\n",
    "        cpf_cnpj_com_mascara_prestador = None\n",
    "        cpf_cnpj_sem_mascara_prestador = None            \n",
    "            \n",
    "            \n",
    "    telefone_str = None\n",
    "\n",
    "    #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "    telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', raw_texto_pil)\n",
    "    if telefone_match: \n",
    "        telefone_str = telefone_match.group(1)\n",
    "        # Remover quebras de linha\n",
    "        telefone_str = telefone_str.replace('.', '')\n",
    "        telefone_str = telefone_str.replace('\\n', '')\n",
    "                \n",
    "        telefone_prestatador_frame = telefone_str\n",
    "    else:\n",
    "        telefone_prestatador_frame = None \n",
    "\n",
    "             \n",
    "    return cpf_cnpj_com_mascara_prestador, cpf_cnpj_sem_mascara_prestador, telefone_prestatador_frame, raw_texto_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta e o principio da melhor funcao do mundo\n",
    "def extracao_pipeline(qualquer_df, fase, atividade, status):\n",
    "    \n",
    "    linhas_analise = []\n",
    "    bloco_1_list = []\n",
    "    bloco_2_list = []\n",
    "    bloco_3_list = []\n",
    "    imagens_list = []   \n",
    "    \n",
    "    pre_processo = ['nro_nota', 'competencia', 'dt_hr_emissao', 'codigo_verificacao', 'ajustar_nome', 'split_paginas', 'ajustar_imagem', 'buscar_nome_prefeitura', 'enviar_canceladas', 'enviar_listagens']\n",
    "    \n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    \n",
    "    fase_processo_atual = fase\n",
    "    atividade_processo_atual = atividade\n",
    "    status_documento_atual = status\n",
    "    \n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        message_erro = []\n",
    "        \n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        document_unique_id = idx\n",
    "        seq_df = row['seq']\n",
    "        batch_name = row['batch']\n",
    "        fase_processo = row['fase_processo']\n",
    "        nome_atividade = row['nome_atividade']\n",
    "        status_documento = row['status_documento']\n",
    "        original_file_name = row['original_file_name']\n",
    "        file_directory = row['directory']\n",
    "        level = row['level']\n",
    "        d_type = row['level']\n",
    "        document_type = row['document_type']\n",
    "        pdf_pesquisavel = row['pdf_pesquisavel']\n",
    "        one_page_doc = row['one_page']\n",
    "        modelo = row['modelo']\n",
    "        \n",
    "        prefeitura = row['prefeitura']\n",
    "        \n",
    "        parent_document_unique_id = row['parent_document_unique_id']\n",
    "        file_path = row['file_path']\n",
    "        file_hash = row['file_hash']\n",
    "        \n",
    "        # 2. Busca modelo\n",
    "        if (not status_documento == 'NAO_PROCESSAR') or (not document_type == 'outros') or (not one_page_doc == True):\n",
    "            \n",
    "            \n",
    "            # 1. Analisa se PDF e pesquisavel\n",
    "            if atividade_processo_atual == 'extracao_cabecalho':\n",
    "                #print(f' 1 - seq: {seq_df} | file: {original_file_name} |status_documento: {status_documento} pdf_pesquisavel: {pdf_pesquisavel}\\n')\n",
    "                if status_documento == status_documento_atual:\n",
    "                    if pdf_pesquisavel == True:\n",
    "                        \n",
    "                        nro_nota_frame, competencia_frame, dt_hr_emissao_frame, codigo_verificacao_frame, message_erro, section = extract_fields_cabecalho_pdf_pesquisavel(row, file_path)\n",
    "\n",
    "                    elif pdf_pesquisavel == False:\n",
    "                        \n",
    "                        doc2convert = original_file_name\n",
    "                       \n",
    "                        image_2work, image_resized_name = convertResize(doc2convert, file_path, image_resized_path)\n",
    "                        #image_2work, name_image_2work = convertResize(doc2convert, file_path, image_resized_path)\n",
    "                        \n",
    "                        #print(f' 0 - seq: {seq_df} | file: {original_file_name} |status_documento: {status_documento} pdf_pesquisavel: {pdf_pesquisavel} {name_image_2work}\\n')\n",
    "                        nro_nota_frame, competencia_frame, dt_hr_emissao_frame, codigo_verificacao_frame, message_erro, section, raw_texto = extract_fields_cabecalho_raster_pdf(row, file_path, image_2work)\n",
    "                        \n",
    "                        print(f'seq_df: {seq_df} | {original_file_name} | nro_nota: {nro_nota_frame} | competencia: {competencia_frame} | dt_hr_emissao: {dt_hr_emissao_frame} |  codigo_verificacao: {codigo_verificacao_frame}    raw_texto: raw_texto')\n",
    "        \n",
    "                    status_message = 'processado'\n",
    "                    qualquer_df.loc[idx, 'date_time'] = time_now\n",
    "                    qualquer_df.loc[idx, 'nro_nota'] = nro_nota_frame\n",
    "                    qualquer_df.loc[idx, 'competencia'] = competencia_frame\n",
    "                    qualquer_df.loc[idx, 'dt_hr_emissao'] = dt_hr_emissao_frame\n",
    "                    qualquer_df.loc[idx, 'codigo_verificacao'] = codigo_verificacao_frame \n",
    "                    \n",
    "            \n",
    "            elif atividade_processo_atual == 'extracao_prestador':\n",
    "            #print(f' 1 - seq: {seq_df} | file: {original_file_name} |status_documento: {status_documento} pdf_pesquisavel: {pdf_pesquisavel}\\n')\n",
    "                if status_documento == status_documento_atual:\n",
    "                    if pdf_pesquisavel == True:\n",
    "                        \n",
    "                        nro_nota_frame, competencia_frame, dt_hr_emissao_frame, codigo_verificacao_frame, message_erro, section = extract_fields_cabecalho_pdf_pesquisavel(row, file_path)\n",
    "\n",
    "                    elif pdf_pesquisavel == False:\n",
    "                        \n",
    "                        doc2convert = original_file_name\n",
    "                        \n",
    "                        image_2work, image_resized_name = convertResize(doc2convert, file_path, image_resized_path)\n",
    "                        #image_2work, name_image_2work = convertResize(doc2convert, file_path, image_resized_path)\n",
    "                        \n",
    "                        #print(f' 0 - seq: {seq_df} | file: {original_file_name} |status_documento: {status_documento} pdf_pesquisavel: {pdf_pesquisavel} {name_image_2work}\\n')\n",
    "                        cpf_cnpj_com_mascara_prestador, cpf_cnpj_sem_mascara_prestador, telefone_prestatador_frame, raw_texto_pil = extract_fields_cnpj_prestador_raster_pdf(row, file_path, image_2work)\n",
    "                        \n",
    "                        print(f'seq_df: {seq_df} | file: {original_file_name} | {cpf_cnpj_com_mascara_prestador} ')\n",
    "                        \n",
    "                        inscricao_municipal_prestador_frame, inscricao_estadual_prestador_frame, msg, raw_texto_pil  = extract_fields_inscricao_prestador_raster_pdf(row, file_path, image_2work)\n",
    "                        \n",
    "                        print(f'seq_df: {seq_df} | file: {original_file_name} | {inscricao_municipal_prestador_frame} ')\n",
    "                        \n",
    "                        #cpf_cnpj_com_mascara_prestador, cpf_cnpj_sem_mascara_prestador, telefone_prestatador_frame, insc_muni_prest, insc_estad_prest, razao_prest, nome_fan_prest, end_prest, email_prest = extract_fields_prestador_raster_pdf(row, file_path, image_2work)\n",
    "                        # \n",
    "                        \n",
    "                        \n",
    "        \n",
    "                        # status_message = 'processado'\n",
    "                        # qualquer_df.loc[idx, 'date_time'] = time_now\n",
    "                        # qualquer_df.loc[idx, 'nro_nota'] = nro_nota_frame\n",
    "                        # qualquer_df.loc[idx, 'competencia'] = competencia_frame\n",
    "                        # qualquer_df.loc[idx, 'dt_hr_emissao'] = dt_hr_emissao_frame\n",
    "                        # qualquer_df.loc[idx, 'codigo_verificacao'] = codigo_verificacao_frame         \n",
    "                    \n",
    "                \n",
    "\n",
    "                                    \n",
    "\n",
    "\n",
    "                i += 1\n",
    "    total_map = i - 1\n",
    "    print(\"Documentos mapeados: \", total_map)\n",
    "    \n",
    "    return raw_texto_pil\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3B. XXX Efetuo a extracao de dados\n",
    "# analisar_pdf_pesquisavel\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'extracao_prestador' #'pesquisar_prefeitura' # pesquisar_prefeitura  pesquisar_modelo    mensagem_status = \"Reavaliar_PDF_Pesquisavel\"\n",
    "status = 'OK'\n",
    "\n",
    "#raw_texto = extracao_pipeline(df_analise_pipe, fase, atividade, status)\n",
    "raw_texto = extracao_pipeline(subset_df_analise_pipe, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Recentemente criada para estrair dados Prestador - TEM QUE REFACTORIZAR\n",
    "def bloco_prestador(row, file_path):\n",
    "    \n",
    "    message_erro = []        \n",
    "    # 2. PRESTADOR DE SERVIÇO\n",
    "    # Definir retângulo de interesse\n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "    nf_data_prestador = {}\n",
    "    x0 = 0\n",
    "    y0 = 100\n",
    "    x1 = 600\n",
    "    y1 = 236  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    \n",
    "    nf_data_prestador = Extc.extract_fields_prestador(text)\n",
    "    try:\n",
    "        razao_social = nf_data_prestador['razao_social']\n",
    "        telefone = nf_data_prestador['telefone']\n",
    "        cpf_cnpj_com_mascara = nf_data_prestador['cpf_cnpj_com_mascara']\n",
    "        cpf_cnpj_sem_mascara = nf_data_prestador['cpf_cnpj_sem_mascara']\n",
    "        inscricao_municipal = nf_data_prestador['inscricao_municipal']\n",
    "        inscricao_estadual = nf_data_prestador['inscricao_estadual']\n",
    "        nome_fantasia = nf_data_prestador['nome_fantasia']\n",
    "        endereco = nf_data_prestador['endereco']\n",
    "        email = nf_data_prestador['email']\n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} |  {e}\")\n",
    "        message_erro.append(msg)\n",
    "        razao_social = \" \"\n",
    "        telefone = \" \"\n",
    "        cpf_cnpj_com_mascara = \" \"\n",
    "        cpf_cnpj_sem_mascara = \" \"\n",
    "        inscricao_municipal = \" \"\n",
    "        inscricao_estadual = \" \"\n",
    "        razao_social = \" \"\n",
    "        nome_fantasia = \" \"\n",
    "        endereco = \" \"\n",
    "        email = \" \"\n",
    "    pdf_document.close()    \n",
    "    return razao_social, telefone, cpf_cnpj_com_mascara, cpf_cnpj_sem_mascara, inscricao_municipal, inscricao_estadual, nome_fantasia, endereco, email, message_erro   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3A. XXX Efetuo a extracao de dados\n",
    "# analisar_pdf_pesquisavel\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'extracao_cabecalho' #'pesquisar_prefeitura' # pesquisar_prefeitura  pesquisar_modelo    mensagem_status = \"Reavaliar_PDF_Pesquisavel\"\n",
    "status = 'OK'\n",
    "\n",
    "#raw_texto = extracao_pipeline(df_analise_pipe, fase, atividade, status)\n",
    "raw_texto = extracao_pipeline(subset_df_analise_pipe, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "subset_df_analise_pipe = df_analise_pipe.query('seq == 59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Salvando o DF (IMPORTANTE)\n",
    "df_analise_pipe.to_excel(\"df_mapeamento_e_analise2.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Criando uma nova coluna no DF\n",
    "df_analise_pipe.insert(loc=17, column='s_act', value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_images(imagens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustando dados do DF e efetuando queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise_pipe['pdf_pesquisavel'] = df_analise_pipe.apply(lambda row: row['coluna1'] * 2 if row['coluna2'] > 0 else row['coluna1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizei o DF baseado em condiçao de outra coluna\n",
    "df_analise_pipe['pdf_pesquisavel'] = df_analise_pipe.apply(lambda row: False if row['status_documento'] == \"Template_encontrado\" else row['pdf_pesquisavel'], axis=1)\n",
    "\n",
    "\n",
    "# Podemos chegar de forma mais rapida\n",
    "df_analise_pipe.loc[df_analise_pipe['status_documento'] == \"Template_encontrado\", 'pdf_pesquisavel'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df_analise_pipe.quert('nro_nota')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efetuando pesquisa no DF\n",
    "df_analise_pipe.query('pdf_pesquisavel == False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise_pipe.query('nro_nota == 13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Para Analise de PDF Pesquisavel, uei uma copia do df_analise_pipe\n",
    "subset_df_analise_pipe = df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "subset_df_analise_pipe = df_analise_pipe.query('seq < 21 & seq > 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Criando uma nova coluna no DF\n",
    "df_analise_pipe.insert(loc=13, column='model_frame', value=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>1.</b> Efetuando subset do df para pesquisa </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Usando iloc para Filtrar um Número Específico de Linhas\n",
    "subset_df_analise_pipe = df_analise_pipe.iloc[:10]\n",
    "\n",
    "subset_df_analise_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Usando loc para Filtrar Baseado em Condições\n",
    "subset_df = df_scan_pipe.loc[df_scan_pipe['directory'] == 'camaleao']\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert file_hash == file_hash2, \"Os arquivos são diferentes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Usando iloc para Filtrar um Número Específico de Linhas\n",
    "subset_df = df_scan_pipe.iloc[:10]\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "subset_df = df_scan_pipe.query('directory == \"root_dir\" & seq < 10 & seq > 7')\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Filtrando Linhas Baseadas em Valores em uma Lista\n",
    "\n",
    "valores = [11, 16, 30, 41]\n",
    "subset_df = df_scan_pipe[df_scan_pipe['seq'].isin(valores)]\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registro_específico = df_scan_pipe.loc['e1f4b1af-30f3-45d2-85a7-1bb895bd5325']\n",
    "\n",
    "registro_específico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Famigerado Outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    acoes_necessarias = {}\n",
    "                        \n",
    "                    if nro_nota_frame == \" \":\n",
    "                        acoes_necessarias['nro_nota'] = True # 'nro_nota'\n",
    "\n",
    "                    if competencia_frame == \" \":\n",
    "                        acoes_necessarias['competencia'] = True # 'competencia'\n",
    "\n",
    "                    if dt_hr_emissao_frame == \" \":\n",
    "                        acoes_necessarias['dt_hr_emissao'] = True #'dt_hr_emissao'\n",
    "\n",
    "                    if codigo_verificacao_frame == \" \":\n",
    "                        acoes_necessarias['codigo_verificacao'] = True #'codigo_verificacao'\n",
    "\n",
    "                    info_list = []\n",
    "                    seq_list = []\n",
    "                    for action in pre_processo:\n",
    "                        if action in acoes_necessarias:\n",
    "                            seq = pre_processo.index(action) + 1  # Pega a sequência da ação baseada na lista pre_processo\n",
    "                            n = seq - 1\n",
    "                            seq_list.append(seq)\n",
    "                            info = pre_processo[n]\n",
    "                            info_list.append(info)\n",
    "                            #print(f' 1 - seq_list: {seq_list} | info_list: {info_list} ')\n",
    "                            \n",
    "                            if (len(seq_list) > 4) and (1 not in seq_list):\n",
    "                                status_documento = \"REPROCESSAR\"\n",
    "                            elif (len(set(seq_list)) < len(seq_list)):\n",
    "                                status_documento = \"REVISAR - Erros duplicados\"\n",
    "                            elif (4 in seq_list) and (10 in seq_list):\n",
    "                                status_documento = \"REJEITAR - Erros críticos presentes\"\n",
    "                            else:\n",
    "                                status_documento = \"OK\"\n",
    "                                \n",
    "                            # numero_erros = ','.join(seq_list) \n",
    "                            # descricao_erros = ','.join(info_list) \n",
    "                            \n",
    "                            numero_erros = ','.join(map(str, seq_list))\n",
    "                            descricao_erros = ','.join(info_list)\n",
    "\n",
    "                            \n",
    "                            qualquer_df.loc[idx, 's_act'] = numero_erros\n",
    "                            qualquer_df.loc[idx, 'process_action'] = descricao_erros\n",
    "                            qualquer_df.loc[idx, 'status_documento'] = status_documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    # if razao_social == \" \":\n",
    "                    #     acoes_necessarias['razao_social'] = True #'razao_social'\n",
    "\n",
    "                    # if cpf_cnpj_com_mascara == \" \":\n",
    "                    #     acoes_necessarias['cpf_cnpj_com_mascara'] = True #'cpf_cnpj_com_mascara'     \n",
    "\n",
    "                    # if not pdf_realmente_pequisavel:\n",
    "                    #     acoes_necessarias['ajustar_imagem'] =  True\n",
    "                        \n",
    "                    # if not doc_1_page:\n",
    "                    #     acoes_necessarias['split_paginas'] = True\n",
    "                    \n",
    "                    # if new_name != original_file_name:\n",
    "                    #     acoes_necessarias['ajustar_nome'] =  True\n",
    "                        \n",
    "                    # if not nome_prefeitura:\n",
    "                    #     acoes_necessarias['buscar_nome_prefeitura'] = True    \n",
    "                        \n",
    "                    # if \"cancelada\" in original_file_name.lower():\n",
    "                    #     acoes_necessarias['enviar_canceladas'] = True\n",
    "                        \n",
    "                    # if \"listagem\" in original_file_name.lower():\n",
    "                    #     acoes_necessarias['enviar_listagens'] = True                   \n",
    "                   \n",
    "                   \n",
    "                   #time_now = cron.timenow_pt_BR()\n",
    "                    #bloco_1 = {\n",
    "                    #     'document_unique_id': idx,\n",
    "                    #     'seq': i,\n",
    "                    #     'date_time': cron.timenow_pt_BR(),\n",
    "                    #     'fase_processo': fase_atual,\n",
    "                    #     'nome_atividade': nome_atividade,\n",
    "                    #     'batch': batch_name,\n",
    "                    #     'original_file_name': original_file_name,\n",
    "                    #     'directory': file_dir,\n",
    "                    #     'pdf_pesq': pdf_realmente_pequisavel,\n",
    "                    #     'status_documento': status_documento, \n",
    "                    #     's_act': seq_list,\n",
    "                    #     'process_action': info_list,\n",
    "                    # }    \n",
    "                    # bloco_1_list.append(bloco_1)\n",
    "                    \n",
    "                    # bloco_2 = { \n",
    "                    #     'document_unique_id': idx,   \n",
    "                    #     'one_page': doc_1_page,\n",
    "                    #     'prefeitura': nome_prefeitura,\n",
    "                    #     'nro_nota': nro_nota,\n",
    "                    #     'competencia':competencia,\n",
    "                    #     'dt_hr_emissao':dt_hr_emissao,\n",
    "                    #     'codigo_verificacao':codigo_verificacao,\n",
    "                    #     'pages': nro_paginas,\n",
    "                    #     'razao_social':razao_social,\n",
    "                    #     'telefone': telefone,\n",
    "                    # }    \n",
    "                    # bloco_2_list.append(bloco_2)\n",
    "                    \n",
    "                    # bloco_3 = {\n",
    "                    #     'document_unique_id': idx,    \n",
    "                    #     'cpf_cnpj_com_mascara': cpf_cnpj_com_mascara,\n",
    "                    #     'cpf_cnpj_sem_mascara': cpf_cnpj_sem_mascara,\n",
    "                    #     'inscricao_municipal': inscricao_municipal,\n",
    "                    #     'inscricao_estadual': inscricao_estadual,\n",
    "                    #     'razao_social': razao_social,\n",
    "                    #     'nome_fantasia': nome_fantasia,\n",
    "                    #     'endereco': endereco,\n",
    "                    #     'email': email,\n",
    "                    #     'aditional_information': message_erro,\n",
    "                    #     'file_hash': file_hash,\n",
    "                    #     'file_path': file_path,\n",
    "                    # }\n",
    "                    # bloco_3_list.append(bloco_3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Criando um DataFrame de exemplo\n",
    "data = {'A': [1, 2], 'B': [3, 4], 'Link': ['http://www.google.com', 'http://www.github.com']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Criando um escritor de Excel\n",
    "with pd.ExcelWriter('path_to_file.xlsx', engine='xlsxwriter') as writer:\n",
    "    \n",
    "    # Escrevendo o DataFrame para o Excel\n",
    "    df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "    \n",
    "    # Obtendo o objeto workbook e sheet\n",
    "    workbook  = writer.book\n",
    "    worksheet = writer.sheets['Sheet1']\n",
    "    \n",
    "    # Adicionando um formato de hyperlink\n",
    "    hyperlink_format = workbook.add_format({'hyperlink': True, 'color': 'blue', 'underline': 1})\n",
    "    \n",
    "    # Fazendo um loop através dos links e aplicando o formato de hyperlink\n",
    "    for row_num, link in enumerate(df['Link'], start=1):\n",
    "        worksheet.write_url(row_num, df.shape[1] - 1, link, hyperlink_format)\n",
    "\n",
    "# Agora, na coluna \"Link\" do seu arquivo Excel, você terá links clicáveis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prefeitura(model, father, values):\n",
    "    \n",
    "    tipo = \"sframe_field\"\n",
    "    data_extrated_prefeitura = {}\n",
    "    #print(tipo)\n",
    "\n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model) & (frames_nf_v4_df['father'] == father) & (frames_nf_v4_df['type'] == tipo)]\n",
    "\n",
    "    for index_sframe, row_sframe in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        label_value = row_sframe['label']\n",
    "        \n",
    "        #print(\"label_value\", label_value)\n",
    "        \n",
    "        if label_value == \"nome_prefeitura\":\n",
    "            reference_value = row_sframe['reference']\n",
    "            for value in values:\n",
    "                result = process_line(value, reference_value, label_value)\n",
    "                if result:\n",
    "                    data_extrated_prefeitura.update(result)\n",
    "        elif label_value == \"secretaria\":\n",
    "            reference_value = row_sframe['reference']\n",
    "            for value in values:\n",
    "                result = process_line(value, reference_value, label_value)\n",
    "                if result:\n",
    "                    data_extrated_prefeitura.update(result) \n",
    "        elif label_value == \"tipo_nota_fiscal\":\n",
    "            reference_value = row_sframe['reference']  \n",
    "            for value in values:\n",
    "                result = process_line(value, reference_value, label_value)\n",
    "                if result:\n",
    "                    data_extrated_prefeitura.update(result)\n",
    "                    \n",
    "    return data_extrated_prefeitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        try:\n",
    "            os.remove(file_delete_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Nao congui deletar: {e}\") \n",
    "            \n",
    "            df_analise_pipe, documentos = scan_pipeline_documentos(documentos_scan_path, batch_name, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        try:\n",
    "            os.remove(file_delete_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Nao congui deletar: {e}\") \n",
    "            \n",
    "            df_analise_pipe, documentos = scan_pipeline_documentos(documentos_scan_path, batch_name, fase, atividade, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta e o principio da melhor funcao do mundo\n",
    "def analise_extracao_pipeline(qualquer_df, fase, atividade, status):\n",
    "    \n",
    "   \n",
    "    linhas_analise = []\n",
    "    bloco_1_list = []\n",
    "    bloco_2_list = []\n",
    "    bloco_3_list = []   \n",
    "    \n",
    "    pre_processo = ['ajustar_nome', 'split_paginas', 'ajustar_imagem', 'buscar_nome_prefeitura', 'enviar_canceladas', 'enviar_listagens']\n",
    "    \n",
    "    \n",
    "    \n",
    "    fase_processo_atual = fase\n",
    "    atividade_processo_atual = atividade\n",
    "    status_documento_atual = status\n",
    "    \n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        message_erro = []\n",
    "        \n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        document_unique_id = idx\n",
    "        batch_name = row['batch']\n",
    "        batch_name = row['batch']\n",
    "        fase_processo = row['fase_processo']\n",
    "        nome_atividade = row['nome_atividade']\n",
    "        status_documento = row['status_documento']\n",
    "        original_file_name = row['original_file_name']\n",
    "        file_directory = row['directory']\n",
    "        level = row['level']\n",
    "        d_type = row['level']\n",
    "        document_type = row['document_type']\n",
    "        parent_document_unique_id = row['parent_document_unique_id']\n",
    "        file_path = row['file_path']\n",
    "        file_hash = row['file_hash']\n",
    "        \n",
    "        # Chama a função conv_filename\n",
    "        #new_name = conv_filename(original_file_name)\n",
    "\n",
    "\n",
    "        # 2. Busca modelo\n",
    "        image_on_the_fly, image_resized_name = convertResizeAnalise_1page(original_file_name, file_path, image_resized_path)\n",
    "        nome_prefeitura = pequisaModel(image_on_the_fly)\n",
    "      \n",
    "        # nro_nota, competencia, dt_hr_emissao, codigo_verificacao, message_erro = bloco_prefeitura(row, file_path)\n",
    "        # razao_social, telefone, cpf_cnpj_com_mascara, cpf_cnpj_sem_mascara, inscricao_municipal, inscricao_estadual, nome_fantasia, endereco, email, message_erro = bloco_prestador(row, file_path)    \n",
    "\n",
    "        # acoes_necessarias = {}\n",
    "        \n",
    "        # if not doc_1_page:\n",
    "        #     acoes_necessarias['split_paginas'] = nro_paginas\n",
    "        \n",
    "        # if nro_nota == \" \":\n",
    "        #     acoes_necessarias['nro_nota'] = True # 'nro_nota'\n",
    "\n",
    "        # if competencia == \" \":\n",
    "        #     acoes_necessarias['competencia'] = True # 'competencia'\n",
    "\n",
    "        # if dt_hr_emissao == \" \":\n",
    "        #     acoes_necessarias['dt_hr_emissao'] = True #'dt_hr_emissao'\n",
    "\n",
    "        # if codigo_verificacao == \" \":\n",
    "        #     acoes_necessarias['codigo_verificacao'] = True #'codigo_verificacao'\n",
    "\n",
    "        # if razao_social == \" \":\n",
    "        #     acoes_necessarias['razao_social'] = True #'razao_social'\n",
    "\n",
    "        # if cpf_cnpj_com_mascara == \" \":\n",
    "        #     acoes_necessarias['cpf_cnpj_com_mascara'] = True #'cpf_cnpj_com_mascara'     \n",
    "\n",
    "        # if not pdf_realmente_pequisavel:\n",
    "        #     acoes_necessarias['ajustar_imagem'] =  True\n",
    "            \n",
    "        # if not doc_1_page:\n",
    "        #     acoes_necessarias['split_paginas'] = True\n",
    "        \n",
    "        # if new_name != original_file_name:\n",
    "        #     acoes_necessarias['ajustar_nome'] =  True\n",
    "            \n",
    "        # if not nome_prefeitura:\n",
    "        #     acoes_necessarias['buscar_nome_prefeitura'] = True    \n",
    "            \n",
    "        # if \"cancelada\" in original_file_name.lower():\n",
    "        #     acoes_necessarias['enviar_canceladas'] = True\n",
    "            \n",
    "        # if \"listagem\" in original_file_name.lower():\n",
    "        #     acoes_necessarias['enviar_listagens'] = True\n",
    "                \n",
    "        # info_list = []\n",
    "        # seq_list = []\n",
    "        # for action in pre_processo:\n",
    "        #     if action in acoes_necessarias:\n",
    "        #         seq = pre_processo.index(action) + 1  # Pega a sequência da ação baseada na lista pre_processo\n",
    "        #         n = seq - 1\n",
    "        #         seq_list.append(seq)\n",
    "        #         info = pre_processo[n]\n",
    "        #         info_list.append(info)\n",
    "        #         if (len(seq_list) > 4) and (1 not in seq_list):\n",
    "        #             status_documento = \"REPROCESSAR\"\n",
    "        #         elif (len(set(seq_list)) < len(seq_list)):\n",
    "        #             status_documento = \"REVISAR - Erros duplicados\"\n",
    "        #         elif (4 in seq_list) and (10 in seq_list):\n",
    "        #             status_documento = \"REJEITAR - Erros críticos presentes\"\n",
    "        #         else:\n",
    "        #             status_documento = \"OK\"  \n",
    "                        \n",
    "        #time_now = cron.timenow_pt_BR()\n",
    "        # bloco_1 = {\n",
    "        #     'document_unique_id': idx,\n",
    "        #     'seq': i,\n",
    "        #     'date_time': cron.timenow_pt_BR(),\n",
    "        #     'fase_processo': fase_atual,\n",
    "        #     'nome_atividade': nome_atividade,\n",
    "        #     'batch': batch_name,\n",
    "        #     'original_file_name': original_file_name,\n",
    "        #     'directory': file_dir,\n",
    "        #     'pdf_pesq': pdf_realmente_pequisavel,\n",
    "        #     'status_documento': status_documento, \n",
    "        #     's_act': seq_list,\n",
    "        #     'process_action': info_list,\n",
    "        # }    \n",
    "        # bloco_1_list.append(bloco_1)\n",
    "        \n",
    "        # bloco_2 = { \n",
    "        #     'document_unique_id': idx,   \n",
    "        #     'one_page': doc_1_page,\n",
    "        #     'prefeitura': nome_prefeitura,\n",
    "        #     'nro_nota': nro_nota,\n",
    "        #     'competencia':competencia,\n",
    "        #     'dt_hr_emissao':dt_hr_emissao,\n",
    "        #     'codigo_verificacao':codigo_verificacao,\n",
    "        #     'pages': nro_paginas,\n",
    "        #     'razao_social':razao_social,\n",
    "        #     'telefone': telefone,\n",
    "        # }    \n",
    "        # bloco_2_list.append(bloco_2)\n",
    "        \n",
    "        # bloco_3 = {\n",
    "        #     'document_unique_id': idx,    \n",
    "        #     'cpf_cnpj_com_mascara': cpf_cnpj_com_mascara,\n",
    "        #     'cpf_cnpj_sem_mascara': cpf_cnpj_sem_mascara,\n",
    "        #     'inscricao_municipal': inscricao_municipal,\n",
    "        #     'inscricao_estadual': inscricao_estadual,\n",
    "        #     'razao_social': razao_social,\n",
    "        #     'nome_fantasia': nome_fantasia,\n",
    "        #     'endereco': endereco,\n",
    "        #     'email': email,\n",
    "        #     'aditional_information': message_erro,\n",
    "        #     'file_hash': file_hash,\n",
    "        #     'file_path': file_path,\n",
    "        # }\n",
    "        # bloco_3_list.append(bloco_3) \n",
    "\n",
    "        \n",
    "        i += 1\n",
    "    total_map = i - 1\n",
    "    print(\"Documentos mapeados: \", total_map)\n",
    "    \n",
    "    # bloco_1_df = pd.DataFrame(bloco_1_list)\n",
    "    # bloco_2_df = pd.DataFrame(bloco_2_list)\n",
    "    # bloco_3_df = pd.DataFrame(bloco_3_list)\n",
    "    \n",
    "    # df_map_analise = pd.concat([bloco_1_df.set_index('document_unique_id'), \n",
    "    #                             bloco_2_df.set_index('document_unique_id'), \n",
    "    #                             bloco_3_df.set_index('document_unique_id')], axis=1)\n",
    "\n",
    "    # df_map_analise = pd.DataFrame(linhas_analise)\n",
    "    \n",
    "    # return df_map_analise, bloco_1_list, bloco_2_list, bloco_3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTANTE - COPIA APENAS PARA REFERENCIA ######\n",
    "#            NAO AJUSTAR SEM FALAR COMIGO          #\n",
    "####################################################\n",
    "# 7. XXX Pesquisa prefeitura no documento (dando as coordenadas) e efetuando o OCR\n",
    "# 1. Geraçao do hash do arquivo\n",
    "def generate_file_hash(file_path):\n",
    "    # Abre o arquivo em modo de leitura de bytes\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # Lê o conteúdo do arquivo\n",
    "        file_data = f.read()\n",
    "        # Utiliza o algoritmo SHA-256 para gerar o hash\n",
    "        file_hash = hashlib.sha256(file_data).hexdigest()\n",
    "    return file_hash\n",
    "\n",
    "# 2. Geraçao do Unique_id do arquivo\n",
    "def generate_unique_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "\n",
    "def add_source_entry(batch_name, file_path, file, type, level, parent_unique_id):\n",
    "    #unique_id = generate_unique_id(type)\n",
    "    unique_id = generate_unique_id()\n",
    "    time_now = cron.timenow_pt_BR()   \n",
    "    file_hash = generate_file_hash(file_path) \n",
    "    if level == 1:\n",
    "        parent_unique_id = unique_id\n",
    "    data = {\n",
    "        'Batch': batch_name,\n",
    "        'Data': time_now,\n",
    "        'File': file,\n",
    "        'Type': type,\n",
    "        'Level': level,\n",
    "        'Unique_ID': unique_id,\n",
    "        'Parent_Unique_ID': parent_unique_id,\n",
    "        'Hash': file_hash,\n",
    "        'File_Path': file_path\n",
    "    }\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fase_processo = ['analise', 'pre-processamento', 'validacao', 'processamento', 'homologacao']\n",
    "atividade = ['scan_analise', 'split-paginas', 'pre_process', 'validacao', 'processamento', 'homologacao']\n",
    "status_documento = ['nao_processar', 'mapear', 'analisar', 'processar', 'avaliar', 'reprocessar', 'homologar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE - TRATAMENTO DE PATH\n",
    "\n",
    "file_name = \"Listagem de NFS_e _Sintético IM 237881_104871.pdf\"\n",
    "\n",
    "file_path2 = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_17/Listagem de NFS_e _Sintético IM 237881_104871.pdf\"\n",
    "\n",
    "diretorio_dir = os.path.relpath(file_path2, file_name)\n",
    "\n",
    "diretorio = os.path.basename(file_path2)\n",
    "\n",
    "sub_dir_path = os.path.normpath(file_path2)\n",
    "\n",
    "sub_dir = os.path.join(root_output_dir, folder_name)\n",
    "\n",
    "sub_dir_path = os.path.normpath(file_path2)\n",
    "\n",
    "diretorio = os.path.relpath(file_path2, documentos_scan_path)\n",
    "\n",
    "sub_dir = os.path.join(root_output_dir, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando multiplos parametros (um dicionario)\n",
    "df_id_relations = pd.DataFrame({\n",
    "    'Batch': ['Batch_17', 'Batch_18'],\n",
    "    'Unique_ID': [1, 2],\n",
    "    'File': ['File1', 'File2'],\n",
    "    'Type': ['Type1', 'Type2'],\n",
    "    'Level': [1, 2],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_scan_pipe.iterrows():\n",
    "    # Aqui, idx será o Unique_ID da linha atual\n",
    "    atual_unique_id = idx\n",
    "    original_file_name = row['original_file_name']\n",
    "    print(f'processing: {idx} | file: {original_file_name:>75} | unique_id:{atual_unique_id:>30}')\n",
    "    \n",
    "    \n",
    "    # ... (seu código de processamento)\n",
    "\n",
    "\n",
    "df_scan_pipe.to_excel('df_scan_pipe3.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Estabelece o pipeline de documentos\n",
    "def abertura_pipeline(index, batch_name, file, documentos_extracao_path, folder_name, file_path, content, parent_unique_id):\n",
    "    \n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    status_pro = \"para_analise\"\n",
    "    new_unique_id = generate_unique_id(content)\n",
    "    \n",
    "    return {\n",
    "        'index': index,\n",
    "        'data_processamento': time_now,\n",
    "        'Batch': batch_name,\n",
    "        'diretorio_origem': folder_name,\n",
    "        'nome_arquivo': file,\n",
    "        'diretorio_origem': file_path,\n",
    "        'status': status_pro,\n",
    "        'Unique_ID': new_unique_id,\n",
    "        'hash_value': generate_file_hash(file_path),\n",
    "        'Parent_Unique_ID': parent_unique_id\n",
    "    }\n",
    "\n",
    "\n",
    "# 3. função para ajustar documentos no pipeline antes de serem processados\n",
    "def pipeline_preprocessamento(documentos_extracao_path, content, unique_id):\n",
    "    rows_list = []\n",
    "    i = 0\n",
    "    for root, dirs, files in os.walk(documentos_extracao_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                i += 1\n",
    "                new_row = abertura_pipeline(i, batch_name, file, documentos_extracao_path, folder_name, file_path, content, parent_unique_id)\n",
    "                rows_list.append(new_row)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao criar nova row: {e}\")\n",
    "    \n",
    "    df_documento_recebido = pd.DataFrame(rows_list)\n",
    "    return df_documento_recebido    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_documento_recebido = pipeline_preprocessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id = df_source[df_source['Batch'] == batch_name]['Unique_ID'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. XXX Estabelece o pipeline de documentos\n",
    "def abertura_pipeline(index, batch_name, file, documentos_extracao_path, folder_name, file_path):\n",
    "    action_itens = {}\n",
    "    \n",
    "    status_pro = \"para_analise\"\n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'index': index,\n",
    "        'data_processamento': time_now,\n",
    "        'Batch': batch_name,\n",
    "        'diretorio_origem': folder_name,\n",
    "        'nome_arquivo': file,\n",
    "        'diretorio_origem': file_path,\n",
    "        'status': status_pro,\n",
    "    }\n",
    "\n",
    "\n",
    "# 3. funcao para ajustar documentos no pipeline antes de serem processados\n",
    "def pipeline_preprocessamento():\n",
    "    rows_list = []\n",
    "    i = 0\n",
    "    for root, dirs, files in os.walk(documentos_extracao_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                i += 1\n",
    "                new_row = abertura_pipeline(i, batch_name, file, documentos_extracao_path, folder_name, file_path)\n",
    "                rows_list.append(new_row)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao criar nova row: {e}\") \n",
    "            \n",
    "    \n",
    "    df_documento_recebido = pd.DataFrame(rows_list)\n",
    "    \n",
    "    return df_documento_recebido"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tables-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
