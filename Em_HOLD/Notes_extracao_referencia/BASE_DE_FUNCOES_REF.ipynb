{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACKUP DE FUNCS IMPORTANTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Funcoes Reseva pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. funçao que MOVE documentos e gera add_log_transaction_entry para df_log_transctions\n",
    "def move_doc_processed_file(batch_name, src_path, tgt_path):\n",
    "    \n",
    "    function = \"move_doc_processed_file\"\n",
    "    source_path = src_path\n",
    "    file = os.path.basename(source_path)\n",
    "    sub_dir = os.path.join(tgt_path, batch_name)\n",
    "    destination_path = os.path.join(sub_dir, file)\n",
    "    document_action = \"move_processed_file\"\n",
    "    transaction_detail = (f'document {file} moved by: {function}')\n",
    "    df_move = pd.DataFrame()\n",
    "    try:\n",
    "        document_unique_id = get_document_id_by_file(batch_name, file)\n",
    "        check_and_create_folder(destination_path)\n",
    "        shutil.move(source_path, destination_path)\n",
    "        sucess = True\n",
    "        move_log = add_log_transaction_entry(document_unique_id, batch_name, file, document_action, src_path, tgt_path, transaction_detail, sucess)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao mover documento: {e}\")\n",
    "        sucess = False\n",
    "    \n",
    "    return move_log    \n",
    "\n",
    "# 8. Função para adicionar um novo registro em df_source\n",
    "def add_source_entry(batch_name, file_path, file, type, level, parent_unique_id):\n",
    "    #unique_id = generate_unique_id(type)\n",
    "    unique_id = generate_unique_id()\n",
    "    time_now = cron.timenow_pt_BR()   \n",
    "    file_hash = generate_file_hash(file_path) \n",
    "    if level == 1:\n",
    "        parent_unique_id = unique_id\n",
    "    data = {\n",
    "        'Batch': batch_name,\n",
    "        'Data': time_now,\n",
    "        'File': file,\n",
    "        'Type': type,\n",
    "        'Level': level,\n",
    "        'Unique_ID': unique_id,\n",
    "        'Parent_Unique_ID': parent_unique_id,\n",
    "        'Hash': file_hash,\n",
    "        'File_Path': file_path\n",
    "    }\n",
    "    return data\n",
    "\n",
    "# 9. Add nova linha para atualizar df_log_transctions\n",
    "def add_log_transaction_entry(document_unique_id,batch_name, file, document_action, src_path, tgt_path, transaction_detail, sucess=True):\n",
    "\n",
    "    data_log = {\n",
    "        'Dt_Time': cron.timenow_pt_BR(),\n",
    "        'Batch': batch_name,\n",
    "        'File' : file,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Action': document_action,\n",
    "        'Sorce': src_path,\n",
    "        'Target': tgt_path,\n",
    "        'Transction_Detail': transaction_detail,\n",
    "        'Sucess': sucess,    \n",
    "    }\n",
    "    \n",
    "        \n",
    "    return data_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resrva oficial Aplkicacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = \"1. CABECALHO\"\n",
    "label = \"1_frame_dados_nf\"\n",
    "pdf_pesquisavel = True\n",
    "tipo = \"frame\"\n",
    "model_map = 'SPA'\n",
    "\n",
    "coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel, model=model_map, tipo=tipo, label=label, section=section)\n",
    "\n",
    "#coordinates = get_coordinates_filter_pdf_pesquisavel(model_map=model_map, tipo=tipo, label=label, section=section)\n",
    "\n",
    "x0, y0, x1, y1 = coordinates[0]\n",
    "\n",
    "print(f'\\n3. x0: {x0}, y0: {y0}, x1: {x1}, y1: {y1} \\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = \"1. CABECALHO\"\n",
    "label = \"1_frame_dados_nf\"\n",
    "tipo = \"frame\"\n",
    "model_map = 'SPA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_boxes_info = frames_nf_v4_df[((frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == tipo) & (frames_nf_v4_df['section_json'] == section) & (frames_nf_v4_df['label'] == label))]\n",
    "\n",
    "for idx_frame, row_frame in filtered_boxes_info.iterrows():\n",
    "    extracted_text_box = None\n",
    "    reference = row_frame['reference']\n",
    "    x0, y0, x1, y1 = row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']\n",
    "    index = idx_frame\n",
    "    id = row_frame['id']\n",
    "    \n",
    "    print(f'\\nid: {id} | reference: {reference} x0: {x0}, y0: {y0}, x1: {x1}, y1: {y1} \\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_coordinates_filter_pdf_pesquisavel(model_map, tipo, label, section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta e o principio da melhor funcao do mundo\n",
    "def extracao_pipeline(qualquer_df, fase, atividade, status, debug):\n",
    "    \n",
    "    doc_info = {}\n",
    "    resumo = {}\n",
    "    row_teste_info = []\n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    func_fase = fase\n",
    "    func_atividade = atividade\n",
    "    func_status = status\n",
    "    lista_dicts = []\n",
    "    conf_processo = {}\n",
    "    lista_conferencia = []\n",
    "   \n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        dados_iniciais = {}\n",
    "        row_info = row.to_dict()\n",
    "        message_erro = []\n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        map_document_unique_id = idx\n",
    "        map_seq = row['seq']\n",
    "        map_batch_name = row['batch']\n",
    "        map_fase_processo = row['fase_processo']\n",
    "        map_nome_atividade = row['nome_atividade']\n",
    "        map_status_documento = row['status_documento']\n",
    "        map_original_file_name = row['original_file_name']\n",
    "        map_directory = row['directory']\n",
    "        map_one_page = row['one_page']\n",
    "        map_palavra_chave = row['palavra_chave']\n",
    "        map_document_tag = row['document_tag']\n",
    "        map_action_item = row['action_item']\n",
    "        map_level = row['level']\n",
    "        file_path = row['file_path']\n",
    "        row_info['document_unique_id'] = map_document_unique_id\n",
    "        # if (map_status_documento != 'NO_PROCESS'):\n",
    "        # Nivel 1 - Definindo que documentos serao tratados    \n",
    "        if map_status_documento == 'PREPROCESS_EXTRACT':\n",
    "            \n",
    "            # 0. DADOS GERAIS DOCUMENTO\n",
    "            section = \"0. DADOS INICIAIS\"\n",
    "            try:\n",
    "                valores = {}\n",
    "                # 0.1. Busco prefeitura, de/para e modelo - se nao achar seta status documento para NO_PROCESS\n",
    "                valores = processar_dados_iniciais(idx, row, row_info, section, map_directory, map_original_file_name, file_path, debug)\n",
    "            except Exception as e:\n",
    "                msg = (f'Erro ao processar_dados_iniciais: {e}')\n",
    "            finally:\n",
    "                row_info.update(valores)\n",
    "            \n",
    "            map_status_documento_row_info = row_info.get('status_documento')\n",
    "            \n",
    "            # Nivel 2 - Definindo que os documentos legiveis serao tratados\n",
    "            if map_status_documento_row_info == 'PREPROCESS_EXTRACT':\n",
    "                prefeitura_map = row_info.get('prefeitura')\n",
    "                pdf_pesquisavel_map = row_info.get('pdf_pesquisavel')\n",
    "                de_para_map = row_info.get('de_para_pm')\n",
    "                model_map = row_info.get('model')\n",
    "                \n",
    "                \n",
    "                if not pdf_pesquisavel_map:\n",
    "                    # 1 XXX NOVO PROCESSO DE TRATAMENTO DE IMAGEM - Convertendo a imagem para numpy array\n",
    "                    print(\"irei gerar a imagem_np\")\n",
    "                    imagem_gray, image_resized_name = convert_resize_gray(map_original_file_name, file_path, image_resized_path)\n",
    "                    imagem_gray_rgb = imagem_gray.convert(\"RGB\")\n",
    "                    imagem_gray_np = np.array(imagem_gray_rgb)\n",
    "                    row_info['image_np'] = imagem_gray_np\n",
    "                \n",
    "                # 1. CABECALHO\n",
    "                # try:\n",
    "                section = \"1. CABECALHO\"\n",
    "                valores = {}\n",
    "                #valores_P = {}\n",
    "                f_0 = 1\n",
    "                f_1 = 1\n",
    "                mapping_method = \"frame_&_sframe_field\" # significa que as coordenadas estao em frames e os valores dos campos nos sframe_fields\n",
    "                context_mapping = \"data_cabecalho\"\n",
    "                def_replace = True \n",
    "                \n",
    "                if pdf_pesquisavel_map:\n",
    "                    valores = extrai_cabecalho_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                    row_info.update(valores) \n",
    "                else:\n",
    "                    valores = processar_cabecalho_R_PDF(idx, row, row_info, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, map_original_file_name, file_path, debug)   \n",
    "                    row_info.update(valores)\n",
    "         \n",
    "                #status_documento_row_info = row_info.get('status_documento')\n",
    "                action_item_row_info = row_info.get('action_item')\n",
    "                information_row_info = row_info.get('informations')   \n",
    "                \n",
    "                # Nivel 3 - Definindo que os documentos legiveis serao tratados realmente\n",
    "                if action_item_row_info == 'BREAK_PROCESS':\n",
    "                    #msg = (f'Processo inicial: {map_batch_name} | {map_original_file_name} | diretorio: {map_directory} - information_row_info: {information_row_info}')\n",
    "                    print(f'\\nINFELIZMENTE - seq: {map_seq} doc: {map_original_file_name} dir: {map_directory} - NAO SERA PROCESSADO  | inf: {information_row_info} \\n\\n')\n",
    "                    #row_info['action_item'] = \"BREAK_PROCESS\"\n",
    "                    #row_info['informations'] = msg\n",
    "                    # logging.error(msg)\n",
    "                    lista_dicts.append(row_info)\n",
    "                    continue \n",
    "                    \n",
    "                elif action_item_row_info == 'CONTINUE_PROCESS':\n",
    "                    \n",
    "                    print(f'\\nEBA, BORA CONTINUAR - seq: {map_seq} - proxima section: | PDF Pesquisavel: {pdf_pesquisavel_map} doc: {map_original_file_name} dir: {map_directory} | action_item: {action_item_row_info} | inf: {information_row_info} \\n\\n')\n",
    "                    print()\n",
    "                    print(valores)\n",
    "\n",
    "                #lista_dicts.append(row_info)\n",
    "            elif map_status_documento == 'NO_PROCESS':\n",
    "                msg = (f'Documento sem qualidade para pesquisa inicial: {map_batch_name} | {map_original_file_name} | diretorio: {map_directory}')\n",
    "                row_info['informations'] = msg  \n",
    "                row_info['action_item'] = \"BREAK_PROCESS\"          \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        elif map_status_documento == 'NO_PROCESS':\n",
    "            msg = (f'Documento nao sera tratado neste escopo: {map_batch_name} | {map_original_file_name} | diretorio: {map_directory}')\n",
    "            row_info['action_item'] = \"NO_PROCESS\"    \n",
    "            row_info['informations'] = msg \n",
    "        \n",
    "        lista_dicts.append(row_info)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    logging.info(f'processamento finalizado para: {batch_name}') \n",
    "    print(f'processamento de {i} documentos')\n",
    "    \n",
    "    novo_df = pd.DataFrame(lista_dicts)\n",
    "    \n",
    "    return novo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_21/SAO PEDRO DA ALDEIA_PDF_31282023_2257/11756286/NF CRJ PRIMEIRA QUINZENA DE JULHO DE 2023.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_txt = pesquisa_prefeitura_pdf_pesquisavel(file_path, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         \n",
    "       msg = (f'Processo inicial: {map_batch_name} | {map_original_file_name:>25} | diretorio: {map_directory}')\n",
    "            logging.error(msg)\n",
    "            row_info['informations'] = msg\n",
    "            lista_dicts.append(row_info)\n",
    "            continue\n",
    "         if debug:\n",
    "                print(f'\\n1. map_status_documento_row_info: {map_status_documento_row_info}\\n\\n')\n",
    "                \n",
    "                           if debug:\n",
    "                print(f'\\n1. apos esecutar processar_dados_iniciais: valores:\\n\\n{valores}\\n\\n')\n",
    "                \n",
    "                \n",
    "                print(f'\\nseq: {map_seq} | PDF Pesquisavel: {pdf_pesquisavel_map} doc: {map_original_file_name} dir: {map_directory} valores: {valores}\\n')\n",
    "                \n",
    "                \n",
    "                \n",
    "                            print(f'\\n1. processo inicial: {map_status_documento_row_info} - doc: {map_original_file_name} | diretorio: {map_directory} \\n')\n",
    "            'NO_PROCESS':\n",
    "                msg = (f'Processo inicial: {batch_name} | {map_original_file_name:>25} | diretorio: {map_directory}')\n",
    "                logging.error(msg)\n",
    "                row_info['informations'] = msg\n",
    "                lista_dicts.append(row_info)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrai_cabecalho_PDF_P(idx, row, row_info, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "    \n",
    "    nf_data_cabecalho = {}\n",
    "    section = \"1 - CABECALHO\"\n",
    "    pdf_pesquisavel_map = True\n",
    "    lista_erros = []\n",
    "    batch_name = row['batch']\n",
    "    print(f'\\n\\n2. dentro da funçao extrai_cabecalho_PDF: batch_name: {batch_name}\\n\\n')\n",
    "    \n",
    "    label = \"1_frame_dados_nf\"\n",
    "\n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    tipo = \"frame\"\n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    #print(label)\n",
    "    #print(x0,x1,y0,y1)\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    nf_data_cabecalho = novaextra.extract_fields_cabecalho(idx, row, row_info, text, original_file_name, debug)\n",
    "    \n",
    "    \n",
    "    pdf_document.close()\n",
    "    return nf_data_cabecalho\n",
    "\n",
    "\n",
    "\n",
    "    return data_extrated_prefeitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta e o principio da melhor funcao do mundo\n",
    "def extracao_pipeline(qualquer_df, fase, atividade, status, debug):\n",
    "    \n",
    "    doc_info = {}\n",
    "    resumo = {}\n",
    "    row_teste_info = []\n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    func_fase = fase\n",
    "    func_atividade = atividade\n",
    "    func_status = status\n",
    "    lista_dicts = []\n",
    "    conf_processo = {}\n",
    "    lista_conferencia = []\n",
    "   \n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        dados_iniciais = {}\n",
    "        row_info = row.to_dict()\n",
    "        message_erro = []\n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        map_document_unique_id = idx\n",
    "        map_seq = row['seq']\n",
    "        map_batch_name = row['batch']\n",
    "        map_fase_processo = row['fase_processo']\n",
    "        map_nome_atividade = row['nome_atividade']\n",
    "        map_status_documento = row['status_documento']\n",
    "        map_original_file_name = row['original_file_name']\n",
    "        map_directory = row['directory']\n",
    "        map_one_page = row['one_page']\n",
    "        map_palavra_chave = row['palavra_chave']\n",
    "        map_document_tag = row['document_tag']\n",
    "        map_action_item = row['action_item']\n",
    "        map_level = row['level']\n",
    "        file_path = row['file_path']\n",
    "        # if (map_status_documento != 'NO_PROCESS'):\n",
    "            #1. Buscar Prefeitura, de/para e modelo\n",
    "        if map_status_documento == 'NO_PROCESS':\n",
    "            msg = (f'Processo inicial: {batch_name} | {map_original_file_name:>25} | diretorio: {map_directory}')\n",
    "            logging.error(msg)\n",
    "            row_info['informations'] = msg\n",
    "            lista_dicts.append(row_info)\n",
    "            continue\n",
    "        \n",
    "        elif map_status_documento == 'PREPROCESS_EXTRACT':   \n",
    "            \n",
    "            valores = {}\n",
    "            valores = processar_dados_iniciais(idx, row, row_info, map_directory, map_original_file_name, file_path, debug)\n",
    "            if debug:\n",
    "                print(f'\\n1. apos esecutar processar_dados_iniciais: valores:\\n\\n{valores}\\n\\n')\n",
    "            if valores:\n",
    "                row_info.update(valores)\n",
    "            map_status_documento_row_info = row_info.get('status_documento')\n",
    "            if debug:\n",
    "                print(f'\\n1. map_status_documento_row_info: {map_status_documento_row_info}\\n\\n')\n",
    "            \n",
    "            print(f'\\n1. processo inicial: {map_status_documento_row_info} - doc: {map_original_file_name} | diretorio: {map_directory} \\n')\n",
    "            if map_status_documento_row_info == 'NO_PROCESS':\n",
    "                msg = (f'Processo inicial: {batch_name} | {map_original_file_name:>25} | diretorio: {map_directory}')\n",
    "                logging.error(msg)\n",
    "                row_info['informations'] = msg\n",
    "                lista_dicts.append(row_info)\n",
    "                continue\n",
    "            \n",
    "            elif map_status_documento_row_info == 'PREPROCESS_EXTRACT':\n",
    "                # 0. DADOS GERAIS DOCUMENTO\n",
    "                prefeitura_map = row_info['prefeitura']\n",
    "                pdf_pesquisavel_map = valores['pdf_pesquisavel']\n",
    "                de_para_map = valores['de_para_pm']\n",
    "                model_map = valores['model']\n",
    "                #cnpj_map = dados_iniciais['cnpj']\n",
    "                if debug:\n",
    "                    print(f'\\n1. map_original_file_name: {map_original_file_name} | pdf_pesquisavel: {pdf_pesquisavel_map} | diretorio: {map_directory} | de_para_map: {de_para_map} | model: {model_map} | PM: {prefeitura_map}\\n')\n",
    "                \n",
    "                row_info['document_unique_id'] = map_document_unique_id\n",
    "                if not pdf_pesquisavel_map:\n",
    "                    # 1 XXX NOVO PROCESSO DE TRATAMENTO DE IMAGEM - Convertendo a imagem para numpy array\n",
    "                    print(\"irei gerar a imagem_np\")\n",
    "                    imagem_gray, image_resized_name = convert_resize_gray(map_original_file_name, file_path, image_resized_path)\n",
    "                    imagem_gray_rgb = imagem_gray.convert(\"RGB\")\n",
    "                    imagem_gray_np = np.array(imagem_gray_rgb)\n",
    "                    row_info['image_np'] = imagem_gray_np\n",
    "\n",
    "                # 1. CABECALHO\n",
    "                section = \"1. CABECALHO\"\n",
    "                valores = {}\n",
    "                f_0 = 1\n",
    "                f_1 = 1\n",
    "                mapping_method = \"frame_&_sframe_field\" # significa que as coordenadas estao em frames e os valores dos campos nos sframe_fields\n",
    "                context_mapping = \"data_cabecalho\"\n",
    "                def_replace = True\n",
    "                \n",
    "                if pdf_pesquisavel_map:\n",
    "                    valores = extrai_cabecalho_PDF_P(idx, row, row_info, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                    print(f'\\nseq: {map_seq} | PDF Pesquisavel: {pdf_pesquisavel_map} doc: {map_original_file_name} dir: {map_directory} valores: {valores}\\n')    \n",
    "                else:\n",
    "                    valores = processar_cabecalho_R_PDF(idx, row, row_info, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, map_original_file_name, file_path, debug)\n",
    "                    \n",
    "                print(f'\\nseq: {map_seq} | PDF Pesquisavel: {pdf_pesquisavel_map} doc: {map_original_file_name} dir: {map_directory} valores:\\n\\n{valores}\\n\\n\\n')\n",
    "                \n",
    "                if valores:\n",
    "                    row_info.update(valores)\n",
    "                       \n",
    "                \n",
    "                map_status_documento_row_info = row_info.get('status_documento')\n",
    "                \n",
    "                print(f'\\n2. Cabecalho: {map_status_documento_row_info} - doc: {map_original_file_name} | diretorio: {map_directory} \\n')\n",
    "                \n",
    "                if map_status_documento_row_info == 'NO_PROCESS':\n",
    "                    msg = (f'Processo inicial: {batch_name} | {map_original_file_name:>25} | diretorio: {map_directory}')\n",
    "                    logging.error(msg)\n",
    "                    row_info['informations'] = msg\n",
    "                    #lista_dicts.append(row_info)\n",
    "                    continue\n",
    "                \n",
    "                elif map_status_documento_row_info == 'PREPROCESS_EXTRACT':\n",
    "                    \n",
    "                    msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)  \n",
    "\n",
    "                    #     #if map_status_documento_row_info == 'PREPROCESS_EXTRACT':        \n",
    "                    #     # map_status_documento_valores = valores['status_documento']\n",
    "                    #     # if map_status_documento_valores:\n",
    "                            \n",
    "                    # guarda_texto_doc = {}\n",
    "                    # guarda_texto_doc, linhas = cria_guarda_doc_ref_R_PDF(idx, row, de_para_map, model_map, map_original_file_name, file_path, image_resized_path, debug)\n",
    "            \n",
    "                    # # 2. PRESTADOR DE SERVIÇO\n",
    "                    # section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "                    # valores = {}\n",
    "                    # erros_prestador = {}\n",
    "                    # data_tomador = {}\n",
    "                    # f_0 = 1\n",
    "                    # f_1 = 1\n",
    "                    # if pdf_pesquisavel_map:\n",
    "                    #     valores = extrai_prestador_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                    # else:\n",
    "                    #     valores = extrai_prestador_R_PDF(idx, row, row_info, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                    \n",
    "                    # if not isinstance(valores, dict):\n",
    "                    #     msg_erro = (f\"\\nErro na linha {idx}: 'valores' não é um dicionário. Tipo: {type(valores)}, Valor: {valores}\")\n",
    "                    # else:\n",
    "                    #     row_info.update(valores)\n",
    "                        \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)                                                    \n",
    "\n",
    "                    # # 3. TOMADOR DE SERVIÇO\n",
    "                    # section = \"3. TOMADOR DE SERVIÇO\"\n",
    "                    # valores = {}\n",
    "                    # erros = []\n",
    "                    # data_tomador = {}\n",
    "                    # f_0 = 1\n",
    "                    # f_1 = 1.01\n",
    "                    \n",
    "                    # if pdf_pesquisavel_map:\n",
    "                    #     valores = extrai_tomador_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                    # else:   \n",
    "                    #     valores = extrai_tomador_R_PDF(idx, row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                        \n",
    "                    # if not isinstance(valores, dict):\n",
    "                    #     print(f\"\\nErro na linha {idx}: 'valores' não é um dicionário. Tipo: {type(valores)}, Valor: {valores}\")\n",
    "                    # else:\n",
    "                    #     row_info.update(valores)\n",
    "                        \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)    \n",
    "                        \n",
    "                        \n",
    "                    \n",
    "                    # # 4. DESCRIMINACAO DOS SERVIÇOS\n",
    "                    # section = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "                    # valores = {}\n",
    "                    # nf_data_servico = {} \n",
    "                    # f_0 = 1\n",
    "                    # f_1 = 1\n",
    "                    \n",
    "                    # if pdf_pesquisavel_map:\n",
    "                    #     nf_data_servico = processar_servicos_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, map_original_file_name, file_path)\n",
    "                    # else:\n",
    "                    #     label = \"discriminacao_servicos\"\n",
    "                    #     tipo = \"field_box\"\n",
    "                    #     def_replace = True\n",
    "                        \n",
    "                    #     # ItSs  working\n",
    "                    #     texto_extraido = extracao_documento_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_map, model_map, def_replace, map_original_file_name)\n",
    "                    #     row_info[label] = texto_extraido\n",
    "                        \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)     \n",
    "\n",
    "\n",
    "                    # try:\n",
    "                    #     texto_extraido = nf_data_servico['discriminacao_servicos'] \n",
    "                    #     row_info['discriminacao_servicos'] = texto_extraido \n",
    "                    # except Exception as e:\n",
    "                    #     msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "                    #     discrimanacao_servico = \"Descricao nao encontrada\"\n",
    "                    #     row_info['discriminacao_servicos'] = texto_extraido\n",
    "\n",
    "                    \n",
    "                    # # 5. VALOR TOTAL\n",
    "                    # section = \"5. VALOR TOTAL\"\n",
    "                    # valores = {}\n",
    "                    # if pdf_pesquisavel_map:\n",
    "                    #     valores = processar_valor_total_PDF_P(row, pdf_pesquisavel_map, model_map, map_original_file_name, file_path, debug)\n",
    "                    # else:\n",
    "                    #     label = \"valor_total_nota\"\n",
    "                    #     tipo = \"field_box\"\n",
    "                    #     def_replace = True\n",
    "                    #     texto_extraido = extracao_documento_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_map, model_map, def_replace, map_original_file_name, debug)\n",
    "                    #     if texto_extraido: \n",
    "                    #         valor_total_match = re.search(r'R\\$ ([\\d,.]+)', texto_extraido)\n",
    "                    #         if valor_total_match:\n",
    "                    #             valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "                    #             try:\n",
    "                    #                 valores['valor_total_nota'] = float(valor_total_sem_formatacao)\n",
    "                    #             except Exception as e:\n",
    "                    #                 msg = (f'Processo inicial: {batch_name} | {map_original_file_name:>25} | diretorio: {map_directory} | {e}')\n",
    "                    #                 valores['valor_total_nota'] = 0.0\n",
    "                    #                 logging.error(f\" {batch_name} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")\n",
    "                    \n",
    "                    # if not isinstance(valores, dict):\n",
    "                    #     msg_erro = (f\"\\nErro na linha {idx}: 'valores' não é um dicionário. Tipo: {type(valores)}, Valor: {valores}\")\n",
    "                    # else:\n",
    "                    #     row_info.update(valores)\n",
    "                        \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)   \n",
    "\n",
    "                    \n",
    "                    # # 6. CNAE e Item da Lista de Serviços \n",
    "                    # section = \"6. CNAE e Item da Lista de Serviços\"\n",
    "                    # f_0_cnae = 0.95\n",
    "                    # f_1_cnae = 1.15\n",
    "                    # f_0_it = 0.95     #0.95\n",
    "                    # f_1_it = 1.15    # 1\n",
    "                    # if pdf_pesquisavel_map:\n",
    "                    #     cnae_value, item_servico_value = extrai_consiste_cnae_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0_cnae, f_1_cnae, f_0_it, f_1_it, map_original_file_name, file_path, debug)\n",
    "                    # else:\n",
    "                        \n",
    "                    #     tipo = \"sframe_field\"\n",
    "                    #     def_replace = True                  \n",
    "                    #     label = \"cnae\"\n",
    "                    #     cnae_value = extracao_documento_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_map, model_map, def_replace, map_original_file_name)\n",
    "                    #     if cnae_value:\n",
    "                    #         row_info[label] = cnae_value\n",
    "                            \n",
    "                    #     label = \"item_lista_servicos\"\n",
    "                    #     item_lista_value = extracao_documento_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_map, model_map, def_replace, map_original_file_name)\n",
    "                    #     if item_lista_value:\n",
    "                    #         row_info[label] = cnae_value\n",
    "                    #         #print(f'cnae: {cnae_value} | item_lista_servicos: {item_lista_value}')\n",
    "                    \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)         \n",
    "\n",
    "                    # # 7. VALORES E IMPOSTOS\n",
    "                    # section = \"7. VALORES E IMPOSTOS\"\n",
    "                    # valores = {}\n",
    "                    # nf_data_valores = {}\n",
    "                    # lista_impostos = []\n",
    "                    # f_0 = 1\n",
    "                    # f_1 = 1\n",
    "                    # if pdf_pesquisavel_map:\n",
    "                    #     valores = extrai_valores_impostos_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "                    #     row_info.update(valores)\n",
    "                    \n",
    "                    # else:\n",
    "                    #     tipo = \"field_box\"\n",
    "                    #     father_value = \"5_frame_valores_impostos\"\n",
    "                    #     valores = extracao_impostos_R_PDF(section, tipo, father_value, de_para_map, model_map, map_original_file_name, file_path)\n",
    "                    #     #print(valores)\n",
    "                    #     row_info.update(valores)\n",
    "                        \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)   \n",
    "                        \n",
    "                    # # 8. DADOS COMPLEMENTARES\n",
    "                    # section = '8. DADOS COMPLEMENTARES'\n",
    "                    # nf_data_dados_complementares = {}\n",
    "                    # f_0 = 1\n",
    "                    # f_1 = 1\n",
    "                    # if pdf_pesquisavel_map:\n",
    "                    #     nf_data_valores = extrai_dados_complementares_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "                    # else:\n",
    "                        \n",
    "                    #     tipo = \"field_box\"\n",
    "                    #     father_value = \"6_section_inf_complementares_criticas\"  \n",
    "                    #     valores = extracao_complementares_R_PDF(row, section, tipo, father_value, de_para_map, model_map, map_original_file_name, file_path)\n",
    "                    #     if valores:\n",
    "                    #         row_info.update(valores)    \n",
    "                    # try:\n",
    "                    #     row_info['dados_complementares'] = nf_data_dados_complementares['dados_complementares']\n",
    "                    # except Exception as e:\n",
    "                    #     msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "                    #     row_info['dados_complementares'] = None\n",
    "                    \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)     \n",
    "                    \n",
    "                    # # 9. OUTRAS INFORMAÇOES / CRITICAS\n",
    "                    # section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "                    # tipo = \"field_box\"\n",
    "                    # father_value = \"5_frame_inf_criticas\"\n",
    "                    # valores = {} \n",
    "                    # nf_data_outras_informacoes = {}\n",
    "                    # f_0 = 1\n",
    "                    # f_1 = 1\n",
    "                    # if pdf_pesquisavel_map:\n",
    "                    #     valores = extrai_outras_informacoes_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "                    #     if valores:\n",
    "                    #         row_info.update(valores)\n",
    "                    # else:\n",
    "                    #     section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "                    #     tipo = \"field_box\"\n",
    "                    #     father_value = \"5_frame_inf_criticas\"\n",
    "                    #     valores = extracao_inforacoes_criticas_R_PDF(section, tipo, father_value, de_para_map, model_map, map_original_file_name, file_path)\n",
    "                    #     if valores:\n",
    "                    #         row_info.update(valores)\n",
    "                            \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)          \n",
    "                            \n",
    "                    \n",
    "                    # # 10. OBSERVACOES  \n",
    "                    # section = \"10. OBSERVACOES\"  \n",
    "                    # data_observacao = {}\n",
    "                    # valores = {}\n",
    "                    # f_0 = 0.9\n",
    "                    # f_1 = 1.1\n",
    "                    # if pdf_pesquisavel_map:\n",
    "                    #     valores = extrai_outras_informacoes_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "                    #     if valores:\n",
    "                    #         row_info.update(valores)\n",
    "                    # else:\n",
    "                    #     section = '10. OBSERVACOES'\n",
    "                    #     tipo = \"field_box\"\n",
    "                    #     father_value = \"6_section_inf_complementares_criticas\"  \n",
    "                    #     valores = extracao_observacao_R_PDF(row, section, tipo, father_value, de_para_map, model_map, map_original_file_name, file_path) \n",
    "                    #     if valores:\n",
    "                    #         row_info.update(valores)\n",
    "                    \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    logging.info(msg)            \n",
    "                    lista_dicts.append(row_info)\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    logging.info(f'processamento finalizado para: {batch_name}') \n",
    "    \n",
    "    novo_df = pd.DataFrame(lista_dicts)\n",
    "    return novo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando DF para analises\n",
    "df.set_index('document_unique_id', inplace=True)\n",
    "ordem_status = ['PREPROCESS_EXTRACT', 'NO_PROCESS', 'root_analise']\n",
    "df['status_documento'] = pd.Categorical(df['status_documento'], categories=ordem_status, ordered=True)\n",
    "df.sort_values(by=['status_documento', 'seq'], ascending=[True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracao_pipeline(qualquer_df, fase, atividade, status, debug):\n",
    "    \n",
    "    doc_info = {}\n",
    "    resumo = {}\n",
    "    row_teste_info = []\n",
    "\n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        dados_iniciais = {}\n",
    "        row_info = row.to_dict()\n",
    "        message_erro = []\n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        map_document_unique_id = idx\n",
    "        map_seq = row['seq']\n",
    "        map_batch_name = row['batch']\n",
    "        map_fase_processo = row['fase_processo']\n",
    "    \n",
    "        if map_status_documento == 'PREPROCESS_EXTRACT':\n",
    "            \n",
    "            valores = {}\n",
    "            valores = processar_dados_iniciais(idx, row, row_info, map_directory, map_original_file_name, file_path, debug)\n",
    "            if debug:\n",
    "                print(f'\\n1. apos esecutar processar_dados_iniciais: valores:\\n\\n{valores}\\n\\n')\n",
    "            if valores:\n",
    "                row_info.update(valores)\n",
    "            map_status_documento_row_info = row_info.get('status_documento')\n",
    "            \n",
    "            if map_status_documento_row_info == 'NO_PROCESS':\n",
    "                msg = (f'Processo inicial: {batch_name} | {map_original_file_name:>25} | diretorio: {map_directory}')\n",
    "                logging.error(msg)\n",
    "                row_info['informations'] = msg\n",
    "                lista_dicts.append(row_info)\n",
    "                return\n",
    "            \n",
    "            elif map_status_documento_row_info == 'PREPROCESS_EXTRACT':\n",
    "                # 0. DADOS GERAIS DOCUMENTO\n",
    "                prefeitura_map = row_info['prefeitura']\n",
    "                pdf_pesquisavel_map = valores['pdf_pesquisavel']\n",
    "                de_para_map = valores['de_para_pm']\n",
    "                model_map = valores['model']\n",
    "                #cnpj_map = dados_iniciais['cnpj']\n",
    "                row_info['document_unique_id'] = map_document_unique_id\n",
    "                \n",
    "                if not pdf_pesquisavel_map:\n",
    "                    # 1 XXX NOVO PROCESSO DE TRATAMENTO DE IMAGEM - Convertendo a imagem para numpy array\n",
    "                    print(\"irei gerar a imagem_np\")\n",
    "                    imagem_gray, image_resized_name = convert_resize_gray(map_original_file_name, file_path, image_resized_path)\n",
    "                    imagem_gray_rgb = imagem_gray.convert(\"RGB\")\n",
    "                    imagem_gray_np = np.array(imagem_gray_rgb)\n",
    "                    row_info['image_np'] = imagem_gray_np\n",
    "                 \n",
    "                \n",
    "                # 1. CABECALHO\n",
    "                section = \"1. CABECALHO\"\n",
    "                valores = {}\n",
    "                f_0 = 1\n",
    "                f_1 = 1\n",
    "                if pdf_pesquisavel_map:\n",
    "                    valores = extrai_cabecalho_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                    if valores:\n",
    "                        row_info.update(valores)\n",
    "                else:\n",
    "                    mapping_method = \"frame_&_sframe_field\" # significa que as coordenadas estao em frames e os valores dos campos nos sframe_fields\n",
    "                    context_mapping = \"data_cabecalho\"\n",
    "                    def_replace = True\n",
    "                    valores = processar_cabecalho_R_PDF(idx, row, row_info, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, map_original_file_name, file_path, debug)\n",
    "                    if valores:\n",
    "                        row_info.update(valores)\n",
    "                        map_status_documento_row_info = row_info['status_documento']\n",
    "                    \n",
    "                    msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    if debug:\n",
    "                        print(msg)\n",
    "                    logging.info(msg)  \n",
    "\n",
    "           \n",
    "                guarda_texto_doc = {}\n",
    "                guarda_texto_doc, linhas = cria_guarda_doc_ref_R_PDF(idx, row, de_para_map, model_map, map_original_file_name, file_path, image_resized_path, debug)\n",
    "        \n",
    "                # 2. PRESTADOR DE SERVIÇO\n",
    "                section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "                valores = {}\n",
    "                f_0 = 1\n",
    "                f_1 = 1\n",
    "                if pdf_pesquisavel_map:\n",
    "                    valores = extrai_prestador_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                else:\n",
    "                    valores = extrai_prestador_R_PDF(idx, row, row_info, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "       \n",
    "                # 3. TOMADOR DE SERVIÇO\n",
    "                section = \"3. TOMADOR DE SERVIÇO\"\n",
    "                valores = {}\n",
    "                erros = []\n",
    "              \n",
    "                \n",
    "                # 4. DESCRIMINACAO DOS SERVIÇOS\n",
    "                section = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "                valores = {}\n",
    "                nf_data_servico = {} \n",
    "                f_0 = 1\n",
    "                f_1 = 1\n",
    "            \n",
    "                # 6. CNAE e Item da Lista de Serviços \n",
    "                section = \"6. CNAE e Item da Lista de Serviços\"\n",
    "          \n",
    "\n",
    "                # 7. VALORES E IMPOSTOS\n",
    "                section = \"7. VALORES E IMPOSTOS\"\n",
    "                valores = {}\n",
    "                nf_data_valores = {}\n",
    "          \n",
    "                # 8. DADOS COMPLEMENTARES\n",
    "                section = '8. DADOS COMPLEMENTARES'\n",
    "                nf_data_dados_complementares = {}\n",
    "          \n",
    "                # 9. OUTRAS INFORMAÇOES / CRITICAS\n",
    "                section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "                tipo = \"field_box\"\n",
    "       \n",
    "                # 10. OBSERVACOES  \n",
    "                section = \"10. OBSERVACOES\"  \n",
    "                data_observacao = {}\n",
    "        \n",
    "   \n",
    "                            \n",
    "                lista_dicts.append(row_info)\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "\n",
    "    novo_df = pd.DataFrame(lista_dicts)\n",
    "    logging.info(f'processamento finalizado para: {batch_name}')  \n",
    "     \n",
    "    return novo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    # if debug:\n",
    "                    #     x0 = 0\n",
    "                    #     y0 = 0\n",
    "                    #     plt.figure(figsize=(25, 25))\n",
    "                    #     plt.imshow(imagem_gray_np)\n",
    "                    #     plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "                    #     plt.text(x0 + 1, y0 + 10, map_original_file_name, color='black', fontsize=20)\n",
    "                    #     #plt.text(original_file_name, color='black', fontsize=7)\n",
    "                    #     plt.show()\n",
    "                    # row_info['image_np'] = imagem_gray_np\n",
    "                    # conf_processo['image_np'] = imagem_gray_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    #if valores:\n",
    "                        #print(f'\\ncabecalho PDF_Pesquisavel: {map_original_file_name} - diretorio: {map_directory} valores:\\n\\n{valores}\\n\\n\\n')\n",
    "                        #row_info.update(valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_coordinates_filter_R_PDF(model_map, tipo, label, section):\n",
    "    \n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model_map, type=tipo, label=label, section_json=section)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [(row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1'])]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_coordinates_filter_pdf_pesquisavel(model_map, tipo, label, section):\n",
    "    \n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model_map, type=tipo, label=label, section_json=section)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [(row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p'])]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. XXX Nova funçao de conversao de PDF para imagem tam 2X\n",
    "def convertResize2(idx, row, original_file_name, file_path, image_resized_path):\n",
    "    \n",
    "    \n",
    "    name_image = conv_filename_no_ext(original_file_name)\n",
    "    \n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(name_image)}.jpg')\n",
    "    #print(f'image_resized_name: {image_resized_name}\\n')\n",
    "    # 3. Conversao para imagem\n",
    "    pages = convert_from_path(file_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((4134, 5846))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "    return  image_resized_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de apresentaçao da imagem\n",
    "x0 = 0\n",
    "y0 = 0\n",
    "plt.figure(figsize=(25, 25))\n",
    "plt.imshow(imagem_gray_np)\n",
    "plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "plt.text(x0 + 1, y0 + 10,original_file_name, color='black', fontsize=20)\n",
    "#plt.text(original_file_name, color='black', fontsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mapping_method == \"frame_&_sframe_field\":\n",
    "    tipo_4_coordinates = \"frame\"\n",
    "    tipo_4_filter = \"sframe_field\"\n",
    "coordinates = get_coordinates_filter_by_context(pdf_pesquisavel_map, model, context_mapping, tipo_4_coordinates)\n",
    "x0, y0, x1, y1 = coordinates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_file_name = \"Doria Marinho 0297 Raquel.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = filtrar_df(df_conf0, original_file_name=original_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de apresentaçao da imagem\n",
    "x0 = 0\n",
    "y0 = 0\n",
    "plt.figure(figsize=(25, 25))\n",
    "plt.imshow(result['image_np'].values[0])\n",
    "plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "plt.text(x0 + 1, y0 + 10,original_file_name, color='black', fontsize=20)\n",
    "#plt.text(original_file_name, color='black', fontsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_np = result['image_np'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = int(x0)\n",
    "y0 = int(y0)\n",
    "x1 = int(x1)    \n",
    "y1 = int(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_image_np = imagem_gray_np[y0:y1, x0:x1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de apresentaçao da imagem\n",
    "x0 = 0\n",
    "y0 = 0\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(cropped_image_np)\n",
    "plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "#plt.text(x0 + 1, y0 + 10,original_file_name, color='black', fontsize=20)\n",
    "#plt.text(original_file_name, color='black', fontsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = frames_nf_v4_df[frames_nf_v4_df['type'] == 'field_box']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_info = {}\n",
    "i = 1\n",
    "for idx, row in boxes.iterrows():\n",
    "    x0 = int(row['x0'])\n",
    "    y0 = int(row['y0'])\n",
    "    x1 = int(row['x1'])\n",
    "    y1 = int(row['y1'])\n",
    "    cropped_image_np = imagem_gray_np[y0:y1, x0:x1]\n",
    "    \n",
    "    boxes_info[f'box_{i}'] = {\n",
    "        'coordinates': (x0, y0, x1, y1),\n",
    "        'image': cropped_image_np,\n",
    "        # ... qualquer outra informação que você deseja armazenar\n",
    "    }\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_info['box_1']['coordinates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_info['box_1']['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for box in boxes_info:\n",
    "    x0, y0, x1, y1 = boxes_info[box]['coordinates']\n",
    "    image = boxes_info[box]['image']\n",
    "    # plt.figure(figsize=(25, 25))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8\n",
    "\n",
    "x0 = 0\n",
    "y0 = 0\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(boxes_info[f'box_{i}']['image'])\n",
    "plt.text(x0 , y0,boxes_info[f'box_{i}']['coordinates'], color='green', fontsize=7)\n",
    "plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize um dicionário vazio para armazenar as informações dos \"boxes\"\n",
    "boxes_info = {}\n",
    "\n",
    "# Suponha que você está em um loop onde está processando vários \"boxes\"\n",
    "for i, box in enumerate(boxes):\n",
    "    # Obtenha as coordenadas do \"box\"\n",
    "    x0, y0, x1, y1 = coordinates[0] \n",
    "    # Corte a área do \"box\" da imagem original\n",
    "    cropped_image = imagem_gray_np[y0:y1, x0:x1]\n",
    "    \n",
    "    # Armazene as informações do \"box\" no dicionário\n",
    "    boxes_info[f'box_{i}'] = {\n",
    "        'coordinates': (x0, y0, x1, y1),\n",
    "        'image': cropped_image,\n",
    "        # ... qualquer outra informação que você deseja armazenar\n",
    "    }\n",
    "\n",
    "# Agora, `boxes_info` contém informações detalhadas sobre cada \"box\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = \"10. OBSERVACOES\"\n",
    "tipo = \"field_box\"\n",
    "father_value = \"5_frame_inf_criticas\"\n",
    "marcador_ini = \"Observação:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta e o principio da melhor funcao do mundo - SIM ELA E MAS PRECISA FICAR AQUI?\n",
    "def extracao_pipeline(qualquer_df, fase, atividade, status):\n",
    "    \n",
    "    linhas_analise = []\n",
    "    bloco_1_list = []\n",
    "    bloco_2_list = []\n",
    "    bloco_3_list = []\n",
    "    imagens_list = []  \n",
    "    new_data = [] \n",
    "    \n",
    "    pre_processo = ['nro_nota', 'competencia', 'dt_hr_emissao', 'codigo_verificacao', 'ajustar_nome', 'split_paginas', 'ajustar_imagem', 'buscar_nome_prefeitura', 'enviar_canceladas', 'enviar_listagens']\n",
    "    \n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    \n",
    "    fase_processo_atual = fase\n",
    "    atividade_processo_atual = atividade\n",
    "    status_documento_atual = status\n",
    "    \n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        message_erro = []\n",
    "        \n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        document_unique_id = idx\n",
    "        seq_df = row['seq']\n",
    "        batch_name = row['batch']\n",
    "        fase_processo = row['fase_processo']\n",
    "        nome_atividade = row['nome_atividade']\n",
    "        status_documento = row['status_documento']\n",
    "        original_file_name = row['original_file_name']\n",
    "        file_directory = row['directory']\n",
    "        level = row['level']\n",
    "        d_type = row['level']\n",
    "        document_type = row['document_type']\n",
    "        pdf_pesquisavel = row['pdf_pesquisavel']\n",
    "        one_page_doc = row['one_page']\n",
    "        modelo = row['modelo']\n",
    "        \n",
    "        prefeitura = row['prefeitura']\n",
    "        \n",
    "        parent_document_unique_id = row['parent_document_unique_id']\n",
    "        file_path = row['file_path']\n",
    "        file_hash = row['file_hash']\n",
    "        \n",
    "        # 2. Busca modelo\n",
    "        if (not status_documento == 'NAO_PROCESSAR') or (not document_type == 'outros') or (not one_page_doc == True):\n",
    "            \n",
    "            if atividade_processo_atual == 'extracao_prestador':\n",
    "            #print(f' 1 - seq: {seq_df} | file: {original_file_name} |status_documento: {status_documento} pdf_pesquisavel: {pdf_pesquisavel}\\n')\n",
    "                if status_documento == status_documento_atual:\n",
    "         \n",
    "                    print(f'seq_df: {seq_df} status_documento: {status_documento} | modelo: {modelo:>20} | file: {original_file_name:>40} | prefeitura: {prefeitura:>55} | {pdf_pesquisavel}\\n')\n",
    "                    \n",
    "                    result_list = []\n",
    "                    \n",
    "                    dfcnpj_prestador = {}\n",
    "                    dfincricao_prestador = {}\n",
    "                    dfdados_prestador = {}\n",
    "                        \n",
    "                    doc2convert = original_file_name\n",
    "                    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "                    modelo = 'SAO_PEDRO_SUPERMIX'\n",
    "                    f_tipo = 'frame'\n",
    "                    \n",
    "                    dfcnpj_prestador, dfincricao_prestador, dfdados_prestador, textoextraido  = processar_dados_dados_documentos(row, original_file_name, file_path, pdf_pesquisavel, section, modelo, f_tipo)\n",
    "                    \n",
    "                    print(f'\\n {dfcnpj_prestador}\\n{dfincricao_prestador}\\n{dfdados_prestador}\\n')\n",
    "                    \n",
    "               \n",
    "      \n",
    "\n",
    "\n",
    "                i += 1\n",
    "              \n",
    "    return textoextraido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_valores = linhas\n",
    "# Exemplo de uso\n",
    "marcador1 = 'VALOR SERVIÇOS'  # SAO PEDRO DA ALDEIA\n",
    "marcador2 = 'DADOS COMPLEMENTARES'\n",
    "lista_val = encontrar_valores3(texto_valores, marcador1, marcador2)\n",
    "\n",
    "\n",
    "print(f'\\nlista_val: {lista_val}  -->  {len(lista_val)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota 1\n",
    "file_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_20/MAGE_PDF_31282023_2254/10597/NFS.299 07.2023 FLOC.pdf\"\n",
    "doc2convert = \"NFS.299 07.2023 FLOC.pdf\"\n",
    "\n",
    "image_2work, image_path = convertResize(doc2convert, file_path, image_resized_path)\n",
    "imagem_gray = image_2work.convert('L')\n",
    "texto_doc = (pytesseract.image_to_string(imagem_gray, lang='por'))\n",
    "text_splited = texto_doc.split('\\n')\n",
    "texto_ref = [x for x in text_splited if x.strip()]\n",
    "texto_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_valores = texto_ref\n",
    "\n",
    "# Exemplo de uso\n",
    "marcador1 = 'VALOR SERVIÇOS:'  # SAO PEDRO DA ALDEIA\n",
    "marcador2 = 'LOCAL INCIDÊNCIA'\n",
    "lista_val = encontrar_valores3(texto_valores, marcador1, marcador2)\n",
    "\n",
    "\n",
    "print(f'\\nlista_val: {lista_val}  -->  {len(lista_val)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota 2\n",
    "file_path = \"pipeline_extracao_documentos/7_Avaliar_processamento/Batch_2/SAO PEDRO DA ALDEIA_PDF_17282023_1532/11282686/2023251 - RAZÃO ORGANIZAÇÃO CONTABIL - LTDA.pdf\"\n",
    "doc2convert = \"2023251 - RAZÃO ORGANIZAÇÃO CONTABIL - LTDA.pdf\"\n",
    "\n",
    "image_2work, image_path = convertResize(doc2convert, file_path, image_resized_path)\n",
    "imagem_gray = image_2work.convert('L')\n",
    "texto = (pytesseract.image_to_string(imagem_gray, lang='por'))\n",
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_valores = linhas\n",
    "\n",
    "# Exemplo de uso\n",
    "marcador1 = 'VALOR SERVIÇOS:'  # SAO PEDRO DA ALDEIA\n",
    "marcador2 = 'LOCAL INCIDÊNCIA'\n",
    "lista_val = encontrar_valores2(texto_valores, marcador1, marcador2)\n",
    "lista_val\n",
    "\n",
    "print(f'\\nlista_val: {lista_val}  -->  {len(lista_val)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota 3\n",
    "file_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_20/MAGE_PDF_31282023_2254/1004403/NF 11 IGREJA EVANGÉLICA BATISTA ETERNA VIDA.pdf\"\n",
    "doc2convert = \"NF 11 IGREJA EVANGÉLICA BATISTA ETERNA VIDA.pdf\"\n",
    "\n",
    "image_2work, image_path = convertResize(doc2convert, file_path, image_resized_path)\n",
    "imagem_gray = image_2work.convert('L')\n",
    "texto_doc = (pytesseract.image_to_string(imagem_gray, lang='por'))\n",
    "text_splited = texto_doc.split('\\n')\n",
    "texto_refNota3 = [x for x in text_splited if x.strip()]\n",
    "texto_refNota3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_valores = texto_refNota3\n",
    "\n",
    "# Exemplo de uso\n",
    "marcador1 = 'VALOR SERVIÇOS:'  # SAO PEDRO DA ALDEIA\n",
    "marcador2 = 'LOCAL INCIDÊNCIA'\n",
    "lista_val = encontrar_valores2(texto_valores, marcador1, marcador2)\n",
    "lista_val\n",
    "\n",
    "print(f'\\nlista_val: {lista_val}  -->  {len(lista_val)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota 4 - SAO PEDRO DA ALDEIA\n",
    "file_path = \"pipeline_extracao_documentos/0_arquivos_teste_pipeline/pdf_raster/Sao_predo_aldeia/2023143.pdf\"\n",
    "doc2convert = \"2023143.pdf\"\n",
    "\n",
    "image_2work, image_path = convertResize(doc2convert, file_path, image_resized_path)\n",
    "imagem_gray = image_2work.convert('L')\n",
    "texto_doc = (pytesseract.image_to_string(imagem_gray, lang='por'))\n",
    "text_splited = texto_doc.split('\\n')\n",
    "texto_refNota4 = [x for x in text_splited if x.strip()]\n",
    "texto_refNota4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_valores = texto_refNota4\n",
    "\n",
    "# Exemplo de uso\n",
    "marcador1 = 'VALOR SERVIÇOS:'  # SAO PEDRO DA ALDEIA\n",
    "marcador2 = 'LOCAL INCIDÊNCIA'\n",
    "lista_val = encontrar_valores2(texto_valores, marcador1, marcador2)\n",
    "lista_val\n",
    "\n",
    "print(f'\\nlista_val: {lista_val}  -->  {len(lista_val)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota 5 - Mesquita\n",
    "file_path = \"pipeline_extracao_documentos/0_arquivos_teste_pipeline/pdf_pesquisavel/Mesquita/2023__5.pdf\"\n",
    "doc2convert = \"2023__5.pdf\"\n",
    "\n",
    "image_2work, image_path = convertResize(doc2convert, file_path, image_resized_path)\n",
    "imagem_gray = image_2work.convert('L')\n",
    "texto_doc = (pytesseract.image_to_string(imagem_gray, lang='por'))\n",
    "text_splited = texto_doc.split('\\n')\n",
    "texto_refNota5 = [x for x in text_splited if x.strip()]\n",
    "texto_refNota5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_valores = texto_refNota5\n",
    "\n",
    "# Exemplo de uso\n",
    "marcador1 = 'VALOR SERVIÇOS:'  # SAO PEDRO DA ALDEIA\n",
    "marcador2 = 'SIMPLES NACIONAL'\n",
    "lista_val = encontrar_valores2(texto_valores, marcador1, marcador2)\n",
    "lista_val\n",
    "\n",
    "print(f'\\nlista_val: {lista_val}  -->  {len(lista_val)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_valores = texto_refNota5\n",
    "\n",
    "# Exemplo de uso\n",
    "marcador1 = 'DESC. COND:'  # SAO PEDRO DA ALDEIA\n",
    "marcador2 = 'LOCAL INCIDÊNCIA'\n",
    "lista_val = encontrar_valores2(texto_valores, marcador1, marcador2)\n",
    "lista_val\n",
    "\n",
    "print(f'\\nlista_val: {lista_val}  -->  {len(lista_val)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Valor total da nota\n",
    "marcador_inicio = 'VALOR TOTAL DA NOTA:'\n",
    "marcador_fim = 'CNAE -'\n",
    "texto_valores = texto_refNota5\n",
    "\n",
    "resultado = extrair_texto_entre_marcadores(texto_valores, marcador1, marcador2)\n",
    "valor_total = resultado.replace(marcador_inicio, \"\").strip()\n",
    "valor_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_20/fwdnotasfaltantesnosistemadeemissoim20734_106187/Doria Marinho 0295 Carlos Leandro.pdf\"\n",
    "original_file_name = os.path.basename(file_path)\n",
    "\n",
    "section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "tipo = \"field_box\"\n",
    "father_value = \"5_frame_inf_criticas\"\n",
    "model_map = \"MAGE\"\n",
    "de_para_pm = \"PM_MAGE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_box_valores = {}\n",
    "data_box_valores['secao'] = section\n",
    "\n",
    "#print(f'father_value: {father_value}, section: {section}, tipo: {tipo}, model_map: {model_map}, de_para_pm: {de_para_pm}, original_file_name: {original_file_name}\\n')\n",
    "# trato a imagem logo no começo\n",
    "imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "\n",
    "# Estabeleco o filtro\n",
    "filtered_boxes_info = frames_nf_v4_df[((frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == tipo) & (frames_nf_v4_df['section_json'] == section) & (frames_nf_v4_df['father'] == father_value))]\n",
    "\n",
    "for idx_frame, row_frame in filtered_boxes_info.iterrows():\n",
    "    extracted_text_box = None\n",
    "    reference = row_frame['reference']\n",
    "    x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "    index = idx_frame\n",
    "    \n",
    "    label = row_frame['label']\n",
    "    image_gray_croped = imagem_gray.crop((x0, y0, x1, y1))\n",
    "    extracted_text_box = (pytesseract.image_to_string(image_gray_croped, lang='por'))\n",
    "    #print(extracted_text_box)\n",
    "    texto = extracted_text_box\n",
    "    if label == \"exigibilidade_iss\":\n",
    "        texto = extracted_text_box\n",
    "        linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "        marcador = \"EXIGIBILIDADE ISS\"\n",
    "        value  = encontrar_valor_por_marcador(marcador, linhas)\n",
    "        data_box_valores[label] = value\n",
    "        \n",
    "    if label == \"regime_tributacao\":\n",
    "        texto = extracted_text_box\n",
    "        linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "        marcador = \"REGIME TRIBUTAÇÃO\"\n",
    "        value = encontrar_valor_por_marcador(marcador, linhas)\n",
    "        data_box_valores[label] = value\n",
    "        \n",
    "    if label == \"simples_nacional\":\n",
    "        texto = extracted_text_box\n",
    "        linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "        marcador = \"SIMPLES NACIONAL\"\n",
    "        value = encontrar_valor_por_marcador(marcador, linhas) \n",
    "        data_box_valores[label] = value \n",
    "    \n",
    "    if label == \"issqn_retido\":\n",
    "        texto = extracted_text_box \n",
    "        linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "        marcador = \"ISSQN RETIDO\"\n",
    "        valor = encontrar_valor_por_marcador(marcador, linhas)\n",
    "        data_box_valores[label] = value \n",
    "        \n",
    "        \n",
    "    if label == \"local_pretacao_servico\":\n",
    "        texto = extracted_text_box\n",
    "        texto = texto_local_pretacao_servico.replace('\\n\\n', \" \").strip()\n",
    "        linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "        marcador = \"LOCAL. PRESTAÇÃO SERVIÇO\"\n",
    "        value = encontrar_valor_por_marcador(marcador, linhas)\n",
    "        data_box_valores[label] = value \n",
    "        \n",
    "    if label == \"local_incidencia\":\n",
    "        texto = extracted_text_box\n",
    "        linhas = [linha for linha in texto.split('\\n') if linha.strip()] \n",
    "        marcador = \"LOCAL INCIDÊNCIA\"\n",
    "        value = encontrar_valor_por_marcador(marcador, linhas)  \n",
    "        data_box_valores[label] = value              \n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> EXIGIBILIDADE ISS </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = texto_exigibilidade_iss\n",
    "linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcador = \"EXIGIBILIDADE ISS\"\n",
    "valor = encontrar_valor_por_marcador(marcador, linhas)\n",
    "valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> REGIME TRIBUTAÇÃO </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = texto_regime_tributacao\n",
    "linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcador = \"REGIME TRIBUTAÇÃO\"\n",
    "valor = encontrar_valor_por_marcador(marcador, linhas)\n",
    "valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> SIMPLES NACIONAL </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = texto_simples_nacional\n",
    "linhas = [linha for linha in texto.split('\\n') if linha.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcador = \"SIMPLES NACIONAL\"\n",
    "valor = encontrar_valor_por_marcador(marcador, linhas)\n",
    "valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> ISSQN RETIDO </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = texto_issqn_retido\n",
    "linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcador = \"ISSQN RETIDO\"\n",
    "valor = encontrar_valor_por_marcador(marcador, linhas)\n",
    "valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> LOCAL. PRESTAÇÃO SERVIÇO </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = texto_local_pretacao_servico.replace('\\n\\n', \" \").strip()\n",
    "linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcador = \"LOCAL. PRESTAÇÃO SERVIÇO\"\n",
    "valor = encontrar_valor_por_marcador(marcador, linhas)\n",
    "valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> LOCAL INCIDÊNCIA </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = texto_local_incidencia\n",
    "linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcador = \"LOCAL INCIDÊNCIA\"\n",
    "valor = encontrar_valor_por_marcador(marcador, linhas)\n",
    "valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string_with_exceptions(s, phrases):\n",
    "    safe_phrases = {phrase: phrase.replace(' ', '_') for phrase in phrases}\n",
    "\n",
    "    for phrase, safe_phrase in safe_phrases.items():\n",
    "        s = s.replace(phrase, safe_phrase)\n",
    "\n",
    "    words = s.split()\n",
    "\n",
    "    for safe_phrase, phrase in safe_phrases.items():\n",
    "        words = [word.replace(safe_phrase, phrase) for word in words]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_phrase(safe_word, safe_phrases_to_original):\n",
    "    return safe_phrases_to_original.get(safe_word, safe_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando a função\n",
    "safe_phrases_to_original = {\"Microempresa_municipal\": \"Microempresa municipal\", \"Magé_-_RJ\": \"RJ Magé\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_phrase = get_original_phrase(\"Microempresa_municipal\", safe_phrases_to_original)\n",
    "print(original_phrase)  # Deve imprimir: \"Sociedade Limitada\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_word_from_list(word_list, word_to_remove):\n",
    "    return [word for word in word_list if word != word_to_remove]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando a função\n",
    "word_list = [\"VALOR_IR:\", \"RJ_Magé\", \"DESC._COND:\"]\n",
    "word_to_remove = \"DESC._COND:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word_list = remove_word_from_list(impostos, word_to_remove)\n",
    "print(new_word_list)  # Deve imprimir: [\"RJ_Magé\", \"outra_palavra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    secao = \"6. CNAE e Item da Lista de Serviços\"\n",
    "    try:\n",
    "        nd_data_CNAE = {}\n",
    "        nd_data_CNAE['secao'] = secao\n",
    "        f_frame_name = \"4_frame_cnae_itens_servico\"   \n",
    "        Texto_extraido = executa_model_frame(model, secao, f_frame_name)\n",
    "        text_splited = Texto_extraido.split('\\n')\n",
    "        # Processando CNAE\n",
    "        cnae_line = [line for line in text_splited if 'CNAE' in line][0]\n",
    "        cnae_number = int(extract_number(cnae_line))\n",
    "        cnae_value = cnae_dict.get(cnae_number, \"Valor não encontrado\")\n",
    "        if cnae_value == 'Valor não encontrado':\n",
    "            cnae_value = processa_cnae_outros(cnae_line)\n",
    "            cnae_value = cnae_value.upper()\n",
    "            nd_data_CNAE['cnae'] = cnae_value\n",
    "        else:\n",
    "            cnae_value = cnae_value.upper()\n",
    "            cnae_value = str(cnae_number) + \" - \" + cnae_value\n",
    "            nd_data_CNAE['cnae'] = cnae_value\n",
    "            nd_data_CNAE['item_lista_servicos'] = item_servico_value\n",
    "    except Exception as e:\n",
    "        print(f\"Erro busca cnae: {e}\")    \n",
    "\n",
    "    try:\n",
    "        item_servico_line = [line for line in text_splited if 'Item da Lista de Serviços' in line][0]\n",
    "        item_servico_number = float(extract_number(item_servico_line))\n",
    "        item_servico_value = item_servico_dict.get(item_servico_number, \"Valor não encontrado\")\n",
    "        item_servico_value = item_servico_value.upper()\n",
    "        item_servico_value = str(item_servico_number) + \" - \" + item_servico_value\n",
    "        nd_data_CNAE['item_lista_servicos'] = item_servico_value\n",
    "    except Exception as e:\n",
    "        print(f\"Erro busca Itens de servico: {e}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # templates = {\n",
    "    #     (\"PM_MAGE\", \"30.693.231/0001-99\"): \"MAGE_MAICON\",\n",
    "    #     (\"PM_MAGE\", \"23.317.112/0001-76\"): \"MAGE_MFF\",\n",
    "    #     (\"PM_MAGE\", \"06.083.610/0001-82\"): \"MAGE_DORIA\",      \n",
    "    #     (\"PM_MAGE\", None): \"MAGE\",\n",
    "    #     (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", \"47.945.459/0001-21\"): \"SAO_PEDRO_GOAT\",\n",
    "    #     (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", \"68.687.722/0001-08\"): \"SAO_PEDRO_GM\",\n",
    "    #     (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", \"34.230.979/0038-06\"): \"SAO_PEDRO_SUPERMIX\",\n",
    "    #     (\"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\", None): \"SAO_PEDRO\",\n",
    "    #     (\"Pague agora com o seu Pix\", None): \"NAO_PROCESSAR\",\n",
    "    #     # ... adicione outras combinações aqui\n",
    "    # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        label_value = row_frame['label']\n",
    "        reference_value = row_frame['reference']\n",
    "        f_type = row_frame['type']\n",
    "        frame_id = row_frame['id']\n",
    "        if row_frame['label']  == \"1_frame_prefeitura_nf\":\n",
    "            x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "            texto_extraido = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "            #print(texto_extraido)\n",
    "            label_value = row_frame['label']\n",
    "            #print(\"label_value\", label_value)\n",
    "            values = texto_extraido.split('\\n')\n",
    "            for index_sframe, row_sframe in filtered_sframe_field_nf_v4_df.iterrows():\n",
    "                label_value = row_sframe['label']\n",
    "                #print(\"label_value\", label_value)\n",
    "                if label_value == \"nome_prefeitura\":\n",
    "                    reference_value = row_sframe['reference']\n",
    "                    for value in values:\n",
    "                        result = process_line(value, reference_value, label_value)\n",
    "                        if result:\n",
    "                            data_extrated_prefeitura.update(result)\n",
    "                elif label_value == \"secretaria\":\n",
    "                    reference_value = row_sframe['reference']\n",
    "                    for value in values:\n",
    "                        result = process_line(value, reference_value, label_value)\n",
    "                        if result:\n",
    "                            data_extrated_prefeitura.update(result) \n",
    "                elif label_value == \"tipo_nota_fiscal\":\n",
    "                    reference_value = row_sframe['reference']  \n",
    "                    for value in values:\n",
    "                        result = process_line(value, reference_value, label_value)\n",
    "                        if result:\n",
    "                            data_extrated_prefeitura.update(result)\n",
    "        \n",
    "        if row_frame['label']  == \"1_frame_dados_nf\":\n",
    "            x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "            texto_extraido = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "            #print(texto_extraido)\n",
    "            text_splited = texto_extraido_cabecalho(texto_extraido)\n",
    "            keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "            string_pesquisa = \"Número da Nota:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "            data_extrated_prefeitura['numero_nota_fiscal'] = texto\n",
    "\n",
    "            string_pesquisa = \"Competência:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_extrated_prefeitura['competencia'] = texto\n",
    "\n",
    "            string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_extrated_prefeitura['dt_hr_emissao'] = texto\n",
    "\n",
    "            string_pesquisa = \"Código Verificação:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_extrated_prefeitura['codigo_verificacao'] = texto\n",
    "            #print(f'{frame_id:>5}  {f_type:>20} | {label_value:>30} |  ref:{reference_value:>30}  | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. XXX Ajusta texto\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF RASTER NO CABECALHO\n",
    "def texto_extraido_cabecalho(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    text_splited = [s.replace(\")\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF_Pesquisavel-  NO CABECALHO\n",
    "def texto_extraido_nf(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_coordinates(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    print(x0, y0, x1, y1)\n",
    "    frame_image\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=config).strip()\n",
    "    return extracted_text\n",
    "\n",
    "def extract_text_from_coordinates_2(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    print(x0, y0, x1, y1)\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config_1).strip()\n",
    "    return extracted_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dados_comple_obs(modelo, frame_father, section):\n",
    "    \n",
    "    data_dados_complementares = {}\n",
    "    #frame_label = frame_father\n",
    "    \n",
    "    # 1. Filtrando o frames_info para buscar os dados de corte\n",
    "    filtered_frames_info = frames_info[(frames_info['label'] == frame_father) & (frames_info['model'] == modelo)]\n",
    "\n",
    "    # 2. Filtrando o sframe_fields_info para buscar os dados dos campos que estao nos frames\n",
    "    filtered_sframe_fields_info = sframe_fields_info[(sframe_fields_info['father'] == frame_father) & (sframe_fields_info['model'] == modelo)]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "        \n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        \n",
    "        print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_frame['seq'], row_frame['model'], row_frame['father'], row_frame['label'], row_frame['reference'], row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'] ))\n",
    "        for index_field, row_field in filtered_sframe_fields_info.iterrows():\n",
    "            #print(\"{:<5} {:<10} {:<30} {:<20} {:<20}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference']))\n",
    "            \n",
    "            if frame_father == \"5_frame_dados_complementares\":\n",
    "                nf_data_dados_complementares = {}\n",
    "                nf_data_dados_complementares['section'] = section\n",
    "                \n",
    "                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^DADOS COMPLEMENTARES', '', extracted_text_box, count=1)\n",
    "                if text == '':\n",
    "                    text = None\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text\n",
    "                else:    \n",
    "                    # Extrair texto dentro do retângulo\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "                    \n",
    "                return nf_data_dados_complementares                \n",
    "                \n",
    "            elif frame_father == \"5_frame_observacao\":\n",
    "                nf_data_observacao = {}\n",
    "                nf_data_observacao['section'] = section \n",
    "                                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^Observação:', '', extracted_text_box, count=1)\n",
    "\n",
    "                # Remover quebras de linha\n",
    "                text = text.replace('\\n', ' ')\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                nf_data_observacao['observacao'] = text.strip()\n",
    "                \n",
    "                return nf_data_observacao "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields_tomador_cnpj(text):\n",
    "    nf_data_tomador_cnpj = {}\n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in text:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "                        nf_data_tomador_cnpj['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                        nf_data_tomador_cnpj['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "\n",
    "    \n",
    "    # Extrair Telefone\n",
    "    telefone_match = re.search(r'Telefone:\\s+(.+)', text)\n",
    "    if telefone_match:\n",
    "        telefone_str = telefone_match.group(1)\n",
    "        if telefone_str == 'Inscrição Estadual:':\n",
    "            nf_data_tomador_cnpj['telefone'] = None  # Valor padrão quando não há correspondência\n",
    "        elif telefone_str == '':\n",
    "            nf_data_tomador_cnpj['telefone'] = None  # Valor padrão quando não há correspondência\n",
    "                    \n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['telefone'] = telefone_match.group(1)\n",
    "            \n",
    "    \n",
    "    # Extrair Inscrição Municipal\n",
    "    inscricao_municipal_match = re.search(r'Inscrição Municipal:\\s+(.+)', text)\n",
    "    if inscricao_municipal_match:\n",
    "        inscricao_municipal_str = inscricao_municipal_match.group(1)\n",
    "        if inscricao_municipal_str == \"Telefone:\": \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = inscricao_municipal_str\n",
    "    \n",
    "    insc_municipal_match = re.search(r'INSC:MUNICIPAL:\\s+(.+)', text)\n",
    "    if insc_municipal_match:\n",
    "        insc_municipal_str = insc_municipal_match.group(1)\n",
    "        if insc_municipal_str == \"Telefone:\":\n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = insc_municipal_str\n",
    "    else:\n",
    "        nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "            \n",
    "    \n",
    "    return nf_data_tomador_cnpj \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao importante - process_line\n",
    "def process_line(value, reference, label):\n",
    "    name_match = re.search(fr'{reference} (.+)', value)\n",
    "    if name_match:\n",
    "        extracted_value = reference + \" \" + name_match.group(1)\n",
    "        return {label: extracted_value}\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_fb_outras_inf(modelo, father_value, section):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    filtered_boxes_info = field_boxes_info[(field_boxes_info['father'] == father_value) & (field_boxes_info['model'] == model)]\n",
    "    # Iterate nas informações dos boxes de fields e extraia o texto de cada field\n",
    "    for index_field, row_field in filtered_boxes_info.iterrows():\n",
    "        \n",
    "        string_pesquisa = row_field['reference']\n",
    "        x0, y0, x1, y1 = row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        label = row_field['label']\n",
    "        #print(f'extracted_text_box {extracted_text_box}, label {label}')\n",
    "        text = extracted_text_box.replace('\\n', '')\n",
    "        if text.startswith(string_pesquisa):\n",
    "            #print(\"aqui:\", text)\n",
    "            text = text[len(label):].strip()\n",
    "            data_box_valores[label] = text\n",
    "    \n",
    "    return   data_box_valores  \n",
    "\n",
    "\n",
    "def extract_text_from_frame(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=config).strip()\n",
    "    return extracted_text \n",
    "\n",
    "\n",
    "# 2. Efetua OCR no documento (area parao do texto da NF)\n",
    "def ocr_RasterPDF_free(image_name, vx0, vy0, vx1, vy1):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = vx0\n",
    "    y0 = vy0\n",
    "    x1= vx1\n",
    "    y1 = vy1\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "    return values, extracted_text_frame \n",
    "\n",
    "\n",
    "# 3. Efetua OCR no documento (area parao do texto da NF)\n",
    "def ocr_RasterPDF(image_name):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = 406\n",
    "    y0 = 0\n",
    "    x1= 1540\n",
    "    y1 = 380\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "    return values, extracted_text_frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. XXX Baseado no texto extraido, cria um dicionario com os dados extraidos\n",
    "def cria_guarda_doc_ref_R_PDF(idx, row, de_para_pm, model_map, original_file_name, file_path, image_resized_path):\n",
    "    \n",
    "    guarda_texto_doc = {}\n",
    "    \n",
    "    texto_documento = extracao_texto_ref(idx, row, model_map, original_file_name, file_path, image_resized_path)\n",
    "    \n",
    "    guarda_texto_doc['document_unique_id'] = idx\n",
    "    guarda_texto_doc['original_file_name'] = original_file_name\n",
    "    guarda_texto_doc['texto_documento'] = texto_documento\n",
    "    \n",
    "    return guarda_texto_doc, texto_documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrai_geral_data_R_PDF(idx, row, de_para_pm, model_map, marcador_inicio, marcador_fim, def_replace, original_file_name, file_path):\n",
    "    \n",
    "    nf_data_geral = {}\n",
    "    x0 = 1\n",
    "    y0 = 1\n",
    "    x1 = 2067\n",
    "    y1 = 2923\n",
    "    image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "    texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "    text_splited = texto_extraido.split('\\n')\n",
    "    \n",
    "    marcador_inicio = marcador_inicio\n",
    "    marcador_fim = marcador_fim\n",
    "    resultado = extrair_texto_entre_marcadores(text_splited, marcador_inicio, marcador_fim)\n",
    "    if def_replace:\n",
    "        resultado = resultado.replace(marcador_inicio, \"\").strip()\n",
    "        return resultado\n",
    "    else:\n",
    "        return resultado    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_20/fwdnotasfaltantesnosistemadeemissoim20734_106187/Doria Marinho 0301 Ultrascan.pdf\"\n",
    "def_replace = True\n",
    "\n",
    "#section = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "marcador_inicio = 'DISCRIMINAÇÃO DOS SERVIÇOS'\n",
    "marcador_fim = 'VALOR TOTAL DA NOTA:'\n",
    "\n",
    "original_file_name = \"Doria Marinho 0301 Ultrascan.pdf\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = 'MAGE_DORIA'\n",
    "section = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "label = \"discriminacao_servicos\"\n",
    "tipo = \"field_box\"\n",
    "original_file_name = \"Doria Marinho 0301 Ultrascan.pdf\"\n",
    "de_para_pm = \"PM_MAGE\"\n",
    "\n",
    "file_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_20/fwdnotasfaltantesnosistemadeemissoim20734_106187/Doria Marinho 0296 Vanisa (Cancelada).pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def extrai_servicos_R_PDF(idx, row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path):\n",
    "    \n",
    "    section = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "    tipo = \"field_box\"\n",
    "    label = \"discriminacao_servicos\"\n",
    "    nf_data_servico = {}\n",
    "    pdf_pesquisavel_map = False\n",
    "    \n",
    " \n",
    "    image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "    \n",
    "    coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    #print(f'1. : (coordenadas originais):  x0:{x0} y0:{y0} x1:{x1} y1:{y1}\\n')\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    print(f'coordenadas:   x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0:{f_0}, f_1: {f_1}\\n')\n",
    "    texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "    #text = texto_extraido.replace('\\n', ' ')\n",
    "    \n",
    "    print(f'texto_extraido:\\n{texto_extraido}\\n')\n",
    "    label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "    # if text.startswith(label):\n",
    "    #   text = text[len(label):].strip()\n",
    "    #   nf_data_servico['discriminacao_servicos'] = text        \n",
    "      \n",
    "          \n",
    "    return nf_data_servico   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriçao\n",
    "marcador_inicio = 'DISCRIMINAÇÃO DOS SERVIÇOS'\n",
    "marcador_fim = 'VALOR TOTAL DA NOTA:'\n",
    "\n",
    "resultado = extrair_texto_entre_marcadores(text_splited, marcador_inicio, marcador_fim)\n",
    "resultado = resultado.replace(marcador_inicio, \"\").strip()\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outras Informacoes criticas\n",
    "marcador_inicio = 'EXIGIBILIDADE ISS REGIME TRIBUTAÇÃO SIMPLES NACIONAL ISSQN RETIDO LOCAL. PRESTAÇÃO LOCAL INCIDÊNCIA'\n",
    "marcador_fim = 'Observação:'\n",
    "\n",
    "resultado = extrair_texto_entre_marcadores(text_splited, marcador_inicio, marcador_fim)\n",
    "resultado\n",
    "resultado = resultado.replace(marcador_inicio, \"\").strip()\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outras Informacoes criticas\n",
    "marcador_inicio = 'EXIGIBILIDADE ISS REGIME TRIBUTAÇÃO SIMPLES NACIONAL ISSQN RETIDO LOCAL. PRESTAÇÃO LOCAL INCIDÊNCIA'\n",
    "marcador_fim = 'Observação:'\n",
    "\n",
    "resultado = extrair_texto_entre_marcadores(text_splited, marcador_inicio, marcador_fim)\n",
    "resultado\n",
    "resultado = resultado.replace(marcador_inicio, \"\").strip()\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valores dos impostos\n",
    "marcador_inicio = 'i Fi\"'\n",
    "marcador_fim = 'DADOS COMPLEMENTARES'\n",
    "\n",
    "resultado = extrair_texto_entre_marcadores(text_splited, marcador_inicio, marcador_fim)\n",
    "resultado\n",
    "resultado = resultado.replace(marcador_inicio, \"\").strip()\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = resultado.replace(marcador_inicio, \"\").strip()\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITEM DA LISTA DE SERVICO\n",
    "marcador_inicio = 'Item da Lista de Serviços -'\n",
    "marcador_fim = 'VALOR SERVIÇOS:'\n",
    "\n",
    "resultado = extrair_texto_entre_marcadores(text_splited, marcador_inicio, marcador_fim)\n",
    "resultado = resultado.replace(marcador_inicio, \"\").strip()\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNAE\n",
    "marcador_inicio = 'CNAE - '\n",
    "marcador_fim = 'Item da Lista de Serviços -'\n",
    "\n",
    "resultado = extrair_texto_entre_marcadores(text_splited, marcador_inicio, marcador_fim)\n",
    "resultado = resultado.replace(marcador_inicio, \"\").strip()\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valor total da nota\n",
    "marcador_inicio = 'VALOR TOTAL DA NOTA:'\n",
    "marcador_fim = 'CNAE -'\n",
    "\n",
    "resultado = extrair_texto_entre_marcadores(text_splited, marcador_inicio, marcador_fim)\n",
    "resultado = resultado.replace(marcador_inicio, \"\").strip()\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriçao\n",
    "marcador_inicio = 'DISCRIMINAÇÃO DOS SERVIÇOS'\n",
    "marcador_fim = 'VALOR TOTAL DA NOTA:'\n",
    "\n",
    "resultado = extrair_texto_entre_marcadores(text_splited, marcador_inicio, marcador_fim)\n",
    "resultado = resultado.replace(marcador_inicio, \"\").strip()\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Razao Social\n",
    "marcador_inicio = 'Nome/Razão Social:'\n",
    "marcador_fim = 'Nome de Fantasia:'\n",
    "\n",
    "resultado = extrair_texto_entre_marcadores(text_splited, marcador_inicio, marcador_fim)\n",
    "resultado = resultado.replace(marcador_inicio, \"\").strip()\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = resultado.replace(marcador_inicio, \"\").strip()\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codigo de verificaçao\n",
    "marcador_inicio = 'Código Verificação'\n",
    "marcador_fim = 'PRESTADOR DE SERVIÇOS'\n",
    "\n",
    "resultado = extrair_texto_entre_marcadores(text_splited, marcador_inicio, marcador_fim)\n",
    "resultado = resultado.replace(marcador_inicio, \"\").strip()\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splited = [x for x in text_splited if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_texto_entre_marcadores(texto, marcador_inicio, marcador_fim):\n",
    "    try:\n",
    "        # Encontra os índices dos marcadores de início e fim\n",
    "        indice_inicio = next(i for i, s in enumerate(texto) if s.startswith(marcador_inicio)) + 1\n",
    "        indice_fim = next(i for i, s in enumerate(texto) if s.startswith(marcador_fim))\n",
    "        \n",
    "        # Extrai e retorna o texto entre os marcadores\n",
    "        return \" \".join(texto[indice_inicio:indice_fim])\n",
    "    except StopIteration:\n",
    "        # Retorna None se algum dos marcadores não for encontrado\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if indice_inicio == indice_fim:\n",
    "    inicio = texto[indice_inicio].find(marcador_inicio) + len(marcador_inicio)\n",
    "    fim = texto[indice_fim].find(marcador_fim)\n",
    "    resultado = texto[indice_inicio][inicio:fim].strip()\n",
    "    resultado = resultado.replace(marcador_inicio, \"\").strip()  # Remove o marcador de início\n",
    "    return resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcador_inicio = 'TOMADOR DE SERVIÇOS'\n",
    "marcador_fim = 'Telefone'\n",
    "\n",
    "resultado = extrair_texto_entre_marcadores(text_splited, marcador_inicio, marcador_fim)\n",
    "resultado = resultado.replace(marcador_inicio, \"\").strip()\n",
    "resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcador_inicio = 'VALOR TOTAL DA NOTA'\n",
    "marcador_fim = 'CNAE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcador_inicio = 'DISCRIMINAÇÃO DOS SERVIÇOS'\n",
    "marcador_fim = 'VALOR TOTAL DA NOTA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = extrair_texto_entre_marcadores(text_splited, marcador_inicio, marcador_fim)\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcador_inicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = resultado.replace(marcador_inicio, \"\").strip()\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrai_prestador_R_PDF_RESERVA(idx, row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path):\n",
    "\n",
    "    nf_data_prestador = {}\n",
    "    dic_erros = {}\n",
    "    message_erro = []\n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = False\n",
    "    nf_data_prestador['secao'] = \"2. PRESTADOR DE SERVIÇO\"\n",
    "\n",
    "    process = ['2_frame_cnpj_prestador', '2_frame_inscricao_prestador', '2_frame_dados_prestador']\n",
    "    tipo = \"frame\"\n",
    "\n",
    "    image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "    \n",
    "    for label in process:\n",
    "        seq = process.index(label) + 1\n",
    "        if label == \"2_frame_cnpj_prestador\":\n",
    "            coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "            #print(f'texto_extraido: {texto_extraido}\\n\\n')\n",
    "            text_splited = texto_extraido_nf(texto_extraido)\n",
    "            \n",
    "            keyword_list = ['CPF/CNPJ:', 'Telefone:']\n",
    "            string_pesquisa = \"CPF/CNPJ:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', texto)\n",
    "            if cpf_cnpj_formatado_match:\n",
    "                nf_data_prestador['p_cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                nf_data_prestador['p_cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "            \n",
    "            try:\n",
    "                string_pesquisa = \"Telefone:\"  \n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                print(texto) \n",
    "                telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', texto) \n",
    "                if telefone_match: \n",
    "                    telefone_str = telefone_match.group(1)\n",
    "                    # Remover quebras de linha\n",
    "                    telefone_str = telefone_str.replace('.', '')\n",
    "                    telefone_str = telefone_str.replace('\\n', '')\n",
    "                            \n",
    "                    nf_data_prestador['p_telefone'] = telefone_str\n",
    "                else:\n",
    "                    nf_data_prestador['p_telefone'] = None\n",
    "            except Exception as e:\n",
    "                msg = (f\"doc: | {e}\")\n",
    "                message_erro.append(msg)\n",
    "                nf_data_prestador['p_telefone'] = None   \n",
    "                \n",
    "        if label == \"2_frame_inscricao_prestador\":\n",
    "            coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "            \n",
    "            print(f'texto_extraido: {texto_extraido} | {original_file_name} \\n\\n')\n",
    "            try:\n",
    "                text_splited = texto_extraido.split('\\n')\n",
    "                text_splited = texto_extraido(texto_extraido)\n",
    "                text_splited = [x for x in text_splited if x.strip()]\n",
    "                keyword_list = ['Inscrição Municipal:', 'Inscrição Estadual:']\n",
    "                string_pesquisa = \"Inscrição Municipal:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                nf_data_prestador['p_inscricao_municipal'] = texto\n",
    "                \n",
    "                string_pesquisa = \"Inscrição Estadual:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                nf_data_prestador['p_inscricao_estadual'] = texto\n",
    "            \n",
    "            except Exception as e:\n",
    "                new_row = {\n",
    "                    \"row_index\": idx,  # Substitua 'index' pela variável que contém o índice da linha atual\n",
    "                    \"erro_inscricao\": str(e),\n",
    "                    \"file\": original_file_name,\n",
    "                    \"process_label\": label\n",
    "                }   \n",
    "                dic_erros.update(new_row)\n",
    "                nf_data_prestador['p_inscricao_municipal'] = None\n",
    "                nf_data_prestador['p_inscricao_estadual'] = None\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    return nf_data_prestador, dic_erros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcador_inicio = marc_ini[0]\n",
    "marcador_fim = marc_fim[0]\n",
    "resultado = extrair_texto_entre_marcadores(texto_documento_uso, marcador_inicio, marcador_fim)\n",
    "discriminacao_servico = resultado.replace(marcador_inicio, \"\").strip()\n",
    "discriminacao_servico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrai_prestador_R_PDF(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path):\n",
    "\n",
    "    nf_data_prestador = {}\n",
    "    message_erro = []\n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = False\n",
    "    nf_data_prestador['secao'] = \"2. PRESTADOR DE SERVIÇO\"\n",
    "\n",
    "    process = ['2_frame_cnpj_prestador', '2_frame_inscricao_prestador', '2_frame_dados_prestador']\n",
    "    tipo = \"frame\"\n",
    "\n",
    "    image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "    \n",
    "    for label in process:\n",
    "        seq = process.index(label) + 1\n",
    "        if label == \"2_frame_cnpj_prestador\":\n",
    "            coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "            #print(f'texto_extraido: {texto_extraido}\\n\\n')\n",
    "            text_splited = texto_extraido_nf(texto_extraido)\n",
    "            \n",
    "            keyword_list = ['CPF/CNPJ:', 'Telefone:']\n",
    "            string_pesquisa = \"CPF/CNPJ:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', texto)\n",
    "            if cpf_cnpj_formatado_match:\n",
    "                nf_data_prestador['p_cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                nf_data_prestador['p_cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "            \n",
    "            try:\n",
    "                string_pesquisa = \"Telefone:\"  \n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                print(texto) \n",
    "                telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', texto) \n",
    "                if telefone_match: \n",
    "                    telefone_str = telefone_match.group(1)\n",
    "                    # Remover quebras de linha\n",
    "                    telefone_str = telefone_str.replace('.', '')\n",
    "                    telefone_str = telefone_str.replace('\\n', '')\n",
    "                            \n",
    "                    nf_data_prestador['p_telefone'] = telefone_str\n",
    "                else:\n",
    "                    nf_data_prestador['p_telefone'] = None\n",
    "            except Exception as e:\n",
    "                msg = (f\"doc: | {e}\")\n",
    "                message_erro.append(msg)\n",
    "                nf_data_prestador['p_telefone'] = None   \n",
    "                \n",
    "        if label == \"2_frame_inscricao_prestador\":\n",
    "            coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "            print(f'texto_extraido: {texto_extraido} | {original_file_name} \\n\\n')\n",
    "            try:\n",
    "                text_splited = texto_extraido(texto_extraido)\n",
    "                keyword_list = ['Inscrição Municipal:', 'Inscrição Estadual:']\n",
    "                string_pesquisa = \"Inscrição Municipal:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                nf_data_prestador['p_inscricao_municipal'] = texto\n",
    "                \n",
    "                string_pesquisa = \"Inscrição Estadual:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                nf_data_prestador['p_inscricao_estadual'] = texto\n",
    "            \n",
    "            except Exception as e:\n",
    "                msg = (f\"doc: {original_file_name} | {e}\")\n",
    "                nf_data_prestador['p_inscricao_municipal'] = None\n",
    "                nf_data_prestador['p_inscricao_estadual'] = None\n",
    "                message_erro.append(msg)\n",
    "            \n",
    "            \n",
    "            \n",
    "         \n",
    "            \n",
    "\n",
    "\n",
    "    return nf_data_prestador  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Remover quebras de linha e rótulo\n",
    "        text = text.replace('\\n', ' ')\n",
    "        if text.startswith(label):\n",
    "            text = text[len(label):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PRESTADOR DE SERVIÇO\n",
    "def extrai_prestador_R_PDF(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path):\n",
    "    \n",
    "    nf_data_prestador = {}\n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = False\n",
    "    \n",
    "    process = ['2_frame_cnpj_prestador', '2_frame_inscricao_prestador', '2_frame_dados_prestador']\n",
    "\n",
    "    tipo = \"frame\"\n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    #print(label)\n",
    "    #print(x0,x1,y0,y1)\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    nf_data_prestador = novaextra.extract_fields_prestador(text)\n",
    "\n",
    "           \n",
    "\n",
    "    pdf_document.close()\n",
    "\n",
    "    return nf_data_prestador \n",
    "\n",
    "# 2.B PRESTADOR DE SERVIÇO - RASTER\n",
    "def processar_prestador_raster_pdf(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = False\n",
    "    \n",
    "    tipo= \"frame\"\n",
    "    message_erro = []\n",
    "    nf_dados_prestador = {}\n",
    "    model = row['model']\n",
    "    print(model)\n",
    "\n",
    "    process_prestador = ['2_frame_cnpj_prestador', '2_frame_inscricao_prestador', '2_frame_dados_prestador']\n",
    "    image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "    for father in process_prestador:\n",
    "            label = father\n",
    "            if label == \"2_frame_cnpj_prestador\": \n",
    "                coordinates = get_coordinates_filter(model_map, tipo, label, section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                extract_text = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                #text_splited = texto_extraido_nf(extract_text)\n",
    "                if \"CPF/CNPJ:\" in extract_text:\n",
    "                    cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', extract_text)\n",
    "                    if cpf_cnpj_formatado_match:\n",
    "                        texto = cpf_cnpj_formatado_match.group(1)\n",
    "                    \n",
    "                        nf_dados_prestador['cpf_cnpj_com_mascara'] = texto\n",
    "                        nf_dados_prestador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "                else:\n",
    "                    nf_dados_prestador['cpf_cnpj_com_mascara'] = 'None'\n",
    "                    nf_dados_prestador['cpf_cnpj_sem_mascara'] = 'None'           \n",
    "                        \n",
    "                telefone_str = None\n",
    "                #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "                telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', extract_text)\n",
    "                if telefone_match: \n",
    "                    telefone_str = telefone_match.group(1)\n",
    "                    # Remover quebras de linha\n",
    "                    telefone_str = telefone_str.replace('.', '')\n",
    "                    telefone_str = telefone_str.replace('\\n', '')\n",
    "                            \n",
    "                    nf_dados_prestador['telefone'] = telefone_str\n",
    "                else:\n",
    "                    nf_dados_prestador['telefone'] = 'None'\n",
    "                    \n",
    "            # if father_to_process == \"2_frame_cnpj_prestador\":\n",
    "            elif label == '2_frame_inscricao_prestador':\n",
    "                coordinates = get_coordinates_filter(model_map, tipo, label, section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                extract_text = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                \n",
    "                text_splited = texto_extraido_nf(extract_text)\n",
    "                \n",
    "                \n",
    "                # Extrair Inscrição Estadual\n",
    "                inscricao_estadual_match = re.search(r'Inscrição Estadual:\\s+(.+)', extract_text)\n",
    "                if inscricao_estadual_match:\n",
    "                    inscricao_estadual_str = inscricao_estadual_match.group(1)\n",
    "                    if inscricao_estadual_str == 'Nome/Razão Social:':\n",
    "                        nf_inscr_estadual = 'None'\n",
    "                        nf_dados_prestador['inscricao_estadual'] = \"NONE\"\n",
    "                    else:\n",
    "                        nf_inscr_estadual = inscricao_estadual_match.group(1)\n",
    "                        nf_dados_prestador['inscricao_estadual'] = inscricao_estadual_match.group(1) \n",
    "                \n",
    "\n",
    "            elif label == '2_frame_dados_prestador':\n",
    "                dados_prestador = {}\n",
    "                coordinates = get_coordinates_filter(model_map, tipo, label, section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                \n",
    "                extract_text = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                dados_prestador = extract_fields_prestador(extract_text)\n",
    "                \n",
    "                text_splited = texto_extraido_nf(extract_text)\n",
    "                keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "                string_pesquisa = \"Nome/Razão Social:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            \n",
    "                nf_dados_prestador['razao_social'] = texto\n",
    "\n",
    "                string_pesquisa = \"Nome de Fantasia:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                nf_dados_prestador['nome_fantasia'] = texto\n",
    "                \n",
    "                string_pesquisa = \"Endereço:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                nf_dados_prestador['endereco'] = texto\n",
    "                \n",
    "                string_pesquisa = \"E-mail:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                nf_dados_prestador['email'] = texto\n",
    "                \n",
    "    return nf_dados_prestador  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_prestador_raster_pdf(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = False\n",
    "    \n",
    "    tipo= \"frame\"\n",
    "    message_erro = []\n",
    "    nf_dados_prestador = {}\n",
    "    model = row['model']\n",
    "    print(model)\n",
    "\n",
    "    process_prestador = ['2_frame_cnpj_prestador', '2_frame_inscricao_prestador', '2_frame_dados_prestador']\n",
    "    image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "    for father in process_prestador:\n",
    "            label = father\n",
    "            if label == \"2_frame_cnpj_prestador\": \n",
    "                coordinates = get_coordinates_filter(model_map, tipo, label, section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                extract_text = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                #text_splited = texto_extraido_nf(extract_text)\n",
    "                if \"CPF/CNPJ:\" in extract_text:\n",
    "                    cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', extract_text)\n",
    "                    if cpf_cnpj_formatado_match:\n",
    "                        texto = cpf_cnpj_formatado_match.group(1)\n",
    "                    \n",
    "                        nf_dados_prestador['cpf_cnpj_com_mascara'] = texto\n",
    "                        nf_dados_prestador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "                else:\n",
    "                    nf_dados_prestador['cpf_cnpj_com_mascara'] = 'None'\n",
    "                    nf_dados_prestador['cpf_cnpj_sem_mascara'] = 'None'           \n",
    "                        \n",
    "                telefone_str = None\n",
    "                #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "                telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', extract_text)\n",
    "                if telefone_match: \n",
    "                    telefone_str = telefone_match.group(1)\n",
    "                    # Remover quebras de linha\n",
    "                    telefone_str = telefone_str.replace('.', '')\n",
    "                    telefone_str = telefone_str.replace('\\n', '')\n",
    "                            \n",
    "                    nf_dados_prestador['telefone'] = telefone_str\n",
    "                else:\n",
    "                    nf_dados_prestador['telefone'] = 'None'\n",
    "                    \n",
    "            # if father_to_process == \"2_frame_cnpj_prestador\":\n",
    "            elif label == '2_frame_inscricao_prestador':\n",
    "                coordinates = get_coordinates_filter(model_map, tipo, label, section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                extract_text = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                \n",
    "                text_splited = texto_extraido_nf(extract_text)\n",
    "                \n",
    "                \n",
    "                # Extrair Inscrição Estadual\n",
    "                inscricao_estadual_match = re.search(r'Inscrição Estadual:\\s+(.+)', extract_text)\n",
    "                if inscricao_estadual_match:\n",
    "                    inscricao_estadual_str = inscricao_estadual_match.group(1)\n",
    "                    if inscricao_estadual_str == 'Nome/Razão Social:':\n",
    "                        nf_inscr_estadual = 'None'\n",
    "                        nf_dados_prestador['inscricao_estadual'] = \"NONE\"\n",
    "                    else:\n",
    "                        nf_inscr_estadual = inscricao_estadual_match.group(1)\n",
    "                        nf_dados_prestador['inscricao_estadual'] = inscricao_estadual_match.group(1) \n",
    "                \n",
    "\n",
    "            elif label == '2_frame_dados_prestador':\n",
    "                dados_prestador = {}\n",
    "                coordinates = get_coordinates_filter(model_map, tipo, label, section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                \n",
    "                extract_text = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                dados_prestador = extract_fields_prestador(extract_text)\n",
    "                \n",
    "                text_splited = texto_extraido_nf(extract_text)\n",
    "                keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "                string_pesquisa = \"Nome/Razão Social:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            \n",
    "                nf_dados_prestador['razao_social'] = texto\n",
    "\n",
    "                string_pesquisa = \"Nome de Fantasia:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                nf_dados_prestador['nome_fantasia'] = texto\n",
    "                \n",
    "                string_pesquisa = \"Endereço:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                nf_dados_prestador['endereco'] = texto\n",
    "                \n",
    "                string_pesquisa = \"E-mail:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                nf_dados_prestador['email'] = texto\n",
    "                \n",
    "    return nf_dados_prestador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrai_cnpj_R_PDF(texto_extraido, original_file_name):\n",
    "    \n",
    "    nf_values_prestador = {}\n",
    "    message_erro = []\n",
    "    try:\n",
    "        keyword_list = ['CPF/CNPJ:', 'Telefone:']\n",
    "        string_pesquisa = \"CPF/CNPJ:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', texto)\n",
    "        cpf_cnpj_formatado_match\n",
    "        if cpf_cnpj_formatado_match:\n",
    "            nf_values_prestador['p_cpf_cnpj_com_mascara']= cpf_cnpj_formatado_match.group(1)\n",
    "            nf_values_prestador['p_cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "        print(cpf_cnpj_formatado, cpf_cnpj_sem_mascara) \n",
    "    \n",
    "        string_pesquisa = \"Telefone:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', texto)\n",
    "        if telefone_match: \n",
    "            telefone_str = telefone_match.group(1)\n",
    "            # Remover quebras de linha\n",
    "            telefone_str = telefone_str.replace('.', '')\n",
    "            telefone_str = telefone_str.replace('\\n', '')\n",
    "            \n",
    "        else:\n",
    "            telefone_str = 'None'\n",
    "            \n",
    "        nf_values_prestador['p_telefone'] = telefone_str    \n",
    "            \n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} | {e}\")\n",
    "        # nf_values_prestador['cpf_cnpj_com_mascara'] = 'None'\n",
    "        # nf_values_prestador['cpf_cnpj_sem_mascara'] = 'None'\n",
    "        # nf_values_prestador['telefone'] = 'None'\n",
    "        # message_erro.append(msg)\n",
    "    \n",
    "    print(f'telefone_str: {telefone_str} | cpf_cnpj_formatado: {cpf_cnpj_formatado} | cpf_cnpj_sem_mascara: {cpf_cnpj_sem_mascara}')\n",
    "    \n",
    "    return nf_values_prestador  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. CNAE e Item da Lista de Serviços\n",
    "def extrai_consiste_cnae_PDF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0_cnae, f_1_cnae, f_0_it, f_1_it, original_file_name, file_path):\n",
    "\n",
    "    nf_data_CNAE = {}\n",
    "    cnae_value = None\n",
    "    item_servico_value = None\n",
    "    \n",
    "    nf_data_CNAE['Secao'] = \"6. CNAE e Item da Lista de Serviços\"\n",
    "\n",
    "    process = [\"cnae\", 'item_lista_servicos']\n",
    "\n",
    "    section = \"6. CNAE e Item da Lista de Serviços\"\n",
    "    pdf_pesquisavel_map = True\n",
    "        \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "        \n",
    "    tipo = \"sframe_field\"\n",
    "    message_erro = []\n",
    "    nf_dados_prestador = {}\n",
    "    # model = row['model']\n",
    "    for father in process:\n",
    "        label = father\n",
    "        if label == \"cnae\": \n",
    "            tipo = \"sframe_field\"\n",
    "            #print(model_map)\n",
    "            coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            y0 = y0 * f_0_cnae\n",
    "            y1 = y1 * f_1_cnae\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            text_extraido_cnae = text\n",
    "            #print(f'model: {model_map} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6}\\n\\n {text}')\n",
    "            nf_data_CNAE_match = re.search(r'CNAE\\s+(.+)', text)\n",
    "            if nf_data_CNAE_match:\n",
    "                # Remove a primeira ocorrência de \"CNAE:\"\n",
    "                nf_data_CNAE_str = re.sub(r'^CNAE - ', '', text, count=1)\n",
    "                # Remover quebras de linha\n",
    "                nf_data_CNAE_str = nf_data_CNAE_str.replace('\\n', ' ')\n",
    "                \n",
    "                cnae_number = int(extract_number(nf_data_CNAE_str))\n",
    "                cnae_value = cnae_dict.get((de_para_pm, cnae_number),(0))\n",
    "                if cnae_value != 0:\n",
    "                    cnae_value = cnae_value.upper()\n",
    "                    cnae_value = str(cnae_number) + \" - \" + cnae_value\n",
    "                    #print(f' CNAE: {cnae_value} - Prefeitura: {de_para_pm}')\n",
    "            \n",
    "        \n",
    "        elif label == \"item_lista_servicos\":\n",
    "            tipo = \"sframe_field\"\n",
    "            #print(model_map)\n",
    "\n",
    "            coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            print(f'1. label: {label} | coordenadas originais: x0:{x0} y0:{y0} x1:{x1} y1:{y1} item: {original_file_name}')\n",
    "            \n",
    "            y0 = y0 * f_0_it\n",
    "            y1 = y1 * f_1_it\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            \n",
    "            print(f'2. coordenadas ajustadas: x0:{x0} y0:{y0} x1:{x1} y1:{y1} item: {original_file_name} - text:\\n{text}\\n\\n')\n",
    "            \n",
    "            #print(f'{text} - {label} x0:{x0} y0:{y0} x1:{x1} y1:{y1}')\n",
    "            text_extraido_itens = text\n",
    "            #print(f'model: {model_map} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6}\\n\\n {text}')\n",
    "            nf_item_lista_servicos_match = re.search(r'Item da Lista de Serviços\\s+(.+)', text)\n",
    "            print(f'1: nf_item_lista_servicos_match: {nf_item_lista_servicos_match}\\n')\n",
    "            if nf_item_lista_servicos_match:\n",
    "                nf_item_lista_servicos_str = re.sub(r'^Item da Lista de Serviços - ', '', text, count=1)\n",
    "                print(f'2: nf_item_lista_servicos_str: {nf_item_lista_servicos_str}\\n') \n",
    "                item_servico_cod = float(extract_number(nf_item_lista_servicos_str))\n",
    "                print(f'3: item_servico_cod: {item_servico_cod}\\n') \n",
    "                item_servico_value, cnae_associado = item_servico_dict.get((de_para_pm, item_servico_cod), (\"Valor não encontrado\"))\n",
    "                print(f'4: item_servico_value: {item_servico_value} | cnae_associado: {cnae_associado} \\n') \n",
    "                item_servico_value = item_servico_value.upper()\n",
    "                item_servico_value = str(item_servico_cod) + \" - \" + item_servico_value\n",
    "\n",
    "    pdf_document.close()\n",
    "\n",
    "    return cnae_value, item_servico_value    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6C Processo de tratamento do CNAE anterior e sem pesquisa no dicionario\n",
    "def processa_cnae_outros(text):\n",
    "    \n",
    "    nf_data_CNAE_match = re.search(r'CNAE\\s+(.+)', text)\n",
    "    if nf_data_CNAE_match:\n",
    "        try:\n",
    "            # Remove a primeira ocorrência de \"CNAE:\"\n",
    "            nf_data_CNAE_str = re.sub(r'^CNAE - ', '', text, count=1)\n",
    "            # Remover quebras de linha\n",
    "            nf_data_CNAE_str = nf_data_CNAE_str.replace('\\n', ' ')\n",
    "            return nf_data_CNAE_str \n",
    "        except Exception as e:\n",
    "            print(f\"Erro busca cnae: {e}\") \n",
    "        \n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Dados do Prestador de Servico\n",
    "def processar_dados_prestador(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    \n",
    "    nf_dados_prestador = {}\n",
    "    prestador_list = []\n",
    "    f_tipo = 'frame'\n",
    "    message_erro = []\n",
    "    # if pdf_pesquisavel_map:\n",
    "    \n",
    "    nf_dados_prestador['pdf_pesquisavel'] = pdf_pesquisavel_map\n",
    "    nf_dados_prestador['original_file_name'] = original_file_name\n",
    "    \n",
    "    data_box_valores = {'secao': section, 'erros': []}\n",
    "   \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['section_json'] == section) & (frames_nf_v4_df['type'] == f_tipo)]\n",
    "    \n",
    "    if pdf_pesquisavel_map:\n",
    "        pdf_document = fitz.open(file_path)\n",
    "        page_number = 0\n",
    "        page = pdf_document[page_number]\n",
    "    else:\n",
    "        image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        frame_id = row_frame['id']\n",
    "        label = row_frame['label']\n",
    "        \n",
    "        x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "        \n",
    "        #print(f'doc {\"pdf_pesquisavel_map\" if pdf_pesquisavel_map else \"raster_pdf\"}: {label:>30} | id: {frame_id:>3} | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6} ')\n",
    "        \n",
    "        if pdf_pesquisavel_map:\n",
    "            texto_extraido = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            #data_box_valores['processo'] = \"PDF_PESQUISAVEL\"\n",
    "            #texto_to_process = texto_pdf\n",
    "        else:\n",
    "            texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "            \n",
    "            if label == '2_frame_cnpj_prestador':\n",
    "                # Extrair CPF/CNPJ com máscara 1\n",
    "                try:\n",
    "                    \n",
    "                    if \"CPF/CNPJ:\" in texto_extraido:\n",
    "                        print(f'doc: {texto_extraido} pdf_pesqui: {pdf_pesquisavel_map} | {original_file_name}')\n",
    "                        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', texto_extraido)\n",
    "                        if cpf_cnpj_formatado_match:\n",
    "                            nf_dados_prestador['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                            nf_dados_prestador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "                    else:\n",
    "                        nf_dados_prestador['cpf_cnpj_com_mascara'] = 'None'\n",
    "                        nf_dados_prestador['cpf_cnpj_sem_mascara'] = 'None'           \n",
    "                            \n",
    "                    telefone_str = None\n",
    "                    #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "                    telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', texto_extraido)\n",
    "                    if telefone_match: \n",
    "                        telefone_str = telefone_match.group(1)\n",
    "                        # Remover quebras de linha\n",
    "                        telefone_str = telefone_str.replace('.', '')\n",
    "                        telefone_str = telefone_str.replace('\\n', '')\n",
    "                                \n",
    "                        nf_dados_prestador['telefone'] = telefone_str\n",
    "                    else:\n",
    "                        nf_dados_prestador['telefone'] = 'None'\n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {original_file_name} | {e}\")\n",
    "                    nf_dados_prestador['cpf_cnpj_com_mascara'] = 'None'\n",
    "                    nf_dados_prestador['cpf_cnpj_sem_mascara'] = 'None'\n",
    "                    nf_dados_prestador['telefone'] = 'None'\n",
    "                    message_erro.append(msg)       \n",
    "            \n",
    "            elif label == '2_frame_inscricao_prestador':\n",
    "                #print(f'extract_fields_inscricao  {texto_extraido}')\n",
    "                try:\n",
    "                    print(f'doc: {texto_extraido} pdf_pesqui: {pdf_pesquisavel_map} | {original_file_name}')\n",
    "                    text_splited = texto_extraido(texto_extraido)\n",
    "                    keyword_list = ['Inscrição Municipal:', 'Inscrição Estadual:']\n",
    "                    string_pesquisa = \"Inscrição Municipal:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['inscricao_municipal'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Inscrição Estadual:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['inscricao_estadual'] = texto\n",
    "                \n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {original_file_name} | {e}\")\n",
    "                    nf_dados_prestador['inscricao_municipal'] = \" \"\n",
    "                    nf_dados_prestador['inscricao_estadual'] = \" \"\n",
    "                    message_erro.append(msg)\n",
    "            \n",
    "            elif label == '2_frame_dados_prestador':\n",
    "                # print(f' extract_fields_dados {texto_extraido}')\n",
    "                try:\n",
    "                    print(f'doc: {texto_extraido} pdf_pesqui: {pdf_pesquisavel_map} | {original_file_name}')\n",
    "                    text_splited = texto_extraido(texto_extraido)\n",
    "                    keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "                    string_pesquisa = \"Nome/Razão Social:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['razao_social'] = texto\n",
    "\n",
    "                    string_pesquisa = \"Nome de Fantasia:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['nome_fantasia'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Endereço:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['endereco'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"E-mail:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['email'] = texto\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {original_file_name} | {e}\")\n",
    "                    nf_dados_prestador['razao_social'] = \" \"\n",
    "                    nf_dados_prestador['nome_fantasia'] = \" \"\n",
    "                    nf_dados_prestador['endereco'] = \" \"\n",
    "                    nf_dados_prestador['email'] = \" \"\n",
    "                    message_erro.append(msg)\n",
    "        prestador_list.append(nf_dados_prestador)\n",
    "                \n",
    "    if pdf_pesquisavel_map:\n",
    "        pdf_document.close()\n",
    "    \n",
    "    print(nf_dados_prestador)    \n",
    "        \n",
    "    return prestador_list, message_erro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_prestador_raster_pdf(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):    \n",
    "    \n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = False\n",
    "    \n",
    "    tipo= \"frame\"\n",
    "    message_erro = []\n",
    "    nf_dados_prestador = {}\n",
    "    \n",
    "    process_prestador = ['2_frame_cnpj_prestador', '2_frame_inscricao_prestador', '2_frame_dados_prestador']\n",
    "\n",
    "    image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "    \n",
    "    for father in process_prestador:\n",
    "        father_to_process = father\n",
    "        if father_to_process == \"2_frame_cnpj_prestador\":\n",
    "            \n",
    "            \n",
    "            x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "            texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "            \n",
    "            data_prestador = {}\n",
    "            f_father = father_to_process\n",
    "            section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "            \n",
    "            filtered_frame_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['label'] == father_to_process) & (frames_nf_v4_df['type'] == tipo)]\n",
    "            \n",
    "            Texto_extraido = extract_dados_from_frame(model, f_father, section)\n",
    "            print(Texto_extraido)\n",
    "        \n",
    "        \n",
    "        elif frame_pesquisa == \"2_frame_cnpj_prestador\":\n",
    "        data_prestador = {}\n",
    "        f_father = \"2_frame_cnpj_prestador\"\n",
    "        section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "        Texto_extraido = extract_dados_from_frame(model, f_father, section)\n",
    "        print(Texto_extraido)\n",
    "        \n",
    "        print(father_to_process)\n",
    "        for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "            label_value = row_frame['label']\n",
    "            reference_value = row_frame['reference']\n",
    "            f_type = row_frame['type']\n",
    "            frame_id = row_frame['id']\n",
    "            \n",
    "            if label_value == '2_frame_cnpj_prestador':\n",
    "                \n",
    "                x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "                texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                \n",
    "                print(f'label: {label_value:>10}  {x0}, {y0}, {x1}, {y1} | ')\n",
    "                # Extrair CPF/CNPJ com máscara 1\n",
    "                try:\n",
    "                    if \"CPF/CNPJ:\" in texto_extraido:\n",
    "                        print(f'doc: {texto_extraido} pdf_pesqui: {pdf_pesquisavel_map} | {original_file_name}')\n",
    "                        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', texto_extraido)\n",
    "                        if cpf_cnpj_formatado_match:\n",
    "                            nf_dados_prestador['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                            nf_dados_prestador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "                    else:\n",
    "                        nf_dados_prestador['cpf_cnpj_com_mascara'] = 'None'\n",
    "                        nf_dados_prestador['cpf_cnpj_sem_mascara'] = 'None'           \n",
    "                            \n",
    "                    telefone_str = None\n",
    "                    #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "                    telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', texto_extraido)\n",
    "                    if telefone_match: \n",
    "                        telefone_str = telefone_match.group(1)\n",
    "                        # Remover quebras de linha\n",
    "                        telefone_str = telefone_str.replace('.', '')\n",
    "                        telefone_str = telefone_str.replace('\\n', '')\n",
    "                                \n",
    "                        nf_dados_prestador['telefone'] = telefone_str\n",
    "                    else:\n",
    "                        nf_dados_prestador['telefone'] = 'None'\n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {original_file_name} | {e}\")\n",
    "                    nf_dados_prestador['cpf_cnpj_com_mascara'] = 'None'\n",
    "                    nf_dados_prestador['cpf_cnpj_sem_mascara'] = 'None'\n",
    "                    nf_dados_prestador['telefone'] = 'None'\n",
    "                    message_erro.append(msg) \n",
    "                     \n",
    "            elif label_value == '2_frame_inscricao_prestador':\n",
    "                \n",
    "                x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "                texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                \n",
    "                #print(f'extract_fields_inscricao  {texto_extraido}')\n",
    "                try:\n",
    "                    print(f'doc: {texto_extraido} pdf_pesqui: {pdf_pesquisavel_map} | {original_file_name}')\n",
    "                    text_splited = texto_extraido(texto_extraido)\n",
    "                    keyword_list = ['Inscrição Municipal:', 'Inscrição Estadual:']\n",
    "                    string_pesquisa = \"Inscrição Municipal:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['inscricao_municipal'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Inscrição Estadual:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['inscricao_estadual'] = texto\n",
    "                \n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {original_file_name} | {e}\")\n",
    "                    nf_dados_prestador['inscricao_municipal'] = \" \"\n",
    "                    nf_dados_prestador['inscricao_estadual'] = \" \"\n",
    "                    message_erro.append(msg)\n",
    "            \n",
    "            \n",
    "            elif label_value == '2_frame_dados_prestador':\n",
    "                # print(f' extract_fields_dados {texto_extraido}')\n",
    "                x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "                texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                \n",
    "                try:\n",
    "                    print(f'doc: {texto_extraido} pdf_pesqui: {pdf_pesquisavel_map} | {original_file_name}')\n",
    "                    text_splited = texto_extraido(texto_extraido)\n",
    "                    keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "                    string_pesquisa = \"Nome/Razão Social:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['razao_social'] = texto\n",
    "\n",
    "                    string_pesquisa = \"Nome de Fantasia:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['nome_fantasia'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Endereço:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['endereco'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"E-mail:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['email'] = texto\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {original_file_name} | {e}\")\n",
    "                    nf_dados_prestador['razao_social'] = \" \"\n",
    "                    nf_dados_prestador['nome_fantasia'] = \" \"\n",
    "                    nf_dados_prestador['endereco'] = \" \"\n",
    "                    nf_dados_prestador['email'] = \" \"\n",
    "                    message_erro.append(msg)\n",
    "        #prestador_list.append(nf_dados_prestador)\n",
    "                \n",
    "\n",
    " \n",
    "        \n",
    "    return nf_dados_prestador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_prestador_raster_pdf(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = False\n",
    "    \n",
    "    tipo= \"frame\"\n",
    "    message_erro = []\n",
    "    nf_dados_prestador = {}\n",
    "    \n",
    "    process_prestador = ['2_frame_cnpj_prestador', '2_frame_inscricao_prestador', '2_frame_dados_prestador']\n",
    "\n",
    "    image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "    \n",
    "    for father in process_prestador:\n",
    "        father_to_process = father\n",
    "        \n",
    "        filtered_frame_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['label'] == father_to_process) & (frames_nf_v4_df['type'] == tipo)]\n",
    "        print(father_to_process)\n",
    "        for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "            label_value = row_frame['label']\n",
    "            reference_value = row_frame['reference']\n",
    "            f_type = row_frame['type']\n",
    "            frame_id = row_frame['id']\n",
    "            \n",
    "            if label_value == '2_frame_cnpj_prestador':\n",
    "                \n",
    "                x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "                texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                \n",
    "                print(f'label: {label_value:>10}  {x0}, {y0}, {x1}, {y1} | ')\n",
    "                # Extrair CPF/CNPJ com máscara 1\n",
    "                try:\n",
    "                    if \"CPF/CNPJ:\" in texto_extraido:\n",
    "                        print(f'doc: {texto_extraido} pdf_pesqui: {pdf_pesquisavel_map} | {original_file_name}')\n",
    "                        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', texto_extraido)\n",
    "                        if cpf_cnpj_formatado_match:\n",
    "                            nf_dados_prestador['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                            nf_dados_prestador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "                    else:\n",
    "                        nf_dados_prestador['cpf_cnpj_com_mascara'] = 'None'\n",
    "                        nf_dados_prestador['cpf_cnpj_sem_mascara'] = 'None'           \n",
    "                            \n",
    "                    telefone_str = None\n",
    "                    #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "                    telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', texto_extraido)\n",
    "                    if telefone_match: \n",
    "                        telefone_str = telefone_match.group(1)\n",
    "                        # Remover quebras de linha\n",
    "                        telefone_str = telefone_str.replace('.', '')\n",
    "                        telefone_str = telefone_str.replace('\\n', '')\n",
    "                                \n",
    "                        nf_dados_prestador['telefone'] = telefone_str\n",
    "                    else:\n",
    "                        nf_dados_prestador['telefone'] = 'None'\n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {original_file_name} | {e}\")\n",
    "                    nf_dados_prestador['cpf_cnpj_com_mascara'] = 'None'\n",
    "                    nf_dados_prestador['cpf_cnpj_sem_mascara'] = 'None'\n",
    "                    nf_dados_prestador['telefone'] = 'None'\n",
    "                    message_erro.append(msg) \n",
    "                     \n",
    "            elif label_value == '2_frame_inscricao_prestador':\n",
    "                \n",
    "                x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "                texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                \n",
    "                #print(f'extract_fields_inscricao  {texto_extraido}')\n",
    "                try:\n",
    "                    print(f'doc: {texto_extraido} pdf_pesqui: {pdf_pesquisavel_map} | {original_file_name}')\n",
    "                    text_splited = texto_extraido(texto_extraido)\n",
    "                    keyword_list = ['Inscrição Municipal:', 'Inscrição Estadual:']\n",
    "                    string_pesquisa = \"Inscrição Municipal:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['inscricao_municipal'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Inscrição Estadual:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['inscricao_estadual'] = texto\n",
    "                \n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {original_file_name} | {e}\")\n",
    "                    nf_dados_prestador['inscricao_municipal'] = \" \"\n",
    "                    nf_dados_prestador['inscricao_estadual'] = \" \"\n",
    "                    message_erro.append(msg)\n",
    "            \n",
    "            \n",
    "            elif label_value == '2_frame_dados_prestador':\n",
    "                # print(f' extract_fields_dados {texto_extraido}')\n",
    "                x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "                texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "                \n",
    "                try:\n",
    "                    print(f'doc: {texto_extraido} pdf_pesqui: {pdf_pesquisavel_map} | {original_file_name}')\n",
    "                    text_splited = texto_extraido(texto_extraido)\n",
    "                    keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "                    string_pesquisa = \"Nome/Razão Social:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['razao_social'] = texto\n",
    "\n",
    "                    string_pesquisa = \"Nome de Fantasia:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['nome_fantasia'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"Endereço:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['endereco'] = texto\n",
    "                    \n",
    "                    string_pesquisa = \"E-mail:\"\n",
    "                    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                    nf_dados_prestador['email'] = texto\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    msg = (f\"doc: {original_file_name} | {e}\")\n",
    "                    nf_dados_prestador['razao_social'] = \" \"\n",
    "                    nf_dados_prestador['nome_fantasia'] = \" \"\n",
    "                    nf_dados_prestador['endereco'] = \" \"\n",
    "                    nf_dados_prestador['email'] = \" \"\n",
    "                    message_erro.append(msg)\n",
    "        #prestador_list.append(nf_dados_prestador)\n",
    "                \n",
    "\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Dados do Prestador de Servico\n",
    "def processar_dados_prestador_raster_pdf(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    \n",
    "    \n",
    "    prestador_list = []\n",
    "    f_tipo = 'frame'\n",
    "    message_erro = []\n",
    "    # if pdf_pesquisavel_map:\n",
    "    data_box_valores = {'secao': section, 'erros': []}\n",
    "   \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['section_json'] == section) & (frames_nf_v4_df['type'] == f_tipo)]\n",
    "    \n",
    "    \n",
    "        \n",
    "  \n",
    "     \n",
    "    else:\n",
    "        image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        frame_id = row_frame['id']\n",
    "        label = row_frame['label']\n",
    "        \n",
    "        x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "        \n",
    "        #print(f'doc {\"pdf_pesquisavel_map\" if pdf_pesquisavel_map else \"raster_pdf\"}: {label:>30} | id: {frame_id:>3} | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6} ')\n",
    "        \n",
    "        if pdf_pesquisavel_map:\n",
    "            texto_extraido = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            #data_box_valores['processo'] = \"PDF_PESQUISAVEL\"\n",
    "            #texto_to_process = texto_pdf\n",
    "        else:\n",
    "            texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "            \n",
    "    return texto_extraido "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields_cnpj(texto_extraido, original_file_name): \n",
    "  \n",
    "    message_erro = []\n",
    "    \n",
    "    data_nf_cnpj_prestador = {}\n",
    "      \n",
    "    print(f'2 extract_fields_cnpj:  {texto_extraido}')\n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in texto_extraido:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', texto_extraido)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "            data_nf_cnpj_prestador['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "            data_nf_cnpj_prestador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "    else:\n",
    "        data_nf_cnpj_prestador['cpf_cnpj_com_mascara'] = None\n",
    "        data_nf_cnpj_prestador['cpf_cnpj_sem_mascara'] = None            \n",
    "            \n",
    "            \n",
    "    telefone_str = None\n",
    "\n",
    "    #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "    telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', texto_extraido)\n",
    "    if telefone_match: \n",
    "        telefone_str = telefone_match.group(1)\n",
    "        # Remover quebras de linha\n",
    "        telefone_str = telefone_str.replace('.', '')\n",
    "        telefone_str = telefone_str.replace('\\n', '')\n",
    "                \n",
    "        data_nf_cnpj_prestador['telefone'] = telefone_str\n",
    "    else:\n",
    "        data_nf_cnpj_prestador['telefone'] = None \n",
    "    \n",
    "    #print(data_nf_cnpj_prestador)    \n",
    "\n",
    "             \n",
    "    return data_nf_cnpj_prestador\n",
    "\n",
    "\n",
    "def extract_fields_inscricao(texto_extraido, original_file_name):\n",
    "    \n",
    "    data_nf_incricao_prestador = {} \n",
    "    message_erro = []\n",
    "    \n",
    "    print(f'extract_fields_inscricao  {texto_extraido}')\n",
    "    try:\n",
    "        text_splited = texto_extraido(texto_extraido)\n",
    "        keyword_list = ['Inscrição Municipal:', 'Inscrição Estadual:']\n",
    "        string_pesquisa = \"Inscrição Municipal:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        data_nf_incricao_prestador['inscricao_municipal'] = texto\n",
    "        \n",
    "        string_pesquisa = \"Inscrição Estadual:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        data_nf_incricao_prestador['inscricao_estadual'] = texto\n",
    "      \n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} | {e}\")\n",
    "        data_nf_incricao_prestador['inscricao_municipal'] = \" \"\n",
    "        data_nf_incricao_prestador['inscricao_estadual'] = \" \"\n",
    "        message_erro.append(msg)\n",
    "        \n",
    "    #print(data_nf_incricao_prestador)   \n",
    "    \n",
    "    return data_nf_incricao_prestador\n",
    "\n",
    "\n",
    "def extract_fields_dados(texto_extraido, original_file_name):\n",
    "    \n",
    "    data_nf_dados_prestador = {}\n",
    "    message_erro = []\n",
    "    print(f' extract_fields_dados {texto_extraido}')\n",
    "    try:\n",
    "\n",
    "        keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "        \n",
    "        string_pesquisa = \"Nome/Razão Social:\"\n",
    "        text_splited = texto_extraido(texto_extraido)\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        data_nf_dados_prestador['razao_social'] = texto\n",
    "\n",
    "        string_pesquisa = \"Nome de Fantasia:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        data_nf_dados_prestador['nome_fantasia'] = texto\n",
    "        \n",
    "        string_pesquisa = \"Endereço:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        data_nf_dados_prestador['endereco'] = texto\n",
    "        \n",
    "        string_pesquisa = \"E-mail:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        data_nf_dados_prestador['email'] = texto\n",
    "        \n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} | {e}\")\n",
    "        data_nf_dados_prestador['razao_social'] = \" \"\n",
    "        data_nf_dados_prestador['nome_fantasia'] = \" \"\n",
    "        data_nf_dados_prestador['endereco'] = \" \"\n",
    "        data_nf_dados_prestador['email'] = \" \"\n",
    "        message_erro.append(msg)\n",
    "        \n",
    "    #print(data_nf_dados_prestador)\n",
    "    \n",
    "    return data_nf_dados_prestador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. XXX Pesquisa prefeitura no documento (dando as coordenadas) e efetuando o OCR\n",
    "def pequisaTextoDoc(image_name):\n",
    "\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = 0\n",
    "    y0 = 0\n",
    "    x1= 2066\n",
    "    y1 = 2922\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "    \n",
    "    return extracted_text_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. XXX Analisa nro de paginas\n",
    "def analisa_nro_pages(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    pages = pdf_document.pages() # generator object\n",
    "\n",
    "    page_nro = []\n",
    "    for page in pages:\n",
    "        page_nro.append(page)\n",
    "        \n",
    "    nro_paginas = len(page_nro)    \n",
    "    if nro_paginas > 1:\n",
    "        doc_1_page = False\n",
    "        return doc_1_page, nro_paginas    \n",
    "    else:\n",
    "        doc_1_page = True\n",
    "        return doc_1_page, nro_paginas  \n",
    "    pdf_document.close()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# XXX FUNCAO DE SPLIT\n",
    "def split_documentos(qualquer_df, outro_tgt_df, num_linhas_df, fase, atividade, status):\n",
    "    \n",
    "    documentos_splitados = []\n",
    "    doc_info = {}\n",
    "    rows_list = []\n",
    "    documentos = []\n",
    "    #output_dir = os.path.join(documentos_scan_path, batch_name)\n",
    "\n",
    "    fase_processo = fase\n",
    "    atividade_processo = atividade\n",
    "    status_documento = status\n",
    "    \n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    \n",
    "    i = num_linhas_df + 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        message_erro = []\n",
    "        batch_name = row['batch']\n",
    "        original_file_name = row['original_file_name']\n",
    "        folder_name = row['directory']\n",
    "        file_path = row['file_path']\n",
    "        level = row['level']\n",
    "        d_type = row['type']\n",
    "        document_type = row['document_type']\n",
    "        # document_unique_id = row['document_unique_id']\n",
    "        new_level = level + 1\n",
    "        try:\n",
    "            pdf = fitz.open(file_path)\n",
    "            # Número total de páginas no PDF\n",
    "            total_pages = len(pdf)\n",
    "        except Exception as e:\n",
    "            print(f\"Nao congui abrir o PDF: {e}\")    \n",
    "\n",
    "        # Nome base para os arquivos de saída\n",
    "        base_name = file_path.split('.')[0]  # Remove a extensão do arquivo\n",
    "        \n",
    "        file_to_delete = file_path\n",
    "\n",
    "        # Loop para criar um novo PDF para cada página\n",
    "        for page_num in range(total_pages):\n",
    "            # Cria um novo objeto PDF\n",
    "            new_pdf = fitz.open()\n",
    "            # Adiciona a página atual ao novo PDF\n",
    "            new_pdf.insert_pdf(pdf, from_page=page_num, to_page=page_num)\n",
    "            # Nome do novo arquivo PDF\n",
    "            new_pdf_name = f\"{base_name}_page_{page_num + 1}.pdf\"\n",
    "            # Salva o novo PDF\n",
    "            new_pdf.save(new_pdf_name)\n",
    "            # Fecha o novo PDF\n",
    "            new_pdf.close()\n",
    "            status_documento = 'splitado'\n",
    "            name_pdf_splited = os.path.basename(new_pdf_name)\n",
    "            \n",
    "            nova_linha = {\n",
    "                'seq': i,\n",
    "                'date_time': time_now,\n",
    "                'batch': batch_name,\n",
    "                'fase_processo': fase_processo,\n",
    "                'nome_atividade': atividade_processo,\n",
    "                'status_documento': status_documento,\n",
    "                'document_unique_id': generate_unique_id(),\n",
    "                'original_file_name': name_pdf_splited,\n",
    "                'directory': folder_name,\n",
    "                'one_page': True,\n",
    "                'pages': 1,                    \n",
    "                'level': new_level,\n",
    "                'type': d_type,\n",
    "                'document_type': document_type,\n",
    "                'parent_document_unique_id': idx,\n",
    "                'file_path': new_pdf_name,\n",
    "                'file_hash': generate_file_hash(new_pdf_name),\n",
    "            }\n",
    "            rows_list.append(nova_linha)\n",
    "            i += 1\n",
    "        outro_tgt_df.loc[idx, 'status_documento'] = \"NAO_PROCESSAR\"    \n",
    "            \n",
    "\n",
    "    total_split = i - 1\n",
    "                      \n",
    "      \n",
    "    df_split = pd.DataFrame(rows_list)\n",
    "    \n",
    "    return df_split\n",
    "\n",
    "\n",
    "# 1.XXX  Acao 1 - Ler todo o pipeline de documentos recebidos\n",
    "def scan_pipeline_documentos(documentos_scan_path, batch_name, fase, atividade, status):\n",
    "\n",
    "    doc_info = {}\n",
    "    resumo = {}\n",
    "    documentos = []\n",
    "    output_dir = os.path.join(documentos_scan_path, batch_name)\n",
    "\n",
    "    fase_processo = fase\n",
    "    atividade_processo = atividade\n",
    "    status_documento = status\n",
    "    \n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    \n",
    "    #print(f'1. Fase atual: {fase_atual} | Nome atividade: {nome_atividade} | Status documento: {status_documento}')\n",
    "    \n",
    "    # Just for DEV\n",
    "    parent_document_unique_id = generated_parent_document_unique_id\n",
    "\n",
    "    i = 1\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        folder_name = os.path.basename(root)\n",
    "        #print(folder_name)\n",
    "        for file in files:\n",
    "            level = 3\n",
    "            file_path = os.path.join(root, file)\n",
    "            new_path_name = os.path.join(output_dir, file)\n",
    "            diretorio = os.path.basename(file_path)\n",
    "            if folder_name == batch_name:\n",
    "                folder_name = \"root_dir\"\n",
    "            doc_one_page, nro_pgs = analisa_nro_pages(file_path)\n",
    "            #print(f'2. doc_one_page: {doc_one_page} | nro_pgs: {nro_pgs}') \n",
    "            type = 'document_file'\n",
    "            document_type = \"provavel_NFSe\"\n",
    "            #print(f'\\nfile: {file} | diretorio: {folder_name}\\n{file_path} ')\n",
    "            if folder_name not in doc_info:\n",
    "                doc_info[folder_name] = {\"data\": []}\n",
    "            doc_info[folder_name][\"data\"].append({\n",
    "                \"seq\": i,\n",
    "                \"date_time\": time_now,\n",
    "                \"batch\": batch_name,\n",
    "                \"fase_processo\": fase_processo,\n",
    "                \"nome_atividade\": atividade_processo,\n",
    "                \"original_file_name\": file,\n",
    "                \"status_documento\": status_documento,\n",
    "                \"one_page\": doc_one_page,\n",
    "                \"pages\": nro_pgs,\n",
    "                \"directory\": folder_name,\n",
    "                \"file_path\": file_path,\n",
    "                \"level\": level,\n",
    "                \"type\": type,\n",
    "                \"document_type\": document_type,\n",
    "                \"document_unique_id\": generate_unique_id(),\n",
    "                \"parent_document_unique_id\": parent_document_unique_id,\n",
    "                \"file_hash\": generate_file_hash(file_path)\n",
    "            })\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    \n",
    "    documentos.append(doc_info)\n",
    "    total_scan = i - 1\n",
    "    #print(\"Documentos scaneados: \", total_scan)\n",
    "    \n",
    "    # Criaçao do df_scan_pipe\n",
    "    document_row = {}\n",
    "    rows_list = []\n",
    "    for documento in documentos:\n",
    "        #print(documento)\n",
    "        for diretorio, dados in documento.items():\n",
    "            #print(f'Diretório: {diretorio}\\n')\n",
    "            for arquivo in dados['data']:\n",
    "                nova_linha = {\n",
    "                    'seq': arquivo['seq'],\n",
    "                    'date_time': arquivo['date_time'],\n",
    "                    'batch': arquivo['batch'],\n",
    "                    'fase_processo': arquivo['fase_processo'],\n",
    "                    'nome_atividade': arquivo['nome_atividade'],\n",
    "                    'status_documento': arquivo['status_documento'],\n",
    "                    'document_unique_id': arquivo[\"document_unique_id\"],\n",
    "                    'original_file_name': arquivo['original_file_name'],\n",
    "                    'directory': arquivo['directory'],\n",
    "                    'one_page': arquivo['one_page'],\n",
    "                    'pages': arquivo['pages'],\n",
    "                    'pdf_pesquisavel': None,\n",
    "                    'prefeitura': None,\n",
    "                    'cnpj': None,\n",
    "                    'modelo': None,\n",
    "                    'level': arquivo['level'],\n",
    "                    'type': arquivo['type'],\n",
    "                    'document_type': arquivo['document_type'],\n",
    "                    'parent_document_unique_id': arquivo['parent_document_unique_id'],\n",
    "                    'file_path': arquivo[\"file_path\"],\n",
    "                    'file_hash': arquivo[\"file_hash\"],\n",
    "                }\n",
    "                rows_list.append(nova_linha)\n",
    "    \n",
    "    df_trans_pipe = pd.DataFrame(rows_list)            \n",
    "    \n",
    "    return df_trans_pipe, documentos\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#generated_parent_document_unique_id = generate_unique_id()  \n",
    "\n",
    "# Processo de deleçao e atualizacao de documentos\n",
    "#e_deleta_peloamor(df_docs_splitados)\n",
    "\n",
    "#me_atualiza_logo_vai_2(novo_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confirma_pdf_pequisavel(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 4\n",
    "    x1 = 600\n",
    "    y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    if text:\n",
    "       page_number = 0\n",
    "       pdf_realmente_pequisavel = True\n",
    "       #print(page_number)\n",
    "    else:\n",
    "       page_number = 1\n",
    "       pdf_realmente_pequisavel = False\n",
    "       #print(page_number)\n",
    "    \n",
    "    try:\n",
    "        page = pdf_document[page_number]\n",
    "        x0 = 0\n",
    "        y0 = 0\n",
    "        x1 = 600\n",
    "        y1 = 110\n",
    "\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        nf_data_cabecalho = Extc.extract_fields_cabecalho(text)\n",
    "        \n",
    "        if nf_data_cabecalho:\n",
    "            pdf_realmente_pequisavel = True\n",
    "        else:\n",
    "            pdf_realmente_pequisavel = False\n",
    "            \n",
    "      \n",
    "        return pdf_realmente_pequisavel, page_number\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao abrir pagina do PDF: {e}\")\n",
    "        return pdf_realmente_pequisavel, page_number\n",
    "    pdf_document.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_images(imagens_list):\n",
    "    for image in imagens_list:\n",
    "        try:\n",
    "            os.remove(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Nao congui deletar: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demais funcoes para guarda e analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pesquisa_prefeitura_pdf_pesquisavel(file_path):    \n",
    "    \n",
    "    # Carregar o arquivo PDF\n",
    "    pdf_document = fitz.open(file_path)\n",
    "\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 4\n",
    "    x1 = 600\n",
    "    y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    \n",
    "    #print(text)\n",
    "    \n",
    "    if text:\n",
    "       page_number = 0\n",
    "       #print(page_number)\n",
    "    else:\n",
    "       page_number = 1\n",
    "       #print(page_number)\n",
    "    \n",
    "    pdf_document.close()\n",
    "   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confirma_pdf_pequisavel3(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 4\n",
    "    x1 = 600\n",
    "    y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    if text:\n",
    "       page_number = 0\n",
    "       pdf_realmente_pequisavel = True\n",
    "       #print(page_number)\n",
    "    else:\n",
    "       page_number = 1\n",
    "       pdf_realmente_pequisavel = False\n",
    "       #print(page_number)\n",
    "    \n",
    "    try:\n",
    "        page = pdf_document[page_number]\n",
    "        x0 = 0\n",
    "        y0 = 0\n",
    "        x1 = 600\n",
    "        y1 = 110\n",
    "\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        nf_data_cabecalho = Extc.extract_fields_cabecalho(text)\n",
    "        \n",
    "        if nf_data_cabecalho:\n",
    "            pdf_realmente_pequisavel = True\n",
    "        else:\n",
    "            pdf_realmente_pequisavel = False\n",
    "            \n",
    "      \n",
    "        return pdf_realmente_pequisavel, page_number\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao abrir pagina do PDF: {e}\")\n",
    "        return pdf_realmente_pequisavel, page_number\n",
    "    pdf_document.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao deletar documentos pelo DF\n",
    "def me_deleta_peloamor(df_deleta):\n",
    "    \n",
    "    for idx, row in df_deleta.iterrows():\n",
    "        file_delete_path = row['file_path']\n",
    "        try:\n",
    "            os.remove(file_delete_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Nao congui deletar: {e}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao deletar documentos pelo DF\n",
    "def me_atualiza_logo_vai_2(qualquer_df):\n",
    "    \n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        file_delete_path = row['file_path']\n",
    "        try:\n",
    "            df_temp_pipe.loc[idx, 'status_documento'] = \"NAO PROCESSAR\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Nao congui atualizar: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertResize(doc2convert, document_path, image_resized_path):\n",
    "    \n",
    "    \n",
    "    name_image = conv_filename_no_ext(doc2convert)\n",
    "    \n",
    "    # # 1. remocao do sufixo .pdf\n",
    "    # if doc2convert.split(\".\")[1].islower():\n",
    "    #     nameImage= doc2convert.removesuffix(\".pdf\")\n",
    "    # else:\n",
    "    #     nameImage= doc2convert.removesuffix(\".PDF\")\n",
    "        \n",
    "    # print(f'nameImage: {nameImage}\\n')    \n",
    "    \n",
    "    # 2. construo um novo nome para o documento imagem\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(name_image)}.jpg')\n",
    "    #print(f'image_resized_name: {image_resized_name}\\n')\n",
    "    # 3. Conversao para imagem\n",
    "    pages = convert_from_path(document_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((2067, 2923))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "        image_2work = resized_pages[0]\n",
    "        \n",
    "    return image_2work, image_resized_name\n",
    "\n",
    "\n",
    "# 1. converte nome do arquivo\n",
    "def conv_filename(title):\n",
    "    \n",
    "    # Divide o título em nome e extensão\n",
    "    name, extension = title.rsplit('.', 1) if '.' in title else (title, \"\")\n",
    "\n",
    "    # Remove acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substiti espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remove quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converte para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    # Adiciona a extensão de volta, se houver\n",
    "    if extension:\n",
    "        filename += '.' + extension.lower()\n",
    "\n",
    "    return filename\n",
    "\n",
    "def conv_filename_no_ext(title):\n",
    "    \n",
    "    # Divida o título em nome e extensão (mas ignore a extensão)\n",
    "    name = title.rsplit('.', 1)[0] if '.' in title else title\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    return filename "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcoes para validar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pesquisa prefeitura no documento\n",
    "def pequisa_model_raster_pdf(image_name):\n",
    "\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = 406\n",
    "    y0 = 0\n",
    "    x1= 1540\n",
    "    y1 = 380\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "\n",
    "    # 5. Interacao para pesquisar prefeitura\n",
    "    for value in values:\n",
    "        nome_prefeitura_match = re.search(r'PREFEITURA (.+)', value)\n",
    "        if nome_prefeitura_match:\n",
    "            nome_prefeitura = \"PREFEITURA \" + nome_prefeitura_match.group(1) \n",
    "            return  nome_prefeitura  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_cabecalho(row, model_map, original_file_name, file_path, frame_type):\n",
    "    \n",
    "    \n",
    "    section = \"1 - CABECALHO\"\n",
    "    f_frame_label = \"1_frame_dados_nf\"\n",
    "    data_box_valores = {'secao': section}\n",
    "    nf_data_cabecalho = {}\n",
    "    \n",
    "    section = \"1 - CABECALHO\"\n",
    "    f_tipo = 'frame'\n",
    "    f_frame_label = \"1_frame_dados_nf\"\n",
    "    data_extrated_prefeitura = {}\n",
    "    \n",
    "    \n",
    "    1_frame_dados_nf\n",
    "    \n",
    " \n",
    "    message_erro = []\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['section_json'] == section)]\n",
    "    \n",
    "    image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        if row_frame['father'] == \"1_frame_prefeitura_nf\":\n",
    "        \n",
    "        if row_frame['label'] == \"1_frame_dados_nf\":\n",
    "            \n",
    "            \n",
    "            \n",
    "            tipo = \"sframe_field\"\n",
    "            data_extrated_prefeitura = {}\n",
    "   \n",
    "\n",
    "            filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model) & (frames_nf_v4_df['father'] == father) & (frames_nf_v4_df['type'] == tipo)]\n",
    "\n",
    "            for index_sframe, row_sframe in filtered_frames_nf_v4_df.iterrows():\n",
    "                \n",
    "                label_value = row_sframe['label']\n",
    "                \n",
    "                #print(\"label_value\", label_value)\n",
    "                \n",
    "                if label_value == \"nome_prefeitura\":\n",
    "                    reference_value = row_sframe['reference']\n",
    "                    for value in values:\n",
    "                        result = process_line(value, reference_value, label_value)\n",
    "                        if result:\n",
    "                            data_extrated_prefeitura.update(result)\n",
    "                elif label_value == \"secretaria\":\n",
    "                    reference_value = row_sframe['reference']\n",
    "                    for value in values:\n",
    "                        result = process_line(value, reference_value, label_value)\n",
    "                        if result:\n",
    "                            data_extrated_prefeitura.update(result) \n",
    "                elif label_value == \"tipo_nota_fiscal\":\n",
    "                    reference_value = row_sframe['reference']  \n",
    "                    for value in values:\n",
    "                        result = process_line(value, reference_value, label_value)\n",
    "                        if result:\n",
    "                            data_extrated_prefeitura.update(result)\n",
    "                            \n",
    "            return data_extrated_prefeitura\n",
    "            \n",
    "        \n",
    "        frame_id = row_frame['id']\n",
    "            label = row_frame['label']\n",
    "            x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "        \n",
    "        \n",
    "        #print(f'doc {\"pdf_pesquisavel\" if pdf_pesquisavel else \"raster_pdf\"}: {label:>30} | id: {frame_id:>3} | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6} ')\n",
    "      \n",
    "        \n",
    "        text_splited = texto_extraido_cabecalho(texto_extraido)\n",
    "        keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "        string_pesquisa = \"Número da Nota:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "        nro_nota_frame = texto\n",
    "        #print(nro_nota_frame)\n",
    "        nf_data_cabecalho['numero_nota_fiscal'] = nro_nota_frame\n",
    "        \n",
    "        string_pesquisa = \"Competência:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        competencia_frame = texto\n",
    "        nf_data_cabecalho['competencia'] = competencia_frame \n",
    "        #print(competencia_frame)\n",
    "        \n",
    "        string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        dt_hr_emissao_frame = texto\n",
    "        nf_data_cabecalho['dt_hr_emissao'] = dt_hr_emissao_frame \n",
    "        \n",
    "        string_pesquisa = \"Código Verificação:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        codigo_ver_frame = texto\n",
    "        nf_data_cabecalho['codigo_verificacao'] = codigo_ver_frame \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "    # if pdf_pesquisavel:\n",
    "    #     pdf_document.close()\n",
    "        \n",
    "    return nf_data_cabecalho\n",
    "\n",
    "\n",
    "\n",
    "def cabecalho_prefeitura():\n",
    "    valor_dict = {}\n",
    "    dados_prefeitura = {}\n",
    "    f_frame_name = \"1_frame_prefeitura_nf\"\n",
    "    # 1. funçao basica de modelo \n",
    "    texto = executa_model_frame(model, secao, f_frame_name)\n",
    "    text_splited = texto.split('\\n')\n",
    "    \n",
    "    valor_dict = extract_prefeitura(model, f_frame_name, text_splited)\n",
    "    if valor_dict:\n",
    "        dados_prefeitura.update(valor_dict)\n",
    "        \n",
    "        \n",
    "    return dados_prefeitura \n",
    "                \n",
    "def cabecalho_dados():\n",
    "\n",
    "    valor = {}   \n",
    "    f_frame_name = \"1_frame_dados_nf\"\n",
    "    \n",
    "    dadinho_dados_nf = {}\n",
    "    \n",
    "    # 1. funçao basica de modelo \n",
    "    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "    text_splited = texto_extraido(texto)\n",
    "    keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "    string_pesquisa = \"Número da Nota:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "    dadinho_dados_nf['numero_nota_fiscal'] = texto\n",
    "\n",
    "\n",
    "    string_pesquisa = \"Competência:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['competencia'] = texto\n",
    "    \n",
    "    \n",
    "    string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['dt_hr_emissao'] = texto\n",
    "    \n",
    "    \n",
    "    string_pesquisa = \"Código Verificação:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['codigo_verificacao'] = texto\n",
    "    \n",
    "    return dadinho_dados_nf\n",
    "\n",
    "#4. Extrai prefeitura\n",
    "def extract_prefeitura(model, father, values):\n",
    "    \n",
    "    tipo = \"sframe_field\"\n",
    "    data_extrated_prefeitura = {}\n",
    "    #print(tipo)\n",
    "\n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model) & (frames_nf_v4_df['father'] == father) & (frames_nf_v4_df['type'] == tipo)]\n",
    "\n",
    "    for index_sframe, row_sframe in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        label_value = row_sframe['label']\n",
    "        \n",
    "        #print(\"label_value\", label_value)\n",
    "        \n",
    "        if label_value == \"nome_prefeitura\":\n",
    "            reference_value = row_sframe['reference']\n",
    "            for value in values:\n",
    "                result = process_line(value, reference_value, label_value)\n",
    "                if result:\n",
    "                    data_extrated_prefeitura.update(result)\n",
    "        elif label_value == \"secretaria\":\n",
    "            reference_value = row_sframe['reference']\n",
    "            for value in values:\n",
    "                result = process_line(value, reference_value, label_value)\n",
    "                if result:\n",
    "                    data_extrated_prefeitura.update(result) \n",
    "        elif label_value == \"tipo_nota_fiscal\":\n",
    "            reference_value = row_sframe['reference']  \n",
    "            for value in values:\n",
    "                result = process_line(value, reference_value, label_value)\n",
    "                if result:\n",
    "                    data_extrated_prefeitura.update(result)\n",
    "                    \n",
    "    return data_extrated_prefeitura\n",
    "\n",
    "\n",
    "\n",
    "def cabecalho_prefeitura():\n",
    "    valor_dict = {}\n",
    "    dados_prefeitura = {}\n",
    "    f_frame_name = \"1_frame_prefeitura_nf\"\n",
    "    # 1. funçao basica de modelo \n",
    "    texto = executa_model_frame(model, secao, f_frame_name)\n",
    "    text_splited = texto.split('\\n')\n",
    "    \n",
    "    valor_dict = extract_prefeitura(model, f_frame_name, text_splited)\n",
    "    if valor_dict:\n",
    "        dados_prefeitura.update(valor_dict)\n",
    "        \n",
    "        \n",
    "    return dados_prefeitura \n",
    "                \n",
    "def cabecalho_dados():\n",
    "\n",
    "    valor = {}   \n",
    "    f_frame_name = \"1_frame_dados_nf\"\n",
    "    \n",
    "    dadinho_dados_nf = {}\n",
    "    \n",
    "    # 1. funçao basica de modelo \n",
    "    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "    text_splited = texto_extraido(texto)\n",
    "    keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "    string_pesquisa = \"Número da Nota:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "    dadinho_dados_nf['numero_nota_fiscal'] = texto\n",
    "\n",
    "\n",
    "    string_pesquisa = \"Competência:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['competencia'] = texto\n",
    "    \n",
    "    \n",
    "    string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['dt_hr_emissao'] = texto\n",
    "    \n",
    "    \n",
    "    string_pesquisa = \"Código Verificação:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['codigo_verificacao'] = texto\n",
    "    \n",
    "    return dadinho_dados_nf  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_cabecalho(row, model_map, pdf_pesquisavel_map, original_file_name, file_path, frame_type):\n",
    "    \n",
    "    \n",
    "    section = \"1 - CABECALHO\"\n",
    "    f_frame_label = \"1_frame_dados_nf\"\n",
    "    data_box_valores = {'secao': section}\n",
    "    nf_data_cabecalho = {}\n",
    "    \n",
    " \n",
    "    message_erro = []\n",
    "    \n",
    "    model = model_map\n",
    "    pdf_pesquisavel = pdf_pesquisavel_map\n",
    "    \n",
    "    if pdf_pesquisavel_map:\n",
    "        f_tipo = 'frame'\n",
    "        \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model) & (frames_nf_v4_df['type'] == frame_type) & (frames_nf_v4_df['label'] == f_frame_label)]\n",
    "    \n",
    "    if pdf_pesquisavel:\n",
    "        print(' ')\n",
    "        # pdf_document = fitz.open(file_path)\n",
    "        # # Página do PDF\n",
    "        # page_number = 0  # Defina o número da página que deseja analisar\n",
    "        # page = pdf_document[page_number]\n",
    "        # x0, y0, x1, y1 = (0, 0, 600, 110)\n",
    "        # text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        # nf_data_cabecalho = extract_fields_cabecalho(text)\n",
    "        \n",
    "        # pdf_document.close()\n",
    "    else:\n",
    "        image_2work, image_resized_name = convertResize(original_file_name, file_path, image_resized_path)\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        frame_id = row_frame['id']\n",
    "        label = row_frame['label']\n",
    "        x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "        \n",
    "        #print(f'doc {\"pdf_pesquisavel\" if pdf_pesquisavel else \"raster_pdf\"}: {label:>30} | id: {frame_id:>3} | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6} ')\n",
    "        \n",
    "        if pdf_pesquisavel:\n",
    "            # texto_extraido = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            pdf_document = fitz.open(file_path)\n",
    "            # Página do PDF\n",
    "            page_number = 0  # Defina o número da página que deseja analisar\n",
    "            page = pdf_document[page_number]\n",
    "            #x0, y0, x1, y1 = (0, 0, 600, 110)\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            nf_data_cabecalho = extract_fields_cabecalho(text)\n",
    "            #x0, y0, x1, y1 = (0, 0, 600, 110)\n",
    "            #text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            #data_servico = extract_fields_cabecalho()\n",
    "            nf_data_cabecalho['original_file_name'] = original_file_name\n",
    "            nf_data_cabecalho['pdf_pesquisavel'] = pdf_pesquisavel_map\n",
    "            \n",
    "            pdf_document.close()\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        else:\n",
    "            texto_extraido = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "            print(f'RASTER PDF file: {original_file_name:>50} | doc {\"pdf_pesquisavel\" if pdf_pesquisavel else \"raster_pdf\"}: {label:>30} | id: {frame_id:>3} | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6}\\n')\n",
    "            #print(texto_extraido)\n",
    "            \n",
    "        try:\n",
    "            text_splited = texto_extraido_cabecalho(texto_extraido)\n",
    "            keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "            string_pesquisa = \"Número da Nota:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "            nro_nota_frame = texto\n",
    "            #print(nro_nota_frame)\n",
    "            nf_data_cabecalho['numero_nota_fiscal'] = nro_nota_frame\n",
    "            \n",
    "            string_pesquisa = \"Competência:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            competencia_frame = texto\n",
    "            nf_data_cabecalho['competencia'] = competencia_frame \n",
    "            #print(competencia_frame)\n",
    "            \n",
    "            string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            dt_hr_emissao_frame = texto\n",
    "            nf_data_cabecalho['dt_hr_emissao'] = dt_hr_emissao_frame \n",
    "            \n",
    "            string_pesquisa = \"Código Verificação:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            codigo_ver_frame = texto\n",
    "            nf_data_cabecalho['codigo_verificacao'] = codigo_ver_frame \n",
    "            \n",
    "            \n",
    "            \n",
    "            #print(codigo_ver_frame)\n",
    "            print(f'label: {label} | pdf_pesq.: {pdf_pesquisavel} | nro: {nro_nota_frame} | competencia: {competencia_frame} | dt_hr_emissao: {dt_hr_emissao_frame} | codigo_ver: {codigo_ver_frame} | file: {original_file_name}\\n')\n",
    "            print(text_splited)\n",
    "            print()\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            # erros_cabecalho = {}\n",
    "            msg = (f\"doc: {original_file_name} | {e}\")\n",
    "            nro_nota_frame = \" \"\n",
    "            competencia_frame = \" \"\n",
    "            dt_hr_emissao_frame = \" \"\n",
    "            codigo_ver_frame = \" \"\n",
    "            message_erro.append(msg)\n",
    "    \n",
    "    # if pdf_pesquisavel:\n",
    "    #     pdf_document.close()\n",
    "        \n",
    "    return nf_data_cabecalho\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cabecalho_prefeitura():\n",
    "    valor_dict = {}\n",
    "    dados_prefeitura = {}\n",
    "    f_frame_name = \"1_frame_prefeitura_nf\"\n",
    "    # 1. funçao basica de modelo \n",
    "    texto = executa_model_frame(model, secao, f_frame_name)\n",
    "    text_splited = texto.split('\\n')\n",
    "    \n",
    "    valor_dict = extract_prefeitura(model, f_frame_name, text_splited)\n",
    "    if valor_dict:\n",
    "        dados_prefeitura.update(valor_dict)\n",
    "        \n",
    "        \n",
    "    return dados_prefeitura \n",
    "                \n",
    "def cabecalho_dados():\n",
    "\n",
    "    valor = {}   \n",
    "    f_frame_name = \"1_frame_dados_nf\"\n",
    "    \n",
    "    dadinho_dados_nf = {}\n",
    "    \n",
    "    # 1. funçao basica de modelo \n",
    "    texto = executa_model_frame(model, secao, f_frame_name)    \n",
    "    text_splited = texto_extraido(texto)\n",
    "    keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "    string_pesquisa = \"Número da Nota:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "    dadinho_dados_nf['numero_nota_fiscal'] = texto\n",
    "\n",
    "\n",
    "    string_pesquisa = \"Competência:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['competencia'] = texto\n",
    "    \n",
    "    \n",
    "    string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['dt_hr_emissao'] = texto\n",
    "    \n",
    "    \n",
    "    string_pesquisa = \"Código Verificação:\"\n",
    "    texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "    dadinho_dados_nf['codigo_verificacao'] = texto\n",
    "    \n",
    "    return dadinho_dados_nf   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields_cabecalho_pdf_pesquisavel(row, file_path):\n",
    "    \n",
    "    message_erro = [] \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "    \n",
    "    \n",
    "    model_src = row['modelo']\n",
    "    original_file_name = row['original_file_name']\n",
    "    tipo = \"frame\"\n",
    "    section = '1 - CABECALHO'\n",
    "    label = '1_frame_prefeitura_nf'\n",
    "\n",
    "    x0, y0, x1, y1 = pesquisa_frame_coordenadas_pdf_pesquisa(model_src, tipo, section, label)\n",
    "\n",
    "    try:\n",
    "        texto = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        \n",
    "        text_splited = texto_extraido_nf(texto)\n",
    "        keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "        string_pesquisa = \"Número da Nota:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "        nro_nota_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Competência:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        competencia_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        dt_hr_emissao_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Código Verificação:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        codigo_verificacao_frame = texto\n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} | {e}\")\n",
    "        nro_nota_frame = \" \"\n",
    "        competencia_frame = \" \"\n",
    "        dt_hr_emissao_frame = \" \"\n",
    "        codigo_verificacao_frame = \" \"\n",
    "        message_erro.append(msg)\n",
    "            \n",
    "    pdf_document.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields_cabecalho_raster_pdf(row, file_path, image_2work): \n",
    "  \n",
    "    message_erro = []\n",
    "  \n",
    "  \n",
    "    model_src = row['modelo']\n",
    "    original_file_name = row['original_file_name'] \n",
    "    \n",
    "    model = row['modelo']   \n",
    "    secao = \"1 - CABECALHO\"\n",
    "    section = \"1 - CABECALHO\"\n",
    "    f_frame_name = \"1_frame_dados_nf\"\n",
    "    f_tipo = 'frame'\n",
    "        \n",
    "    raw_texto_pil = executa_model_frame(model, secao, f_frame_name, f_tipo, image_2work) # executa_model_frame\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        text_splited = texto_extraido_cabecalho(raw_texto_pil)\n",
    "        keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "        string_pesquisa = \"Número da Nota:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "        nro_nota_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Competência:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        competencia_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        dt_hr_emissao_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Código Verificação:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        codigo_verificacao_frame = texto\n",
    "        \n",
    "    except Exception as e:\n",
    "        # erros_cabecalho = {}\n",
    "        msg = (f\"doc: {original_file_name} | {e}\")\n",
    "        nro_nota_frame = \" \"\n",
    "        competencia_frame = \" \"\n",
    "        dt_hr_emissao_frame = \" \"\n",
    "        codigo_verificacao_frame = \" \"\n",
    "        message_erro.append(msg)\n",
    "        \n",
    "        \n",
    "    return nro_nota_frame, competencia_frame, dt_hr_emissao_frame, codigo_verificacao_frame, message_erro, section, raw_texto_pil \n",
    "\n",
    "def extract_fields_cabecalho_pdf_pesquisavel(row, file_path):\n",
    "    \n",
    "    message_erro = [] \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "    \n",
    "    \n",
    "    model_src = row['modelo']\n",
    "    original_file_name = row['original_file_name']\n",
    "    tipo = \"frame\"\n",
    "    section = '1 - CABECALHO'\n",
    "    label = '1_frame_prefeitura_nf'\n",
    "\n",
    "    x0, y0, x1, y1 = pesquisa_frame_coordenadas_pdf_pesquisa(model_src, tipo, section, label)\n",
    "\n",
    "    try:\n",
    "        texto = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        \n",
    "        text_splited = texto_extraido_nf(texto)\n",
    "        keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "        string_pesquisa = \"Número da Nota:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "        nro_nota_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Competência:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        competencia_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        dt_hr_emissao_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Código Verificação:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        codigo_verificacao_frame = texto\n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} | {e}\")\n",
    "        nro_nota_frame = \" \"\n",
    "        competencia_frame = \" \"\n",
    "        dt_hr_emissao_frame = \" \"\n",
    "        codigo_verificacao_frame = \" \"\n",
    "        message_erro.append(msg)\n",
    "            \n",
    "    pdf_document.close()\n",
    "    #print(row)\n",
    "    return nro_nota_frame, competencia_frame, dt_hr_emissao_frame, codigo_verificacao_frame, message_erro, section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields_cabecalho_raster_pdf(row, file_path, image_2work): \n",
    "  \n",
    "    message_erro = []\n",
    "  \n",
    "  \n",
    "    model_src = row['modelo']\n",
    "    original_file_name = row['original_file_name'] \n",
    "    \n",
    "    model = row['modelo']   \n",
    "    secao = \"1 - CABECALHO\"\n",
    "    section = \"1 - CABECALHO\"\n",
    "    f_frame_name = \"1_frame_dados_nf\"\n",
    "    f_tipo = 'frame'\n",
    "        \n",
    "    raw_texto_pil = executa_model_frame(model, secao, f_frame_name, f_tipo, image_2work) # executa_model_frame\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        text_splited = texto_extraido_cabecalho(raw_texto_pil)\n",
    "        keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "        string_pesquisa = \"Número da Nota:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "        nro_nota_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Competência:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        competencia_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        dt_hr_emissao_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Código Verificação:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        codigo_verificacao_frame = texto\n",
    "        \n",
    "    except Exception as e:\n",
    "        # erros_cabecalho = {}\n",
    "        msg = (f\"doc: {original_file_name} | {e}\")\n",
    "        nro_nota_frame = \" \"\n",
    "        competencia_frame = \" \"\n",
    "        dt_hr_emissao_frame = \" \"\n",
    "        codigo_verificacao_frame = \" \"\n",
    "        message_erro.append(msg)\n",
    "        \n",
    "        \n",
    "    return nro_nota_frame, competencia_frame, dt_hr_emissao_frame, codigo_verificacao_frame, message_erro, section, raw_texto_pil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields_cabecalho_pdf_pesquisavel(row, file_path):\n",
    "    \n",
    "    message_erro = [] \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "    \n",
    "    \n",
    "    model_src = row['modelo']\n",
    "    original_file_name = row['original_file_name']\n",
    "    tipo = \"frame\"\n",
    "    section = '1 - CABECALHO'\n",
    "    label = '1_frame_prefeitura_nf'\n",
    "\n",
    "    x0, y0, x1, y1 = pesquisa_frame_coordenadas_pdf_pesquisa(model_src, tipo, section, label)\n",
    "\n",
    "    try:\n",
    "        texto = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        \n",
    "        text_splited = texto_extraido_nf(texto)\n",
    "        keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "\n",
    "        string_pesquisa = \"Número da Nota:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)         \n",
    "        nro_nota_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Competência:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        competencia_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"ata e Hora da Emissão:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        dt_hr_emissao_frame = texto\n",
    "        \n",
    "        string_pesquisa = \"Código Verificação:\"\n",
    "        texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "        codigo_verificacao_frame = texto\n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} | {e}\")\n",
    "        nro_nota_frame = \" \"\n",
    "        competencia_frame = \" \"\n",
    "        dt_hr_emissao_frame = \" \"\n",
    "        codigo_verificacao_frame = \" \"\n",
    "        message_erro.append(msg)\n",
    "            \n",
    "    pdf_document.close()\n",
    "    #print(row)\n",
    "    return nro_nota_frame, competencia_frame, dt_hr_emissao_frame, codigo_verificacao_frame, message_erro, section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. funçao basica de modelo \n",
    "def pesquisa_model_frame(model_src, tipo):\n",
    "\n",
    "    data_dados_model = {}\n",
    "    dados_modelo = []\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_src) & (frames_nf_v4_df['type'] == tipo)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        frame_model = row_frame['model']\n",
    "        # frame_prefeitura = row_frame['prefeitura']\n",
    "        # frame_label = row_frame['label']\n",
    "        # frame_section_json = row_frame['section_json']\n",
    "        # frame_cnpj = row_frame['cnpj']\n",
    "        \n",
    "        # if frame_model:\n",
    "        #     modelo_frame = True\n",
    "        # else:    \n",
    "        #     modelo_frame = False\n",
    "\n",
    "                  \n",
    "  \n",
    "    return frame_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX funçao basica de modelo \n",
    "def pesquisa_frame_coordenadas_pdf_pesquisa(model_src, tipo, section, label):\n",
    "\n",
    "    tipo = tipo\n",
    "    data_dados_model = {}\n",
    "    dados_modelo = []\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_src) & (frames_nf_v4_df['type'] == tipo) & (frames_nf_v4_df['section_json'] == section) & (frames_nf_v4_df['label'] == label)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        frame_model = row_frame['model']\n",
    "        frame_label = row_frame['label']\n",
    " \n",
    "        # data for PDF Pesquisavel\n",
    "        x0, y0, x1, y1 = row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']\n",
    " \n",
    "    return x0, y0, x1, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Avaliar Funçao basica de modelo \n",
    "def pesquisa_model_frame_coordenadas(model_src, tipo, tipo_processo_pdf):\n",
    "\n",
    "    data_dados_model = {}\n",
    "    dados_modelo = []\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_src) & (frames_nf_v4_df['type'] == tipo)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        \n",
    "        frame_model = row_frame['model']\n",
    "        frame_prefeitura = row_frame['prefeitura']\n",
    "        frame_label = row_frame['label']\n",
    "        frame_section_json = row_frame['section_json']\n",
    "        frame_cnpj = row_frame['cnpj']\n",
    "        \n",
    "        if frame_model:\n",
    "            modelo = True\n",
    "        \n",
    "        if tipo_processo_pdf == 'pdf_pesquisavel':\n",
    "            try:\n",
    "                # data for PDF Pesquisavel\n",
    "                x0, y0, x1, y1 = row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']\n",
    "                \n",
    "                data_dados_model = {\n",
    "                    \"prefeitura\": frame_prefeitura,\n",
    "                    \"model\": frame_model,\n",
    "                    \"label\": frame_label,\n",
    "                    \"section_json\": frame_section_json,\n",
    "                    \"cnpj\": frame_cnpj,\n",
    "                    \"x0\": x0,\n",
    "                    \"y0\": y0,\n",
    "                    \"x1\": x1,\n",
    "                    \"y1\": y1,\n",
    "                }\n",
    "                dados_modelo.append(data_dados_model)   \n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao carregar coordenadas do frame: {e}\")\n",
    "                x0, y0, x1, y1 = 0, 0, 0, 0 \n",
    "                \n",
    "\n",
    "        elif tipo_processo_pdf == 'raster_pdf':\n",
    "            try:\n",
    "                # data for Raster_PDF\n",
    "                x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "                \n",
    "                data_dados_model = {\n",
    "                    \"prefeitura\": frame_prefeitura,\n",
    "                    \"model\": frame_model,\n",
    "                    \"label\": frame_label,\n",
    "                    \"section_json\": frame_section_json,\n",
    "                    \"cnpj\": frame_cnpj,\n",
    "                    \"x0\": x0,\n",
    "                    \"y0\": y0,\n",
    "                    \"x1\": x1,\n",
    "                    \"y1\": y1,\n",
    "                }\n",
    "                dados_modelo.append(data_dados_model)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao carregar coordenadas do frame: {e}\")\n",
    "                x0, y0, x1, y1 = 0, 0, 0, 0 \n",
    "                  \n",
    "  \n",
    "    return dados_modelo, modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_raw = {}\n",
    "i = 0\n",
    "for  doc in raw_document_list:\n",
    "    dict_raw = raw_document_list[i]\n",
    "    print(i)\n",
    "    print(dict_raw['pdf_pesquisavel'])\n",
    "    print(dict_raw['original_file_name'])\n",
    "    print(dict_raw['texto_extraido'])\n",
    "    #print(f'\\nraw_document_list[i]: {raw_document_list[i]}\\n\\n')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pesquisando os dados para o processo de extracao - Raster\n",
    "\n",
    "#f_type = 'frame'\n",
    "f_type = 'field_box'\n",
    "\n",
    "filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['de_para_pm'] == de_para_pm) & (frames_nf_v4_df['model'] == model2search) & (frames_nf_v4_df['type'] == f_type)]\n",
    "\n",
    "for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "    seq = row_frame['seq']\n",
    "    de_para = row_frame['de_para_pm']\n",
    "    model = row_frame['model']  \n",
    "    print(f'{seq:>4} {de_para:>8} | {model:>10} | {row_frame[\"type\"]:>15} | {row_frame[\"label\"]:>35} | {row_frame[\"section_json\"]:>35} | {row_frame[\"x0\"]:>7} | {row_frame[\"y0\"]:>7} | {row_frame[\"x1\"]:>7} | {row_frame[\"y1\"]:>7}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pesquisando os dados para o processo de extracao - PDF Pesquisavel\n",
    "\n",
    "f_type = 'frame'\n",
    "\n",
    "filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['de_para_pm'] == de_para_pm) & (frames_nf_v4_df['model'] == model2search) & (frames_nf_v4_df['type'] == f_type)]\n",
    "\n",
    "\n",
    "for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "    seq = row_frame['seq']\n",
    "    de_para = row_frame['de_para_pm']\n",
    "    model = row_frame['model']  \n",
    "    print(f'{seq:>4} {de_para:>8} | {model:>10} | {row_frame[\"type\"]:>15} | {row_frame[\"label\"]:>35} | {row_frame[\"section_json\"]:>35} | {row_frame[\"x0_p\"]:>7} | {row_frame[\"y0_p\"]:>8} | {row_frame[\"x1_p\"]:>8} | {row_frame[\"y1_p\"]:>8}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model2search) & (frames_nf_v4_df['type'] == tipo)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Criando um DataFrame de exemplo\n",
    "data = {'A': [1, 2], 'B': [3, 4], 'C': [5, 6]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Criando uma lista para armazenar os dicionários que representam cada linha\n",
    "lista_dicts = []\n",
    "\n",
    "# Iterando sobre as linhas do DataFrame e adicionando cada linha à lista\n",
    "for index, row in df.iterrows():\n",
    "    # Convertendo a linha a um dicionário\n",
    "    row_dict = row.to_dict()\n",
    "    \n",
    "    # Adicionando novas colunas ao dicionário\n",
    "    row_dict['D'] = row['A'] * row['B']\n",
    "    row_dict['E'] = row['B'] + row['C']\n",
    "    \n",
    "    # Adicionando o dicionário à lista\n",
    "    lista_dicts.append(row_dict)\n",
    "\n",
    "# Criando um novo DataFrame a partir da lista de dicionários\n",
    "novo_df = pd.DataFrame(lista_dicts)\n",
    "\n",
    "# Exibindo o novo DataFrame\n",
    "print(novo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. TOMADOR DE SERVIÇO\n",
    "def extract_fields_tomador(text):\n",
    "    nf_data_tomador = {}\n",
    "    \n",
    "    \n",
    "    nf_data_tomador['secao'] = \"3. TOMADOR DE SERVIÇO\"\n",
    "    \n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in text:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "                        nf_data_tomador['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                        nf_data_tomador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "\n",
    "        \n",
    "    # Extrair RG    \n",
    "    rg_match = re.search(r'RG:\\s+(.+)', text)   \n",
    "    if rg_match:\n",
    "        rg_str = rg_match.group(1)\n",
    "        if rg_str == 'Telefone:':\n",
    "            nf_data_tomador['rg'] = \"NONE\"  # Valor padrão quando não há correspondência\n",
    "        else:    \n",
    "            nf_data_tomador['rg'] = rg_match.group(1)  \n",
    " \n",
    "        \n",
    "    # Extrair Telefone\n",
    "    telefone_match = re.search(r'Telefone:\\s+(.+)', text)\n",
    "    if telefone_match:\n",
    "        telefone_str = telefone_match.group(1)\n",
    "        if telefone_str == 'Inscrição Estadual:':\n",
    "            nf_data_tomador['telefone'] = \"NONE\"  # Valor padrão quando não há correspondência\n",
    "        else:    \n",
    "            nf_data_tomador['telefone'] = telefone_match.group(1)\n",
    "     \n",
    "\n",
    "    # Extrair Inscrição Municipal\n",
    "    inscricao_municipal_match = re.search(r'Inscrição Municipal:\\s+(.+)', text)\n",
    "    if inscricao_municipal_match:\n",
    "        nf_data_tomador['inscricao_municipal'] = inscricao_municipal_match.group(1)\n",
    "                \n",
    "                \n",
    "                \n",
    "    # Extrair Inscrição Estadual\n",
    "    inscricao_estadual_match = re.search(r'Inscrição Estadual:\\s+(.+)', text)\n",
    "    if inscricao_estadual_match:\n",
    "        inscricao_estadual_str = inscricao_estadual_match.group(1)\n",
    "        if inscricao_estadual_str == 'Nome/Razão Social:':\n",
    "            nf_data_tomador['inscricao_estadual'] = \"NONE\"\n",
    "        else:    \n",
    "            nf_data_tomador['inscricao_estadual'] = inscricao_estadual_match.group(1)   \n",
    "                \n",
    "    \n",
    "    # Nome/Razão Social:\n",
    "    razao_social_match = re.search(r'Nome/Razão Social:\\s+(.+)', text)\n",
    "    if razao_social_match:\n",
    "        nf_data_tomador['razao_social'] = razao_social_match.group(1)                                                \n",
    "                \n",
    "    # Endereço:\n",
    "    endereco_match = re.search(r'Endereço:\\s+(.+)', text)\n",
    "    if endereco_match:\n",
    "        nf_data_tomador['endereco'] = endereco_match.group(1) \n",
    "    \n",
    "    # E-mail:\n",
    "    email_match = re.search(r'E-mail:\\s+(.+)', text)\n",
    "    if email_match:\n",
    "        nf_data_tomador['email'] = email_match.group(1) \n",
    "    else:\n",
    "        nf_data_tomador['email'] = \"NONE\"  # Valor padrão quando não há correspondência    \n",
    "\n",
    "    return nf_data_tomador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Extrair EXIGIBILIDADE ISS:\n",
    "    try:\n",
    "        exigibilidade_iss_match = re.search(r'EXIGIBILIDADE ISS\\s+(.+)', text)\n",
    "        if exigibilidade_iss_match:\n",
    "            exigibilidade_iss_value = exigibilidade_iss_match.group(1).strip()\n",
    "            nf_data_outras_informacoes['exigibilidade_iss'] = exigibilidade_iss_value\n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "        exigibilidade_iss_value = ' '\n",
    "\n",
    "    try:\n",
    "        # Extrair REGIME TRIBUTAÇÃO:\n",
    "        regime_tributacao_match = re.search(r'REGIME TRIBUTAÇÃO\\s+(.+)', text)\n",
    "        if regime_tributacao_match:\n",
    "            regime_tributacao_value = regime_tributacao_match.group(1).strip()\n",
    "            nf_data_outras_informacoes['regime_tributacao'] = regime_tributacao_value\n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "        regime_tributacao_value = ' '    \n",
    "    \n",
    "    try:\n",
    "        # Extrair SIMPLES NACIONAL:\n",
    "        simples_nacional_match = re.search(r'SIMPLES NACIONAL\\s+(.+)', text)\n",
    "        if simples_nacional_match:\n",
    "            simples_nacional_value = simples_nacional_match.group(1).strip()\n",
    "            nf_data_outras_informacoes['simples_nacional'] = simples_nacional_value  \n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "        simples_nacional_value = ' '     \n",
    "        \n",
    "      \n",
    "    try:\n",
    "        # Extrair ISSQN RETIDO:\n",
    "        local_prestacao_servico_match = re.search(r'ISSQN RETIDO\\s+(.+)', text)\n",
    "        if local_prestacao_servico_match:\n",
    "            local_prestacao_servico_value = local_prestacao_servico_match.group(1).strip()\n",
    "            nf_data_outras_informacoes['issqn_retido'] = local_prestacao_servico_value      \n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "        exigibilidade_iss_value = ' ' \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # Extrair LOCAL PRESTAÇÃO SERVIÇO:\n",
    "        local_prestacao_servico_match = re.search(r'LOCAL\\. PRESTAÇÃO\\s+SERVIÇO\\s+(.+)', text)\n",
    "        if local_prestacao_servico_match:\n",
    "            local_prestacao_servico_value = local_prestacao_servico_match.group(1).strip()\n",
    "            nf_data_outras_informacoes['local_prestacao_servico'] = local_prestacao_servico_value\n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "        local_prestacao_servico_value = ' ' \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # Extrair LOCAL INCIDÊNCIA:\n",
    "        local_incidencia_match = re.search(r'LOCAL INCIDÊNCIA\\s+(.+)', text)\n",
    "        if local_incidencia_match:\n",
    "            local_incidencia_value = local_incidencia_match.group(1).strip()\n",
    "            nf_data_outras_informacoes['local_incidencia'] = local_incidencia_value\n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "        local_incidencia_value = ' '    \n",
    "    \n",
    "  \n",
    "    \n",
    "    pdf_document.close()\n",
    "    return nf_data_outras_informacoes\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PRESTADOR DE SERVIÇO\n",
    "def extract_fields_prestador(text): # Função para extrair campos e valores dentro de um retângulo\n",
    "    nf_data_prestador = {}\n",
    "    \n",
    "    nf_data_prestador['secao'] = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    \n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in text:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "                        nf_data_prestador['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                        nf_data_prestador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "\n",
    "    # Extrair Inscrição Municipal\n",
    "    inscricao_municipal_match = re.search(r'Inscrição Municipal:\\s+(.+)', text)\n",
    "    if inscricao_municipal_match:\n",
    "        nf_data_prestador['inscricao_municipal'] = inscricao_municipal_match.group(1)\n",
    "        \n",
    "               \n",
    "    # Extrair Inscrição Estadual\n",
    "    #if \"Inscrição Estadual:\" in text:\n",
    "    \n",
    "    # Extrair Inscrição Estadual\n",
    "    inscricao_estadual_match = re.search(r'Inscrição Estadual:\\s+(.+)', text)\n",
    "    if inscricao_estadual_match:\n",
    "        inscricao_estadual_str = inscricao_estadual_match.group(1)\n",
    "        if inscricao_estadual_str == 'Nome/Razão Social:':\n",
    "            nf_data_prestador['inscricao_estadual'] = \"NONE\"\n",
    "        else:    \n",
    "            nf_data_prestador['inscricao_estadual'] = inscricao_estadual_match.group(1)       \n",
    "        \n",
    "                \n",
    "    \n",
    "\n",
    "    # Extrair Telefone\n",
    "    #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "    telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', text)\n",
    "    if telefone_match: \n",
    "        telefone_str = telefone_match.group(1)\n",
    "        # Remover quebras de linha\n",
    "        telefone_str = telefone_str.replace('.', '')\n",
    "        telefone_str = telefone_str.replace('\\n', '')\n",
    "                \n",
    "        nf_data_prestador['telefone'] = telefone_str\n",
    "    else:\n",
    "        nf_data_prestador['telefone'] = \"NONE\"\n",
    "\n",
    "         \n",
    "                \n",
    "    # Nome/Razão Social:\n",
    "    razao_social_match = re.search(r'Nome/Razão Social:\\s+(.+)', text)\n",
    "    if razao_social_match:\n",
    "        nf_data_prestador['razao_social'] = razao_social_match.group(1)  \n",
    "                \n",
    "    # Nome de Fantasia:\n",
    "    nome_fantasia_match = re.search(r'Nome de Fantasia:\\s+(.+)', text)\n",
    "    if nome_fantasia_match:\n",
    "        nf_data_prestador['nome_fantasia'] = nome_fantasia_match.group(1)                                    \n",
    "                \n",
    "            \n",
    "    # Endereço:\n",
    "    endereco_match = re.search(r'Endereço:\\s+(.+)', text)\n",
    "    if endereco_match:\n",
    "        nf_data_prestador['endereco'] = endereco_match.group(1) \n",
    "    \n",
    "    # E-mail:\n",
    "    email_match = re.search(r'E-mail:\\s+(.+)', text)\n",
    "    if email_match:\n",
    "        nf_data_prestador['email'] = email_match.group(1)  \n",
    "    else:\n",
    "        nf_data_prestador['email'] = \"NONE\"  # Valor padrão quando não há correspondência\n",
    "   \n",
    "        \n",
    "\n",
    "    return nf_data_prestador\n",
    "\n",
    "## VERIFICAR\n",
    "def extract_fields_cnpj(texto_extraido, original_file_name): \n",
    "  \n",
    "    message_erro = []\n",
    "    \n",
    "    data_nf_cnpj_prestador = {}\n",
    "      \n",
    "    #print(f'2 extract_fields_cnpj:  {texto_extraido}')\n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in texto_extraido:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', texto_extraido)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "            data_nf_cnpj_prestador['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "            data_nf_cnpj_prestador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "    else:\n",
    "        data_nf_cnpj_prestador['cpf_cnpj_com_mascara'] = None\n",
    "        data_nf_cnpj_prestador['cpf_cnpj_sem_mascara'] = None            \n",
    "            \n",
    "            \n",
    "    telefone_str = None\n",
    "\n",
    "    #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "    telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', texto_extraido)\n",
    "    if telefone_match: \n",
    "        telefone_str = telefone_match.group(1)\n",
    "        # Remover quebras de linha\n",
    "        telefone_str = telefone_str.replace('.', '')\n",
    "        telefone_str = telefone_str.replace('\\n', '')\n",
    "                \n",
    "        data_nf_cnpj_prestador['telefone'] = telefone_str\n",
    "    else:\n",
    "        data_nf_cnpj_prestador['telefone'] = None \n",
    "    \n",
    "    #print(data_nf_cnpj_prestador)    \n",
    "\n",
    "             \n",
    "    return data_nf_cnpj_prestador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERRIFICAR (under cosntruction)\n",
    "def processar_servicos_pdf_pesquisavel_2(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    nf_data_servico = {}\n",
    "    \n",
    "    process = ['4_frame_descricao_totais']\n",
    "    \n",
    "    section = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "    pdf_pesquisavel_map = True\n",
    "        \n",
    "    tipo= \"frame\"\n",
    "    message_erro = []\n",
    "    nf_dados_prestador = {}\n",
    "    model = row['model']\n",
    "    print(model)\n",
    "\n",
    "    for father in process_prestador:\n",
    "        label = father\n",
    "        if label == \"4_frame_descricao_totais\": \n",
    "            coordinates = get_coordinates_filter(model_map, tipo, label, section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            extract_text = extract_text_PIL(image_2work, (x0, y0, x1, y1))\n",
    "            #text_splited = texto_extraido_nf(extract_text)\n",
    "            if \"CPF/CNPJ:\" in extract_text:\n",
    "                cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', extract_text)\n",
    "                if cpf_cnpj_formatado_match:\n",
    "                    texto = cpf_cnpj_formatado_match.group(1)\n",
    "                \n",
    "                    nf_dados_prestador['cpf_cnpj_com_mascara'] = texto\n",
    "                    nf_dados_prestador['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "            else:\n",
    "                nf_dados_prestador['cpf_cnpj_com_mascara'] = 'None'\n",
    "                nf_dados_prestador['cpf_cnpj_sem_mascara'] = 'None' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. VALORES E IMPOSTOS\n",
    "def extract_fields_impostos(text):\n",
    "    nf_data_valores = {}\n",
    "    #nf_data_valores['secao'] = \"7. VALORES E IMPOSTOS\"\n",
    "    \n",
    "    # Extrair VALOR SERVIÇOS:\n",
    "    valor_servicos_match = re.search(r'VALOR SERVIÇOS:\\s+(.+)', text)\n",
    "    if valor_servicos_match:\n",
    "        valor_servicos_str = valor_servicos_match.group(1)\n",
    "        valor_servicos_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_servicos_str)\n",
    "        if valor_servicos_sem_formato:\n",
    "            valor_servicos_sem_formatacao = valor_servicos_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_servicos'] = float(valor_servicos_sem_formatacao)\n",
    "        else:\n",
    "            nf_data_valores['valor_servicos'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "  \n",
    "  \n",
    "    # Extrair VALOR DEDUÇÃO:\n",
    "    valor_deducao_match = re.search(r'DEDUÇÃO:\\s+(.+)', text)\n",
    "    if valor_deducao_match:\n",
    "        valor_deducao_str = valor_deducao_match.group(1)\n",
    "        valor_deducao_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_deducao_str)\n",
    "        if valor_deducao_sem_formato:\n",
    "            valor_deducao_sem_formato = valor_deducao_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_deducao'] = float(valor_deducao_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_deducao'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "        \n",
    "        \n",
    "    # Extrair DESC. INCOND:\n",
    "    valor_desc_match = re.search(r'DESC. INCOND:\\s+(.+)', text)\n",
    "    if valor_desc_match:\n",
    "        valor_desc_str = valor_desc_match.group(1)\n",
    "        valor_desc_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_desc_str)\n",
    "        if valor_desc_sem_formato:\n",
    "            valor_desc_sem_formato = valor_desc_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['desc_incond'] = float(valor_desc_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['desc_incond'] = 0.0  # Valor não encontrado ou não está no formato esperado        \n",
    "        \n",
    "\n",
    "    # Extrair BASE DE CÁLCULO:\n",
    "    valor_calculo_match = re.search(r'CÁLCULO:\\s+(.+)', text)\n",
    "    if valor_calculo_match:\n",
    "        valor_calculo_str = valor_calculo_match.group(1)\n",
    "        valor_calculo_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_calculo_str)\n",
    "        if valor_calculo_sem_formato:\n",
    "            valor_calculo_sem_formato = valor_calculo_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['base_calculo'] = float(valor_calculo_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['base_calculo'] = 0.0  # Valor não encontrado ou não está no formato esperado    \n",
    "\n",
    "\n",
    "\n",
    "    # Extrair ALÍQUOTA:\n",
    "    valor_aliquota_match = re.search(r'ALÍQUOTA:\\s+(.+)', text)\n",
    "    if valor_aliquota_match:\n",
    "        valor_aliquota_str = valor_aliquota_match.group(1)\n",
    "        valor_aliquota_sem_formato = re.search(r'([\\d.,]+)%', valor_aliquota_str)  # Ajuste aqui\n",
    "        if valor_aliquota_sem_formato:\n",
    "            valor_aliquota_sem_formato = valor_aliquota_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['aliquota'] = float(valor_aliquota_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['aliquota'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "\n",
    "\n",
    "    # Extrair VALOR ISS:\n",
    "    valor_iss_match = re.search(r'VALOR ISS:\\s+(.+)', text)\n",
    "    if valor_iss_match:\n",
    "        valor_iss_str = valor_iss_match.group(1)\n",
    "        valor_iss_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_iss_str)\n",
    "        if valor_iss_sem_formato:\n",
    "            valor_iss_sem_formato = valor_iss_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_iss'] = float(valor_iss_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_iss'] = 0.0  # Valor não encontrado ou não está no formato esperado \n",
    "\n",
    "    # Extrair VALOR ISS RETIDO:\n",
    "    valor_iss_retido_match = re.search(r'RETIDO:\\s+(.+)', text)\n",
    "    if valor_iss_match:\n",
    "        valor_iss_retido_str = valor_iss_retido_match.group(1)\n",
    "        valor_iss_retido_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_iss_retido_str)\n",
    "        if valor_iss_retido_sem_formato:\n",
    "            valor_iss_retido_sem_formato = valor_iss_retido_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_iss_retido'] = float(valor_iss_retido_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_iss_retido'] = 0.0  # Valor não encontrado ou não está no formato esperado \n",
    "\n",
    "    # Extrair VALOR DESC. COND:\n",
    "    valor_desc_cond_match = re.search(r'DESC. COND:\\s+(.+)', text)\n",
    "    if valor_desc_cond_match:\n",
    "        valor_desc_cond_str = valor_desc_cond_match.group(1)\n",
    "        valor_desc_cond_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_desc_cond_str)\n",
    "        if valor_desc_cond_sem_formato:\n",
    "            valor_desc_cond_sem_formato = valor_desc_cond_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['desc_cond'] = float(valor_desc_cond_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['desc_cond'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "    \n",
    "    # Extrair VALOR PIS:\n",
    "    valor_pis_match = re.search(r'VALOR PIS:\\s+(.+)', text)\n",
    "    if valor_pis_match:\n",
    "        valor_pis_str = valor_pis_match.group(1)\n",
    "        valor_pis_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_pis_str)\n",
    "        if valor_pis_sem_formato:\n",
    "            valor_pis_sem_formato = valor_pis_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_pis'] = float(valor_pis_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_pis'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "    \n",
    "    # Extrair VALOR COFINS:\n",
    "    valor_cofins_match = re.search(r'VALOR COFINS:\\s+(.+)', text)\n",
    "    if valor_cofins_match:\n",
    "        valor_cofins_str = valor_cofins_match.group(1)\n",
    "        valor_cofins_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_cofins_str)\n",
    "        if valor_cofins_sem_formato:\n",
    "            valor_cofins_sem_formato = valor_cofins_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_cofins'] = float(valor_cofins_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_cofins'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "            \n",
    "    # Extrair VALOR IR:\n",
    "    valor_ir_match = re.search(r'VALOR IR:\\s+(.+)', text)\n",
    "    if valor_ir_match:\n",
    "        valor_ir_str = valor_ir_match.group(1)\n",
    "        valor_ir_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_ir_str)\n",
    "        if valor_ir_sem_formato:\n",
    "            valor_ir_sem_formato = valor_ir_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_ir'] = float(valor_ir_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_ir'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "            \n",
    "    # Extrair VALOR INSS:\n",
    "    valor_inss_match = re.search(r'VALOR INSS:\\s+(.+)', text)\n",
    "    if valor_inss_match:\n",
    "        valor_inss_str = valor_inss_match.group(1)\n",
    "        valor_inss_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_inss_str)\n",
    "        if valor_inss_sem_formato:\n",
    "            valor_inss_sem_formato = valor_inss_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_inss'] = float(valor_inss_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_inss'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "            \n",
    "    # Extrair VALOR CSLL:\n",
    "    valor_csll_match = re.search(r'VALOR CSLL:\\s+(.+)', text)\n",
    "    if valor_csll_match:\n",
    "        valor_csll_str = valor_csll_match.group(1)\n",
    "        valor_csll_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_csll_str)\n",
    "        if valor_csll_sem_formato:\n",
    "            valor_csll_sem_formato = valor_csll_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_csll'] = float(valor_csll_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_csll'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "    \n",
    "    # Extrair OUTRAS RETENÇÕES:\n",
    "    outras_retencoes_match = re.search(r'OUTRAS RETENÇÕES:\\s+(.+)', text)\n",
    "    if outras_retencoes_match:\n",
    "        outras_retencoes_str = outras_retencoes_match.group(1)\n",
    "        outras_retencoes_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', outras_retencoes_str)\n",
    "        if outras_retencoes_sem_formato:\n",
    "            outras_retencoes_sem_formato = outras_retencoes_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['outras_retencoes'] = float(outras_retencoes_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['outras_retencoes'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "    \n",
    "    \n",
    "    # Extrair VALOR LÍQUIDO:\n",
    "    valor_liquido_match = re.search(r'VALOR LÍQUIDO:\\s+(.+)', text)\n",
    "    if valor_liquido_match:\n",
    "        valor_liquido_str = valor_liquido_match.group(1)\n",
    "        valor_liquido_sem_formato = re.search(r'R\\$\\s*([\\d.,]+)', valor_liquido_str)\n",
    "        if valor_liquido_sem_formato:\n",
    "            valor_liquido_sem_formato = valor_liquido_sem_formato.group(1).replace('.', '').replace(',', '.').strip()\n",
    "            nf_data_valores['valor_liquido'] = float(valor_liquido_sem_formato)\n",
    "        else:\n",
    "            nf_data_valores['valor_liquido'] = 0.0  # Valor não encontrado ou não está no formato esperado\n",
    "            \n",
    "    #print(nf_data_valores)        \n",
    "        \n",
    "\n",
    "    return nf_data_valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CABECALHO\n",
    "def extract_fields_cabecalho(text):\n",
    "    nf_data_cabecalho = {}\n",
    "    # Extrair Nome da Prefeitura\n",
    "    nome_prefeitura_match = re.search(r'PREFEITURA (.+)', text)\n",
    "    if nome_prefeitura_match:\n",
    "        nome_prefeitura = \"PREFEITURA \" + nome_prefeitura_match.group(1)\n",
    "        nf_data_cabecalho['nome_prefeitura'] = nome_prefeitura\n",
    "    else:\n",
    "        nf_data_cabecalho['nome_prefeitura'] = None    \n",
    "\n",
    "    # Extrair Tipo de NF\n",
    "    tipo_nf_match = re.search(r'NOTA FISCAL (.+)', text)\n",
    "    if tipo_nf_match:\n",
    "        tipo_nf = \"NOTA FISCAL \" + tipo_nf_match.group(1)\n",
    "        nf_data_cabecalho['tipo_nota_fiscal'] = tipo_nf\n",
    "    else:\n",
    "        nf_data_cabecalho['tipo_nota_fiscal'] = None    \n",
    "    \n",
    "    # Extrair Número da Nota\n",
    "    numero_nota_match = re.search(r'Número da Nota:\\s+(\\d+)', text)\n",
    "    if numero_nota_match:\n",
    "        nr_nro_nf = numero_nota_match.group(1)\n",
    "        nf_data_cabecalho['numero_nota_fiscal'] = numero_nota_match.group(1)\n",
    "    else:\n",
    "        nf_data_cabecalho['numero_nota_fiscal'] = None\n",
    "        nf_data_erros['cabecalho'] = \"numero_nota\"\n",
    "        nf_lista_erros.append(nf_data_erros)        \n",
    "\n",
    "    # Extrair Competência\n",
    "    competencia_match = re.search(r'Competência:\\s+(.+)', text)\n",
    "    if competencia_match:\n",
    "        nf_data_cabecalho['competencia'] = competencia_match.group(1)\n",
    "    else:\n",
    "        nf_data_cabecalho['competencia'] = None\n",
    "        nf_data_erros['cabecalho'] = \"competencia\"\n",
    "        nf_lista_erros.append(nf_data_erros)\n",
    "            \n",
    "\n",
    "    # Extrair Data e Hora de Emissão\n",
    "    data_emissao_match = re.search(r'Data e Hora da Emissão:\\s+(.+)', text)\n",
    "    if data_emissao_match:\n",
    "        nf_data_cabecalho['dt_hr_emissao'] = data_emissao_match.group(1)\n",
    "    else:\n",
    "        nf_data_cabecalho['dt_hr_emissao'] = None    \n",
    "        \n",
    "    # Extrair Data e Hora de Emissão\n",
    "    codigo_verificacao_match = re.search(r'Código Verificação:\\s+(.+)', text)\n",
    "    if codigo_verificacao_match:\n",
    "        nf_data_cabecalho['codigo_verificacao'] = codigo_verificacao_match.group(1)\n",
    "    else:\n",
    "        nf_data_cabecalho['codigo_verificacao'] = None\n",
    "        nf_data_erros['cabecalho'] = \"codigo_verificacao\"\n",
    "        nf_lista_erros.append(nf_data_erros)        \n",
    "\n",
    "    return nf_data_cabecalho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PRESTADOR DE SERVIÇO\n",
    "def extrai_prestador_PDF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path):\n",
    "    \n",
    "    nf_data_prestador = {}\n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    label = \"2_frame_cnpj_prestador\"\n",
    "\n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    tipo = \"frame\"\n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    #print(label)\n",
    "    #print(x0,x1,y0,y1)\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    nf_data_prestador = novaextra.extract_fields_prestador(text)\n",
    "\n",
    "           \n",
    "\n",
    "    pdf_document.close()\n",
    "\n",
    "    return nf_data_prestador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. VALOR TOTAL\n",
    "def processar_valor_total_PDF_P(row, pdf_pesquisavel_map, model_map, original_file_name, file_path):\n",
    "    \n",
    "    nf_data_valor_total = {}\n",
    "    \n",
    "    process = ['4_frame_valor_total']\n",
    "    \n",
    "    section = \"5. VALOR TOTAL\"\n",
    "    pdf_pesquisavel_map = True\n",
    "        \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "        \n",
    "    tipo= \"frame\"\n",
    "    message_erro = []\n",
    "    nf_dados_prestador = {}\n",
    "    # model = row['model']\n",
    "    for father in process:\n",
    "        label = father\n",
    "        if label == \"4_frame_valor_total\": \n",
    "            print(model_map)\n",
    "            coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            print(f'model: {model_map} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6}\\n {text}')\n",
    "            valor_total_match = re.search(r'R\\$ ([\\d,.]+)', text)\n",
    "            if valor_total_match:\n",
    "                valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "                valor_total_float = float(valor_total_sem_formatacao)\n",
    "                #nf_data_valor_total['valor_total_nota'] =\n",
    "                \n",
    "    pdf_document.close()            \n",
    "                \n",
    "    return valor_total_float  \n",
    "   nf_data_CNAE = {}\n",
    "    nf_data_CNAE['Secao'] = \"6. CNAE e Item da Lista de Serviços\"\n",
    "    \n",
    "    process = [\"cnae\", 'item_lista_servicos']\n",
    "    \n",
    "    section = \"6. CNAE e Item da Lista de Serviços\"\n",
    "    pdf_pesquisavel_map = True\n",
    "        \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "        \n",
    "    tipo = \"sframe_field\"\n",
    "    message_erro = []\n",
    "    nf_dados_prestador = {}\n",
    "    # model = row['model']\n",
    "    for father in process:\n",
    "        label = father\n",
    "        if label == \"cnae\": \n",
    "            tipo = \"sframe_field\"\n",
    "            print(model_map)\n",
    "            coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            print(f'model: {model_map} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6}\\n {text}')\n",
    "        \n",
    "        elif label == \"item_lista_servicos\":\n",
    "            tipo = \"sframe_field\"\n",
    "            print(model_map)\n",
    "            coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            print(f'model: {model_map} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6}\\n {text}')\n",
    "                \n",
    "\n",
    "           "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
