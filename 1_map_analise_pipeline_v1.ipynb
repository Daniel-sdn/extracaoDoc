{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <mark> <b> > 1. </b> Mapeamento e analise do  Pipeline </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1_map_analise_pipeline_v0.ipynb </b> |  Atual notebook com as funçoes de mapeamento e analise do pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import platform\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from urllib import response\n",
    "\n",
    "from outlook_msg import Message\n",
    "import extract_msg\n",
    "import zipfile\n",
    "from pyunpack import Archive\n",
    "import py7zr\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import PyPDF2\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "import locale\n",
    "import time, copy\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import cv2\n",
    "import fitz  # Módulo PyMuPDF\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import Image, ImageDraw\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar\n",
    "import matplotlib.pyplot as plt\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "\n",
    "# Modulos da solucao\n",
    "import modules.cronometro as cron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Config - Mapeamento e Analise do pipeline\n",
    "\n",
    "# 1. XXX Path para planilha de processamento de batches\n",
    "conf_export_plan_path = 'processamentos/processamento_batches/df_conf_export_batch.xlsx'\n",
    "\n",
    "# 2. XXX  Tratando nome de carga do df_processamento\n",
    "map_analise_path = \"processamentos/mapeamento_analise\"\n",
    "\n",
    "# 3. XXX  prefixo de nome do arquivo de exportaçao\n",
    "df_root_pipe_file = \"df_root_\"\n",
    "\n",
    "\n",
    "# 5. Path para documentos para extracao\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "\n",
    "\n",
    "#### Config - E-mail\n",
    "# 1. Caminho do arquivo uma mensagem especifica\n",
    "msg_dir_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails'\n",
    "\n",
    "# 2. Path para arquivos atachados compactados\n",
    "msg_attachment_zip = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/13_attachments'\n",
    "\n",
    "\n",
    "#### Config - messages\n",
    "# 3. Caminho do arquivo uma mensagem especifica\n",
    "msg_outros_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/12_messages'\n",
    "\n",
    "# 4. Path para arquivos recebidos manualmente\n",
    "arquivos_recebidos_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/14_documentos_recebidos'\n",
    "\n",
    "\n",
    "# 6. Path para gestao de imagens resized\n",
    "image_resized_path = \"pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas\"\n",
    "\n",
    "\n",
    "# 12. poppler path\n",
    "poppler_path = \"/home/dani-boy/miniconda3/envs/tables-detr/bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcoes para Analise do pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Geraçao do hash do arquivo\n",
    "def generate_file_hash(file_path):\n",
    "    # Abre o arquivo em modo de leitura de bytes\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # Lê o conteúdo do arquivo\n",
    "        file_data = f.read()\n",
    "        # Utiliza o algoritmo SHA-256 para gerar o hash\n",
    "        file_hash = hashlib.sha256(file_data).hexdigest()\n",
    "    return file_hash\n",
    "\n",
    "\n",
    "\n",
    "# 2. Geraçao do Unique_id do arquivo\n",
    "def generate_unique_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. XXX Busca proximo Batch\n",
    "def busca_proximo_batch():\n",
    "    # Abre o arquivo Excel e lê a coluna 'batch'\n",
    "    df = pd.read_excel(conf_export_plan_path, usecols=[\"batch\"])\n",
    "    # Pega o último valor da coluna 'batch'\n",
    "    last_value = df.iloc[-1, 0]\n",
    "    \n",
    "    # Extraí o número do último batch e adiciona 1 para o próximo\n",
    "    last_number = int(last_value.split(\"_\")[1])\n",
    "    next_number = last_number + 1\n",
    "    \n",
    "    # Forma o nome do próximo batch\n",
    "    next_batch = f\"Batch_{next_number}\"\n",
    "    \n",
    "    return next_batch    \n",
    "\n",
    "# 4. Função para verificar e criar a pasta se não existir\n",
    "def check_and_create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        \n",
    "# 7. funçao que MOVE documentos e gera add_log_transaction_entry para df_log_transctions\n",
    "def move_doc_processed_file(batch_name, src_path, tgt_path):\n",
    "    \n",
    "    function = \"move_doc_processed_file\"\n",
    "    source_path = src_path\n",
    "    file = os.path.basename(source_path)\n",
    "    sub_dir = os.path.join(tgt_path, batch_name)\n",
    "    destination_path = os.path.join(sub_dir, file)\n",
    "    document_action = \"move_processed_file\"\n",
    "    transaction_detail = (f'document {file} moved by: {function}')\n",
    "    df_move = pd.DataFrame()\n",
    "    try:\n",
    "        document_unique_id = get_document_id_by_file(batch_name, file)\n",
    "        check_and_create_folder(destination_path)\n",
    "        shutil.move(source_path, destination_path)\n",
    "        sucess = True\n",
    "        move_log = add_log_transaction_entry(document_unique_id, batch_name, file, document_action, src_path, tgt_path, transaction_detail, sucess)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao mover documento: {e}\")\n",
    "        sucess = False\n",
    "    \n",
    "    return move_log   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. XXX Analisa nro de paginas\n",
    "def analisa_nro_pages(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    pages = pdf_document.pages() # generator object\n",
    "\n",
    "    page_nro = []\n",
    "    for page in pages:\n",
    "        page_nro.append(page)\n",
    "        \n",
    "    nro_paginas = len(page_nro)    \n",
    "    if nro_paginas > 1:\n",
    "        doc_1_page = False\n",
    "        return doc_1_page, nro_paginas    \n",
    "    else:\n",
    "        doc_1_page = True\n",
    "        return doc_1_page, nro_paginas  \n",
    "    pdf_document.close()\n",
    "    \n",
    " \n",
    "\n",
    "# XXX FUNCAO DE SPLIT\n",
    "def split_documentos(qualquer_df, fase, atividade, status):\n",
    "    \n",
    "    documentos_splitados = []\n",
    "    doc_info = {}\n",
    "    rows_list = []\n",
    "    documentos = []\n",
    "    #output_dir = os.path.join(documentos_scan_path, batch_name)\n",
    "    num_linhas_df = qualquer_df.shape[0]\n",
    "\n",
    "    i = num_linhas_df + 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        message_erro = []\n",
    "        nun_pages = row['pages']\n",
    "        batch_name = row['batch']\n",
    "        original_file_name = row['original_file_name']\n",
    "        folder_name = row['directory']\n",
    "        file_path = row['file_path']\n",
    "        level = row['level']\n",
    "        document_type = row['document_type']\n",
    "        doc_action = row['doc_action']\n",
    "        document_unique_id = idx\n",
    "        new_level = level + 1\n",
    "        \n",
    "        if (doc_action == 'splitar') and (status == 'root_analise'):\n",
    "            if nun_pages > 1:\n",
    "                try:\n",
    "                    pdf = fitz.open(file_path)\n",
    "                    # Número total de páginas no PDF\n",
    "                    total_pages = len(pdf)\n",
    "                except Exception as e:\n",
    "                    print(f\"Nao congui abrir o PDF: {e}\")    \n",
    "\n",
    "                # Nome base para os arquivos de saída\n",
    "                base_name = file_path.split('.')[0]  # Remove a extensão do arquivo\n",
    "                file_to_delete = file_path\n",
    "                # Loop para criar um novo PDF para cada página\n",
    "                for page_num in range(total_pages):\n",
    "                    # Cria um novo objeto PDF\n",
    "                    new_pdf = fitz.open()\n",
    "                    # Adiciona a página atual ao novo PDF\n",
    "                    new_pdf.insert_pdf(pdf, from_page=page_num, to_page=page_num)\n",
    "                    # Nome do novo arquivo PDF\n",
    "                    new_pdf_name = f\"{base_name}_page_{page_num + 1}.pdf\"\n",
    "                    # Salva o novo PDF\n",
    "                    new_pdf.save(new_pdf_name)\n",
    "                    # Fecha o novo PDF\n",
    "                    new_pdf.close()\n",
    "                    rotulo = \"prov_nota_fiscal\"\n",
    "                    acao_sugerida = sugestoes_acao.get(rotulo, \"no_defined_action\")\n",
    "                    acao_executada = \"novo_doc_criado\"\n",
    "                    informations = (f'documento criado a partir do split do documento: {original_file_name}')  \n",
    "                    name_pdf_splited = os.path.basename(new_pdf_name)\n",
    "\n",
    "                    new_row = {\n",
    "                        \"seq\": i,\n",
    "                        \"date_time\": cron.timenow_pt_BR(),\n",
    "                        \"batch\": batch_name,\n",
    "                        \"fase_processo\": fase,\n",
    "                        \"nome_atividade\": atividade,\n",
    "                        \"status_documento\": status,\n",
    "                        \"acao_executada\": acao_executada,\n",
    "                        \"original_file_name\": new_pdf_name,\n",
    "                        \"directory\": folder_name,\n",
    "                        \"one_page\": True,\n",
    "                        \"pages\": 1,\n",
    "                        \"document_type\": rotulo,\n",
    "                        \"doc_action\": acao_sugerida,\n",
    "                        \"level\": level,\n",
    "                        \"document_unique_id\": generate_unique_id(),\n",
    "                        \"parent_document_unique_id\": document_unique_id,\n",
    "                        \"file_hash\": generate_file_hash(file_path),\n",
    "                        \"file_path\": file_path,\n",
    "                        \"informations\": informations,\n",
    "                    }\n",
    "                    rows_list.append(new_row)\n",
    "                    i += 1\n",
    "                qualquer_df.loc[idx, 'status_documento'] = \"NAO_PROCESSAR\" \n",
    "                qualquer_df.loc[idx, 'informations'] = \"Paginas splitada em multiplos documentos\" \n",
    "                qualquer_df.loc[idx, 'date_time'] = cron.timenow_pt_BR() \n",
    "    \n",
    "    total_split = i - 1        \n",
    "    df_split = pd.DataFrame(rows_list)\n",
    "    \n",
    "    \n",
    "    return df_split, rows_list\n",
    "\n",
    "\n",
    "# XXX Usando na criacao da imagem \n",
    "def conv_filename_no_ext(title):\n",
    "    \n",
    "    # Divida o título em nome e extensão (mas ignore a extensão)\n",
    "    name = title.rsplit('.', 1)[0] if '.' in title else title\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    return filename \n",
    "\n",
    "\n",
    "\n",
    "def apagar_zone(documentos_extracao_path):\n",
    "    # Para apagar arquivos PDF:Zone\n",
    "    for root, dirs, files in os.walk(documentos_extracao_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            #print(file)\n",
    "            if \":Zone\" in file:\n",
    "                file_to_delete = file_path\n",
    "                os.remove(file_to_delete)\n",
    "                #print(file, \"termina, pode eliminar\")\n",
    "                \n",
    "                \n",
    "def confirma_pdf_pequisavel(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "    # Definir retângulo de interesse\n",
    "    try:\n",
    "        x0 = 0\n",
    "        y0 = 4\n",
    "        x1 = 600\n",
    "        y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "        # Extrair texto dentro do retângulo\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        if text:\n",
    "            page_number = 0\n",
    "            pdf_pequisavel = True\n",
    "        #print(page_number)\n",
    "        else:\n",
    "            page_number = 1\n",
    "            pdf_pequisavel = False\n",
    "        #print(page_number)\n",
    "    except Exception as e:\n",
    "        msg_error = (f\"Erro ao abrir pagina do PDF: {e}\")\n",
    "        pdf_pequisavel = False\n",
    "        pdf_document.close()   \n",
    "         \n",
    "        return pdf_pequisavel\n",
    "                   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def define_rotulo_acao(nome_arquivo):\n",
    "    \n",
    "    for palavra_chave, rotulo in mapeamento_palavras_chave.items():\n",
    "        if palavra_chave.lower() in nome_arquivo.lower():\n",
    "            break\n",
    "    else:\n",
    "        rotulo = 'prov_nota_fiscal' #\"sem_rotulo\"\n",
    "        palavra_chave = 'default'\n",
    "        acao_sugerida = sugestoes_acao.get(rotulo, 'None')\n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "        # palavra_chave = 'None' #\"sem_palavra_chave\"\n",
    "        # acao_sugerida = 'None' #\"sem_acao_sugerida\"\n",
    "        \n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "        #print(f'nome_arquivo: {nome_arquivo} | rotulo: {rotulo}')\n",
    "    if rotulo != 'None': #\"sem_rotulo\"\n",
    "        acao_sugerida = sugestoes_acao.get(rotulo, 'None') # \"Ação não definida\"\n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Dicionário para mapear palavras-chave a rótulos\n",
    "mapeamento_palavras_chave = {\n",
    "    \"relatorio\": \"prov_relatorio\",\n",
    "    \"listagem\": \"prov_listagem\",\n",
    "    \"NF\": \"prov_nota_fiscal\",\n",
    "    \"nf\": \"prov_nota_fiscal\",\n",
    "    \"relatorio\": \"prov_listagem\",\n",
    "    \"sintetico\": \"prov_listagem\",\n",
    "    \"livro\": \"prov_livro_registro\",\n",
    "    \"sintético\": \"prov_listagem\",\n",
    "    \"nota\": \"prov_nota_fiscal\",\n",
    "    \"zip\": \"doc_zip\",\n",
    "    \"rar\": \"doc_rar\",\n",
    "    \"valores\": \"prov_dinheiro\",\n",
    "}\n",
    "\n",
    "# Dicionário mapeando rótulos a ações sugeridas\n",
    "sugestoes_acao = {\n",
    "    \"prov_relatorio\": \"NO_PROCESS\",\n",
    "    \"prov_listagem\": \"NO_PROCESS\",\n",
    "    \"prov_nota_fiscal\": \"NO_PROCESS\",\n",
    "    \"sem_rotulo\": \"MANUAL_REV\",\n",
    "    \"prov_livro_registro\": \"NO_PROCESS\",\n",
    "    \"doc_nao_pdf\": \"verificar\",\n",
    "    \"nao_pdf\": \"NO_PROCESS\",\n",
    "    \"doc_zip\": \"NO_PROCESS\",\n",
    "    \"pdf_mul_paginas\": \"SPLIT\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# 2.Testando\n",
    "nome_arquivo = 'batatinha_quando_nasce.pdf' # 'pre-processamento'\n",
    "#palavra_chave, rotulo, acao_sugerida = define_rotulo_acao(nome_arquivo, debug)\n",
    "#print(f'nome_arquivo: {nome_arquivo:>55} | palavra_chave: {palavra_chave:>20} | rotulo: {rotulo:20} | acao_sugerida: {acao_sugerida:30}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b> 3.0</b>  Mapeamento e Analise do pipeline </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOMENTE PARA TESTE DE FUNCAO\n",
    "fake_parent_document_unique_id = 'f976c128-1f41-4551-bffd-fac687c1c8b2'\n",
    "\n",
    "# Busca proximo Batch caso nao esteja rodando email\n",
    "batch_name = busca_proximo_batch()\n",
    "batch_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.XXX  Acao 1 - Ler todo o pipeline de documentos recebidos - ESSA E A UNICA FUNCAO QUE ITERA NO DIRETORIO\n",
    "def scan_pipeline_documentos(documentos_extracao_path, batch_name, fase, atividade, status):\n",
    "    \n",
    "    doc_info = {}\n",
    "    resumo = {}\n",
    "    raw_document = []\n",
    "    \n",
    "    output_dir = os.path.join(documentos_extracao_path, batch_name)\n",
    "    i = 1\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        folder_name = os.path.basename(root)\n",
    "        #print(folder_name)\n",
    "        for file in files:\n",
    "            nome_arquivo = file\n",
    "            palavra_chave, rotulo, acao_sugerida = define_rotulo_acao(nome_arquivo)\n",
    "            acao_executada = \"Analise\"\n",
    "            informations = ' '    \n",
    "            file_name = file.lower()    \n",
    "            file_path = os.path.join(root, file)\n",
    "            new_path_name = os.path.join(output_dir, file)\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                doc_one_page, nro_pgs = analisa_nro_pages(file_path)\n",
    "                one_page = doc_one_page\n",
    "                num_page = nro_pgs\n",
    "            else:\n",
    "                one_page = False\n",
    "                num_page = 0    \n",
    "            # if doc_one_page:\n",
    "            #     one_page = doc_one_page\n",
    "            # else:\n",
    "            #     one_page = False    \n",
    "            #             rotulo = 'pdf_mul_paginas'\n",
    "            level = 3\n",
    "            diretorio = os.path.basename(file_path)\n",
    "            if folder_name == batch_name:\n",
    "                folder_name = \"root_dir\"\n",
    "                \n",
    "            #print(f'nome_arquivo: {nome_arquivo:>55} | palavra_chave: {palavra_chave:>20} | rotulo: {rotulo:20} | acao_sugerida: {acao_sugerida:30}')    \n",
    "            \n",
    "            new_row = {\n",
    "                \"seq\": i,\n",
    "                \"date_time\": cron.timenow_pt_BR(),\n",
    "                \"batch\": batch_name,\n",
    "                \"fase_processo\": fase,\n",
    "                \"nome_atividade\": atividade,\n",
    "                \"status_documento\": status,\n",
    "                \"acao_executada\": acao_executada,\n",
    "                \"original_file_name\": file,\n",
    "                \"directory\": folder_name,\n",
    "                \"one_page\": one_page,\n",
    "                \"pages\": num_page,\n",
    "                \"palavra_chave\": palavra_chave,\n",
    "                \"document_tag\": rotulo,\n",
    "                \"action_item\": acao_sugerida,\n",
    "                \"level\": level,\n",
    "                \"document_unique_id\": generate_unique_id(),\n",
    "                \"parent_document_unique_id\": fake_parent_document_unique_id,\n",
    "                \"file_hash\": generate_file_hash(file_path),\n",
    "                \"file_path\": file_path,\n",
    "                \"informations\": informations,\n",
    "            }\n",
    "            raw_document.append(new_row)\n",
    "\n",
    "            \n",
    "            # print(f'seq: {i} | file: {file}'\n",
    "            i += 1\n",
    "    df_trans_pipe = pd.DataFrame(raw_document)\n",
    "      \n",
    "                \n",
    "    return df_trans_pipe, raw_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>date_time</th>\n",
       "      <th>batch</th>\n",
       "      <th>fase_processo</th>\n",
       "      <th>nome_atividade</th>\n",
       "      <th>status_documento</th>\n",
       "      <th>acao_executada</th>\n",
       "      <th>original_file_name</th>\n",
       "      <th>directory</th>\n",
       "      <th>one_page</th>\n",
       "      <th>pages</th>\n",
       "      <th>palavra_chave</th>\n",
       "      <th>document_tag</th>\n",
       "      <th>action_item</th>\n",
       "      <th>level</th>\n",
       "      <th>parent_document_unique_id</th>\n",
       "      <th>file_hash</th>\n",
       "      <th>file_path</th>\n",
       "      <th>informations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c4dc0fca-4edf-4547-96e3-26790aae2549</th>\n",
       "      <td>1</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NF CRJ PRIMEIRA QUINZENA DE JULHO DE 2023.pdf</td>\n",
       "      <td>11756286</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>b3acfffea4847108d0064ecbd62a73359961f88741a037...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789136f-6c11-4fe8-a27b-0e38714bbddf</th>\n",
       "      <td>2</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Nota Fiscal Eletrônica Quallit 24072023.pdf</td>\n",
       "      <td>11766341</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>nota</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>ae37292a66dff093838f3cde0da8bea332ec81241b1d5f...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5bed1512-08fc-4397-8a5c-90fe3b8c97f0</th>\n",
       "      <td>3</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NFE 20237.pdf</td>\n",
       "      <td>11624359</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>1976ffe84e27b6bb1a5840018c4a47f6bac483b10f686a...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>db839bcc-55b5-4890-8259-01c447b7b5b5</th>\n",
       "      <td>4</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Ampla.pdf</td>\n",
       "      <td>11777624</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>default</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>ad2522b53349fffd748376bca7fe4b90fd59359971d992...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f41c0878-079b-4520-9168-b376391f2e85</th>\n",
       "      <td>5</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Supernova.pdf</td>\n",
       "      <td>11777624</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>default</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>c26954dad71d508d4b5315f69dc4c6291a1c5bf70548e7...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4ea208e-caaf-41a6-924c-01b02ed0d147</th>\n",
       "      <td>6</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Blue Lord.pdf</td>\n",
       "      <td>11777624</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>default</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>63b6603be7b1df359e67b96e9e128336b46fa3fbcfdadc...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff641182-55a6-496c-be94-d36c212a8518</th>\n",
       "      <td>7</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NF 2023158.pdf</td>\n",
       "      <td>11285853</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>abcfcce04cabd261b30730f04cbb98e030f2a765bb31aa...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b5636ddc-4bb8-4e8a-a0de-867548fd50a5</th>\n",
       "      <td>8</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NF 2023157.pdf</td>\n",
       "      <td>11285853</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>c8c5ae8ccca5e9befb6445e21701dc99266746e877c630...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9bc6aaf8-e05a-49d6-9642-8f44eb69ea95</th>\n",
       "      <td>9</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>nota_07_2023.pdf</td>\n",
       "      <td>11778425</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>nota</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>33bc08587f5c227ab3e035011ed9b20340f99599e65ff7...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d1dc634-8e1f-448d-a3b7-c36a5a678e01</th>\n",
       "      <td>10</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>nf 59.pdf</td>\n",
       "      <td>11359989</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>59274230e7fb7897397bb426ac31d953e2a14def1fd25e...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7fcaf5c2-9b82-4db6-adab-28a6330d836d</th>\n",
       "      <td>11</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>nf 63.pdf</td>\n",
       "      <td>11359989</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>fb05f10a57aaffd529244e962ea6d2dfba001045869b54...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4185b903-3ec1-42e7-8918-2654cf60445c</th>\n",
       "      <td>12</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>nf 60.pdf</td>\n",
       "      <td>11359989</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>5acdb785ad22a4d4fbd2e5b99de2e86300a34e460d4b27...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>af3c7eda-d4a5-4824-a653-dcdf3e355b2c</th>\n",
       "      <td>13</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>nf 58.pdf</td>\n",
       "      <td>11359989</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>d41c76becd4476eb626b63787adb9aaabfbbf08f182612...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1b3ea90b-091f-4bdd-b8c9-7eaf77205c1d</th>\n",
       "      <td>14</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>nf 65.pdf</td>\n",
       "      <td>11359989</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>58f15df43aaaf86ae15e9b4c2869dc236814001385440c...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78cdd67d-260b-4c42-8743-56b8c99705e0</th>\n",
       "      <td>15</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>nf 62.pdf</td>\n",
       "      <td>11359989</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>07edad2d23ed32fbe526111b0dd0c47de7bd9c387a0ea7...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8d624056-939f-493f-9935-34f47cc9a15c</th>\n",
       "      <td>16</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>nf 61.pdf</td>\n",
       "      <td>11359989</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>5514dfbdf73d25069675e75f61c32381e5035a28876635...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56ab9d8f-8a7c-4c49-88ee-5efe75009166</th>\n",
       "      <td>17</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>nf 64.pdf</td>\n",
       "      <td>11359989</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>92b195edf2b2f1b8f08bf22260c231a372c93499f9faba...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d1a1a993-8e78-4eed-9a07-cac03c00cca2</th>\n",
       "      <td>18</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NFS-e 22.pdf</td>\n",
       "      <td>11778003</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>0039c12e1e972817f623e70ce05f54cb037a6a7e55cc3a...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35c0af62-e9ee-4283-9cd1-fe188e505d0b</th>\n",
       "      <td>19</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NFSe-e 23.pdf</td>\n",
       "      <td>11778003</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>940b1ffaf961f72198f07d10909177a62706acf88aa333...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2e2b9dfd-4d7e-42f6-88b3-76ee3e5fcdd5</th>\n",
       "      <td>20</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>5CBB9967-367A-42EE-BCD8-A25F161906E3.PDF</td>\n",
       "      <td>11674905</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>default</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>608adb56abc1d2229cf2930aeca0a2f4a04290474b1c71...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646854db-84d8-48a1-b72a-64df293230b6</th>\n",
       "      <td>21</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NF 202315- SJDI 35 JUL 23.pdf</td>\n",
       "      <td>11777556</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>c0fd28f51a6eb88508566d880c95d8c99490067af70311...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86930a7e-07b0-4995-aa39-f94555e110a9</th>\n",
       "      <td>22</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>resposta.PDF</td>\n",
       "      <td>11779053</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>default</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>c0c55e14c885f423b5cc49a32202e90e1e58bfec85a48e...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dc4ac4dc-d547-4f63-aa67-48481ef250ca</th>\n",
       "      <td>23</td>\n",
       "      <td>23/09/2023 12:52:11</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Heidelberg 21 07 2023 NOTA FISCAL.pdf</td>\n",
       "      <td>11779531</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>nota</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>4073b25641024505cd6a5b7052266f1c494acfe517fcb1...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      seq            date_time     batch  \\\n",
       "document_unique_id                                                         \n",
       "c4dc0fca-4edf-4547-96e3-26790aae2549    1  23/09/2023 12:52:11  Batch_21   \n",
       "1789136f-6c11-4fe8-a27b-0e38714bbddf    2  23/09/2023 12:52:11  Batch_21   \n",
       "5bed1512-08fc-4397-8a5c-90fe3b8c97f0    3  23/09/2023 12:52:11  Batch_21   \n",
       "db839bcc-55b5-4890-8259-01c447b7b5b5    4  23/09/2023 12:52:11  Batch_21   \n",
       "f41c0878-079b-4520-9168-b376391f2e85    5  23/09/2023 12:52:11  Batch_21   \n",
       "d4ea208e-caaf-41a6-924c-01b02ed0d147    6  23/09/2023 12:52:11  Batch_21   \n",
       "ff641182-55a6-496c-be94-d36c212a8518    7  23/09/2023 12:52:11  Batch_21   \n",
       "b5636ddc-4bb8-4e8a-a0de-867548fd50a5    8  23/09/2023 12:52:11  Batch_21   \n",
       "9bc6aaf8-e05a-49d6-9642-8f44eb69ea95    9  23/09/2023 12:52:11  Batch_21   \n",
       "3d1dc634-8e1f-448d-a3b7-c36a5a678e01   10  23/09/2023 12:52:11  Batch_21   \n",
       "7fcaf5c2-9b82-4db6-adab-28a6330d836d   11  23/09/2023 12:52:11  Batch_21   \n",
       "4185b903-3ec1-42e7-8918-2654cf60445c   12  23/09/2023 12:52:11  Batch_21   \n",
       "af3c7eda-d4a5-4824-a653-dcdf3e355b2c   13  23/09/2023 12:52:11  Batch_21   \n",
       "1b3ea90b-091f-4bdd-b8c9-7eaf77205c1d   14  23/09/2023 12:52:11  Batch_21   \n",
       "78cdd67d-260b-4c42-8743-56b8c99705e0   15  23/09/2023 12:52:11  Batch_21   \n",
       "8d624056-939f-493f-9935-34f47cc9a15c   16  23/09/2023 12:52:11  Batch_21   \n",
       "56ab9d8f-8a7c-4c49-88ee-5efe75009166   17  23/09/2023 12:52:11  Batch_21   \n",
       "d1a1a993-8e78-4eed-9a07-cac03c00cca2   18  23/09/2023 12:52:11  Batch_21   \n",
       "35c0af62-e9ee-4283-9cd1-fe188e505d0b   19  23/09/2023 12:52:11  Batch_21   \n",
       "2e2b9dfd-4d7e-42f6-88b3-76ee3e5fcdd5   20  23/09/2023 12:52:11  Batch_21   \n",
       "646854db-84d8-48a1-b72a-64df293230b6   21  23/09/2023 12:52:11  Batch_21   \n",
       "86930a7e-07b0-4995-aa39-f94555e110a9   22  23/09/2023 12:52:11  Batch_21   \n",
       "dc4ac4dc-d547-4f63-aa67-48481ef250ca   23  23/09/2023 12:52:11  Batch_21   \n",
       "\n",
       "                                     fase_processo nome_atividade  \\\n",
       "document_unique_id                                                  \n",
       "c4dc0fca-4edf-4547-96e3-26790aae2549       analise   scan_analise   \n",
       "1789136f-6c11-4fe8-a27b-0e38714bbddf       analise   scan_analise   \n",
       "5bed1512-08fc-4397-8a5c-90fe3b8c97f0       analise   scan_analise   \n",
       "db839bcc-55b5-4890-8259-01c447b7b5b5       analise   scan_analise   \n",
       "f41c0878-079b-4520-9168-b376391f2e85       analise   scan_analise   \n",
       "d4ea208e-caaf-41a6-924c-01b02ed0d147       analise   scan_analise   \n",
       "ff641182-55a6-496c-be94-d36c212a8518       analise   scan_analise   \n",
       "b5636ddc-4bb8-4e8a-a0de-867548fd50a5       analise   scan_analise   \n",
       "9bc6aaf8-e05a-49d6-9642-8f44eb69ea95       analise   scan_analise   \n",
       "3d1dc634-8e1f-448d-a3b7-c36a5a678e01       analise   scan_analise   \n",
       "7fcaf5c2-9b82-4db6-adab-28a6330d836d       analise   scan_analise   \n",
       "4185b903-3ec1-42e7-8918-2654cf60445c       analise   scan_analise   \n",
       "af3c7eda-d4a5-4824-a653-dcdf3e355b2c       analise   scan_analise   \n",
       "1b3ea90b-091f-4bdd-b8c9-7eaf77205c1d       analise   scan_analise   \n",
       "78cdd67d-260b-4c42-8743-56b8c99705e0       analise   scan_analise   \n",
       "8d624056-939f-493f-9935-34f47cc9a15c       analise   scan_analise   \n",
       "56ab9d8f-8a7c-4c49-88ee-5efe75009166       analise   scan_analise   \n",
       "d1a1a993-8e78-4eed-9a07-cac03c00cca2       analise   scan_analise   \n",
       "35c0af62-e9ee-4283-9cd1-fe188e505d0b       analise   scan_analise   \n",
       "2e2b9dfd-4d7e-42f6-88b3-76ee3e5fcdd5       analise   scan_analise   \n",
       "646854db-84d8-48a1-b72a-64df293230b6       analise   scan_analise   \n",
       "86930a7e-07b0-4995-aa39-f94555e110a9       analise   scan_analise   \n",
       "dc4ac4dc-d547-4f63-aa67-48481ef250ca       analise   scan_analise   \n",
       "\n",
       "                                     status_documento acao_executada  \\\n",
       "document_unique_id                                                     \n",
       "c4dc0fca-4edf-4547-96e3-26790aae2549     root_analise        Analise   \n",
       "1789136f-6c11-4fe8-a27b-0e38714bbddf     root_analise        Analise   \n",
       "5bed1512-08fc-4397-8a5c-90fe3b8c97f0     root_analise        Analise   \n",
       "db839bcc-55b5-4890-8259-01c447b7b5b5     root_analise        Analise   \n",
       "f41c0878-079b-4520-9168-b376391f2e85     root_analise        Analise   \n",
       "d4ea208e-caaf-41a6-924c-01b02ed0d147     root_analise        Analise   \n",
       "ff641182-55a6-496c-be94-d36c212a8518     root_analise        Analise   \n",
       "b5636ddc-4bb8-4e8a-a0de-867548fd50a5     root_analise        Analise   \n",
       "9bc6aaf8-e05a-49d6-9642-8f44eb69ea95     root_analise        Analise   \n",
       "3d1dc634-8e1f-448d-a3b7-c36a5a678e01     root_analise        Analise   \n",
       "7fcaf5c2-9b82-4db6-adab-28a6330d836d     root_analise        Analise   \n",
       "4185b903-3ec1-42e7-8918-2654cf60445c     root_analise        Analise   \n",
       "af3c7eda-d4a5-4824-a653-dcdf3e355b2c     root_analise        Analise   \n",
       "1b3ea90b-091f-4bdd-b8c9-7eaf77205c1d     root_analise        Analise   \n",
       "78cdd67d-260b-4c42-8743-56b8c99705e0     root_analise        Analise   \n",
       "8d624056-939f-493f-9935-34f47cc9a15c     root_analise        Analise   \n",
       "56ab9d8f-8a7c-4c49-88ee-5efe75009166     root_analise        Analise   \n",
       "d1a1a993-8e78-4eed-9a07-cac03c00cca2     root_analise        Analise   \n",
       "35c0af62-e9ee-4283-9cd1-fe188e505d0b     root_analise        Analise   \n",
       "2e2b9dfd-4d7e-42f6-88b3-76ee3e5fcdd5     root_analise        Analise   \n",
       "646854db-84d8-48a1-b72a-64df293230b6     root_analise        Analise   \n",
       "86930a7e-07b0-4995-aa39-f94555e110a9     root_analise        Analise   \n",
       "dc4ac4dc-d547-4f63-aa67-48481ef250ca     root_analise        Analise   \n",
       "\n",
       "                                                                 original_file_name  \\\n",
       "document_unique_id                                                                    \n",
       "c4dc0fca-4edf-4547-96e3-26790aae2549  NF CRJ PRIMEIRA QUINZENA DE JULHO DE 2023.pdf   \n",
       "1789136f-6c11-4fe8-a27b-0e38714bbddf    Nota Fiscal Eletrônica Quallit 24072023.pdf   \n",
       "5bed1512-08fc-4397-8a5c-90fe3b8c97f0                                  NFE 20237.pdf   \n",
       "db839bcc-55b5-4890-8259-01c447b7b5b5                                      Ampla.pdf   \n",
       "f41c0878-079b-4520-9168-b376391f2e85                                  Supernova.pdf   \n",
       "d4ea208e-caaf-41a6-924c-01b02ed0d147                                  Blue Lord.pdf   \n",
       "ff641182-55a6-496c-be94-d36c212a8518                                 NF 2023158.pdf   \n",
       "b5636ddc-4bb8-4e8a-a0de-867548fd50a5                                 NF 2023157.pdf   \n",
       "9bc6aaf8-e05a-49d6-9642-8f44eb69ea95                               nota_07_2023.pdf   \n",
       "3d1dc634-8e1f-448d-a3b7-c36a5a678e01                                      nf 59.pdf   \n",
       "7fcaf5c2-9b82-4db6-adab-28a6330d836d                                      nf 63.pdf   \n",
       "4185b903-3ec1-42e7-8918-2654cf60445c                                      nf 60.pdf   \n",
       "af3c7eda-d4a5-4824-a653-dcdf3e355b2c                                      nf 58.pdf   \n",
       "1b3ea90b-091f-4bdd-b8c9-7eaf77205c1d                                      nf 65.pdf   \n",
       "78cdd67d-260b-4c42-8743-56b8c99705e0                                      nf 62.pdf   \n",
       "8d624056-939f-493f-9935-34f47cc9a15c                                      nf 61.pdf   \n",
       "56ab9d8f-8a7c-4c49-88ee-5efe75009166                                      nf 64.pdf   \n",
       "d1a1a993-8e78-4eed-9a07-cac03c00cca2                                   NFS-e 22.pdf   \n",
       "35c0af62-e9ee-4283-9cd1-fe188e505d0b                                  NFSe-e 23.pdf   \n",
       "2e2b9dfd-4d7e-42f6-88b3-76ee3e5fcdd5       5CBB9967-367A-42EE-BCD8-A25F161906E3.PDF   \n",
       "646854db-84d8-48a1-b72a-64df293230b6                  NF 202315- SJDI 35 JUL 23.pdf   \n",
       "86930a7e-07b0-4995-aa39-f94555e110a9                                   resposta.PDF   \n",
       "dc4ac4dc-d547-4f63-aa67-48481ef250ca          Heidelberg 21 07 2023 NOTA FISCAL.pdf   \n",
       "\n",
       "                                     directory  one_page  pages palavra_chave  \\\n",
       "document_unique_id                                                              \n",
       "c4dc0fca-4edf-4547-96e3-26790aae2549  11756286      True      1            NF   \n",
       "1789136f-6c11-4fe8-a27b-0e38714bbddf  11766341      True      1          nota   \n",
       "5bed1512-08fc-4397-8a5c-90fe3b8c97f0  11624359      True      1            NF   \n",
       "db839bcc-55b5-4890-8259-01c447b7b5b5  11777624      True      1       default   \n",
       "f41c0878-079b-4520-9168-b376391f2e85  11777624      True      1       default   \n",
       "d4ea208e-caaf-41a6-924c-01b02ed0d147  11777624      True      1       default   \n",
       "ff641182-55a6-496c-be94-d36c212a8518  11285853      True      1            NF   \n",
       "b5636ddc-4bb8-4e8a-a0de-867548fd50a5  11285853      True      1            NF   \n",
       "9bc6aaf8-e05a-49d6-9642-8f44eb69ea95  11778425      True      1          nota   \n",
       "3d1dc634-8e1f-448d-a3b7-c36a5a678e01  11359989      True      1            NF   \n",
       "7fcaf5c2-9b82-4db6-adab-28a6330d836d  11359989      True      1            NF   \n",
       "4185b903-3ec1-42e7-8918-2654cf60445c  11359989      True      1            NF   \n",
       "af3c7eda-d4a5-4824-a653-dcdf3e355b2c  11359989      True      1            NF   \n",
       "1b3ea90b-091f-4bdd-b8c9-7eaf77205c1d  11359989      True      1            NF   \n",
       "78cdd67d-260b-4c42-8743-56b8c99705e0  11359989      True      1            NF   \n",
       "8d624056-939f-493f-9935-34f47cc9a15c  11359989      True      1            NF   \n",
       "56ab9d8f-8a7c-4c49-88ee-5efe75009166  11359989      True      1            NF   \n",
       "d1a1a993-8e78-4eed-9a07-cac03c00cca2  11778003      True      1            NF   \n",
       "35c0af62-e9ee-4283-9cd1-fe188e505d0b  11778003      True      1            NF   \n",
       "2e2b9dfd-4d7e-42f6-88b3-76ee3e5fcdd5  11674905     False      7       default   \n",
       "646854db-84d8-48a1-b72a-64df293230b6  11777556      True      1            NF   \n",
       "86930a7e-07b0-4995-aa39-f94555e110a9  11779053      True      1       default   \n",
       "dc4ac4dc-d547-4f63-aa67-48481ef250ca  11779531      True      1          nota   \n",
       "\n",
       "                                          document_tag action_item  level  \\\n",
       "document_unique_id                                                          \n",
       "c4dc0fca-4edf-4547-96e3-26790aae2549  prov_nota_fiscal  NO_PROCESS      3   \n",
       "1789136f-6c11-4fe8-a27b-0e38714bbddf  prov_nota_fiscal  NO_PROCESS      3   \n",
       "5bed1512-08fc-4397-8a5c-90fe3b8c97f0  prov_nota_fiscal  NO_PROCESS      3   \n",
       "db839bcc-55b5-4890-8259-01c447b7b5b5  prov_nota_fiscal  NO_PROCESS      3   \n",
       "f41c0878-079b-4520-9168-b376391f2e85  prov_nota_fiscal  NO_PROCESS      3   \n",
       "d4ea208e-caaf-41a6-924c-01b02ed0d147  prov_nota_fiscal  NO_PROCESS      3   \n",
       "ff641182-55a6-496c-be94-d36c212a8518  prov_nota_fiscal  NO_PROCESS      3   \n",
       "b5636ddc-4bb8-4e8a-a0de-867548fd50a5  prov_nota_fiscal  NO_PROCESS      3   \n",
       "9bc6aaf8-e05a-49d6-9642-8f44eb69ea95  prov_nota_fiscal  NO_PROCESS      3   \n",
       "3d1dc634-8e1f-448d-a3b7-c36a5a678e01  prov_nota_fiscal  NO_PROCESS      3   \n",
       "7fcaf5c2-9b82-4db6-adab-28a6330d836d  prov_nota_fiscal  NO_PROCESS      3   \n",
       "4185b903-3ec1-42e7-8918-2654cf60445c  prov_nota_fiscal  NO_PROCESS      3   \n",
       "af3c7eda-d4a5-4824-a653-dcdf3e355b2c  prov_nota_fiscal  NO_PROCESS      3   \n",
       "1b3ea90b-091f-4bdd-b8c9-7eaf77205c1d  prov_nota_fiscal  NO_PROCESS      3   \n",
       "78cdd67d-260b-4c42-8743-56b8c99705e0  prov_nota_fiscal  NO_PROCESS      3   \n",
       "8d624056-939f-493f-9935-34f47cc9a15c  prov_nota_fiscal  NO_PROCESS      3   \n",
       "56ab9d8f-8a7c-4c49-88ee-5efe75009166  prov_nota_fiscal  NO_PROCESS      3   \n",
       "d1a1a993-8e78-4eed-9a07-cac03c00cca2  prov_nota_fiscal  NO_PROCESS      3   \n",
       "35c0af62-e9ee-4283-9cd1-fe188e505d0b  prov_nota_fiscal  NO_PROCESS      3   \n",
       "2e2b9dfd-4d7e-42f6-88b3-76ee3e5fcdd5  prov_nota_fiscal  NO_PROCESS      3   \n",
       "646854db-84d8-48a1-b72a-64df293230b6  prov_nota_fiscal  NO_PROCESS      3   \n",
       "86930a7e-07b0-4995-aa39-f94555e110a9  prov_nota_fiscal  NO_PROCESS      3   \n",
       "dc4ac4dc-d547-4f63-aa67-48481ef250ca  prov_nota_fiscal  NO_PROCESS      3   \n",
       "\n",
       "                                                 parent_document_unique_id  \\\n",
       "document_unique_id                                                           \n",
       "c4dc0fca-4edf-4547-96e3-26790aae2549  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "1789136f-6c11-4fe8-a27b-0e38714bbddf  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "5bed1512-08fc-4397-8a5c-90fe3b8c97f0  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "db839bcc-55b5-4890-8259-01c447b7b5b5  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "f41c0878-079b-4520-9168-b376391f2e85  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "d4ea208e-caaf-41a6-924c-01b02ed0d147  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "ff641182-55a6-496c-be94-d36c212a8518  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "b5636ddc-4bb8-4e8a-a0de-867548fd50a5  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "9bc6aaf8-e05a-49d6-9642-8f44eb69ea95  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "3d1dc634-8e1f-448d-a3b7-c36a5a678e01  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "7fcaf5c2-9b82-4db6-adab-28a6330d836d  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "4185b903-3ec1-42e7-8918-2654cf60445c  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "af3c7eda-d4a5-4824-a653-dcdf3e355b2c  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "1b3ea90b-091f-4bdd-b8c9-7eaf77205c1d  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "78cdd67d-260b-4c42-8743-56b8c99705e0  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "8d624056-939f-493f-9935-34f47cc9a15c  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "56ab9d8f-8a7c-4c49-88ee-5efe75009166  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "d1a1a993-8e78-4eed-9a07-cac03c00cca2  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "35c0af62-e9ee-4283-9cd1-fe188e505d0b  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "2e2b9dfd-4d7e-42f6-88b3-76ee3e5fcdd5  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "646854db-84d8-48a1-b72a-64df293230b6  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "86930a7e-07b0-4995-aa39-f94555e110a9  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "dc4ac4dc-d547-4f63-aa67-48481ef250ca  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "\n",
       "                                                                              file_hash  \\\n",
       "document_unique_id                                                                        \n",
       "c4dc0fca-4edf-4547-96e3-26790aae2549  b3acfffea4847108d0064ecbd62a73359961f88741a037...   \n",
       "1789136f-6c11-4fe8-a27b-0e38714bbddf  ae37292a66dff093838f3cde0da8bea332ec81241b1d5f...   \n",
       "5bed1512-08fc-4397-8a5c-90fe3b8c97f0  1976ffe84e27b6bb1a5840018c4a47f6bac483b10f686a...   \n",
       "db839bcc-55b5-4890-8259-01c447b7b5b5  ad2522b53349fffd748376bca7fe4b90fd59359971d992...   \n",
       "f41c0878-079b-4520-9168-b376391f2e85  c26954dad71d508d4b5315f69dc4c6291a1c5bf70548e7...   \n",
       "d4ea208e-caaf-41a6-924c-01b02ed0d147  63b6603be7b1df359e67b96e9e128336b46fa3fbcfdadc...   \n",
       "ff641182-55a6-496c-be94-d36c212a8518  abcfcce04cabd261b30730f04cbb98e030f2a765bb31aa...   \n",
       "b5636ddc-4bb8-4e8a-a0de-867548fd50a5  c8c5ae8ccca5e9befb6445e21701dc99266746e877c630...   \n",
       "9bc6aaf8-e05a-49d6-9642-8f44eb69ea95  33bc08587f5c227ab3e035011ed9b20340f99599e65ff7...   \n",
       "3d1dc634-8e1f-448d-a3b7-c36a5a678e01  59274230e7fb7897397bb426ac31d953e2a14def1fd25e...   \n",
       "7fcaf5c2-9b82-4db6-adab-28a6330d836d  fb05f10a57aaffd529244e962ea6d2dfba001045869b54...   \n",
       "4185b903-3ec1-42e7-8918-2654cf60445c  5acdb785ad22a4d4fbd2e5b99de2e86300a34e460d4b27...   \n",
       "af3c7eda-d4a5-4824-a653-dcdf3e355b2c  d41c76becd4476eb626b63787adb9aaabfbbf08f182612...   \n",
       "1b3ea90b-091f-4bdd-b8c9-7eaf77205c1d  58f15df43aaaf86ae15e9b4c2869dc236814001385440c...   \n",
       "78cdd67d-260b-4c42-8743-56b8c99705e0  07edad2d23ed32fbe526111b0dd0c47de7bd9c387a0ea7...   \n",
       "8d624056-939f-493f-9935-34f47cc9a15c  5514dfbdf73d25069675e75f61c32381e5035a28876635...   \n",
       "56ab9d8f-8a7c-4c49-88ee-5efe75009166  92b195edf2b2f1b8f08bf22260c231a372c93499f9faba...   \n",
       "d1a1a993-8e78-4eed-9a07-cac03c00cca2  0039c12e1e972817f623e70ce05f54cb037a6a7e55cc3a...   \n",
       "35c0af62-e9ee-4283-9cd1-fe188e505d0b  940b1ffaf961f72198f07d10909177a62706acf88aa333...   \n",
       "2e2b9dfd-4d7e-42f6-88b3-76ee3e5fcdd5  608adb56abc1d2229cf2930aeca0a2f4a04290474b1c71...   \n",
       "646854db-84d8-48a1-b72a-64df293230b6  c0fd28f51a6eb88508566d880c95d8c99490067af70311...   \n",
       "86930a7e-07b0-4995-aa39-f94555e110a9  c0c55e14c885f423b5cc49a32202e90e1e58bfec85a48e...   \n",
       "dc4ac4dc-d547-4f63-aa67-48481ef250ca  4073b25641024505cd6a5b7052266f1c494acfe517fcb1...   \n",
       "\n",
       "                                                                              file_path  \\\n",
       "document_unique_id                                                                        \n",
       "c4dc0fca-4edf-4547-96e3-26790aae2549  pipeline_extracao_documentos/2_documentos_para...   \n",
       "1789136f-6c11-4fe8-a27b-0e38714bbddf  pipeline_extracao_documentos/2_documentos_para...   \n",
       "5bed1512-08fc-4397-8a5c-90fe3b8c97f0  pipeline_extracao_documentos/2_documentos_para...   \n",
       "db839bcc-55b5-4890-8259-01c447b7b5b5  pipeline_extracao_documentos/2_documentos_para...   \n",
       "f41c0878-079b-4520-9168-b376391f2e85  pipeline_extracao_documentos/2_documentos_para...   \n",
       "d4ea208e-caaf-41a6-924c-01b02ed0d147  pipeline_extracao_documentos/2_documentos_para...   \n",
       "ff641182-55a6-496c-be94-d36c212a8518  pipeline_extracao_documentos/2_documentos_para...   \n",
       "b5636ddc-4bb8-4e8a-a0de-867548fd50a5  pipeline_extracao_documentos/2_documentos_para...   \n",
       "9bc6aaf8-e05a-49d6-9642-8f44eb69ea95  pipeline_extracao_documentos/2_documentos_para...   \n",
       "3d1dc634-8e1f-448d-a3b7-c36a5a678e01  pipeline_extracao_documentos/2_documentos_para...   \n",
       "7fcaf5c2-9b82-4db6-adab-28a6330d836d  pipeline_extracao_documentos/2_documentos_para...   \n",
       "4185b903-3ec1-42e7-8918-2654cf60445c  pipeline_extracao_documentos/2_documentos_para...   \n",
       "af3c7eda-d4a5-4824-a653-dcdf3e355b2c  pipeline_extracao_documentos/2_documentos_para...   \n",
       "1b3ea90b-091f-4bdd-b8c9-7eaf77205c1d  pipeline_extracao_documentos/2_documentos_para...   \n",
       "78cdd67d-260b-4c42-8743-56b8c99705e0  pipeline_extracao_documentos/2_documentos_para...   \n",
       "8d624056-939f-493f-9935-34f47cc9a15c  pipeline_extracao_documentos/2_documentos_para...   \n",
       "56ab9d8f-8a7c-4c49-88ee-5efe75009166  pipeline_extracao_documentos/2_documentos_para...   \n",
       "d1a1a993-8e78-4eed-9a07-cac03c00cca2  pipeline_extracao_documentos/2_documentos_para...   \n",
       "35c0af62-e9ee-4283-9cd1-fe188e505d0b  pipeline_extracao_documentos/2_documentos_para...   \n",
       "2e2b9dfd-4d7e-42f6-88b3-76ee3e5fcdd5  pipeline_extracao_documentos/2_documentos_para...   \n",
       "646854db-84d8-48a1-b72a-64df293230b6  pipeline_extracao_documentos/2_documentos_para...   \n",
       "86930a7e-07b0-4995-aa39-f94555e110a9  pipeline_extracao_documentos/2_documentos_para...   \n",
       "dc4ac4dc-d547-4f63-aa67-48481ef250ca  pipeline_extracao_documentos/2_documentos_para...   \n",
       "\n",
       "                                     informations  \n",
       "document_unique_id                                 \n",
       "c4dc0fca-4edf-4547-96e3-26790aae2549               \n",
       "1789136f-6c11-4fe8-a27b-0e38714bbddf               \n",
       "5bed1512-08fc-4397-8a5c-90fe3b8c97f0               \n",
       "db839bcc-55b5-4890-8259-01c447b7b5b5               \n",
       "f41c0878-079b-4520-9168-b376391f2e85               \n",
       "d4ea208e-caaf-41a6-924c-01b02ed0d147               \n",
       "ff641182-55a6-496c-be94-d36c212a8518               \n",
       "b5636ddc-4bb8-4e8a-a0de-867548fd50a5               \n",
       "9bc6aaf8-e05a-49d6-9642-8f44eb69ea95               \n",
       "3d1dc634-8e1f-448d-a3b7-c36a5a678e01               \n",
       "7fcaf5c2-9b82-4db6-adab-28a6330d836d               \n",
       "4185b903-3ec1-42e7-8918-2654cf60445c               \n",
       "af3c7eda-d4a5-4824-a653-dcdf3e355b2c               \n",
       "1b3ea90b-091f-4bdd-b8c9-7eaf77205c1d               \n",
       "78cdd67d-260b-4c42-8743-56b8c99705e0               \n",
       "8d624056-939f-493f-9935-34f47cc9a15c               \n",
       "56ab9d8f-8a7c-4c49-88ee-5efe75009166               \n",
       "d1a1a993-8e78-4eed-9a07-cac03c00cca2               \n",
       "35c0af62-e9ee-4283-9cd1-fe188e505d0b               \n",
       "2e2b9dfd-4d7e-42f6-88b3-76ee3e5fcdd5               \n",
       "646854db-84d8-48a1-b72a-64df293230b6               \n",
       "86930a7e-07b0-4995-aa39-f94555e110a9               \n",
       "dc4ac4dc-d547-4f63-aa67-48481ef250ca               "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. XXX Crio o DF df_scan_pipe\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'scan_analise'\n",
    "status = 'root_analise'\n",
    "\n",
    "documentos = []\n",
    "\n",
    "#df_trans_pipe = pd.DataFrame()\n",
    "\n",
    "df_root_pipe, documentos = scan_pipeline_documentos(documentos_extracao_path, batch_name, fase, atividade, status)\n",
    "\n",
    "\n",
    "# 0. Acertando o Index\n",
    "df_root_pipe.set_index('document_unique_id', inplace=True)\n",
    "df_root_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulando od DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "df_root_pipe.query('one_page == False & palavra_chave == \"sem_palavra_chave\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. XXX Usando loc para Filtrar Baseado em one_page == False\n",
    "df_pages_2_split = df_root_pipe[df_root_pipe['one_page'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. XXX SE deseja importar o DF df_analise_pipe\n",
    "\n",
    "df_analise_pipe_path = \"Em_HOLD/df_mapeamento_e_analise2.xlsx\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria df_documento_recebido\n",
    "df_analise_pipe = pd.read_excel(df_analise_pipe_path)\n",
    "\n",
    "# Ajusta o indice\n",
    "df_analise_pipe.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2B. XXX Efetuo a analise do pipeline de documentos e inicio da extracao de dados\n",
    "# analisar_pdf_pesquisavel\n",
    "fase = 'analise' # 'pre-processamento'\n",
    "atividade = 'Reavaliar_PDF_Pesquisavel' #'pesquisar_prefeitura' # pesquisar_prefeitura  pesquisar_modelo    mensagem_status = \"Reavaliar_PDF_Pesquisavel\"\n",
    "status = 'mapear'\n",
    "\n",
    "imagens_list = analise_extracao_pipeline(subset_df_analise_pipe, df_analise_pipe, fase, atividade, status)\n",
    "\n",
    "\n",
    "if imagens_list:\n",
    "    remove_images(imagens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usando query para Filtrar Baseado em Condições Complexas\n",
    "subset_df_analise_pipe = df_analise_pipe.query('seq == 59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Salvando o DF (IMPORTANTE)\n",
    "df_analise_pipe.to_excel(\"df_mapeamento_e_analise2.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Criando uma nova coluna no DF\n",
    "df_analise_pipe.insert(loc=17, column='s_act', value=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exportando o map_analise_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Definiçao do path para salvar o arquivo\n",
    "file_path_root_pipe = os.path.join(map_analise_path, df_root_pipe_file + batch_name + \".xlsx\")\n",
    "\n",
    "# 2. XXX Salvando o arquivo de df: df_root_pipe\n",
    "df_root_pipe.to_excel(file_path_root_pipe, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESERVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch</th>\n",
       "      <th>Data</th>\n",
       "      <th>File</th>\n",
       "      <th>Type</th>\n",
       "      <th>Level</th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Parent_Unique_ID</th>\n",
       "      <th>Hash</th>\n",
       "      <th>File_Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Batch, Data, File, Type, Level, Unique_ID, Parent_Unique_ID, Hash, File_Path]\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "\n",
    "# 8. Função para adicionar um novo registro em df_source\n",
    "def add_source_entry(batch_name, file_path, file, type, level, parent_unique_id):\n",
    "    #unique_id = generate_unique_id(type)\n",
    "    unique_id = generate_unique_id()\n",
    "    time_now = cron.timenow_pt_BR()   \n",
    "    file_hash = generate_file_hash(file_path) \n",
    "    if level == 1:\n",
    "        parent_unique_id = unique_id\n",
    "    data = {\n",
    "        'Batch': batch_name,\n",
    "        'Data': time_now,\n",
    "        'File': file,\n",
    "        'Type': type,\n",
    "        'Level': level,\n",
    "        'Unique_ID': unique_id,\n",
    "        'Parent_Unique_ID': parent_unique_id,\n",
    "        'Hash': file_hash,\n",
    "        'File_Path': file_path\n",
    "    }\n",
    "    return data\n",
    "\n",
    "# 9. Add nova linha para atualizar df_log_transctions\n",
    "def add_log_transaction_entry(document_unique_id,batch_name, file, document_action, src_path, tgt_path, transaction_detail, sucess=True):\n",
    "\n",
    "    data_log = {\n",
    "        'Dt_Time': cron.timenow_pt_BR(),\n",
    "        'Batch': batch_name,\n",
    "        'File' : file,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Action': document_action,\n",
    "        'Sorce': src_path,\n",
    "        'Target': tgt_path,\n",
    "        'Transction_Detail': transaction_detail,\n",
    "        'Sucess': sucess,    \n",
    "    }\n",
    "    \n",
    "        \n",
    "    return data_log\n",
    "\n",
    "\n",
    "# 10. Consulta multiparametros\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# 12. Busca filhos - simples\n",
    "def get_children(batch, file_path):\n",
    "    \n",
    "    file = os.path.basename(file_path)\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    # Buscando todos dos filhos de um documento\n",
    "    return filtrar_df(df_id_relations, Batch=batch, Parent_Unique_ID=document_unique_id)\n",
    "\n",
    "\n",
    "# 13. Busca pai -simples\n",
    "def get_father(batch, file_path):\n",
    "    \n",
    "    file = os.path.basename(file_path)\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    # Buscando todos dos filhos de um documento\n",
    "    return filtrar_df(df_id_relations, Batch=batch, Unique_ID=parent_unique_id)\n",
    "\n",
    "\n",
    "# 14. Pesquiso pai pelo Unique_ID e trago um dict\n",
    "def get_father_data_by_children_file(batch, file):\n",
    "    \n",
    "    src_result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    src_parent_unique_id = src_result['Parent_Unique_ID'].values[0]\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=src_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }\n",
    "\n",
    "\n",
    "# 15. Pesquiso pai pelo Unique_ID (document_parent_unique_id) e cria DICT\n",
    "def get_father_by_unique_id(batch, document_parent_unique_id):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=document_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }\n",
    "    \n",
    "# 16. Pesquiso pai pelo file do filho e cria DICT\n",
    "def get_father_data_by_children_file(batch, file):\n",
    "    \n",
    "    src_result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    src_parent_unique_id = src_result['Parent_Unique_ID'].values[0]\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=src_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }        \n",
    "        \n",
    "\n",
    "# 17. Busca o 'Unique_ID' para definir o Parent_Unique_ID sem considerar 'Level'\n",
    "def get_parent_unique_id(df_id_relations, batch_name, file, type):\n",
    "    try:\n",
    "        parent_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type)]['Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        parent_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return parent_unique_id\n",
    "\n",
    "\n",
    "# 18. funcao para trazer somente o 'Unique_ID'\n",
    "def get_document_unique_id(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)]['Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        document_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# 19. funcao para trazer somente o 'Parent_Unique_ID'\n",
    "def get_document_parent_unique_id(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_parent_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)]['Parent_Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        document_parent_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_parent_unique_id\n",
    "\n",
    "\n",
    "# 20. funçao para trazer toda a row de df_id_relations para o documento\n",
    "def get_document_id_relations(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_id_relations = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)].values[0]\n",
    "    except IndexError:\n",
    "        document_id_relations = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_id_relations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# EXEMPLOS de Pesquisa DFss\n",
    "    # get_document_unique_id(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "    # # Busca somente o 'Parent_Unique_ID'\n",
    "    # get_document_parent_unique_id(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "\n",
    "    # #Busca todos os dados da row do documento encontrado\n",
    "    # document_id_relations = get_document_id_relations(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "    # document_batch = document_id_relations[0]\n",
    "    # document_date = document_id_relations[1]\n",
    "    # document_name = document_id_relations[2]\n",
    "    # document_type = document_id_relations[3]\n",
    "    # document_level = document_id_relations[4]\n",
    "    # document_unique_id = document_id_relations[5]\n",
    "    # document_parent_unique_id = document_id_relations[6]\n",
    "    # document_hash = document_id_relations[7]\n",
    "    # document_path = document_id_relations[8]\n",
    "\n",
    "    # # Insercao de um registro pela func add_source_entry\n",
    "    # file_path = \"pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/SPA 15082023.rar\"\n",
    "\n",
    "    # file = os.path.basename(file_path)\n",
    "\n",
    "    # type = \"compressed_file_attachment\"\n",
    "\n",
    "    # level = 1\n",
    "\n",
    "    # parent_unique_id = ''\n",
    "\n",
    "    # # Adicionando um novo registro (substitua 'batch_name' e 'email' conforme necessário)\n",
    "    # new_entry = add_source_entry(batch_name, file_path, file, type, level, parent_unique_id)\n",
    "\n",
    "    # df_id_relations = df_id_relations.append(new_entry, ignore_index=True)\n",
    "\n",
    "    # df_id_relations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. Criaçao do DataFrame para armazenar as relações de Unique_ID e Parent_Unique_ID\n",
    "df_id_relations = pd.DataFrame(columns=['Batch', 'Data' ,'File', 'Type', 'Level', 'Unique_ID', 'Parent_Unique_ID', 'Hash', 'File_Path'])\n",
    "\n",
    "# 2. Criaçao do DataFrame para df_start_pipe:\n",
    "#df_start_pipe = pd.DataFrame(columns=['Batch', 'Data' ,'File', 'Type', 'Level', 'Unique_ID', 'dt_hora', 'de', 'assunto', 'email', 'Hash'])\n",
    "\n",
    "def get_template_version(model):\n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model)\n",
    "    if not row_frame.empty:\n",
    "            # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "            version = [((row_frame.iloc[0]['version']))]\n",
    "            \n",
    "    return version[0]  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# XXXpara buscar melhor as coordendas dos FRAMES\n",
    "def get_coordinates_filter(pdf_pesquisavel_map, model, tipo, label, section):\n",
    "    \n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model, type=tipo, label=label, section_json=section)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [((row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p']) if pdf_pesquisavel_map else (row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1']))]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#generated_parent_document_unique_id = generate_unique_id()  \n",
    "\n",
    "# Processo de deleçao e atualizacao de documentos\n",
    "#e_deleta_peloamor(df_docs_splitados)\n",
    "\n",
    "#me_atualiza_logo_vai_2(novo_df)\n",
    "\n",
    "# apagar_zone(documentos_extracao_path)\n",
    "\n",
    "\n",
    "# 5. XXX Ajusta textoYYY\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF_Pesquisavel-  NO CABECALHO\n",
    "def texto_extraido_nf(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF RASTER NO CABECALHO\n",
    "def texto_extraido_cabecalho(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    text_splited = [s.replace(\")\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#1. funcao: find_value_after_keyword_out_frame_up\n",
    "def find_value_after_keyword_out_frame_up(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "        if index + 1 < len(text_list) and text_list[index + 1] not in default_keyword_list:\n",
    "            return text_list[index + 1]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            for default_keyword in default_keyword_list:\n",
    "                if default_keyword in text_list:\n",
    "                    # Caso especial para 'Nome/Razão Social:'\n",
    "                    if keyword == 'Nome/Razão Social:':\n",
    "                        return text_list[0]\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#2. find_value_after_keyword_out_frame_down  \n",
    "def find_value_after_keyword_out_frame_down(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o índice seguinte está dentro da lista\n",
    "        if index + 1 < len(text_list):\n",
    "            # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            try:\n",
    "                index = text_list.index(default_keyword_list[-1])\n",
    "                return text_list[index - 1]\n",
    "            except ValueError:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "\n",
    "#3. find_value_after_keyword_fuzz\n",
    "def find_value_after_keyword_fuzz(keyword, text_list, default_keyword_list=None, fuzziness_threshold=80):\n",
    "    closest_match = None\n",
    "    closest_match_score = 0\n",
    "    \n",
    "    for i, text in enumerate(text_list):\n",
    "        score = fuzz.ratio(keyword, text)\n",
    "        \n",
    "        if score > closest_match_score:\n",
    "            closest_match_score = score\n",
    "            closest_match = text\n",
    "        \n",
    "        if closest_match_score > fuzziness_threshold:\n",
    "            break\n",
    "\n",
    "    if closest_match_score > fuzziness_threshold:\n",
    "        index = text_list.index(closest_match)\n",
    "        if index + 1 < len(text_list):\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pesquisa_keyword(string_pesquisa, text_splited, keyword_list):\n",
    "    resultado_extraido_fuzz = find_value_after_keyword_fuzz(string_pesquisa, text_splited, keyword_list)\n",
    "\n",
    "    if resultado_extraido_fuzz == None:\n",
    "        resultado_extraido_frame_up = find_value_after_keyword_out_frame_up(string_pesquisa, text_splited, keyword_list)\n",
    "        if resultado_extraido_frame_up == None:\n",
    "            resultado_extraido_frame_down = find_value_after_keyword_out_frame_down(string_pesquisa, text_splited, keyword_list)\n",
    "            resultado_extraido = resultado_extraido_frame_down\n",
    "        else:\n",
    "            resultado_extraido = resultado_extraido_frame_up\n",
    "    else:\n",
    "        resultado_extraido = resultado_extraido_fuzz\n",
    "\n",
    "    # Verifica se o resultado extraído é uma das palavras-chave, indicando um erro\n",
    "    if resultado_extraido in keyword_list:\n",
    "        resultado_extraido = None\n",
    "\n",
    "    return resultado_extraido\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# XXX Pequenos mas poderosos\n",
    "def extract_text_PIL(image, coordinates):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    image_croped = image.crop((x0, y0, x1, y1))\n",
    "    texto_extraido = pytesseract.image_to_string(image_croped, lang='por', config='--psm 6')\n",
    "    return texto_extraido \n",
    "\n",
    "\n",
    "# 10. Consulta multiparametros\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# Funcao importante - process_line\n",
    "def process_line(value, reference, label):\n",
    "    name_match = re.search(fr'{reference} (.+)', value)\n",
    "    if name_match:\n",
    "        extracted_value = reference + \" \" + name_match.group(1)\n",
    "        return {label: extracted_value}\n",
    "    return None\n",
    "\n",
    "# Função para extrair número da string\n",
    "def extract_number(text):\n",
    "    match = re.search(r'\\b\\d+(\\.\\d+)?\\b', text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def corrigir_email(texto):\n",
    "    # Padrão de regex para identificar e-mails potenciais\n",
    "    padrao_email = re.compile(r'[a-zA-Z0-9_.+-]+[)!Q@][a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+')\n",
    "    \n",
    "    # Encontrar todos os padrões que se assemelham a um e-mail\n",
    "    possiveis_emails = padrao_email.findall(texto)\n",
    "    \n",
    "    for email in possiveis_emails:\n",
    "        # Se \"@\" não estiver presente, tentamos corrigir substituindo \")\" ou \"!\" por \"@\"\n",
    "        if \"@\" not in email:\n",
    "            email_corrigido = email.replace(\")\", \"@\").replace(\"Q\", \"@\")\n",
    "            texto = texto.replace(email, email_corrigido)\n",
    "    \n",
    "    return texto\n",
    "\n",
    "\n",
    "\n",
    "# 1 XXX Crio o DF  cnae_x_item_servico_df\n",
    "cnae_x_item_servico_df = pd.read_excel(cnae_dict_path)\n",
    "\n",
    "# Mapeando prefeitura e CNAE para a descrição do CNAE\n",
    "cnae_dict = dict(zip(zip(cnae_x_item_servico_df['PREFE'], cnae_x_item_servico_df['CNA_NUMERO']), cnae_x_item_servico_df['CNA_NOME']))\n",
    "\n",
    "# Mapeando prefeitura e item de serviço para a descrição do item de serviço e o CNAE associado\n",
    "item_servico_dict = dict(zip(zip(cnae_x_item_servico_df['PREFE'], cnae_x_item_servico_df['ATV_CODIGO']), zip(cnae_x_item_servico_df['ATV_DESCRICAO'], cnae_x_item_servico_df['CNA_NUMERO'])))\n",
    "\n",
    "\n",
    "# 2. XXX  Tratando o CNAE com dict criado\n",
    "def processa_cnae_dict(Texto_extraido, de_para_pm, debug):\n",
    "\n",
    "    text_splited = Texto_extraido.split('\\n')\n",
    "    # Processando CNAE\n",
    "    cnae_lines = [line for line in text_splited if 'CNAE' in line]\n",
    "\n",
    "    if cnae_lines:\n",
    "        cnae_line = cnae_lines[0]\n",
    "        #print(f'cnae_line: {cnae_line}')\n",
    "        \n",
    "        cnae_number = int(extract_number(cnae_line))\n",
    "        \n",
    "        cnae_value = cnae_dict.get((de_para_pm, cnae_number),(\"Valor não encontrado\"))\n",
    "        if cnae_value != \"Valor não encontrado\":\n",
    "            cnae_value = cnae_value.upper()\n",
    "            cnae_value = str(cnae_number) + \" - \" + cnae_value\n",
    "            return cnae_value\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        cnae_value = processa_cnae_outros(Texto_extraido)\n",
    "        cnae_number = int(extract_number(cnae_value))\n",
    "\n",
    "        cnae_value = cnae_dict.get((de_para_pm, cnae_number),(\"Valor não encontrado\"))\n",
    "        if cnae_value != \"Valor não encontrado\":\n",
    "            cnae_value = cnae_value.upper()\n",
    "            cnae_value = str(cnae_number) + \" - \" + cnae_value\n",
    "            return cnae_value\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "     \n",
    "\n",
    "# 3. XXX  Tratando Item de Servico com dict criado\n",
    "def processa_itens_servico_dict(Texto_extraido, de_para_pm, debug):\n",
    "    \n",
    "    text_splited = Texto_extraido.split('\\n')\n",
    "    # Encontrando a linha que contém o texto desejado\n",
    "    item_servico_lines = [line for line in text_splited if 'Item da Lista de Serviços' in line]\n",
    "    #print(f'item_servico_lines (fora do if): {item_servico_lines}')\n",
    "    # Verificando se encontramos uma linha válida\n",
    "    if item_servico_lines:\n",
    "        #print(f'item_servico_lines: {item_servico_lines}')\n",
    "        item_servico_line = item_servico_lines[0]\n",
    "        item_servico_cod = float(extract_number(item_servico_line))\n",
    "        item_servico, cnae_associado = item_servico_dict.get((de_para_pm, item_servico_cod), (\"Valor não encontrado\", None))\n",
    "        item_servico = item_servico.upper()\n",
    "        item_servico_value = str(item_servico_cod) + \" - \" + item_servico\n",
    "        return item_servico_value\n",
    "    \n",
    "    else:\n",
    "        #print(\"Linha com 'Item da Lista de Serviços' não encontrada\")\n",
    "        item_servico_line = processa_item_sevico_outros(Texto_extraido)\n",
    "        if item_servico_line:\n",
    "            item_servico_cod = float(extract_number(item_servico_line))\n",
    "            item_servico, cnae_associado = item_servico_dict.get((de_para_pm, item_servico_cod), (\"Valor não encontrado\", None))\n",
    "            item_servico = item_servico.upper()\n",
    "            item_servico_value = str(item_servico_cod) + \" - \" + item_servico\n",
    "            return item_servico_value\n",
    "        \n",
    "        else:\n",
    "            return None\n",
    "        #return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_id_relations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tables-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
