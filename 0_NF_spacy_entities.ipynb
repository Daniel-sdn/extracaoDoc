{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0_entities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando modulos gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from io import StringIO\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import PyPDF2\n",
    "\n",
    "import locale\n",
    "import time, copy\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import cv2\n",
    "import fitz  # Módulo PyMuPDF\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import Image, ImageDraw\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar\n",
    "import matplotlib.pyplot as plt\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "# import modules.extrai_pdf_pesquisavel as Extc\n",
    "import modules.cronometro as cron\n",
    "import modules.nova_extracao_pdf_pesquisavel as novaextra \n",
    "import modules.trata_model as tmod\n",
    "import modules.trata_pdf as tpdf\n",
    "import modules.utils as utl\n",
    "\n",
    "\n",
    "import json\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "\n",
    "# 5. Path para documentos para extracao\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "# 6. Path para gestao de imagens resized\n",
    "image_resized_path = \"pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas\"\n",
    "\n",
    "\n",
    "# 12. poppler path\n",
    "poppler_path = \"/home/dani-boy/miniconda3/envs/tables-detr/bin\"\n",
    "\n",
    "\n",
    "\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================== 1. CABECALHO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Data e Hora de Emissão:\n",
    "data_hora_emissao_pattern = [\n",
    "    {\"LOWER\": \"data\"},\n",
    "    {\"LOWER\": \"e\"},\n",
    "    {\"LOWER\": \"hora\"},\n",
    "    {\"LOWER\": \"da\"},\n",
    "    {\"LOWER\": \"emissão\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"SHAPE\": \"dd/dd/dddd\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"SHAPE\": \"dd:dd:dd\"}\n",
    "]\n",
    "matcher.add(\"DATA_HORA_EMISSAO\", [data_hora_emissao_pattern])\n",
    "\n",
    "\n",
    "# 4. Código de Verificação:\n",
    "codigo_verificacao_pattern = [\n",
    "    {\"LOWER\": \"código\"},\n",
    "    {\"LOWER\": \"verificação\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_ASCII\": True, \"LENGTH\": 9}\n",
    "]\n",
    "matcher.add(\"CODIGO_VERIFICACAO\", [codigo_verificacao_pattern])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#========================================  5. VALOR TOTAL\n",
    "valor_total_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"total\"},\n",
    "    {\"LOWER\": \"da\", \"OP\": \"?\"},\n",
    "    {\"LOWER\": \"nota\", \"OP\": \"?\"},\n",
    "    {\"TEXT\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_TOTAL\", [valor_total_pattern])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#======================================== 7. VALORES E IMPOSTOS\n",
    "# 1. VALOR_SERVICOS\n",
    "valor_servicos_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"serviços\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "\n",
    "matcher.add(\"VALOR_SERVICOS\", [valor_servicos_pattern])\n",
    "\n",
    "\n",
    "# 2. VALOR DEDUÇÃO:\n",
    "valor_deducao_pattern = [\n",
    "    {\"LOWER\": \"dedução\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "\n",
    "matcher.add(\"VALOR_DEDUCAO\", [valor_deducao_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 3. DESC. INCOND: RASTER_PDF\n",
    "valor_incondR_pattern = [\n",
    "    {\"LOWER\": \"base\"},\n",
    "    {\"LOWER\": \"de\"},\n",
    "    {\"IS_SPACE\": True},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"ORTH\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}    \n",
    "]\n",
    "matcher.add(\"VALOR_INCONDP\", [valor_incondR_pattern])\n",
    "\n",
    "\n",
    "# 3.A DESC. INCOND: - PDF_Pesquisavel   #DESC. INCOND:\n",
    "valor_incond_patternP = [\n",
    "    {\"LOWER\": \"desc\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},\n",
    "    {\"LOWER\": \"incond\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"ORTH\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}    \n",
    "]\n",
    "matcher.add(\"VALOR_INCONDR\", [valor_incond_patternP])\n",
    "\n",
    "\n",
    "\n",
    "# 4. BASE DE CÁLCULO:  RASTER_PDF\n",
    "valor_calculoR_pattern = [\n",
    "    {\"LOWER\": \"cálculo\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_CALCULOR\", [valor_calculoR_pattern])\n",
    "\n",
    "\n",
    "# 4.A BASE DE CÁLCULO:  PDF_P\n",
    "valor_calculoP_pattern = [\n",
    "    {\"LOWER\": \"base\"},\n",
    "    {\"LOWER\": \"de\"},\n",
    "    {\"LOWER\": \"cálculo\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_CALCULOP\", [valor_calculoP_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 5. Alíquota d,dd\n",
    "valor_aliquota_pattern = [\n",
    "    {\"LOWER\": \"alíquota\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"d,dd\", \"OP\": \"?\"},\n",
    "    {\"ORTH\": \"%\"}\n",
    "\n",
    "]\n",
    "matcher.add(\"VALOR_ALIQUOTA\", [valor_aliquota_pattern])\n",
    "\n",
    "# 5.1 Alíquota d\n",
    "valor_aliquota2_pattern = [\n",
    "    {\"LOWER\": \"alíquota\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"d\", \"OP\": \"?\"},\n",
    "    {\"ORTH\": \"%\"}\n",
    "\n",
    "]\n",
    "matcher.add(\"VALOR_ALIQUOTA2\", [valor_aliquota2_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 6. VALOR ISS:\n",
    "valor_iss_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"iss\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_ISS\", [valor_iss_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 7. VALOR ISS RETIDO:\n",
    "valor_issretido_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"iss\"},\n",
    "    {\"LOWER\": \"retido\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_ISSRETIDO\", [valor_issretido_pattern])\n",
    "\n",
    "\n",
    "# 8. DESC. COND:\n",
    "valor_desccond_pattern = [\n",
    "    {\"LOWER\": \"desc\"},\n",
    "    {\"ORTH\": \".\"},\n",
    "    {\"LOWER\": \"cond\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_DESCCOND\", [valor_desccond_pattern])\n",
    "\n",
    "\n",
    "# 9. VALOR PIS:\n",
    "valor_pis_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"pis\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_PIS\", [valor_pis_pattern])\n",
    "\n",
    "\n",
    "# 10. VALOR COFINS:\n",
    "valor_cofins_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"cofins\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_COFINS\", [valor_cofins_pattern])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 11. VALOR IR:\n",
    "valor_ir_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"ir\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_IR\", [valor_ir_pattern])\n",
    "\n",
    "\n",
    "# 12. VALOR INSS:\n",
    "valor_inss_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"inss\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_INSS\", [valor_inss_pattern])\n",
    "\n",
    "\n",
    "# 13. VALOR CSLL:\n",
    "valor_csll_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"csll\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_CSLL\", [valor_csll_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 14. OUTRAS RETENÇÕES:\n",
    "valor_outrasreten_pattern = [\n",
    "    {\"LOWER\": \"outras\"},\n",
    "    {\"LOWER\": \"retenções\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_OUTRAS\", [valor_outrasreten_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 15. VALOR LÍQUIDO:\n",
    "valor_liquido_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"líquido\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_LIQUIDO\", [valor_liquido_pattern])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#======================================== 9. OUTRAS INFORMAÇOES / CRITICAS\n",
    "# 1. EXIGIBILIDADE ISS\n",
    "exigibilidade_iss_pattern = [\n",
    "    {\"LOWER\": \"exigibilidade\"},\n",
    "    {\"LOWER\": \"iss\"},\n",
    "    {\"LOWER\": {\"IN\": [\"exigivel\", \"não exigivel\"]}}\n",
    "]\n",
    "matcher.add(\"EXIGIBILIDADE_ISS\", [exigibilidade_iss_pattern])\n",
    "\n",
    "\n",
    "# 2. REGIME TRIBUTAÇÃO\n",
    "padrao_regime_tributacao = [\n",
    "    {\"LOWER\": \"regime\"},\n",
    "    {\"LOWER\": \"tributação\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"+\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"*\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": \"simples\", \"OP\": \"?\"},\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"REGIME_TRIBUTACAO\", [padrao_regime_tributacao])\n",
    "\n",
    "# 3. SIMPLES NACIONAL = NAO\n",
    "simples_nacional_nao_pattern = [\n",
    "    {\"LOWER\": \"simples\"},\n",
    "    {\"LOWER\": \"nacional\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": \"não\"}\n",
    "]\n",
    "matcher.add(\"SIMPLES_NACIONAL_NAO\", [simples_nacional_nao_pattern])\n",
    "\n",
    "# 3.1 SIMPLES NACIONAL = SIM\n",
    "simples_nacional_pattern = [\n",
    "    {\"LOWER\": \"simples\"},\n",
    "    {\"LOWER\": \"nacional\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": \"sim\", \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"ORTH\": \"(\", \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"ORTH\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"ORTH\": \"%\", \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"ORTH\": \")\", \"OP\": \"?\"}\n",
    "]\n",
    "matcher.add(\"SIMPLES_NACIONAL_SIM\", [simples_nacional_pattern])\n",
    "\n",
    "\n",
    "# 4. ISSQN RETIDO\n",
    "issqn_retido_pattern = [\n",
    "    {\"LOWER\": \"issqn\"},\n",
    "    {\"LOWER\": \"retido\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": {\"IN\": [\"sim\", \"não\"]}}\n",
    "]\n",
    "matcher.add(\"ISSQN_RETIDO\", [issqn_retido_pattern])\n",
    "\n",
    "\n",
    "# 5. LOCAL. PRESTAÇÃO SERVIÇO\n",
    "local_prestacao_servico_pattern = [\n",
    "    {\"LOWER\": \"local\"},\n",
    "    {\"ORTH\": \".\"},\n",
    "    {\"LOWER\": \"prestação\"},\n",
    "    {\"LOWER\": \"serviço\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"+\"},  # para lidar com múltiplos espaços\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"+\"},  # para a cidade\n",
    "    {\"ORTH\": \"-\", \"OP\": \"?\"},\n",
    "    {\"IS_UPPER\": True, \"LENGTH\": 2, \"OP\": \"?\"}  # para a sigla do estado\n",
    "]\n",
    "matcher.add(\"LOCAL_PRESTACAO_SERVICO\", [local_prestacao_servico_pattern])\n",
    "\n",
    "# 6. LOCAL INCIDÊNCIA\n",
    "local_incidencia_pattern = [\n",
    "    {\"LOWER\": \"local\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},\n",
    "    {\"LOWER\": \"incidência\"},\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"+\"},  # Nome da cidade\n",
    "    {\"ORTH\": \"-\", \"OP\": \"?\"},  # Hífen opcional\n",
    "    {\"SHAPE\": \"XX\", \"OP\": \"?\"}  # Sigla do estado\n",
    "]\n",
    "matcher.add(\"LOCAL_INCIDENCIA\", [local_incidencia_pattern])\n",
    "\n",
    "\n",
    "# observacao_pattern = [\n",
    "#     {\"LOWER\": \"observação\"},\n",
    "#     {\"ORTH\": \":\"},\n",
    "#     {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "#     {\"LOWER\": \"-\", \"OP\": \"?\"},\n",
    "#     {\"IS_PRINT\": True, \"OP\": \"+\"}\n",
    "# ]\n",
    "\n",
    "# matcher.add(\"OBSERVACAO\", [observacao_pattern])# 6. Alíquota\n",
    "valor_aliquota_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"iss\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"+\"},\n",
    "    {\"ORTH\": \"\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"},\n",
    "    {\"ORTH\": \"%\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raster PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ocrmypdf(input_file, output_file):\n",
    "    command = [\n",
    "        'ocrmypdf',\n",
    "        '--language', 'por',\n",
    "        '--deskew',\n",
    "        input_file,\n",
    "        output_file\n",
    "    ]\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"OCRmyPDF completed successfully. Output saved to {output_file}.\")\n",
    "    else:\n",
    "        print(f\"OCRmyPDF failed with error: {result.stderr.decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/home/dani-boy/extractNF/pipeline_extracao_documentos/0_arquivos_teste_pipeline/pdf_raster/Mage/nf_689__pmmacae__8a_med_co_22_22_tb_contrato_045_22_1.pdf\"\n",
    "output_file = \"/home/dani-boy/extractNF/processamentos/temp/documento.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCRmyPDF completed successfully. Output saved to /home/dani-boy/extractNF/processamentos/temp/documento.pdf.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'02/08/23, 16:33  Nota Fiscal de Serviços Eletrônica (NFSe) Número da Nota:  PREFEITURA MUNICIPAL DE MAGE  ia  Competência:  SECRETARIA MUNICIPAL DA FAZENDA  Agosto/2023  NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e  Data e Hora da Emissão: 02/08/2023 16:29:00  Código Verificação:  OCDD6FB51  PRESTADOR DE SERVIÇOS CPF/CNPJ:  Inscrição Municipal:  Telefone:  Inscrição Estadual:  33.462.862/0001-95  li:  4337  al  Nome/Razão Social: SÃO MARCOS TERRAPLENAGEM E CONSTRUÇÃO LTDA. Nome de Fantasia: Endereço: RUA DAS MARGARIDAS ,578 ,SANTA DALILA - Magé-RJ E-mail:  TOMADOR DE SERVIÇOS CPF/CNPJ: 29.115.474/0001-60 HINSC:MUNICIPAL:  RG:  |  Telefone:  Inscrição Estadual:  Nome/Razão Social: MUNICÍPIO DE MACAE Endereço: ; PRESIDENTE FELICIANO SODRE Nº 534 PAÇO E-mail: NÃO INFORMADO  MUNICIPAL  BAIRRO:  CENTRO  CIDADE:  MACAE  ;  - RJ CEP: 27913080  DISCRIMINAÇÃO DOS SERVIÇOS Serviço de Recomposição do Pavimento Asfáltico (Tapa Buraco), na cidade de Macaé/RJ, abrangendo Perímetro Urbano, Região Serrana, conf Boletim de Medição 08 — CO 022/2022 — Contrato 045/2022 - . Empenhos 501/2023 e 502/2023. M. Obra (10%)= R$ 104.468,45 Retenção p/ Prev. Social=R$ 11.491,53 Banco: ITAU Agência:  0726 C/Corrente: 35410-3  VALOR TOTAL DA NOTA: R$ 1.044.684,55 CNAE - 4313400 - OBRAS DE TERRAPLENAGEM . é o Item da Lista de Serviços - 7.02 - EXECUÇÃO, POR ADMINISTRAÇÃO, EMPREITADA OU SUBEMPREITADA, DE OBRAS DE CONSTRUÇÃO CIVIL, HIDRAULICA OU ELETRICA E DE OUTRAS OBRAS SEMELHANTES, INCLUSIVE SONDAGEM, PERFURAÇÃO DE POÇOS, ESCAVAÇÃO, DRENAG  = [m]\\' vm La [=]  VALOR SERVIÇOS: R$ 1.044.684,55  VALOR | DEDUÇÃO: R$ 0,00  DESC. INCOND: BASE DE R$ 0,00 CALCULO: R$ 1.044.684,55  ALÍQUOTA: 2%  VALOR ISS: R$ 0,00  VALOR PIS: R$ 0,00  VALOR COFINS: R$ 0,00  VALOR IR: R$ 0,00  VALOR CSLL: R$ 0,00  OUTRAS R$ 0,00  eEd Era a e  [=] Lt  A  =\"  DADOS  VALOR INSS: R$ 11.491,53  VALOR ISS RETIDO: R$ 20.893,69  RETENÇÕES:  DESC. COND: R$ 0,00  VALOR LÍQUIDO: R$ 1.012.299,33  COMPLEMENTARES  OUTRAS INFORMAÇÕES / CRITICAS EXIGIBILIDADE ISS Exigivel  REGIME TRIBUTAÇÃO Sociedade Limitada  SIMPLES NACIONAL Não  ISSQN RETIDO Sim  LOCAL. PRESTAÇÃO SERVIÇO Macaé - RJ  LOCAL INCIDÊNCIA Macaé - RJ  Observação:  - ISS RETIDO PELO TOMADOR DE SERVIÇOS CNPJ: 29.115.474/0001-60 Valor Aproximado dos Tributos Federais R$ 136154,26 (Alíq 13,45), Tributos Estaduais R$ 0,00 (Alíq 0,00 IBPT) e Municipal de R$ 49906,36 (Alíq IBPT 4,93 IBPT)  https://nfs-e.mage.rj.gov.br  \\x0c'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. XXX Executar o comando OCRmyPDF\n",
    "run_ocrmypdf(input_file, output_file)\n",
    "\n",
    "# 2. XXX Executar o comando OCRmyPDF    \n",
    "!pdftotext processamentos/temp/documento.pdf processamentos/temp/txt/documento.txt\n",
    "\n",
    "# 3. XXX Ler o arquivo TXT\n",
    "with open('processamentos/temp/txt/documento.txt', 'r', encoding='utf-8') as arquivo:\n",
    "    texto_OCR_R = arquivo.read()\n",
    "\n",
    "texto_PDF_Raster = texto_OCR_R.replace('\\n', ' ')\n",
    "texto_PDF_Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"SHAPE\": \"dd.ddd.ddd/dddd-dd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "# Função para definir o atributo \"is_cnpj\"\n",
    "@Language.component(\"set_cnpj_attribute\")\n",
    "def set_cnpj_attribute(doc):\n",
    "    for i, token in enumerate(doc):\n",
    "        if i < len(doc) - 1:\n",
    "            next_token = doc[i + 1]\n",
    "            if token.shape_ == \"dd.ddd.ddd/\" and next_token.shape_ == \"dddd-dd\":\n",
    "                token._.is_cnpj = True\n",
    "                next_token._.is_cnpj = True\n",
    "            else:\n",
    "                token._.is_cnpj = False\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registro do atributo 'is_cnpj'\n",
    "Token.set_extension('is_cnpj', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização do Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padrão para encontrar CNPJ\n",
    "pattern = [\n",
    "    {\"SHAPE\": \"dd.ddd.ddd/\"},\n",
    "    {\"SHAPE\": \"dddd-dd\"}\n",
    "]\n",
    "# Adição do padrão ao Matcher\n",
    "matcher.add(\"CNPJ_PATTERN\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para aplicar o matcher\n",
    "@Language.component(\"apply_cnpj_matcher\")\n",
    "def apply_cnpj_matcher(doc):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        for token in span:\n",
    "            token._.is_cnpj = True\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.set_cnpj_attribute(doc)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"set_cnpj_attribute\") # Adicione esta etapa se você quiser definir o atributo manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.apply_cnpj_matcher(doc)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"apply_cnpj_matcher\")  # Adicione esta etapa para aplicar o matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(texto_PDF_Raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  378 | token.text: 33.462.862/, token.shape: dd.ddd.ddd/, token._.is_cnpj: True \n",
      "  389 | token.text: 0001-95, token.shape: dddd-dd, token._.is_cnpj: True \n",
      "  591 | token.text: 29.115.474/, token.shape: dd.ddd.ddd/, token._.is_cnpj: True \n",
      "  602 | token.text: 0001-60, token.shape: dddd-dd, token._.is_cnpj: True \n",
      " 2233 | token.text: 29.115.474/, token.shape: dd.ddd.ddd/, token._.is_cnpj: True \n",
      " 2244 | token.text: 0001-60, token.shape: dddd-dd, token._.is_cnpj: True \n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token._.is_cnpj:\n",
    "        print(f'{token.idx:>5} | token.text: {token.text}, token.shape: {token.shape_}, token._.is_cnpj: {token._.is_cnpj} ') \n",
    "    #print(token.text, token._.is_cnpj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['33.462.862/0001-95', '29.115.474/0001-60', '29.115.474/0001-60']\n"
     ]
    }
   ],
   "source": [
    "cnpjs = []  # Lista para armazenar os CNPJs completos\n",
    "indices = []  # Lista para armazenar os índices dos tokens que são CNPJs\n",
    "\n",
    "for token in doc:\n",
    "    if token._.is_cnpj:\n",
    "        indices.append(token.i)  # Adicione o índice do token à lista\n",
    "\n",
    "# Agrupe as partes do CNPJ\n",
    "for i in range(0, len(indices), 2):  # Vai de dois em dois\n",
    "    try:\n",
    "        first_part = doc[indices[i]].text  # Primeira parte do CNPJ\n",
    "        second_part = doc[indices[i + 1]].text  # Segunda parte do CNPJ\n",
    "        full_cnpj = first_part + second_part  # Concatena as duas partes\n",
    "        cnpjs.append(full_cnpj)\n",
    "    except IndexError:\n",
    "        print(\"Índice fora dos limites. Talvez o CNPJ esteja incompleto.\")\n",
    "        \n",
    "print(cnpjs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exemplo de uso\n",
    "# Suponha que 'nlp' é o seu objeto de linguagem spaCy já carregado\n",
    "# nlp.add_pipe(set_cnpj_attribute)  # Adicione esta etapa se você quiser definir o atributo manualmente\n",
    "# \n",
    "\n",
    "# doc = nlp(\"O CNPJ da empresa é 33.462.862/0001-95.\")\n",
    "# for token in doc:\n",
    "#     print(token.text, token._.is_cnpj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "\n",
    "# 1. CPF/CNPJ:\n",
    "valor_cnpjprestadorR_pattern = [\n",
    "    {\"LOWER\": \"inscricao\"},\n",
    "    {\"LOWER\": \"estadual\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"SHAPE\": \"dd.ddd.ddd/\", \"OP\": \"+\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"SHAPE\": \"dddd-dd\", \"OP\": \"*\"}\n",
    "\n",
    "]\n",
    "matcher.add(\"VALOR_CPF_CNPJ:\", [valor_cnpjprestadorR_pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(texto_PDF_Raster)\n",
    "\n",
    "# Executar o Matcher no Doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Exibir os resultados\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Obter a string de identificação\n",
    "    span = doc[start:end]  # Obter o trecho correspondente\n",
    "    print(f'\\nstring_id: {string_id:>30}   span.text:{span.text:>45} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Pesquisavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Pesquisavel\n",
    "\n",
    "file_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_23/MESQUITA_PDF_31282023_2258/159871/2023 -8.pdf\"\n",
    "\n",
    "\n",
    "pdf_document = fitz.open(file_path)\n",
    "\n",
    "\n",
    "page_number = 0  # Defina o número da página que deseja analisar\n",
    "page = pdf_document[page_number]\n",
    "\n",
    "# Extrair texto dentro do retângulo\n",
    "text_P = page.get_text(\"text\")\n",
    "\n",
    "\n",
    "pdf_document.close()\n",
    "\n",
    "texto_PDF_Pesquisavel = text_P.replace('\\n', ' ')\n",
    "texto_PDF_Pesquisavel \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(texto_PDF_Pesquisavel)\n",
    "\n",
    "# Executar o Matcher no Doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Exibir os resultados\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Obter a string de identificação\n",
    "    span = doc[start:end]  # Obter o trecho correspondente\n",
    "    print(f'\\nstring_id: {string_id:>30}   span.text:{span.text:>45} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texto_amostra = \"ALÍQUOTA: 2,01% VALOR ISS\"\n",
    "\n",
    "texto_amostra = \" Telefone:  Inscrição Estadual:  33.462.862/0001-95  li:  4337 \"\n",
    "\n",
    "doc = nlp(texto_amostra)\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Exibir os resultados\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Obter a string de identificação\n",
    "    span = doc[start:end]  # Obter o trecho correspondente\n",
    "    print(f\"{string_id}: {span.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cnpj(df):\n",
    "    cnpj_list = []\n",
    "    for i in range(len(df) - 1):\n",
    "        if df.loc[i, 'T_shape'] == 'dd.ddd.ddd/' and df.loc[i + 1, 'T_shape'] == 'dddd-dd':\n",
    "            cnpj = df.loc[i, 'T_texto'] + df.loc[i + 1, 'T_texto']\n",
    "            cnpj_list.append(cnpj)\n",
    "    return cnpj_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponhamos que 'df' é o seu DataFrame\n",
    "cnpjs = find_cnpj(df)\n",
    "print(\"CNPJs encontrados:\", cnpjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>T_texto</th>\n",
       "      <th>T_shape</th>\n",
       "      <th>T_is_alpha</th>\n",
       "      <th>T_is_digit</th>\n",
       "      <th>T_is_title</th>\n",
       "      <th>T_is_punct</th>\n",
       "      <th>T_is_sent_start</th>\n",
       "      <th>T_is_right_punct</th>\n",
       "      <th>T_is_stop</th>\n",
       "      <th>T_is_quote</th>\n",
       "      <th>T_is_currency</th>\n",
       "      <th>T_morph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Telefone</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(Gender=Masc, Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Inscrição</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(Gender=Fem, Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Estadual</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>33.462.862/</td>\n",
       "      <td>dd.ddd.ddd/</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(NumType=Card)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0001-95</td>\n",
       "      <td>dddd-dd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(NumType=Range)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>li</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(Mood=Ind, Number=Sing, Person=1, Tense=Past, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>4337</td>\n",
       "      <td>dddd</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(NumType=Card)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id      T_texto      T_shape T_is_alpha T_is_digit T_is_title T_is_punct  \\\n",
       "0    0                                False      False      False      False   \n",
       "1    1     Telefone        Xxxxx       True      False       True      False   \n",
       "2    2            :            :      False      False      False       True   \n",
       "3    3                                False      False      False      False   \n",
       "4    4    Inscrição        Xxxxx       True      False       True      False   \n",
       "5    5     Estadual        Xxxxx       True      False       True      False   \n",
       "6    6            :            :      False      False      False       True   \n",
       "7    7                                False      False      False      False   \n",
       "8    8  33.462.862/  dd.ddd.ddd/      False      False      False      False   \n",
       "9    9      0001-95      dddd-dd      False      False      False      False   \n",
       "10  10                                False      False      False      False   \n",
       "11  11           li           xx       True      False      False      False   \n",
       "12  12            :            :      False      False      False       True   \n",
       "13  13                                False      False      False      False   \n",
       "14  14         4337         dddd      False       True      False      False   \n",
       "\n",
       "   T_is_sent_start T_is_right_punct T_is_stop T_is_quote T_is_currency  \\\n",
       "0             True            False     False      False         False   \n",
       "1             True            False     False      False         False   \n",
       "2            False            False     False      False         False   \n",
       "3            False            False     False      False         False   \n",
       "4            False            False     False      False         False   \n",
       "5            False            False     False      False         False   \n",
       "6            False            False     False      False         False   \n",
       "7            False            False     False      False         False   \n",
       "8             True            False     False      False         False   \n",
       "9            False            False     False      False         False   \n",
       "10           False            False     False      False         False   \n",
       "11            True            False     False      False         False   \n",
       "12           False            False     False      False         False   \n",
       "13           False            False     False      False         False   \n",
       "14           False            False     False      False         False   \n",
       "\n",
       "                                              T_morph  \n",
       "0                                                  ()  \n",
       "1                          (Gender=Masc, Number=Sing)  \n",
       "2                                                  ()  \n",
       "3                                                  ()  \n",
       "4                           (Gender=Fem, Number=Sing)  \n",
       "5                                       (Number=Sing)  \n",
       "6                                                  ()  \n",
       "7                                                  ()  \n",
       "8                                      (NumType=Card)  \n",
       "9                                     (NumType=Range)  \n",
       "10                                                 ()  \n",
       "11  (Mood=Ind, Number=Sing, Person=1, Tense=Past, ...  \n",
       "12                                                 ()  \n",
       "13                                                 ()  \n",
       "14                                     (NumType=Card)  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analisys\n",
    "syntatic = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"T_texto\",\"T_shape\", \"T_is_alpha\", \"T_is_digit\", \"T_is_title\", \"T_is_punct\", \"T_is_sent_start\", \"T_is_right_punct\", \"T_is_stop\", \"T_is_quote\", \"T_is_currency\", \"T_morph\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    syntatic.loc[i,\"id\"] = token.i\n",
    "    syntatic.loc[i,\"T_texto\"] = token.text\n",
    "    syntatic.loc[i,\"T_shape\"] = token.shape_\n",
    "    syntatic.loc[i,\"T_is_alpha\"] = token.is_alpha\n",
    "    syntatic.loc[i,\"T_is_digit\"] = token.is_digit\n",
    "    syntatic.loc[i,\"T_is_title\"] = token.is_title\n",
    "    syntatic.loc[i,\"T_is_punct\"] = token.is_punct\n",
    "    syntatic.loc[i,\"T_is_sent_start\"] = token.is_sent_start\n",
    "    syntatic.loc[i,\"T_is_right_punct\"] = token.is_right_punct\n",
    "    syntatic.loc[i,\"T_is_stop\"] = token.is_stop\n",
    "    syntatic.loc[i,\"T_is_quote\"] = token.is_quote\n",
    "    syntatic.loc[i,\"T_is_currency\"] = token.is_currency\n",
    "    syntatic.loc[i,\"T_morph\"] = token.morph\n",
    "    i = i+1\n",
    "\n",
    "syntatic.head(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntatic.to_excel(\"syntatic.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"IS_DIGIT\": True, \"OP\": \"+\"},\n",
    "{\"ORTH\": \",\", \"OP\": \"?\"},\n",
    "{\"IS_DIGIT\": True, \"OP\": \"*\"},\n",
    "{\"ORTH\": \"%\"}\n",
    "\n",
    "\n",
    "\n",
    "{\"SHAPE\": \"XX\", \"OP\": \"?\"}\n",
    "\n",
    "# {\"LOWER\": \"alíquota\"} busca a palavra \"alíquota\" em qualquer combinação de maiúsculas e minúsculas.\n",
    "# {\"ORTH\": \":\"} busca o caractere \":\".\n",
    "# {\"IS_SPACE\": True, \"OP\": \"*\"} permite zero ou mais espaços.\n",
    "# {\"IS_DIGIT\": True, \"OP\": \"+\"} busca um ou mais dígitos.\n",
    "# {\"ORTH\": \",\", \"OP\": \"?\"} busca zero ou um caractere de vírgula (para números fracionários).\n",
    "# {\"IS_DIGIT\": True, \"OP\": \"*} busca zero ou mais dígitos (novamente, para números fracionários).\n",
    "# {\"ORTH\": \"%\"} busca o símbolo \"%\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Texto</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Tag_explainned</th>\n",
       "      <th>token_POS</th>\n",
       "      <th>POS_explainned</th>\n",
       "      <th>dep</th>\n",
       "      <th>T. Head</th>\n",
       "      <th>dep explained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>02/08/23</td>\n",
       "      <td>02/08/23</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>02/08/23</td>\n",
       "      <td>(Gender=Masc, Number=Plur)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>punct</td>\n",
       "      <td>02/08/23</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>16:33</td>\n",
       "      <td>16:33</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>dep</td>\n",
       "      <td>02/08/23</td>\n",
       "      <td>(Gender=Masc, Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "      <td>space</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>space</td>\n",
       "      <td>dep</td>\n",
       "      <td>16:33</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Nota</td>\n",
       "      <td>Nota</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>flat:name</td>\n",
       "      <td>16:33</td>\n",
       "      <td>(Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>519</td>\n",
       "      <td>IBPT</td>\n",
       "      <td>IBPT</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>appos</td>\n",
       "      <td>Alíq</td>\n",
       "      <td>(Gender=Masc, Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>520</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>punct</td>\n",
       "      <td>Alíq</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>521</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "      <td>space</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>space</td>\n",
       "      <td>dep</td>\n",
       "      <td>)</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>522</td>\n",
       "      <td>https://nfs-e.mage.rj.gov.br</td>\n",
       "      <td>https://nfs-e.mage.rj.gov.br</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>appos</td>\n",
       "      <td>Valor</td>\n",
       "      <td>(Gender=Masc, Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>523</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "      <td>space</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>space</td>\n",
       "      <td>dep</td>\n",
       "      <td>https://nfs-e.mage.rj.gov.br</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>524 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                         Texto                         Lemma    Tag  \\\n",
       "0      0                      02/08/23                      02/08/23   NOUN   \n",
       "1      1                             ,                             ,  PUNCT   \n",
       "2      2                         16:33                         16:33  PROPN   \n",
       "3      3                                                              SPACE   \n",
       "4      4                          Nota                          Nota  PROPN   \n",
       "..   ...                           ...                           ...    ...   \n",
       "519  519                          IBPT                          IBPT  PROPN   \n",
       "520  520                             )                             )  PUNCT   \n",
       "521  521                                                              SPACE   \n",
       "522  522  https://nfs-e.mage.rj.gov.br  https://nfs-e.mage.rj.gov.br  PROPN   \n",
       "523  523                             \n",
       "                             \n",
       "  SPACE   \n",
       "\n",
       "    Tag_explainned token_POS POS_explainned        dep  \\\n",
       "0             noun      NOUN           noun       ROOT   \n",
       "1      punctuation     PUNCT    punctuation      punct   \n",
       "2      proper noun     PROPN    proper noun        dep   \n",
       "3            space     SPACE          space        dep   \n",
       "4      proper noun     PROPN    proper noun  flat:name   \n",
       "..             ...       ...            ...        ...   \n",
       "519    proper noun     PROPN    proper noun      appos   \n",
       "520    punctuation     PUNCT    punctuation      punct   \n",
       "521          space     SPACE          space        dep   \n",
       "522    proper noun     PROPN    proper noun      appos   \n",
       "523          space     SPACE          space        dep   \n",
       "\n",
       "                          T. Head               dep explained  \n",
       "0                        02/08/23  (Gender=Masc, Number=Plur)  \n",
       "1                        02/08/23                          ()  \n",
       "2                        02/08/23  (Gender=Masc, Number=Sing)  \n",
       "3                           16:33                          ()  \n",
       "4                           16:33               (Number=Sing)  \n",
       "..                            ...                         ...  \n",
       "519                          Alíq  (Gender=Masc, Number=Sing)  \n",
       "520                          Alíq                          ()  \n",
       "521                             )                          ()  \n",
       "522                         Valor  (Gender=Masc, Number=Sing)  \n",
       "523  https://nfs-e.mage.rj.gov.br                          ()  \n",
       "\n",
       "[524 rows x 10 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization.to_excel(\"lemmatization.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas = [linha for linha in texto_OCR.split('\\n') if linha.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contas para simulacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessage, HumanMessagePromptTemplate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando Patterns especificos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando e carregando patterns e modelos de patterns e criacao de logs JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_patterns_to_file(patterns, colors, filename):\n",
    "    data = {\"patterns\": patterns, \"colors\": colors}\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, ensure_ascii=True, indent=2)\n",
    "        \n",
    "        \n",
    "        \n",
    "def load_patterns_and_colors(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        patterns = data[\"patterns\"]\n",
    "        colors = data[\"colors\"]\n",
    "    return patterns, colors   \n",
    "\n",
    "\n",
    "# my_str.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/home/wklinux/spaCy/configuracoes/pattern_test.json\"  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns, colors = load_patterns_and_colors(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_patterns_to_file(patterns=patterns, colors=colors, filename=filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Modelo de criaçao de registro JSONL</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONL 'Total do contrato'\n",
    "\n",
    "import json\n",
    "with open('output_daniel.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for _, row in df.iterrows():\n",
    "        summary = f\"Summary: 'Total do contrato\\n\\nSpecific information: Contrato {row['8-nro contrato']} tem a quantidade total de {row['9-qt total contrato']} kilos\\n\\n###\\n\\n\"\n",
    "        customer_message = \"Bom dia, Eu gostaria de saber o total do meu contrato?\\n\"\n",
    "        agent_response = f\"Agent: O total do seu contrato {row['8-nro contrato']} é {row['9-qt total contrato']} kilos. Mais alguma informação?\\n\"\n",
    "        customer_message2 = \"Customer: Não, muito obrigado!\\n\"\n",
    "        agent_response2 = \"Agent: Perfeito, Se precisar de algo é só chamar.\\n\"\n",
    "        json_data = {\"prompt\": summary + customer_message + agent_response + customer_message2 + agent_response2,\"completion\": agent_response2}\n",
    "        \n",
    "        \n",
    "        json_string = json.dumps(json_data, ensure_ascii=False)\n",
    "        print(json_string)\n",
    "        \n",
    "        f.write(json_string + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exemplos de implementacao de patterns com analise sintatica</h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/home/wklinux/spaCy/query_utter.json\"  \n",
    "\n",
    "data = load_json(filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### amostras de textos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicio dados da entidade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisys\n",
    "syntatic = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"T_texto\",\"T_shape\", \"T_is_alpha\", \"T_is_digit\", \"T_is_title\", \"T_is_punct\", \"T_is_sent_start\", \"T_is_right_punct\", \"T_is_stop\", \"T_is_quote\", \"T_is_currency\", \"T_morph\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    syntatic.loc[i,\"id\"] = token.i\n",
    "    syntatic.loc[i,\"T_texto\"] = token.text\n",
    "    syntatic.loc[i,\"T_shape\"] = token.shape_\n",
    "    syntatic.loc[i,\"T_is_alpha\"] = token.is_alpha\n",
    "    syntatic.loc[i,\"T_is_digit\"] = token.is_digit\n",
    "    syntatic.loc[i,\"T_is_title\"] = token.is_title\n",
    "    syntatic.loc[i,\"T_is_punct\"] = token.is_punct\n",
    "    syntatic.loc[i,\"T_is_sent_start\"] = token.is_sent_start\n",
    "    syntatic.loc[i,\"T_is_right_punct\"] = token.is_right_punct\n",
    "    syntatic.loc[i,\"T_is_stop\"] = token.is_stop\n",
    "    syntatic.loc[i,\"T_is_quote\"] = token.is_quote\n",
    "    syntatic.loc[i,\"T_is_currency\"] = token.is_currency\n",
    "    syntatic.loc[i,\"T_morph\"] = token.morph\n",
    "    i = i+1\n",
    "\n",
    "syntatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.start:>2} | {ent.text:>21} | {ent.label_:>12} | {ent.id_:>15}  | {ent.end:>2} || {ent.start_char:>3} | {ent.end_char:>3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ent.orth_ for ent in doc.ents if ent.label_ == \"ENTREGUE\"]\n",
    "label_entregue = [ent.orth_ for ent in doc.ents if ent.label_ == \"ENTREGUE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = doc[22]  \n",
    "print(token.morph, token.text) \n",
    "print(token.morph.get(\"PronType\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_root = [token.text for token in doc if token.dep_ == \"ROOT\"][0]\n",
    "token_root"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = list(doc.noun_chunks)\n",
    "for i in range(len(chunks)):\n",
    "    print(i, chunks[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIAZACAO ESTRUTURA DEP, HEAD, ROOT, LEMMA\n",
    "chunks_valor = []\n",
    "\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "  chunk_text = chunk.text\n",
    "  chunks_valor.append(chunk_text)\n",
    "  chunk_root = chunk.root.text\n",
    "  chunk_root_dep = chunk.root.dep_\n",
    "  chunk_root_head = chunk.root.head.text\n",
    "  chunk_root_head_dep = chunk.root.head.dep_\n",
    "  chunk_root_head_lemma = chunk.root.head.lemma_\n",
    "  chunk_ents = chunk.ents\n",
    "  chunk_root_ent_type = chunk.root.ent_type_\n",
    "  \n",
    "  print(f'1.chunk.text: {chunk_text:>26} | 2.ch.root: {chunk_root:>10} | 3.chunk.root.dep_: {chunk_root_dep:>15} | 4.ch.root.head: {chunk_root_head:>12} | 5.ch.root.head.dep_: {chunk_root_head_dep:>10} |  6.chunk.root.head.lemma_: {chunk_root_head_lemma:>9}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep',\n",
    "                jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_chunk(doc, param, i):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_root_head_lemma_ = (chunk.root.head.lemma_).lower()\n",
    "        if chunk_root_head_lemma_ in [\"gostaria\", \"entregar\", \"entregueir\"]:\n",
    "            if chunk.root.dep_ == param:\n",
    "                return chunk.text, i, chunk.doc\n",
    "            # else:\n",
    "            #     return \"erro\", chunk.root.dep_\n",
    "        elif chunk_root_head_lemma_ in [\"qual\", \"entregar\"]:\n",
    "            if chunk.root.dep_ == \"nsubj:pass\":\n",
    "                return chunk.text, i, chunk.doc\n",
    "        elif chunk_root_head_lemma_ in [\"entregar\", \"entregueir\"]:\n",
    "            if chunk.root.dep_ == \"obj\":\n",
    "                return chunk.text, i, chunk.doc   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_text = [chunk.text for chunk in doc.noun_chunks][0]\n",
    "chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_root_dep = [chunk.root.dep_ for chunk in doc.noun_chunks][0]\n",
    "chunk_root_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_root_head_lemma = [chunk.root.head.lemma_ for chunk in doc.noun_chunks][0].lower()\n",
    "chunk_root_head_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(len(data)):\n",
    "    doc_query = nlp(data[i][\"user_utter\"])\n",
    "    result = relevant_chunk(doc=doc_query, param=chunk_root_dep, i=i)\n",
    "    # print(result)\n",
    "    if result:\n",
    "        bot_utter = nlp(data[i][\"bot_utter\"])\n",
    "        user_keys = nlp(data[i][\"user_keys\"])\n",
    "        user_desire = nlp(data[i][\"user_des\"])\n",
    "        missed_keys = nlp(data[i][\"miss_key\"])      \n",
    "        print(bot_utter)\n",
    "        #print(data[i][\"user_utter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "pos_tagging = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"T_texto\",\"T_lemma_\", \"T_pos_\", \"T_tag_\", \"T_dep_\", \"T_head\", \"T_is_sent_start\", \"T_shape_\", \"T_is_alpha\", \"T_is_stop\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    pos_tagging.loc[i,\"id\"] = token.i\n",
    "    pos_tagging.loc[i,\"T_texto\"] = token.text\n",
    "    pos_tagging.loc[i,\"T_lemma_\"] = token.lemma_\n",
    "    pos_tagging.loc[i,\"T_pos_\"] = token.pos_\n",
    "    pos_tagging.loc[i,\"T_tag_\"] = token.tag_\n",
    "    pos_tagging.loc[i,\"T_dep_\"] = token.dep_\n",
    "    pos_tagging.loc[i,\"T_head\"] = token.head\n",
    "    pos_tagging.loc[i,\"T_is_sent_start\"] = token.is_sent_start\n",
    "    pos_tagging.loc[i,\"T_shape_\"] = token.shape_\n",
    "    pos_tagging.loc[i,\"T_is_alpha\"] = token.is_alpha\n",
    "    pos_tagging.loc[i,\"T_is_stop\"] = token.is_stop\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "pos_tagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outras tratativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/etc/odbcinst.ini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trata_contrato(doc, n_contrato):\n",
    "    #nlp = spacy.blank(\"pt\")\n",
    "    #ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    # ruler.add_patterns(patterns)\n",
    "    # doc = nlp(text)\n",
    "    \n",
    "    tokens_contrato = []\n",
    "    ents_contrato = []\n",
    "    \n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        span = doc.char_span(ent.start_char, ent.end_char, label=ent.label_)\n",
    "        ents_contrato.append(span)\n",
    "        \n",
    "    for token in doc:\n",
    "        start = token.idx\n",
    "        end = start + len(token)\n",
    "        if token.shape_ == \"dddX\":\n",
    "            n_contrato.append(token.text)\n",
    "        tokens_contrato.append((token.text, start, end))\n",
    "        \n",
    "    return doc, tokens_contrato, ents_contrato, n_contrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nros_contratos = []\n",
    "n_contrato = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"CONTRATO\":\n",
    "        ent_texto = nlp(ent.text)\n",
    "        print(ent_texto)\n",
    "        doc, tokens_contrato, ents_contrato, n_contrato = trata_contrato(ent_texto, n_contrato)\n",
    "        \n",
    "        nros_contratos.append(ent.text)\n",
    "        \n",
    "        print(f'{ent.start:>2} | {ent.text:>21} | {ent.label_:>8} | {ent.id_:>15} | {ent.end:>2} || {ent.start_char:>3} | {ent.end_char:>3}')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrato_1 = nros_contratos[0]\n",
    "contrato_2 = nros_contratos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novo_doc = nlp(contrato_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trata_contrato(novo_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Mr. Best flew to New York on Saturday morning.\")\n",
    "span = doc[0:6]\n",
    "ents = list(span.ents)\n",
    "assert ents[0].label == 346\n",
    "assert ents[0].label_ == \"PERSON\"\n",
    "assert ents[0].text == \"Mr. Best\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I live in NewYork\")\n",
    "with doc.retokenize() as retokenizer:\n",
    "    heads = [(doc[3], 1), doc[2]]\n",
    "    attrs = {\"POS\": [\"PROPN\", \"PROPN\"],\n",
    "             \"DEP\": [\"pobj\", \"compound\"]}\n",
    "    retokenizer.split(doc[3], [\"New\", \"York\"], heads=heads, attrs=attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for contrato in range(len(list(nros_contratos))):\n",
    "    doc, tokens, ents = show_ent_new(contrato, patterns=patterns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nros_contratos = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"CONTRATO\":\n",
    "        ent_texto = ent.text\n",
    "        doc_contrato = nlp(ent_texto)\n",
    "        print(doc_contrato)\n",
    "        print()\n",
    "        for token in doc_contrato:\n",
    "            #print(token.i, token.text, token.shape_)\n",
    "            print()\n",
    "            print()\n",
    "            qtd_contratops = token.morph.get(\"Number\")\n",
    "            if \"Plur\" in qtd_contratops:\n",
    "                # print(\"               Ele esta falando mais de um contrato\")\n",
    "                if token.shape_ == \"dddX\":\n",
    "                    nro_contrato = token.text\n",
    "                    nros_contratos.append(nro_contrato)\n",
    "                    print(f'         Ele esta falando de contratos (plural),  esse e o nro do contrato: {nro_contrato}\\n\\n')\n",
    "                else:\n",
    "                    print(f'         Ele esta falando de contratos (plural) -  Nao ha nro de contrato\\n')\n",
    "                    nros_contratos.append(nro_contrato)\n",
    "            else:\n",
    "                # print(\"               ele esta falando no singular - contrato\")    \n",
    "                if token.shape_ == \"dddX\":\n",
    "                    nro_contrato = token.text\n",
    "                    nros_contratos.append(nro_contrato)\n",
    "                    print(f'          Ele esta falando de contrato (singular),  esse e o nro do contrato: {nro_contrato}\\n\\n')\n",
    "                else:\n",
    "                    print(f'          Ele esta falando de contrato (singular) - Nao ha nro de contrato\\n')\n",
    "                    nros_contratos.append(nro_contrato)\n",
    "                    \n",
    "            print()        \n",
    "       \n",
    "# print(nros_contratos)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nros_contratos = []\n",
    "entidades = list(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(entidades)):\n",
    "    print(f'1. idx: {i} | {doc[i].text:>19} | type: {doc[i].ent_type_:>10} {doc[i].ent_id_} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = list(doc.noun_chunks)\n",
    "for i in range(len(chunks)):\n",
    "    # if chunks[i].root.ent_type_ == \"CONTRATO\":\n",
    "        print(f'1. idx: {i} | {chunks[i].text:>19} | {chunks[i].start:>2} {chunks[i].end:>2} | {chunks[i].start_char:>3}  {chunks[i].end_char:>3} || type: {chunks[i].root.ent_type_:>10} {chunks[i].ents} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ent for ent in doc.ents if ent.label_ == \"CONTRATO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ent for ent in doc.ents if ent.label_ == \"NR_CONTRATO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([ent for ent in doc.ents if ent.label_ == \"CONTRATO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = list(doc.noun_chunks)\n",
    "for i in range(len(chunks)):\n",
    "    if chunks[i].root.ent_type_ == \"CONTRATO\":\n",
    "        print(f'1. idx: {i} | {chunks[i].text:>19} | {chunks[i].start:>2} {chunks[i].end:>2} | {chunks[i].start_char:>3}  {chunks[i].end_char:>3} || type: {chunks[i].root.ent_type_:>10} {chunks[i].ents} ')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nros_contratos = []\n",
    "chunks = list(doc.noun_chunks)\n",
    "for i in range(len(chunks)):\n",
    "    if chunks[i].root.ent_type_ == \"CONTRATO\":\n",
    "        print(f'1. idx: {i} | {chunks[i].text:>19} | {chunks[i].start:>2} {chunks[i].end:>2} | {chunks[i].start_char:>3}  {chunks[i].end_char:>3} || type: {chunks[i].root.ent_type_:>10} {chunks[i].ents} ')\n",
    "        print()\n",
    "        doc_contrato = nlp(chunks[i].text)\n",
    "        for token in doc_contrato:\n",
    "            #print(token.i, token.text, token.shape_)\n",
    "            print()\n",
    "            print()\n",
    "            qtd_contratops = token.morph.get(\"Number\")\n",
    "            if \"Plur\" in qtd_contratops:\n",
    "                # print(\"               Ele esta falando mais de um contrato\")\n",
    "                if token.shape_ == \"dddX\":\n",
    "                    nro_contrato = token.text\n",
    "                    nros_contratos.append(nro_contrato)\n",
    "                    print(f'         Ele esta falando de contratos (plural),  esse e o nro do contrato: {nro_contrato}\\n\\n')\n",
    "                else:\n",
    "                    print(f'         Ele esta falando de contratos (plural) -  Nao ha nro de contrato\\n')\n",
    "            else:\n",
    "                # print(\"               ele esta falando no singular - contrato\")    \n",
    "                if token.shape_ == \"dddX\":\n",
    "                    nro_contrato = token.text\n",
    "                    nros_contratos.append(nro_contrato)\n",
    "                    print(f'          Ele esta falando de contrato (singular),  esse e o nro do contrato: {nro_contrato}\\n\\n')\n",
    "                else:\n",
    "                    print(f'          Ele esta falando de contrato (singular) - Nao ha nro de contrato\\n')\n",
    "            print()        \n",
    "       \n",
    "print(nros_contratos)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"CONTRATO\":\n",
    "        doc_contrato = nlp(ent.text)\n",
    "        # print(token.text, token.ent_type_, token.shape_)\n",
    "        qtd_contratops = ent.orth_\n",
    "        #print(doc_contrato)\n",
    "        for token in doc_contrato:\n",
    "            print(token.i, token.text, token.shape_)\n",
    "            print()\n",
    "            print()\n",
    "            qtd_contratops = token.morph.get(\"Number\")\n",
    "            if \"Plur\" in qtd_contratops:\n",
    "                print(\"Ele esta falando mais de um contrato\")\n",
    "                if token.shape_ == \"dddX\":\n",
    "                    nro_contrato = token.text\n",
    "                    print(f'esse e o nro do contrato: {nro_contrato}')\n",
    "                else:\n",
    "                    print(f'Nao ha nro de contrato')\n",
    "            else:\n",
    "                print(\"ele esta falando no singular - contrato\")    \n",
    "                if token.shape_ == \"dddX\":\n",
    "                    nro_contrato = token.text\n",
    "                    print(f'esse e o nro do contrato: {nro_contrato}')\n",
    "                else:\n",
    "                    print(f'Nao ha nro de contrato')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.ent_type_ == \"CONTRATO\":\n",
    "        # print(token.text, token.shape_)\n",
    "        if token.shape_ == \"dddX\":\n",
    "            print(token.text)\n",
    "        # print(token.text, token.shape_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrato = [ent for ent in doc.ents if ent.label_ == \"CONTRATO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"CONTRATO\":\n",
    "        doc_contrato = nlp(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_root = [token.text for token in doc if token.dep_ == \"ROOT\"][0]\n",
    "token_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrato_keys = contrato[0].text.split('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ent for ent in doc.ents if ent.label_ == \"CONTRATO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ent for ent in doc.ents if ent.label_ == \"SAFRA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamento para contratos\n",
    "for token in doc:\n",
    "    if token.ent_type_ == \"CONTRATO\":\n",
    "        # print(token.text, token.ent_type_, token.shape_)\n",
    "        qtd_contratops = token.morph.get(\"Number\")\n",
    "        # print(qtd_contratops)\n",
    "        if \"Plur\" in qtd_contratops:\n",
    "            print(\"Ele esta falando mais de um contrato\")\n",
    "        else:\n",
    "             print(\"Ele esta falando somente de um contrato\")   \n",
    "        if token.shape_ == \"dddX\":\n",
    "            nro_contrato = token.text\n",
    "            print(f'esse e o nro do contrato: {nro_contrato}')\n",
    "        else:\n",
    "            print(f'nao ha numero do contrato')\n",
    "    \n",
    "    # elif token.ent_type_ == \"CONTRATO\":    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_utterance = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"POS\": \"PROPN\", \"LENGTH\": {\">=\": 10}},\n",
    "\n",
    "\n",
    "[\n",
    "  {\"POS\": \"ADJ\", \"OP\": \"*\"},\n",
    "  {\"POS\": \"NOUN\", \"OP\": \"+\"}\n",
    "  {\"POS\": \"PROPN\", \"OP\": \"{2}\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternsContrato = [\n",
    "    {\n",
    "        \"label\":\"CONTRATO\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"contrato\", \"OP\":\"*\"},\n",
    "            {\"SHAPE\": \"dddX\", \"OP\":\"*\"},\n",
    "            {\"LOWER\": \"contratos\", \"OP\":\"*\"},\n",
    "            {\"SHAPE\": \"dddX\", \"OP\":\"*\"}\n",
    "            \n",
    "        ]\n",
    "        \n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens de valor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento de noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_chunk(doc, param, i):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_root_head_lemma_ = (chunk.root.head.lemma_).lower()\n",
    "        if chunk_root_head_lemma_ in [\"gostaria\", \"entregar\", \"entregueir\"]:\n",
    "            if chunk.root.dep_ == param:\n",
    "                return chunk.text, i, chunk.doc\n",
    "            # else:\n",
    "            #     return \"erro\", chunk.root.dep_\n",
    "        elif chunk_root_head_lemma_ in [\"qual\"]:\n",
    "            if chunk.root.dep_ == \"ROOT\":\n",
    "                return chunk.text, i, chunk.doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_text = [chunk.text for chunk in doc.noun_chunks][0]\n",
    "chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_root_dep = [chunk.root.dep_ for chunk in doc.noun_chunks][0]\n",
    "chunk_root_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_root_head_lemma = [chunk.root.head.lemma_ for chunk in doc.noun_chunks][0].lower()\n",
    "chunk_root_head_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = list(doc.noun_chunks)\n",
    "for i in range(len(chunks)):\n",
    "    print(f'1. idx: {i} | {chunks[i].text:>13} | {chunks[i].start:>2} {chunks[i].end:>2} | {chunks[i].start_char:>2}  {chunks[i].end_char:>2} || {chunks[i].root.ent_type_:>10} {chunks[i].ents} ')\n",
    "    \n",
    "    \n",
    "    # print(f'1.chunk.text: {chunk_text:>15} id: {chunk.id_} || {chunk.start:>2} | {chunk.end:>2} || {chunk.start_char:>2} | {chunk.end_char:>2} || 4.chunk_root_ent_type {chunk_root_ent_type:>1} | 5.chunk_ents {chunk_ents}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Preciso saber o saldo do contrato 342F da fazenda Santa Rita\"\n",
    "\n",
    "doc, tokens, ents = show_ent_new(text, patterns=patterns)\n",
    "print(doc)\n",
    "print()\n",
    "## GARANTIR que a ordem dos Tokens esteja correta (ascendente)\n",
    "seq_tokens_id = []\n",
    "seq_tokens_valor = []\n",
    "chunks_valor = []\n",
    "## tokens_ids + Lista de todos os Tokens da frase\n",
    "for token in doc:\n",
    "    seq_tokens_id.append(token.i)\n",
    "    \n",
    "    \n",
    "displacy.render(doc, style=\"ent\", options={\"colors\": colors})\n",
    "\n",
    "displacy.render(doc, style='dep',\n",
    "                jupyter=True, options={'distance': 120})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "  chunk_text = chunk.text\n",
    "  chunks_valor.append(chunk_text)\n",
    "  chunk_root = chunk.root.text\n",
    "  chunk_root_dep = chunk.root.dep_\n",
    "  chunk_root_head = chunk.root.head.text\n",
    "  chunk_root_head_dep = chunk.root.head.dep_\n",
    "  chunk_root_head_lemma = chunk.root.head.lemma_\n",
    "  chunk_ents = chunk.ents\n",
    "  chunk_root_ent_type = chunk.root.ent_type_\n",
    "  \n",
    "  print(f'1.chunk.text: {chunk_text:>15} | 2.ch.root: {chunk_root:>14} | 3.chunk.root.dep_: {chunk_root_dep:>18} | 4.ch.root.head: {chunk_root_head:>12} | 5.ch.root.head.dep_: {chunk_root_head_dep:>10} |  6.chunk.root.head.lemma_: {chunk_root_head_lemma:>9}')\n",
    "\n",
    "print()\n",
    "chunks = list(doc.noun_chunks)\n",
    "for i in range(len(chunks)):\n",
    "    print(f'idx: {i} | {chunks[i].text:>13} | {chunks[i].start:>2} {chunks[i].end:>2} | {chunks[i].start_char:>2}  {chunks[i].end_char:>2} || {chunks[i].root.ent_type_:>10} {chunks[i].ents} ')\n",
    "\n",
    "print()\n",
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_resize_gray(original_file_name, file_path, image_resized_path):\n",
    "\n",
    "    name_image = utl.conv_filename_no_ext(original_file_name)\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(name_image)}.jpg')\n",
    "    pages = convert_from_path(file_path, 500, poppler_path=poppler_path)\n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((4134, 5846))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "    imagem_gray = resized_pages[0].convert('L')\n",
    "    imagem_gray.save(image_resized_name, 'JPEG')\n",
    "\n",
    "    return  imagem_gray, image_resized_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_valortotal2 = {\n",
    "    \"label\": \"TOTALNOTA\",\n",
    "    \"pattern\": [\n",
    "        {\"LOWER\": \"valor\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"total\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"da\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"nota\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"TEXT\": {\"REGEX\": \"R\\$\"}},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"TEXT\": {\"REGEX\": \"[0-9]{1,3}(\\.[0-9]{3})*(,[0-9]{2})?\"}, \"OP\": \"?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "patternsOthers = [{\"label\": \"PERSON\", \"pattern\": \"Daniel\", \"id\": \"pessoa-daniel\"},\n",
    "                  {\"label\": \"PERSON\", \"pattern\": \"Antônio\", \"id\": \"pessoa-antonio\"}] \n",
    " \n",
    "patternsCult = [\n",
    "    {\n",
    "        \"label\":\"CULTURA\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"soja\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"milho\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"sorgo\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"trigo\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"milheto\", \"OP\":\"?\"},  \n",
    "            \n",
    "        ],    \n",
    "    \"id\": \"cultura\"}]\n",
    "\n",
    "patternsValores = [{\"label\":\"VALOR_SERVICO\", \"pattern\": [{\"LOWER\": \"valor\", \"OP\":\"?\"}, {\"LOWER\": \"serviços\", \"OP\":\"*\"}], \"id\": \"val-servico\"},\n",
    "                 {\"label\":\"ENTREGUE\", \"pattern\": [{\"LOWER\": \"quantidade\", \"OP\":\"?\"},{\"LOWER\": \"entregue\",\"OP\":\"*\"}],\"id\": \"qtde-entregue\"},\n",
    "                 {\"label\":\"ENTREGUE\", \"pattern\": [{\"LOWER\": \"quantidade\", \"OP\":\"?\"},{\"LOWER\": \"entreguei\",\"OP\":\"*\"}],\"id\": \"qtde-entregue\"},\n",
    "                 {\"label\":\"ENTREGUE\", \"pattern\": [{\"LOWER\": \"foram\", \"OP\":\"?\"},{\"LOWER\": \"entregues\",\"OP\":\"*\"}],\"id\": \"qtde-entregue\"},\n",
    "                 {\"label\":\"SALDO\", \"pattern\": [{\"LOWER\": \"saldo\",\"OP\":\"*\"}], \"id\": \"qtde-saldo\"}]\n",
    "\n",
    "\n",
    "patternsSafra = [{\"label\":\"SAFRA\", \"pattern\": [{\"LOWER\": \"safra\", \"OP\":\"?\"},{\"LOWER\": \"safras\", \"OP\":\"?\"}], \"id\": \"safra\"}]\n",
    "\n",
    "\n",
    "patternsNroSafra = [{\"label\":\"NR_SAF\", \"pattern\": [{\"SHAPE\": \"dd/dd\", \"OP\":\"*\"}], \"id\": \"nro_safra\"},\n",
    "                    {\"label\":\"NR_SAF\", \"pattern\": [{\"lower\": \"próxima\", \"OP\":\"*\"}], \"id\": \"nro_safra\"},\n",
    "                    {\"label\":\"NR_SAF\", \"pattern\": [{\"lower\": \"passada\", \"OP\":\"*\"}], \"id\": \"nro_safra\"}]   \n",
    "\n",
    "patternsCliente = [{\"label\": \"CLIENTE\", \"pattern\": [{\"LOWER\": \"berdinazzi\"}], \"id\": \"cli-berdinazzi\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"LOWER\": \"lopito\"}], \"id\": \"cli-lopito\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"LOWER\": \"bungue\"}], \"id\": \"cli-bungue\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"bunge\", \"bongue\", \"bumgue\"]}}}], \"id\": \"cli-bungue\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"berdinazi\"]}}}], \"id\": \"cli-berdinazzi\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"matarazzo\"]}}}], \"id\": \"cli-matarazzo\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"mezenga\"]}}}], \"id\": \"cli-mezenga\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"LOWER\": \"rei\"}, {\"LOWER\": \"do\"}, {\"LOWER\": \"gado\"}], \"id\": \"cli-reidogado\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"rei-do-gado\"]}}}], \"id\": \"cli-reidogado\"},   \n",
    "                   ]\n",
    "\n",
    "patternsOthersFazenda = [{\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"santa\"}, {\"LOWER\": \"rita\"}], \"id\": \"faz-santarita\"},\n",
    "                         {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"bela\"}, {\"LOWER\": \"vista\"}], \"id\": \"faz-belavista\"},\n",
    "                         {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"passo\"}, {\"LOWER\": \"fundo\"}], \"id\": \"faz-passo\"},\n",
    "                         {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"minha\"}, {\"LOWER\": \"fazenda\"}], \"id\": \"faz-produtor\"},\n",
    "                        {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"agrobi\"}], \"id\": \"faz-produtor\"}\n",
    "                        ]\n",
    " \n",
    "patternsContrato = [{\"label\": \"CONTRATO\", \"pattern\": [{\"LOWER\": \"contrato\", \"OP\":\"?\"}], \"id\": \"contrato\"},\n",
    "                    {\"label\": \"CONTRATO\", \"pattern\": [{\"LOWER\": \"contratos\", \"OP\":\"?\"}], \"id\": \"contrato\"}]\n",
    "\n",
    "patternsNroContrato = [{\"label\": \"NR_CONT\", \"pattern\": [{\"SHAPE\": \"dddX\", \"OP\":\"*\"}], \"id\": \"nro_contrato\"},\n",
    "                       {\"label\": \"NR_CONT\", \"pattern\": [{\"POS\": \"NUM\", \"SHAPE\": \"ddd\", \"OP\": \"*\"},\n",
    "                                                            {\"POS\": \"PROPN\", \"SHAPE\": \"X\", \"OP\": \"*\"}], \"id\": \"nro_contrato\"}]\n",
    "\n",
    "patternsIntent = [{\"label\": \"INTENT\", \"pattern\": [{\"IS_TITLE\": True, \"OP\":\"*\"}], \"id\": \"user-intent\"},\n",
    "                  {\"label\": \"INTENT\", \"pattern\": [{\"POS\": \"ADJ\", \"OP\":\"*\"}, {\"POS\": \"VERB\", \"OP\":\"*\"}], \"id\": \"user-intent\"},\n",
    "                  {\"label\": \"INTENT\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"preciso\", \"gostaria\", \"informar\"]}}}], \"id\": \"user-intent\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = {\n",
    "        \"CULTURA\": \"linear-gradient(90deg, #2ADB5E, #1FA346)\", \n",
    "        \"TOTAL\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\",\n",
    "        \"ENTREGUE\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\",\n",
    "        \"SALDO\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\", \n",
    "        \"FAZENDA\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\", \n",
    "        \"SAFRA\": \"linear-gradient(90deg, #FFC90E, #BA930A)\", \n",
    "        \"CONTRATO\": \"linear-gradient(90deg, #B5B5B5, #8A8A8A)\",\n",
    "        \"INTENT\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\"}\n",
    "\n",
    "\n",
    "patternsOthers = [{\"label\": \"PERSON\", \"pattern\": \"Daniel\", \"id\": \"daniel\"},\n",
    "                  {\"label\": \"PERSON\", \"pattern\": \"Antônio\", \"id\": \"antônio\"},\n",
    "                  {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"fast\"}, {\"LOWER\": \"innovation\"}], \"id\": \"fast-innovation\"},\n",
    "                  {\"label\": \"ORG\", \"pattern\": {\"LOWER\": \"agrobi\"}, \"id\": \"agrobi\"},\n",
    "                  {\"label\": \"ORG\", \"pattern\": {\"LOWER\": \"AgroBi\"}, \"id\": \"agrobi\"},\n",
    "                  {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"agro\"}, {\"LOWER\": \"bi\"}], \"id\": \"agrobi\"},\n",
    "                  ] \n",
    " \n",
    "\n",
    "patternsCult = [\n",
    "    {\n",
    "        \"label\":\"CULTURA\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"soja\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"milho\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"sorgo\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"trigo\", \"OP\":\"?\"},\n",
    "            \n",
    "        ]    \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "patternsQuant = [\n",
    "    {\n",
    "        \"label\":\"TOTAL\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"total\"},\n",
    "            {\"LOWER\": \"quantidade\", \"OP\":\"?\"},\n",
    "            \n",
    "        ]    \n",
    "    },\n",
    "    {\n",
    "        \"label\":\"ENTREGUE\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"quantidade\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"entregue\",\"OP\":\"?\"},\n",
    "            {\"LOWER\": \"entregues\",\"OP\":\"?\"},\n",
    "            {\"LOWER\": \"entregado\",\"OP\":\"?\"},\n",
    "            {\"LOWER\": \"entreguei\",\"OP\":\"?\"},\n",
    "        ]    \n",
    "    },\n",
    "    {\n",
    "        \"label\":\"SALDO\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"saldo\"},\n",
    "            {\"LOWER\": \"quantidade\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"total\", \"OP\":\"?\"},\n",
    "        ]    \n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "patternsSafra = [\n",
    "    {\n",
    "        \"label\":\"SAFRA\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"safra\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"safras\", \"OP\":\"?\"},\n",
    "            {\"SHAPE\": \"dd/dd\"},\n",
    "        ]    \n",
    "    }\n",
    "]\n",
    "\n",
    "patternsOthersFazenda = [{\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"santa\"}, {\"LOWER\": \"rita\"}], \"id\": \"faz-santa-rita\"},\n",
    "                         {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"bela\"}, {\"LOWER\": \"vista\"}], \"id\": \"faz-bela-vista\"},\n",
    "                         {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"passo\"}, {\"LOWER\": \"fundo\"}], \"id\": \"faz-passo-fundo\"},\n",
    "                         {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"minha\"}, {\"LOWER\": \"fazenda\"}], \"id\": \"faz-produtor\"}\n",
    "                        ]\n",
    " \n",
    "\n",
    "patternsContrato = [\n",
    "    {\n",
    "        \"label\":\"CONTRATO\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"contrato\", \"OP\":\"*\"},\n",
    "            {\"SHAPE\": \"dddX\", \"OP\":\"*\"},\n",
    "            {\"LOWER\": \"contratos\", \"OP\":\"*\"},\n",
    "            {\"SHAPE\": \"dddX\", \"OP\":\"*\"}\n",
    "            \n",
    "        ]\n",
    "        \n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "patterns = patternsCult + patternsQuant + patternsOthersFazenda + patternsSafra + patternsContrato + patternsOthers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"merge_noun_chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_text = (pytesseract.image_to_string(imagem_gray, lang='por'))\n",
    "ocr_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principais funcoes\n",
    "\n",
    "def busca_emitente(remoteJid):\n",
    "    \n",
    "    for entidade in entidades.find({\"remoteJid\": remoteJid}):\n",
    "            nome_entidade = entidade['Nome']\n",
    "            sobrenome_entidade = entidade['Sobrenome']\n",
    "            tipo_entidade = entidade['Tipo']\n",
    "            razao_entidade = entidade['razao']\n",
    "            \n",
    "    return  nome_entidade, sobrenome_entidade, tipo_entidade, razao_entidade\n",
    "\n",
    "\n",
    "def show_ent_new(text, patterns):\n",
    "    #nlp = spacy.blank(\"pt\")\n",
    "    #ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    ruler.add_patterns(patterns)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tokens = []\n",
    "    ents = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        span = doc.char_span(ent.start_char, ent.end_char, label=ent.label_)\n",
    "        ents.append(span)\n",
    "        \n",
    "    for token in doc:\n",
    "        start = token.idx\n",
    "        end = start + len(token)\n",
    "        tokens.append((token.text, start, end))\n",
    "        \n",
    "    return doc, tokens, ents\n",
    "\n",
    "\n",
    "\n",
    "# chunk.text, chunk.start, chunk.end, chunk.root.head.lemma_, chunk.root.dep_, chunk.doc\n",
    "def load_json(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "        \"TOTALNOTA\": \"linear-gradient(90deg, #2ADB5E, #1FA346)\", \n",
    "        \"TOTAL\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\",\n",
    "        \"ENTREGUE\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\",\n",
    "        \"SALDO\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\", \n",
    "        \"FAZENDA\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\", \n",
    "        \"SAFRA\": \"#FFEA7F\",\n",
    "        \"NR_SAF\": \"#CCA10C\", \n",
    "        \"CONTRATO\": \"#AB9BFC\",\n",
    "        \"NR_CONT\": \"#7AECEC\",\n",
    "        \"INTENT\": \"#EE8AF8\",\n",
    "        \"INTENT\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\",\n",
    "        \"PERSON\": \"linear-gradient(90deg, #FFA9FB, #BF7FBC)\",\n",
    "        \"CLIENTE\": \"linear-gradient(90deg, #FFA9FB, #BF7FBC)\"}\n",
    "\n",
    "\n",
    "\n",
    "pattern_valortotal = [{\n",
    "    \"label\": \"TOTALNOTA\",\n",
    "    \"pattern\": [\n",
    "        {\"LOWER\": \"valor\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"total\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"da\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"nota\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"TEXT\": {\"REGEX\": \"R\\$\"}},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"TEXT\": {\"REGEX\": \"[0-9]{1,3}(\\.[0-9]{3})*(,[0-9]{2})?\"}, \"OP\": \"?\"}\n",
    "    ]\n",
    "}]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "patterns = pattern_valortotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Executar o comando OCRmyPDF\n",
    "!ocrmypdf --language por --deskew 'pipeline_extracao_documentos/0_arquivos_teste_pipeline/pdf_raster/Mage/Doria Marinho 0295 Carlos Leandro.pdf' 'output.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ocrmypdf --language por --deskew '/home/dani-boy/extractNF/pipeline_extracao_documentos/0_arquivos_teste_pipeline/pdf_raster/Sao_predo_aldeia/2023143.pdf' 'output.pdf'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
