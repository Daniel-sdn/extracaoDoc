{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0_entities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando modulos gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import PyPDF2\n",
    "\n",
    "import locale\n",
    "import time, copy\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import cv2\n",
    "import fitz  # Módulo PyMuPDF\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import Image, ImageDraw\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar\n",
    "import matplotlib.pyplot as plt\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "# import modules.extrai_pdf_pesquisavel as Extc\n",
    "import modules.cronometro as cron\n",
    "import modules.nova_extracao_pdf_pesquisavel as novaextra \n",
    "import modules.trata_model as tmod\n",
    "import modules.trata_pdf as tpdf\n",
    "import modules.utils as utl\n",
    "\n",
    "\n",
    "import json\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "\n",
    "# 5. Path para documentos para extracao\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "# 6. Path para gestao de imagens resized\n",
    "image_resized_path = \"pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas\"\n",
    "\n",
    "\n",
    "# 12. poppler path\n",
    "poppler_path = \"/home/dani-boy/miniconda3/envs/tables-detr/bin\"\n",
    "\n",
    "\n",
    "\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================== 1. CABECALHO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Data e Hora de Emissão:\n",
    "data_hora_emissao_pattern = [\n",
    "    {\"LOWER\": \"data\"},\n",
    "    {\"LOWER\": \"e\"},\n",
    "    {\"LOWER\": \"hora\"},\n",
    "    {\"LOWER\": \"da\"},\n",
    "    {\"LOWER\": \"emissão\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"SHAPE\": \"dd/dd/dddd\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"SHAPE\": \"dd:dd:dd\"}\n",
    "]\n",
    "matcher.add(\"DATA_HORA_EMISSAO\", [data_hora_emissao_pattern])\n",
    "\n",
    "\n",
    "# 4. Código de Verificação:\n",
    "codigo_verificacao_pattern = [\n",
    "    {\"LOWER\": \"código\"},\n",
    "    {\"LOWER\": \"verificação\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_ASCII\": True, \"LENGTH\": 9}\n",
    "]\n",
    "matcher.add(\"CODIGO_VERIFICACAO\", [codigo_verificacao_pattern])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#========================================  5. VALOR TOTAL\n",
    "valor_total_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"total\"},\n",
    "    {\"LOWER\": \"da\", \"OP\": \"?\"},\n",
    "    {\"LOWER\": \"nota\", \"OP\": \"?\"},\n",
    "    {\"TEXT\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_TOTAL\", [valor_total_pattern])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#======================================== 7. VALORES E IMPOSTOS\n",
    "# 1. VALOR_SERVICOS\n",
    "valor_servicos_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"serviços\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "\n",
    "matcher.add(\"VALOR_SERVICOS\", [valor_servicos_pattern])\n",
    "\n",
    "\n",
    "# 2. VALOR DEDUÇÃO:\n",
    "valor_deducao_pattern = [\n",
    "    {\"LOWER\": \"dedução\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "\n",
    "matcher.add(\"VALOR_DEDUCAO\", [valor_deducao_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 3. DESC. INCOND: RASTER_PDF\n",
    "valor_incondR_pattern = [\n",
    "    {\"LOWER\": \"base\"},\n",
    "    {\"LOWER\": \"de\"},\n",
    "    {\"IS_SPACE\": True},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"ORTH\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}    \n",
    "]\n",
    "matcher.add(\"VALOR_INCONDP\", [valor_incondR_pattern])\n",
    "\n",
    "\n",
    "# 3.A DESC. INCOND: - PDF_Pesquisavel   #DESC. INCOND:\n",
    "valor_incond_patternP = [\n",
    "    {\"LOWER\": \"desc\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},\n",
    "    {\"LOWER\": \"incond\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"ORTH\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}    \n",
    "]\n",
    "matcher.add(\"VALOR_INCONDR\", [valor_incond_patternP])\n",
    "\n",
    "\n",
    "\n",
    "# 4. BASE DE CÁLCULO:  RASTER_PDF\n",
    "valor_calculoR_pattern = [\n",
    "    {\"LOWER\": \"cálculo\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_CALCULOR\", [valor_calculoR_pattern])\n",
    "\n",
    "\n",
    "# 4.A BASE DE CÁLCULO:  PDF_P\n",
    "valor_calculoP_pattern = [\n",
    "    {\"LOWER\": \"base\"},\n",
    "    {\"LOWER\": \"de\"},\n",
    "    {\"LOWER\": \"cálculo\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_CALCULOP\", [valor_calculoP_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 5. Alíquota d,dd\n",
    "valor_aliquota_pattern = [\n",
    "    {\"LOWER\": \"alíquota\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"d,dd\", \"OP\": \"?\"},\n",
    "    {\"ORTH\": \"%\"}\n",
    "\n",
    "]\n",
    "matcher.add(\"VALOR_ALIQUOTA\", [valor_aliquota_pattern])\n",
    "\n",
    "# 5.1 Alíquota d\n",
    "valor_aliquota2_pattern = [\n",
    "    {\"LOWER\": \"alíquota\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"d\", \"OP\": \"?\"},\n",
    "    {\"ORTH\": \"%\"}\n",
    "\n",
    "]\n",
    "matcher.add(\"VALOR_ALIQUOTA2\", [valor_aliquota2_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 6. VALOR ISS:\n",
    "valor_iss_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"iss\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_ISS\", [valor_iss_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 7. VALOR ISS RETIDO:\n",
    "valor_issretido_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"iss\"},\n",
    "    {\"LOWER\": \"retido\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_ISSRETIDO\", [valor_issretido_pattern])\n",
    "\n",
    "\n",
    "# 8. DESC. COND:\n",
    "valor_desccond_pattern = [\n",
    "    {\"LOWER\": \"desc\"},\n",
    "    {\"ORTH\": \".\"},\n",
    "    {\"LOWER\": \"cond\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_DESCCOND\", [valor_desccond_pattern])\n",
    "\n",
    "\n",
    "# 9. VALOR PIS:\n",
    "valor_pis_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"pis\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_PIS\", [valor_pis_pattern])\n",
    "\n",
    "\n",
    "# 10. VALOR COFINS:\n",
    "valor_cofins_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"cofins\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_COFINS\", [valor_cofins_pattern])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 11. VALOR IR:\n",
    "valor_ir_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"ir\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_IR\", [valor_ir_pattern])\n",
    "\n",
    "\n",
    "# 12. VALOR INSS:\n",
    "valor_inss_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"inss\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_INSS\", [valor_inss_pattern])\n",
    "\n",
    "\n",
    "# 13. VALOR CSLL:\n",
    "valor_csll_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"csll\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_CSLL\", [valor_csll_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 14. OUTRAS RETENÇÕES:\n",
    "valor_outrasreten_pattern = [\n",
    "    {\"LOWER\": \"outras\"},\n",
    "    {\"LOWER\": \"retenções\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_OUTRAS\", [valor_outrasreten_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 15. VALOR LÍQUIDO:\n",
    "valor_liquido_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"líquido\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"VALOR_LIQUIDO\", [valor_liquido_pattern])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#======================================== 9. OUTRAS INFORMAÇOES / CRITICAS\n",
    "# 1. EXIGIBILIDADE ISS\n",
    "exigibilidade_iss_pattern = [\n",
    "    {\"LOWER\": \"exigibilidade\"},\n",
    "    {\"LOWER\": \"iss\"},\n",
    "    {\"LOWER\": {\"IN\": [\"exigivel\", \"não exigivel\"]}}\n",
    "]\n",
    "matcher.add(\"EXIGIBILIDADE_ISS\", [exigibilidade_iss_pattern])\n",
    "\n",
    "\n",
    "# 2. REGIME TRIBUTAÇÃO\n",
    "padrao_regime_tributacao = [\n",
    "    {\"LOWER\": \"regime\"},\n",
    "    {\"LOWER\": \"tributação\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"+\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"*\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": \"simples\", \"OP\": \"?\"},\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"REGIME_TRIBUTACAO\", [padrao_regime_tributacao])\n",
    "\n",
    "# 3. SIMPLES NACIONAL = NAO\n",
    "simples_nacional_nao_pattern = [\n",
    "    {\"LOWER\": \"simples\"},\n",
    "    {\"LOWER\": \"nacional\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": \"não\"}\n",
    "]\n",
    "matcher.add(\"SIMPLES_NACIONAL_NAO\", [simples_nacional_nao_pattern])\n",
    "\n",
    "# 3.1 SIMPLES NACIONAL = SIM\n",
    "simples_nacional_pattern = [\n",
    "    {\"LOWER\": \"simples\"},\n",
    "    {\"LOWER\": \"nacional\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": \"sim\", \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"ORTH\": \"(\", \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"ORTH\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"ORTH\": \"%\", \"OP\": \"?\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"ORTH\": \")\", \"OP\": \"?\"}\n",
    "]\n",
    "matcher.add(\"SIMPLES_NACIONAL_SIM\", [simples_nacional_pattern])\n",
    "\n",
    "\n",
    "# 4. ISSQN RETIDO\n",
    "issqn_retido_pattern = [\n",
    "    {\"LOWER\": \"issqn\"},\n",
    "    {\"LOWER\": \"retido\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": {\"IN\": [\"sim\", \"não\"]}}\n",
    "]\n",
    "matcher.add(\"ISSQN_RETIDO\", [issqn_retido_pattern])\n",
    "\n",
    "\n",
    "# 5. LOCAL. PRESTAÇÃO SERVIÇO\n",
    "local_prestacao_servico_pattern = [\n",
    "    {\"LOWER\": \"local\"},\n",
    "    {\"ORTH\": \".\"},\n",
    "    {\"LOWER\": \"prestação\"},\n",
    "    {\"LOWER\": \"serviço\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"+\"},  # para lidar com múltiplos espaços\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"+\"},  # para a cidade\n",
    "    {\"ORTH\": \"-\", \"OP\": \"?\"},\n",
    "    {\"IS_UPPER\": True, \"LENGTH\": 2, \"OP\": \"?\"}  # para a sigla do estado\n",
    "]\n",
    "matcher.add(\"LOCAL_PRESTACAO_SERVICO\", [local_prestacao_servico_pattern])\n",
    "\n",
    "# 6. LOCAL INCIDÊNCIA\n",
    "local_incidencia_pattern = [\n",
    "    {\"LOWER\": \"local\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},\n",
    "    {\"LOWER\": \"incidência\"},\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"+\"},  # Nome da cidade\n",
    "    {\"ORTH\": \"-\", \"OP\": \"?\"},  # Hífen opcional\n",
    "    {\"SHAPE\": \"XX\", \"OP\": \"?\"}  # Sigla do estado\n",
    "]\n",
    "matcher.add(\"LOCAL_INCIDENCIA\", [local_incidencia_pattern])\n",
    "\n",
    "\n",
    "# observacao_pattern = [\n",
    "#     {\"LOWER\": \"observação\"},\n",
    "#     {\"ORTH\": \":\"},\n",
    "#     {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "#     {\"LOWER\": \"-\", \"OP\": \"?\"},\n",
    "#     {\"IS_PRINT\": True, \"OP\": \"+\"}\n",
    "# ]\n",
    "\n",
    "# matcher.add(\"OBSERVACAO\", [observacao_pattern])# 6. Alíquota\n",
    "valor_aliquota_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"iss\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"+\"},\n",
    "    {\"ORTH\": \"\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"},\n",
    "    {\"ORTH\": \"%\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raster PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Executar o comando OCRmyPDF\n",
    "!ocrmypdf --language por --deskew 'pipeline_extracao_documentos/0_arquivos_teste_pipeline/pdf_raster/Mage/Doria Marinho 0295 Carlos Leandro.pdf' 'output.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2KScanning contents     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[32m1/1\u001b[0m \u001b[36m0:00:00\u001b[0m0m\n",
      "\u001b[2K    \u001b[1;36m1\u001b[0m  OMP: Warning #\u001b[1;36m96\u001b[0m: Cannot form a team with \u001b[1;36m4\u001b[0m threads,     \u001b]8;id=737083;file:///home/dani-boy/miniconda3/envs/tables-detr/lib/python3.10/site-packages/ocrmypdf/_exec/tesseract.py\u001b\\\u001b[2mtesseract.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=17298;file:///home/dani-boy/miniconda3/envs/tables-detr/lib/python3.10/site-packages/ocrmypdf/_exec/tesseract.py#273\u001b\\\u001b[2m273\u001b[0m\u001b]8;;\u001b\\\n",
      "using \u001b[1;36m3\u001b[0m instead.                                                \u001b[2m                \u001b[0m\n",
      "\u001b[2K    \u001b[1;36m1\u001b[0m  OMP: Hint Consider unsetting KMP_DEVICE_THREAD_LIMIT     \u001b]8;id=473587;file:///home/dani-boy/miniconda3/envs/tables-detr/lib/python3.10/site-packages/ocrmypdf/_exec/tesseract.py\u001b\\\u001b[2mtesseract.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=274409;file:///home/dani-boy/miniconda3/envs/tables-detr/lib/python3.10/site-packages/ocrmypdf/_exec/tesseract.py#277\u001b\\\u001b[2m277\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[1m(\u001b[0mKMP_ALL_THREADS\u001b[1m)\u001b[0m, KMP_TEAMS_THREAD_LIMIT, and OMP_THREAD_LIMIT \u001b[2m                \u001b[0m\n",
      "\u001b[1m(\u001b[0mif any are set\u001b[1m)\u001b[0m.                                               \u001b[2m                \u001b[0m\n",
      "\u001b[2KOCR                   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[32m1/1\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hPostprocessing\u001b[33m...\u001b[0m                                                   \u001b]8;id=976667;file:///home/dani-boy/miniconda3/envs/tables-detr/lib/python3.10/site-packages/ocrmypdf/_sync.py\u001b\\\u001b[2m_sync.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=27938;file:///home/dani-boy/miniconda3/envs/tables-detr/lib/python3.10/site-packages/ocrmypdf/_sync.py#307\u001b\\\u001b[2m307\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2KRecompressing JPEGs   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[32m0/0\u001b[0m \u001b[36m-:--:--\u001b[0m0m\n",
      "\u001b[2KDeflating JPEGs       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[32m1/1\u001b[0m \u001b[36m0:00:00\u001b[0m-:--\u001b[0m\n",
      "\u001b[2KJBIG2                 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[32m0/0\u001b[0m \u001b[36m-:--:--\u001b[0m-:--\u001b[0m\n",
      "\u001b[?25hImage optimization ratio: \u001b[1;36m1.30\u001b[0m savings: \u001b[1;36m23.3\u001b[0m%                   \u001b]8;id=346067;file:///home/dani-boy/miniconda3/envs/tables-detr/lib/python3.10/site-packages/ocrmypdf/_pipeline.py\u001b\\\u001b[2m_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=166477;file:///home/dani-boy/miniconda3/envs/tables-detr/lib/python3.10/site-packages/ocrmypdf/_pipeline.py#936\u001b\\\u001b[2m936\u001b[0m\u001b]8;;\u001b\\\n",
      "Total file size ratio: \u001b[1;36m0.18\u001b[0m savings: \u001b[1;36m-461.8\u001b[0m%                    \u001b]8;id=869411;file:///home/dani-boy/miniconda3/envs/tables-detr/lib/python3.10/site-packages/ocrmypdf/_pipeline.py\u001b\\\u001b[2m_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=940939;file:///home/dani-boy/miniconda3/envs/tables-detr/lib/python3.10/site-packages/ocrmypdf/_pipeline.py#939\u001b\\\u001b[2m939\u001b[0m\u001b]8;;\u001b\\\n",
      "Output file is a PDF/A-2B \u001b[1m(\u001b[0mas expected\u001b[1m)\u001b[0m                             \u001b]8;id=551507;file:///home/dani-boy/miniconda3/envs/tables-detr/lib/python3.10/site-packages/ocrmypdf/_sync.py\u001b\\\u001b[2m_sync.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=475824;file:///home/dani-boy/miniconda3/envs/tables-detr/lib/python3.10/site-packages/ocrmypdf/_sync.py#405\u001b\\\u001b[2m405\u001b[0m\u001b]8;;\u001b\\\n",
      "The output file size is \u001b[1;36m5.62\u001b[0m× larger than the input file.     \u001b]8;id=463018;file:///home/dani-boy/miniconda3/envs/tables-detr/lib/python3.10/site-packages/ocrmypdf/_validation.py\u001b\\\u001b[2m_validation.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=392334;file:///home/dani-boy/miniconda3/envs/tables-detr/lib/python3.10/site-packages/ocrmypdf/_validation.py#361\u001b\\\u001b[2m361\u001b[0m\u001b]8;;\u001b\\\n",
      "Possible reasons for this include:                            \u001b[2m                  \u001b[0m\n",
      "--deskew was issued, causing transcoding.                     \u001b[2m                  \u001b[0m\n",
      "The optional dependency \u001b[32m'jbig2'\u001b[0m was not found, so some image  \u001b[2m                  \u001b[0m\n",
      "optimizations could not be attempted.                         \u001b[2m                  \u001b[0m\n",
      "The optional dependency \u001b[32m'pngquant'\u001b[0m was not found, so some     \u001b[2m                  \u001b[0m\n",
      "image optimizations could not be attempted.                   \u001b[2m                  \u001b[0m\n",
      "PDF/A conversion was enabled. \u001b[1m(\u001b[0mTry `--output-type pdf`.\u001b[1m)\u001b[0m      \u001b[2m                  \u001b[0m\n",
      "                                                              \u001b[2m                  \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!ocrmypdf --language por --deskew '/home/dani-boy/extractNF/pipeline_extracao_documentos/0_arquivos_teste_pipeline/pdf_raster/Sao_predo_aldeia/2023143.pdf' 'output.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. XXX Executar o comando OCRmyPDF    \n",
    "!pdftotext output.pdf output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ea wmmemna  y' io  PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA SECRETARIA MUNICIPAL DA  a  FAZENDA  TEU UO A QUALIDADE DE VIDA PARA TODOS  NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e  PRESTADOR CPF/CNPJ:  12.852.627/0001-50  2023142 Comperêndis  Julho/2023  Data e Hora da Emissão: 21/07/2023 14:13:00 Código Verificação: 48D30FBCA  DE SERVIÇOS  Inscrição Municipal:  710407  Telefone: 2226274424.. Nome/Razão Social: D. VITORIANO PEREIRA Nome de Fantasia:  Número da Nota:  ai  Inscrição Estadual: 79.562.319  kk  Endereço:  EST PAU FERRO ,SN LT 55 A,CAMPO REDONDO - São Pedro da Aldeia-RJ  E-mail:  D.VITORIANOQLIVE.COM TOMADOR DE SERVIÇOS  CPF/CNPJ:  04.968.448/0001-54  INSC:MUNICIPAL:  |  RG:  Telefone:  Inscrição Estadual:  Nome/Razão Social: RECICLAR - RECICLAGEM ARARUAMA LTDA Endereço: CAJAZEIROS Nº 0 LT 8 QD B BAIRRO: PARQUE E-mail: Não Informado  HOTEL CIDADE: ARARUAMA  SERVIÇO DE SOLDA NO PISTÃO HIDRAULICO . OS 12497  - RJ CEP: 28981382  DISCRIMINAÇÃO DOS SERVIÇOS  VALOR TOTAL DA NOTA: R$ 250,00  CNAE - 2539-0/01 - SERVIÇOS DE USINAGEM, TORNEA RIA E SOLDA Item da Lista de Serviços - 14.05 - RESTAURAÇÃO, RECONDICIONAMENTO, ACONDICIONAMENTO, PINTURA TINGIMENTO, GALVANOPLASTIA, ANODIZAÇÃO, , BENEFICIAMENTO, LAVAGEM, SECAGEM, CORTE, RECORTE, POLIMENTO, PLASTIFICAÇÃO E CONGENERES, DE OBJETOS Q VALOR SERVIÇOS: R$ 250,00  VALOR . DEDUÇÃO: R$ 0,00  DESC. INCOND: BASE DE R$ 0,00 CALCULO: R$ 250,00  ALÍQUOTA: 2%  VALOR ISS: R$ 5,00  VALOR PIS: R$ 0,00  VALOR COFINS: R$ 0,00  VALOR IR: R$ 0,00  VALOR CSLL: R$ 0,00  OUTRAS RETENÇÕES: R$ 0,00  VALOR INSs: R$ 0,00  VALOR ISS RETIDO: R$ 0,00  DESC. COND: R$ 0,00  VALOR LÍQUIDO: R$ 250,00  DADOS COMPLEMENTARES NÃO AUTORIZADO PAGAMENTO VIA DEPOSITO TED/DOC ENCARGOS FINANCEIROS CALCULADOS POR DIA DE ATRASO AO PAGADOR: APOS 10 DIAS VENCIMENTO SUJEITO A INCLUSÃO SERASA OU DO PROTESTO POR NÃO PAGAMENTO SOLICITE A2º VIA DO BOLETO: d.vitorianoQDlive.com OUTRAS INFORMAÇÕES / CRITICAS EXIGIBILIDADE ISS REGIME TRIBUTAÇÃO SIMPLES NACIONAL ISSQN RETIDO LOCAL. PRESTAÇÃO Exigivel Microempresário e Empresa LOCAL INCIDÊNCIA Sim (2% ) Não SERVIÇO São Pedro da Aldeia - RJ de PequenoPorte (ME EPP) São Pedro da Aldeia - RJ Observação: Valor Aproximado dos Tributos Federais R$ 33,62 (Alíq 13,45), Tributos Estaduai s R$ 0,00 (Alíq 0,00 IBPT) e Municipal de R$ 11,83 (Alíq IBPT  4,73 IBPT)  Sistema desenvolvido pela Modernização Pública  \\x0c\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('output.txt', 'r', encoding='utf-8') as arquivo:\n",
    "    texto_OCR_R = arquivo.read()\n",
    "\n",
    "texto_PDF_Raster = texto_OCR_R.replace('\\n', ' ')\n",
    "texto_PDF_Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "string_id:              DATA_HORA_EMISSAO   span.text:  Data e Hora da Emissão: 21/07/2023 14:13:00 \n",
      "\n",
      "string_id:             CODIGO_VERIFICACAO   span.text:                Código Verificação: 48D30FBCA \n",
      "\n",
      "string_id:                    VALOR_TOTAL   span.text:               VALOR TOTAL DA NOTA: R$ 250,00 \n",
      "\n",
      "string_id:                 VALOR_SERVICOS   span.text:                    VALOR SERVIÇOS: R$ 250,00 \n",
      "\n",
      "string_id:                  VALOR_DEDUCAO   span.text:                             DEDUÇÃO: R$ 0,00 \n",
      "\n",
      "string_id:                VALOR_ALIQUOTA2   span.text:                                 ALÍQUOTA: 2% \n",
      "\n",
      "string_id:                      VALOR_ISS   span.text:                           VALOR ISS: R$ 5,00 \n",
      "\n",
      "string_id:                      VALOR_PIS   span.text:                           VALOR PIS: R$ 0,00 \n",
      "\n",
      "string_id:                   VALOR_COFINS   span.text:                        VALOR COFINS: R$ 0,00 \n",
      "\n",
      "string_id:                       VALOR_IR   span.text:                            VALOR IR: R$ 0,00 \n",
      "\n",
      "string_id:                     VALOR_CSLL   span.text:                          VALOR CSLL: R$ 0,00 \n",
      "\n",
      "string_id:                   VALOR_OUTRAS   span.text:                    OUTRAS RETENÇÕES: R$ 0,00 \n",
      "\n",
      "string_id:                     VALOR_INSS   span.text:                          VALOR INSs: R$ 0,00 \n",
      "\n",
      "string_id:                VALOR_ISSRETIDO   span.text:                    VALOR ISS RETIDO: R$ 0,00 \n",
      "\n",
      "string_id:                 VALOR_DESCCOND   span.text:                          DESC. COND: R$ 0,00 \n",
      "\n",
      "string_id:                  VALOR_LIQUIDO   span.text:                     VALOR LÍQUIDO: R$ 250,00 \n",
      "\n",
      "string_id:           SIMPLES_NACIONAL_SIM   span.text:                             SIMPLES NACIONAL \n",
      "\n",
      "string_id:               LOCAL_INCIDENCIA   span.text:                         LOCAL INCIDÊNCIA Sim \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(texto_PDF_Raster)\n",
    "\n",
    "# Executar o Matcher no Doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Exibir os resultados\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Obter a string de identificação\n",
    "    span = doc[start:end]  # Obter o trecho correspondente\n",
    "    print(f'\\nstring_id: {string_id:>30}   span.text:{span.text:>45} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Pesquisavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"27/07/2023, 15:25 Nota Fiscal de Serviços Eletrônica (NFSe) https://nfe.mesquita.rj.gov.br 1/1 PREFEITURA MUNICIPAL DE MESQUITA SECRETARIA MUNICIPAL DA FAZENDA NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e Número da Nota: 20238 Competência: Julho/2023 Data e Hora da Emissão: 27/07/2023 15:21:00 Código Verificação: 3C86CC2F2 PRESTADOR DE SERVIÇOS CPF/CNPJ:  50.921.369/0001-05 Inscrição Municipal:  952538 Telefone:  2297268232.. Inscrição Estadual:   Nome/Razão Social: MEDSORIA CLINICA DE AVALIACAO MEDICA E PSICOLOGICA DO TRAFEGO DE MESQUITA LTDA Nome de Fantasia: Endereço: RUA PROCOPIO ,631 LOJA A ,SANTO ELIAS - Mesquita-RJ E-mail: LARA_VSORIA@HOTMAIL.COM TOMADOR DE SERVIÇOS CPF/CNPJ:  06.047.087/0033-16    |     INSC:MUNICIPAL: 1664085 RG:   Telefone: Inscrição Estadual:   Nome/Razão Social: REDE D'OR SAO LUIZ S.A. Endereço:  OLINDA ELLIS N° 93 BAIRRO: CAMPO GRANDE CIDADE: RIO DE JANEIRO - RJ CEP: 23045160 E-mail: Não Informado DISCRIMINAÇÃO DOS SERVIÇOS Ref a Plantões de Maio, 60h no Setor de Radiologia - Médica: Lara Veiga Soria Catuladeira. VALOR TOTAL DA NOTA: R$ 7.133,60 CNAE - 8630502 - ATIVIDADE MÉDICA AMBULATORIAL COM RECURSOS PARA REALIZAÇÃO DE EXAMES COMPLEMENTARES Item da Lista de Serviços - 4.03 - HOSPITAIS, CLÍNICAS, LABORATÓRIOS, SANATÓRIOS, MANICÔMIOS, CASAS DE SAÚDE, PRONTOS-SOCORROS, AMBULATÓRIOS E CONGÊNERES.   VALOR SERVIÇOS: R$ 7.133,60 VALOR DEDUÇÃO: R$ 0,00 DESC. INCOND: R$ 0,00 BASE DE CÁLCULO: R$ 7.133,60 ALÍQUOTA: 2,01% VALOR ISS: R$ 143,39 VALOR ISS RETIDO: R$ 0,00 DESC. COND: R$ 0,00 ____________________________________________________________________ VALOR PIS: R$ 0,00 VALOR COFINS: R$ 0,00 VALOR IR: R$ 0,00 VALOR INSS: R$ 0,00 VALOR CSLL: R$ 0,00 OUTRAS RETENÇÕES: R$ 0,00 VALOR LÍQUIDO: R$ 7.133,60 DADOS COMPLEMENTARES OUTRAS INFORMAÇÕES / CRITICAS EXIGIBILIDADE ISS Exigivel REGIME TRIBUTAÇÃO Sociedade Limitada SIMPLES NACIONAL Sim ( 2,01% ) ISSQN RETIDO Não LOCAL. PRESTAÇÃO SERVIÇO Mesquita - RJ LOCAL INCIDÊNCIA Mesquita - RJ Observação: LEI DA TRANSPARÊNCIA FISCAL NR. 12.741, DE 8 DE DEZEMBRO DE 2012. - PRESTADOR OPTANTE DO SIMPLES NACIONAL (ALÍQUOTA: 2,01 %) Valor Aproximado dos Tributos Federais R$ 959,47 (Alíq 13,45), Tributos Estaduais R$ 0,00 (Alíq 0,00 IBPT) e Municipal de R$ 159,08 (Alíq IBPT 2,23 IBPT) \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDF Pesquisavel\n",
    "\n",
    "file_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_23/MESQUITA_PDF_31282023_2258/159871/2023 -8.pdf\"\n",
    "\n",
    "\n",
    "pdf_document = fitz.open(file_path)\n",
    "\n",
    "\n",
    "page_number = 0  # Defina o número da página que deseja analisar\n",
    "page = pdf_document[page_number]\n",
    "\n",
    "# Extrair texto dentro do retângulo\n",
    "text_P = page.get_text(\"text\")\n",
    "\n",
    "\n",
    "pdf_document.close()\n",
    "\n",
    "texto_PDF_Pesquisavel = text_P.replace('\\n', ' ')\n",
    "texto_PDF_Pesquisavel \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "string_id:              DATA_HORA_EMISSAO   span.text:  Data e Hora da Emissão: 27/07/2023 15:21:00 \n",
      "\n",
      "string_id:             CODIGO_VERIFICACAO   span.text:                Código Verificação: 3C86CC2F2 \n",
      "\n",
      "string_id:                    VALOR_TOTAL   span.text:             VALOR TOTAL DA NOTA: R$ 7.133,60 \n",
      "\n",
      "string_id:                 VALOR_SERVICOS   span.text:                  VALOR SERVIÇOS: R$ 7.133,60 \n",
      "\n",
      "string_id:                  VALOR_DEDUCAO   span.text:                             DEDUÇÃO: R$ 0,00 \n",
      "\n",
      "string_id:                 VALOR_CALCULOP   span.text:                 BASE DE CÁLCULO: R$ 7.133,60 \n",
      "\n",
      "string_id:                 VALOR_CALCULOR   span.text:                         CÁLCULO: R$ 7.133,60 \n",
      "\n",
      "string_id:                 VALOR_ALIQUOTA   span.text:                              ALÍQUOTA: 2,01% \n",
      "\n",
      "string_id:                      VALOR_ISS   span.text:                         VALOR ISS: R$ 143,39 \n",
      "\n",
      "string_id:                VALOR_ISSRETIDO   span.text:                    VALOR ISS RETIDO: R$ 0,00 \n",
      "\n",
      "string_id:                 VALOR_DESCCOND   span.text:                          DESC. COND: R$ 0,00 \n",
      "\n",
      "string_id:                      VALOR_PIS   span.text:                           VALOR PIS: R$ 0,00 \n",
      "\n",
      "string_id:                   VALOR_COFINS   span.text:                        VALOR COFINS: R$ 0,00 \n",
      "\n",
      "string_id:                       VALOR_IR   span.text:                            VALOR IR: R$ 0,00 \n",
      "\n",
      "string_id:                     VALOR_INSS   span.text:                          VALOR INSS: R$ 0,00 \n",
      "\n",
      "string_id:                     VALOR_CSLL   span.text:                          VALOR CSLL: R$ 0,00 \n",
      "\n",
      "string_id:                   VALOR_OUTRAS   span.text:                    OUTRAS RETENÇÕES: R$ 0,00 \n",
      "\n",
      "string_id:                  VALOR_LIQUIDO   span.text:                   VALOR LÍQUIDO: R$ 7.133,60 \n",
      "\n",
      "string_id:              EXIGIBILIDADE_ISS   span.text:                   EXIGIBILIDADE ISS Exigivel \n",
      "\n",
      "string_id:           SIMPLES_NACIONAL_SIM   span.text:                             SIMPLES NACIONAL \n",
      "\n",
      "string_id:           SIMPLES_NACIONAL_SIM   span.text:                         SIMPLES NACIONAL Sim \n",
      "\n",
      "string_id:           SIMPLES_NACIONAL_SIM   span.text:                       SIMPLES NACIONAL Sim ( \n",
      "\n",
      "string_id:                   ISSQN_RETIDO   span.text:                             ISSQN RETIDO Não \n",
      "\n",
      "string_id:               LOCAL_INCIDENCIA   span.text:                    LOCAL INCIDÊNCIA Mesquita \n",
      "\n",
      "string_id:               LOCAL_INCIDENCIA   span.text:                  LOCAL INCIDÊNCIA Mesquita - \n",
      "\n",
      "string_id:               LOCAL_INCIDENCIA   span.text:               LOCAL INCIDÊNCIA Mesquita - RJ \n",
      "\n",
      "string_id:           SIMPLES_NACIONAL_SIM   span.text:                             SIMPLES NACIONAL \n",
      "\n",
      "string_id:           SIMPLES_NACIONAL_SIM   span.text:                           SIMPLES NACIONAL ( \n",
      "\n",
      "string_id:                 VALOR_ALIQUOTA   span.text:                             ALÍQUOTA: 2,01 % \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(texto_PDF_Pesquisavel)\n",
    "\n",
    "# Executar o Matcher no Doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Exibir os resultados\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Obter a string de identificação\n",
    "    span = doc[start:end]  # Obter o trecho correspondente\n",
    "    print(f'\\nstring_id: {string_id:>30}   span.text:{span.text:>45} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texto_amostra = \"ALÍQUOTA: 2,01% VALOR ISS\"\n",
    "\n",
    "texto_amostra = \"300,00  ALÍQUOTA: 3%  VALOR ISS\"\n",
    "\n",
    "doc = nlp(texto_amostra)\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Exibir os resultados\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Obter a string de identificação\n",
    "    span = doc[start:end]  # Obter o trecho correspondente\n",
    "    print(f\"{string_id}: {span.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisys\n",
    "syntatic = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"T_texto\",\"T_shape\", \"T_is_alpha\", \"T_is_digit\", \"T_is_title\", \"T_is_punct\", \"T_is_sent_start\", \"T_is_right_punct\", \"T_is_stop\", \"T_is_quote\", \"T_is_currency\", \"T_morph\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    syntatic.loc[i,\"id\"] = token.i\n",
    "    syntatic.loc[i,\"T_texto\"] = token.text\n",
    "    syntatic.loc[i,\"T_shape\"] = token.shape_\n",
    "    syntatic.loc[i,\"T_is_alpha\"] = token.is_alpha\n",
    "    syntatic.loc[i,\"T_is_digit\"] = token.is_digit\n",
    "    syntatic.loc[i,\"T_is_title\"] = token.is_title\n",
    "    syntatic.loc[i,\"T_is_punct\"] = token.is_punct\n",
    "    syntatic.loc[i,\"T_is_sent_start\"] = token.is_sent_start\n",
    "    syntatic.loc[i,\"T_is_right_punct\"] = token.is_right_punct\n",
    "    syntatic.loc[i,\"T_is_stop\"] = token.is_stop\n",
    "    syntatic.loc[i,\"T_is_quote\"] = token.is_quote\n",
    "    syntatic.loc[i,\"T_is_currency\"] = token.is_currency\n",
    "    syntatic.loc[i,\"T_morph\"] = token.morph\n",
    "    i = i+1\n",
    "\n",
    "syntatic.head(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntatic.to_excel(\"syntatic.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"IS_DIGIT\": True, \"OP\": \"+\"},\n",
    "{\"ORTH\": \",\", \"OP\": \"?\"},\n",
    "{\"IS_DIGIT\": True, \"OP\": \"*\"},\n",
    "{\"ORTH\": \"%\"}\n",
    "\n",
    "\n",
    "\n",
    "{\"SHAPE\": \"XX\", \"OP\": \"?\"}\n",
    "\n",
    "# {\"LOWER\": \"alíquota\"} busca a palavra \"alíquota\" em qualquer combinação de maiúsculas e minúsculas.\n",
    "# {\"ORTH\": \":\"} busca o caractere \":\".\n",
    "# {\"IS_SPACE\": True, \"OP\": \"*\"} permite zero ou mais espaços.\n",
    "# {\"IS_DIGIT\": True, \"OP\": \"+\"} busca um ou mais dígitos.\n",
    "# {\"ORTH\": \",\", \"OP\": \"?\"} busca zero ou um caractere de vírgula (para números fracionários).\n",
    "# {\"IS_DIGIT\": True, \"OP\": \"*} busca zero ou mais dígitos (novamente, para números fracionários).\n",
    "# {\"ORTH\": \"%\"} busca o símbolo \"%\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization.to_excel(\"lemmatization.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas = [linha for linha in texto_OCR.split('\\n') if linha.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contas para simulacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessage, HumanMessagePromptTemplate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando Patterns especificos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando e carregando patterns e modelos de patterns e criacao de logs JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_patterns_to_file(patterns, colors, filename):\n",
    "    data = {\"patterns\": patterns, \"colors\": colors}\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, ensure_ascii=True, indent=2)\n",
    "        \n",
    "        \n",
    "        \n",
    "def load_patterns_and_colors(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        patterns = data[\"patterns\"]\n",
    "        colors = data[\"colors\"]\n",
    "    return patterns, colors   \n",
    "\n",
    "\n",
    "# my_str.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/home/wklinux/spaCy/configuracoes/pattern_test.json\"  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns, colors = load_patterns_and_colors(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_patterns_to_file(patterns=patterns, colors=colors, filename=filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Modelo de criaçao de registro JSONL</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONL 'Total do contrato'\n",
    "\n",
    "import json\n",
    "with open('output_daniel.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for _, row in df.iterrows():\n",
    "        summary = f\"Summary: 'Total do contrato\\n\\nSpecific information: Contrato {row['8-nro contrato']} tem a quantidade total de {row['9-qt total contrato']} kilos\\n\\n###\\n\\n\"\n",
    "        customer_message = \"Bom dia, Eu gostaria de saber o total do meu contrato?\\n\"\n",
    "        agent_response = f\"Agent: O total do seu contrato {row['8-nro contrato']} é {row['9-qt total contrato']} kilos. Mais alguma informação?\\n\"\n",
    "        customer_message2 = \"Customer: Não, muito obrigado!\\n\"\n",
    "        agent_response2 = \"Agent: Perfeito, Se precisar de algo é só chamar.\\n\"\n",
    "        json_data = {\"prompt\": summary + customer_message + agent_response + customer_message2 + agent_response2,\"completion\": agent_response2}\n",
    "        \n",
    "        \n",
    "        json_string = json.dumps(json_data, ensure_ascii=False)\n",
    "        print(json_string)\n",
    "        \n",
    "        f.write(json_string + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exemplos de implementacao de patterns com analise sintatica</h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/home/wklinux/spaCy/query_utter.json\"  \n",
    "\n",
    "data = load_json(filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### amostras de textos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicio dados da entidade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisys\n",
    "syntatic = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"T_texto\",\"T_shape\", \"T_is_alpha\", \"T_is_digit\", \"T_is_title\", \"T_is_punct\", \"T_is_sent_start\", \"T_is_right_punct\", \"T_is_stop\", \"T_is_quote\", \"T_is_currency\", \"T_morph\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    syntatic.loc[i,\"id\"] = token.i\n",
    "    syntatic.loc[i,\"T_texto\"] = token.text\n",
    "    syntatic.loc[i,\"T_shape\"] = token.shape_\n",
    "    syntatic.loc[i,\"T_is_alpha\"] = token.is_alpha\n",
    "    syntatic.loc[i,\"T_is_digit\"] = token.is_digit\n",
    "    syntatic.loc[i,\"T_is_title\"] = token.is_title\n",
    "    syntatic.loc[i,\"T_is_punct\"] = token.is_punct\n",
    "    syntatic.loc[i,\"T_is_sent_start\"] = token.is_sent_start\n",
    "    syntatic.loc[i,\"T_is_right_punct\"] = token.is_right_punct\n",
    "    syntatic.loc[i,\"T_is_stop\"] = token.is_stop\n",
    "    syntatic.loc[i,\"T_is_quote\"] = token.is_quote\n",
    "    syntatic.loc[i,\"T_is_currency\"] = token.is_currency\n",
    "    syntatic.loc[i,\"T_morph\"] = token.morph\n",
    "    i = i+1\n",
    "\n",
    "syntatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.start:>2} | {ent.text:>21} | {ent.label_:>12} | {ent.id_:>15}  | {ent.end:>2} || {ent.start_char:>3} | {ent.end_char:>3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ent.orth_ for ent in doc.ents if ent.label_ == \"ENTREGUE\"]\n",
    "label_entregue = [ent.orth_ for ent in doc.ents if ent.label_ == \"ENTREGUE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = doc[22]  \n",
    "print(token.morph, token.text) \n",
    "print(token.morph.get(\"PronType\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_root = [token.text for token in doc if token.dep_ == \"ROOT\"][0]\n",
    "token_root"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = list(doc.noun_chunks)\n",
    "for i in range(len(chunks)):\n",
    "    print(i, chunks[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIAZACAO ESTRUTURA DEP, HEAD, ROOT, LEMMA\n",
    "chunks_valor = []\n",
    "\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "  chunk_text = chunk.text\n",
    "  chunks_valor.append(chunk_text)\n",
    "  chunk_root = chunk.root.text\n",
    "  chunk_root_dep = chunk.root.dep_\n",
    "  chunk_root_head = chunk.root.head.text\n",
    "  chunk_root_head_dep = chunk.root.head.dep_\n",
    "  chunk_root_head_lemma = chunk.root.head.lemma_\n",
    "  chunk_ents = chunk.ents\n",
    "  chunk_root_ent_type = chunk.root.ent_type_\n",
    "  \n",
    "  print(f'1.chunk.text: {chunk_text:>26} | 2.ch.root: {chunk_root:>10} | 3.chunk.root.dep_: {chunk_root_dep:>15} | 4.ch.root.head: {chunk_root_head:>12} | 5.ch.root.head.dep_: {chunk_root_head_dep:>10} |  6.chunk.root.head.lemma_: {chunk_root_head_lemma:>9}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep',\n",
    "                jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_chunk(doc, param, i):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_root_head_lemma_ = (chunk.root.head.lemma_).lower()\n",
    "        if chunk_root_head_lemma_ in [\"gostaria\", \"entregar\", \"entregueir\"]:\n",
    "            if chunk.root.dep_ == param:\n",
    "                return chunk.text, i, chunk.doc\n",
    "            # else:\n",
    "            #     return \"erro\", chunk.root.dep_\n",
    "        elif chunk_root_head_lemma_ in [\"qual\", \"entregar\"]:\n",
    "            if chunk.root.dep_ == \"nsubj:pass\":\n",
    "                return chunk.text, i, chunk.doc\n",
    "        elif chunk_root_head_lemma_ in [\"entregar\", \"entregueir\"]:\n",
    "            if chunk.root.dep_ == \"obj\":\n",
    "                return chunk.text, i, chunk.doc   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_text = [chunk.text for chunk in doc.noun_chunks][0]\n",
    "chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_root_dep = [chunk.root.dep_ for chunk in doc.noun_chunks][0]\n",
    "chunk_root_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_root_head_lemma = [chunk.root.head.lemma_ for chunk in doc.noun_chunks][0].lower()\n",
    "chunk_root_head_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(len(data)):\n",
    "    doc_query = nlp(data[i][\"user_utter\"])\n",
    "    result = relevant_chunk(doc=doc_query, param=chunk_root_dep, i=i)\n",
    "    # print(result)\n",
    "    if result:\n",
    "        bot_utter = nlp(data[i][\"bot_utter\"])\n",
    "        user_keys = nlp(data[i][\"user_keys\"])\n",
    "        user_desire = nlp(data[i][\"user_des\"])\n",
    "        missed_keys = nlp(data[i][\"miss_key\"])      \n",
    "        print(bot_utter)\n",
    "        #print(data[i][\"user_utter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "pos_tagging = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"T_texto\",\"T_lemma_\", \"T_pos_\", \"T_tag_\", \"T_dep_\", \"T_head\", \"T_is_sent_start\", \"T_shape_\", \"T_is_alpha\", \"T_is_stop\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    pos_tagging.loc[i,\"id\"] = token.i\n",
    "    pos_tagging.loc[i,\"T_texto\"] = token.text\n",
    "    pos_tagging.loc[i,\"T_lemma_\"] = token.lemma_\n",
    "    pos_tagging.loc[i,\"T_pos_\"] = token.pos_\n",
    "    pos_tagging.loc[i,\"T_tag_\"] = token.tag_\n",
    "    pos_tagging.loc[i,\"T_dep_\"] = token.dep_\n",
    "    pos_tagging.loc[i,\"T_head\"] = token.head\n",
    "    pos_tagging.loc[i,\"T_is_sent_start\"] = token.is_sent_start\n",
    "    pos_tagging.loc[i,\"T_shape_\"] = token.shape_\n",
    "    pos_tagging.loc[i,\"T_is_alpha\"] = token.is_alpha\n",
    "    pos_tagging.loc[i,\"T_is_stop\"] = token.is_stop\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "pos_tagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outras tratativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/etc/odbcinst.ini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trata_contrato(doc, n_contrato):\n",
    "    #nlp = spacy.blank(\"pt\")\n",
    "    #ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    # ruler.add_patterns(patterns)\n",
    "    # doc = nlp(text)\n",
    "    \n",
    "    tokens_contrato = []\n",
    "    ents_contrato = []\n",
    "    \n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        span = doc.char_span(ent.start_char, ent.end_char, label=ent.label_)\n",
    "        ents_contrato.append(span)\n",
    "        \n",
    "    for token in doc:\n",
    "        start = token.idx\n",
    "        end = start + len(token)\n",
    "        if token.shape_ == \"dddX\":\n",
    "            n_contrato.append(token.text)\n",
    "        tokens_contrato.append((token.text, start, end))\n",
    "        \n",
    "    return doc, tokens_contrato, ents_contrato, n_contrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nros_contratos = []\n",
    "n_contrato = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"CONTRATO\":\n",
    "        ent_texto = nlp(ent.text)\n",
    "        print(ent_texto)\n",
    "        doc, tokens_contrato, ents_contrato, n_contrato = trata_contrato(ent_texto, n_contrato)\n",
    "        \n",
    "        nros_contratos.append(ent.text)\n",
    "        \n",
    "        print(f'{ent.start:>2} | {ent.text:>21} | {ent.label_:>8} | {ent.id_:>15} | {ent.end:>2} || {ent.start_char:>3} | {ent.end_char:>3}')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrato_1 = nros_contratos[0]\n",
    "contrato_2 = nros_contratos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novo_doc = nlp(contrato_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trata_contrato(novo_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Mr. Best flew to New York on Saturday morning.\")\n",
    "span = doc[0:6]\n",
    "ents = list(span.ents)\n",
    "assert ents[0].label == 346\n",
    "assert ents[0].label_ == \"PERSON\"\n",
    "assert ents[0].text == \"Mr. Best\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I live in NewYork\")\n",
    "with doc.retokenize() as retokenizer:\n",
    "    heads = [(doc[3], 1), doc[2]]\n",
    "    attrs = {\"POS\": [\"PROPN\", \"PROPN\"],\n",
    "             \"DEP\": [\"pobj\", \"compound\"]}\n",
    "    retokenizer.split(doc[3], [\"New\", \"York\"], heads=heads, attrs=attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for contrato in range(len(list(nros_contratos))):\n",
    "    doc, tokens, ents = show_ent_new(contrato, patterns=patterns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nros_contratos = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"CONTRATO\":\n",
    "        ent_texto = ent.text\n",
    "        doc_contrato = nlp(ent_texto)\n",
    "        print(doc_contrato)\n",
    "        print()\n",
    "        for token in doc_contrato:\n",
    "            #print(token.i, token.text, token.shape_)\n",
    "            print()\n",
    "            print()\n",
    "            qtd_contratops = token.morph.get(\"Number\")\n",
    "            if \"Plur\" in qtd_contratops:\n",
    "                # print(\"               Ele esta falando mais de um contrato\")\n",
    "                if token.shape_ == \"dddX\":\n",
    "                    nro_contrato = token.text\n",
    "                    nros_contratos.append(nro_contrato)\n",
    "                    print(f'         Ele esta falando de contratos (plural),  esse e o nro do contrato: {nro_contrato}\\n\\n')\n",
    "                else:\n",
    "                    print(f'         Ele esta falando de contratos (plural) -  Nao ha nro de contrato\\n')\n",
    "                    nros_contratos.append(nro_contrato)\n",
    "            else:\n",
    "                # print(\"               ele esta falando no singular - contrato\")    \n",
    "                if token.shape_ == \"dddX\":\n",
    "                    nro_contrato = token.text\n",
    "                    nros_contratos.append(nro_contrato)\n",
    "                    print(f'          Ele esta falando de contrato (singular),  esse e o nro do contrato: {nro_contrato}\\n\\n')\n",
    "                else:\n",
    "                    print(f'          Ele esta falando de contrato (singular) - Nao ha nro de contrato\\n')\n",
    "                    nros_contratos.append(nro_contrato)\n",
    "                    \n",
    "            print()        \n",
    "       \n",
    "# print(nros_contratos)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nros_contratos = []\n",
    "entidades = list(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(entidades)):\n",
    "    print(f'1. idx: {i} | {doc[i].text:>19} | type: {doc[i].ent_type_:>10} {doc[i].ent_id_} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = list(doc.noun_chunks)\n",
    "for i in range(len(chunks)):\n",
    "    # if chunks[i].root.ent_type_ == \"CONTRATO\":\n",
    "        print(f'1. idx: {i} | {chunks[i].text:>19} | {chunks[i].start:>2} {chunks[i].end:>2} | {chunks[i].start_char:>3}  {chunks[i].end_char:>3} || type: {chunks[i].root.ent_type_:>10} {chunks[i].ents} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ent for ent in doc.ents if ent.label_ == \"CONTRATO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ent for ent in doc.ents if ent.label_ == \"NR_CONTRATO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([ent for ent in doc.ents if ent.label_ == \"CONTRATO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = list(doc.noun_chunks)\n",
    "for i in range(len(chunks)):\n",
    "    if chunks[i].root.ent_type_ == \"CONTRATO\":\n",
    "        print(f'1. idx: {i} | {chunks[i].text:>19} | {chunks[i].start:>2} {chunks[i].end:>2} | {chunks[i].start_char:>3}  {chunks[i].end_char:>3} || type: {chunks[i].root.ent_type_:>10} {chunks[i].ents} ')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nros_contratos = []\n",
    "chunks = list(doc.noun_chunks)\n",
    "for i in range(len(chunks)):\n",
    "    if chunks[i].root.ent_type_ == \"CONTRATO\":\n",
    "        print(f'1. idx: {i} | {chunks[i].text:>19} | {chunks[i].start:>2} {chunks[i].end:>2} | {chunks[i].start_char:>3}  {chunks[i].end_char:>3} || type: {chunks[i].root.ent_type_:>10} {chunks[i].ents} ')\n",
    "        print()\n",
    "        doc_contrato = nlp(chunks[i].text)\n",
    "        for token in doc_contrato:\n",
    "            #print(token.i, token.text, token.shape_)\n",
    "            print()\n",
    "            print()\n",
    "            qtd_contratops = token.morph.get(\"Number\")\n",
    "            if \"Plur\" in qtd_contratops:\n",
    "                # print(\"               Ele esta falando mais de um contrato\")\n",
    "                if token.shape_ == \"dddX\":\n",
    "                    nro_contrato = token.text\n",
    "                    nros_contratos.append(nro_contrato)\n",
    "                    print(f'         Ele esta falando de contratos (plural),  esse e o nro do contrato: {nro_contrato}\\n\\n')\n",
    "                else:\n",
    "                    print(f'         Ele esta falando de contratos (plural) -  Nao ha nro de contrato\\n')\n",
    "            else:\n",
    "                # print(\"               ele esta falando no singular - contrato\")    \n",
    "                if token.shape_ == \"dddX\":\n",
    "                    nro_contrato = token.text\n",
    "                    nros_contratos.append(nro_contrato)\n",
    "                    print(f'          Ele esta falando de contrato (singular),  esse e o nro do contrato: {nro_contrato}\\n\\n')\n",
    "                else:\n",
    "                    print(f'          Ele esta falando de contrato (singular) - Nao ha nro de contrato\\n')\n",
    "            print()        \n",
    "       \n",
    "print(nros_contratos)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"CONTRATO\":\n",
    "        doc_contrato = nlp(ent.text)\n",
    "        # print(token.text, token.ent_type_, token.shape_)\n",
    "        qtd_contratops = ent.orth_\n",
    "        #print(doc_contrato)\n",
    "        for token in doc_contrato:\n",
    "            print(token.i, token.text, token.shape_)\n",
    "            print()\n",
    "            print()\n",
    "            qtd_contratops = token.morph.get(\"Number\")\n",
    "            if \"Plur\" in qtd_contratops:\n",
    "                print(\"Ele esta falando mais de um contrato\")\n",
    "                if token.shape_ == \"dddX\":\n",
    "                    nro_contrato = token.text\n",
    "                    print(f'esse e o nro do contrato: {nro_contrato}')\n",
    "                else:\n",
    "                    print(f'Nao ha nro de contrato')\n",
    "            else:\n",
    "                print(\"ele esta falando no singular - contrato\")    \n",
    "                if token.shape_ == \"dddX\":\n",
    "                    nro_contrato = token.text\n",
    "                    print(f'esse e o nro do contrato: {nro_contrato}')\n",
    "                else:\n",
    "                    print(f'Nao ha nro de contrato')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.ent_type_ == \"CONTRATO\":\n",
    "        # print(token.text, token.shape_)\n",
    "        if token.shape_ == \"dddX\":\n",
    "            print(token.text)\n",
    "        # print(token.text, token.shape_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrato = [ent for ent in doc.ents if ent.label_ == \"CONTRATO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"CONTRATO\":\n",
    "        doc_contrato = nlp(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_root = [token.text for token in doc if token.dep_ == \"ROOT\"][0]\n",
    "token_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrato_keys = contrato[0].text.split('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ent for ent in doc.ents if ent.label_ == \"CONTRATO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ent for ent in doc.ents if ent.label_ == \"SAFRA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamento para contratos\n",
    "for token in doc:\n",
    "    if token.ent_type_ == \"CONTRATO\":\n",
    "        # print(token.text, token.ent_type_, token.shape_)\n",
    "        qtd_contratops = token.morph.get(\"Number\")\n",
    "        # print(qtd_contratops)\n",
    "        if \"Plur\" in qtd_contratops:\n",
    "            print(\"Ele esta falando mais de um contrato\")\n",
    "        else:\n",
    "             print(\"Ele esta falando somente de um contrato\")   \n",
    "        if token.shape_ == \"dddX\":\n",
    "            nro_contrato = token.text\n",
    "            print(f'esse e o nro do contrato: {nro_contrato}')\n",
    "        else:\n",
    "            print(f'nao ha numero do contrato')\n",
    "    \n",
    "    # elif token.ent_type_ == \"CONTRATO\":    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_utterance = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"POS\": \"PROPN\", \"LENGTH\": {\">=\": 10}},\n",
    "\n",
    "\n",
    "[\n",
    "  {\"POS\": \"ADJ\", \"OP\": \"*\"},\n",
    "  {\"POS\": \"NOUN\", \"OP\": \"+\"}\n",
    "  {\"POS\": \"PROPN\", \"OP\": \"{2}\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternsContrato = [\n",
    "    {\n",
    "        \"label\":\"CONTRATO\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"contrato\", \"OP\":\"*\"},\n",
    "            {\"SHAPE\": \"dddX\", \"OP\":\"*\"},\n",
    "            {\"LOWER\": \"contratos\", \"OP\":\"*\"},\n",
    "            {\"SHAPE\": \"dddX\", \"OP\":\"*\"}\n",
    "            \n",
    "        ]\n",
    "        \n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens de valor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento de noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_chunk(doc, param, i):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_root_head_lemma_ = (chunk.root.head.lemma_).lower()\n",
    "        if chunk_root_head_lemma_ in [\"gostaria\", \"entregar\", \"entregueir\"]:\n",
    "            if chunk.root.dep_ == param:\n",
    "                return chunk.text, i, chunk.doc\n",
    "            # else:\n",
    "            #     return \"erro\", chunk.root.dep_\n",
    "        elif chunk_root_head_lemma_ in [\"qual\"]:\n",
    "            if chunk.root.dep_ == \"ROOT\":\n",
    "                return chunk.text, i, chunk.doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_text = [chunk.text for chunk in doc.noun_chunks][0]\n",
    "chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_root_dep = [chunk.root.dep_ for chunk in doc.noun_chunks][0]\n",
    "chunk_root_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_root_head_lemma = [chunk.root.head.lemma_ for chunk in doc.noun_chunks][0].lower()\n",
    "chunk_root_head_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = list(doc.noun_chunks)\n",
    "for i in range(len(chunks)):\n",
    "    print(f'1. idx: {i} | {chunks[i].text:>13} | {chunks[i].start:>2} {chunks[i].end:>2} | {chunks[i].start_char:>2}  {chunks[i].end_char:>2} || {chunks[i].root.ent_type_:>10} {chunks[i].ents} ')\n",
    "    \n",
    "    \n",
    "    # print(f'1.chunk.text: {chunk_text:>15} id: {chunk.id_} || {chunk.start:>2} | {chunk.end:>2} || {chunk.start_char:>2} | {chunk.end_char:>2} || 4.chunk_root_ent_type {chunk_root_ent_type:>1} | 5.chunk_ents {chunk_ents}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Preciso saber o saldo do contrato 342F da fazenda Santa Rita\"\n",
    "\n",
    "doc, tokens, ents = show_ent_new(text, patterns=patterns)\n",
    "print(doc)\n",
    "print()\n",
    "## GARANTIR que a ordem dos Tokens esteja correta (ascendente)\n",
    "seq_tokens_id = []\n",
    "seq_tokens_valor = []\n",
    "chunks_valor = []\n",
    "## tokens_ids + Lista de todos os Tokens da frase\n",
    "for token in doc:\n",
    "    seq_tokens_id.append(token.i)\n",
    "    \n",
    "    \n",
    "displacy.render(doc, style=\"ent\", options={\"colors\": colors})\n",
    "\n",
    "displacy.render(doc, style='dep',\n",
    "                jupyter=True, options={'distance': 120})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "  chunk_text = chunk.text\n",
    "  chunks_valor.append(chunk_text)\n",
    "  chunk_root = chunk.root.text\n",
    "  chunk_root_dep = chunk.root.dep_\n",
    "  chunk_root_head = chunk.root.head.text\n",
    "  chunk_root_head_dep = chunk.root.head.dep_\n",
    "  chunk_root_head_lemma = chunk.root.head.lemma_\n",
    "  chunk_ents = chunk.ents\n",
    "  chunk_root_ent_type = chunk.root.ent_type_\n",
    "  \n",
    "  print(f'1.chunk.text: {chunk_text:>15} | 2.ch.root: {chunk_root:>14} | 3.chunk.root.dep_: {chunk_root_dep:>18} | 4.ch.root.head: {chunk_root_head:>12} | 5.ch.root.head.dep_: {chunk_root_head_dep:>10} |  6.chunk.root.head.lemma_: {chunk_root_head_lemma:>9}')\n",
    "\n",
    "print()\n",
    "chunks = list(doc.noun_chunks)\n",
    "for i in range(len(chunks)):\n",
    "    print(f'idx: {i} | {chunks[i].text:>13} | {chunks[i].start:>2} {chunks[i].end:>2} | {chunks[i].start_char:>2}  {chunks[i].end_char:>2} || {chunks[i].root.ent_type_:>10} {chunks[i].ents} ')\n",
    "\n",
    "print()\n",
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_resize_gray(original_file_name, file_path, image_resized_path):\n",
    "\n",
    "    name_image = utl.conv_filename_no_ext(original_file_name)\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(name_image)}.jpg')\n",
    "    pages = convert_from_path(file_path, 500, poppler_path=poppler_path)\n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((4134, 5846))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "    imagem_gray = resized_pages[0].convert('L')\n",
    "    imagem_gray.save(image_resized_name, 'JPEG')\n",
    "\n",
    "    return  imagem_gray, image_resized_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_valortotal2 = {\n",
    "    \"label\": \"TOTALNOTA\",\n",
    "    \"pattern\": [\n",
    "        {\"LOWER\": \"valor\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"total\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"da\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"nota\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"TEXT\": {\"REGEX\": \"R\\$\"}},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"TEXT\": {\"REGEX\": \"[0-9]{1,3}(\\.[0-9]{3})*(,[0-9]{2})?\"}, \"OP\": \"?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "patternsOthers = [{\"label\": \"PERSON\", \"pattern\": \"Daniel\", \"id\": \"pessoa-daniel\"},\n",
    "                  {\"label\": \"PERSON\", \"pattern\": \"Antônio\", \"id\": \"pessoa-antonio\"}] \n",
    " \n",
    "patternsCult = [\n",
    "    {\n",
    "        \"label\":\"CULTURA\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"soja\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"milho\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"sorgo\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"trigo\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"milheto\", \"OP\":\"?\"},  \n",
    "            \n",
    "        ],    \n",
    "    \"id\": \"cultura\"}]\n",
    "\n",
    "patternsValores = [{\"label\":\"VALOR_SERVICO\", \"pattern\": [{\"LOWER\": \"valor\", \"OP\":\"?\"}, {\"LOWER\": \"serviços\", \"OP\":\"*\"}], \"id\": \"val-servico\"},\n",
    "                 {\"label\":\"ENTREGUE\", \"pattern\": [{\"LOWER\": \"quantidade\", \"OP\":\"?\"},{\"LOWER\": \"entregue\",\"OP\":\"*\"}],\"id\": \"qtde-entregue\"},\n",
    "                 {\"label\":\"ENTREGUE\", \"pattern\": [{\"LOWER\": \"quantidade\", \"OP\":\"?\"},{\"LOWER\": \"entreguei\",\"OP\":\"*\"}],\"id\": \"qtde-entregue\"},\n",
    "                 {\"label\":\"ENTREGUE\", \"pattern\": [{\"LOWER\": \"foram\", \"OP\":\"?\"},{\"LOWER\": \"entregues\",\"OP\":\"*\"}],\"id\": \"qtde-entregue\"},\n",
    "                 {\"label\":\"SALDO\", \"pattern\": [{\"LOWER\": \"saldo\",\"OP\":\"*\"}], \"id\": \"qtde-saldo\"}]\n",
    "\n",
    "\n",
    "patternsSafra = [{\"label\":\"SAFRA\", \"pattern\": [{\"LOWER\": \"safra\", \"OP\":\"?\"},{\"LOWER\": \"safras\", \"OP\":\"?\"}], \"id\": \"safra\"}]\n",
    "\n",
    "\n",
    "patternsNroSafra = [{\"label\":\"NR_SAF\", \"pattern\": [{\"SHAPE\": \"dd/dd\", \"OP\":\"*\"}], \"id\": \"nro_safra\"},\n",
    "                    {\"label\":\"NR_SAF\", \"pattern\": [{\"lower\": \"próxima\", \"OP\":\"*\"}], \"id\": \"nro_safra\"},\n",
    "                    {\"label\":\"NR_SAF\", \"pattern\": [{\"lower\": \"passada\", \"OP\":\"*\"}], \"id\": \"nro_safra\"}]   \n",
    "\n",
    "patternsCliente = [{\"label\": \"CLIENTE\", \"pattern\": [{\"LOWER\": \"berdinazzi\"}], \"id\": \"cli-berdinazzi\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"LOWER\": \"lopito\"}], \"id\": \"cli-lopito\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"LOWER\": \"bungue\"}], \"id\": \"cli-bungue\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"bunge\", \"bongue\", \"bumgue\"]}}}], \"id\": \"cli-bungue\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"berdinazi\"]}}}], \"id\": \"cli-berdinazzi\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"matarazzo\"]}}}], \"id\": \"cli-matarazzo\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"mezenga\"]}}}], \"id\": \"cli-mezenga\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"LOWER\": \"rei\"}, {\"LOWER\": \"do\"}, {\"LOWER\": \"gado\"}], \"id\": \"cli-reidogado\"},\n",
    "                   {\"label\": \"CLIENTE\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"rei-do-gado\"]}}}], \"id\": \"cli-reidogado\"},   \n",
    "                   ]\n",
    "\n",
    "patternsOthersFazenda = [{\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"santa\"}, {\"LOWER\": \"rita\"}], \"id\": \"faz-santarita\"},\n",
    "                         {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"bela\"}, {\"LOWER\": \"vista\"}], \"id\": \"faz-belavista\"},\n",
    "                         {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"passo\"}, {\"LOWER\": \"fundo\"}], \"id\": \"faz-passo\"},\n",
    "                         {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"minha\"}, {\"LOWER\": \"fazenda\"}], \"id\": \"faz-produtor\"},\n",
    "                        {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"agrobi\"}], \"id\": \"faz-produtor\"}\n",
    "                        ]\n",
    " \n",
    "patternsContrato = [{\"label\": \"CONTRATO\", \"pattern\": [{\"LOWER\": \"contrato\", \"OP\":\"?\"}], \"id\": \"contrato\"},\n",
    "                    {\"label\": \"CONTRATO\", \"pattern\": [{\"LOWER\": \"contratos\", \"OP\":\"?\"}], \"id\": \"contrato\"}]\n",
    "\n",
    "patternsNroContrato = [{\"label\": \"NR_CONT\", \"pattern\": [{\"SHAPE\": \"dddX\", \"OP\":\"*\"}], \"id\": \"nro_contrato\"},\n",
    "                       {\"label\": \"NR_CONT\", \"pattern\": [{\"POS\": \"NUM\", \"SHAPE\": \"ddd\", \"OP\": \"*\"},\n",
    "                                                            {\"POS\": \"PROPN\", \"SHAPE\": \"X\", \"OP\": \"*\"}], \"id\": \"nro_contrato\"}]\n",
    "\n",
    "patternsIntent = [{\"label\": \"INTENT\", \"pattern\": [{\"IS_TITLE\": True, \"OP\":\"*\"}], \"id\": \"user-intent\"},\n",
    "                  {\"label\": \"INTENT\", \"pattern\": [{\"POS\": \"ADJ\", \"OP\":\"*\"}, {\"POS\": \"VERB\", \"OP\":\"*\"}], \"id\": \"user-intent\"},\n",
    "                  {\"label\": \"INTENT\", \"pattern\": [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"preciso\", \"gostaria\", \"informar\"]}}}], \"id\": \"user-intent\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = {\n",
    "        \"CULTURA\": \"linear-gradient(90deg, #2ADB5E, #1FA346)\", \n",
    "        \"TOTAL\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\",\n",
    "        \"ENTREGUE\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\",\n",
    "        \"SALDO\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\", \n",
    "        \"FAZENDA\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\", \n",
    "        \"SAFRA\": \"linear-gradient(90deg, #FFC90E, #BA930A)\", \n",
    "        \"CONTRATO\": \"linear-gradient(90deg, #B5B5B5, #8A8A8A)\",\n",
    "        \"INTENT\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\"}\n",
    "\n",
    "\n",
    "patternsOthers = [{\"label\": \"PERSON\", \"pattern\": \"Daniel\", \"id\": \"daniel\"},\n",
    "                  {\"label\": \"PERSON\", \"pattern\": \"Antônio\", \"id\": \"antônio\"},\n",
    "                  {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"fast\"}, {\"LOWER\": \"innovation\"}], \"id\": \"fast-innovation\"},\n",
    "                  {\"label\": \"ORG\", \"pattern\": {\"LOWER\": \"agrobi\"}, \"id\": \"agrobi\"},\n",
    "                  {\"label\": \"ORG\", \"pattern\": {\"LOWER\": \"AgroBi\"}, \"id\": \"agrobi\"},\n",
    "                  {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"agro\"}, {\"LOWER\": \"bi\"}], \"id\": \"agrobi\"},\n",
    "                  ] \n",
    " \n",
    "\n",
    "patternsCult = [\n",
    "    {\n",
    "        \"label\":\"CULTURA\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"soja\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"milho\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"sorgo\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"trigo\", \"OP\":\"?\"},\n",
    "            \n",
    "        ]    \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "patternsQuant = [\n",
    "    {\n",
    "        \"label\":\"TOTAL\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"total\"},\n",
    "            {\"LOWER\": \"quantidade\", \"OP\":\"?\"},\n",
    "            \n",
    "        ]    \n",
    "    },\n",
    "    {\n",
    "        \"label\":\"ENTREGUE\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"quantidade\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"entregue\",\"OP\":\"?\"},\n",
    "            {\"LOWER\": \"entregues\",\"OP\":\"?\"},\n",
    "            {\"LOWER\": \"entregado\",\"OP\":\"?\"},\n",
    "            {\"LOWER\": \"entreguei\",\"OP\":\"?\"},\n",
    "        ]    \n",
    "    },\n",
    "    {\n",
    "        \"label\":\"SALDO\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"saldo\"},\n",
    "            {\"LOWER\": \"quantidade\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"total\", \"OP\":\"?\"},\n",
    "        ]    \n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "patternsSafra = [\n",
    "    {\n",
    "        \"label\":\"SAFRA\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"safra\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"safras\", \"OP\":\"?\"},\n",
    "            {\"SHAPE\": \"dd/dd\"},\n",
    "        ]    \n",
    "    }\n",
    "]\n",
    "\n",
    "patternsOthersFazenda = [{\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"santa\"}, {\"LOWER\": \"rita\"}], \"id\": \"faz-santa-rita\"},\n",
    "                         {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"bela\"}, {\"LOWER\": \"vista\"}], \"id\": \"faz-bela-vista\"},\n",
    "                         {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"passo\"}, {\"LOWER\": \"fundo\"}], \"id\": \"faz-passo-fundo\"},\n",
    "                         {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"minha\"}, {\"LOWER\": \"fazenda\"}], \"id\": \"faz-produtor\"}\n",
    "                        ]\n",
    " \n",
    "\n",
    "patternsContrato = [\n",
    "    {\n",
    "        \"label\":\"CONTRATO\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"contrato\", \"OP\":\"*\"},\n",
    "            {\"SHAPE\": \"dddX\", \"OP\":\"*\"},\n",
    "            {\"LOWER\": \"contratos\", \"OP\":\"*\"},\n",
    "            {\"SHAPE\": \"dddX\", \"OP\":\"*\"}\n",
    "            \n",
    "        ]\n",
    "        \n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "patterns = patternsCult + patternsQuant + patternsOthersFazenda + patternsSafra + patternsContrato + patternsOthers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"merge_noun_chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_text = (pytesseract.image_to_string(imagem_gray, lang='por'))\n",
    "ocr_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principais funcoes\n",
    "\n",
    "def busca_emitente(remoteJid):\n",
    "    \n",
    "    for entidade in entidades.find({\"remoteJid\": remoteJid}):\n",
    "            nome_entidade = entidade['Nome']\n",
    "            sobrenome_entidade = entidade['Sobrenome']\n",
    "            tipo_entidade = entidade['Tipo']\n",
    "            razao_entidade = entidade['razao']\n",
    "            \n",
    "    return  nome_entidade, sobrenome_entidade, tipo_entidade, razao_entidade\n",
    "\n",
    "\n",
    "def show_ent_new(text, patterns):\n",
    "    #nlp = spacy.blank(\"pt\")\n",
    "    #ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    ruler.add_patterns(patterns)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tokens = []\n",
    "    ents = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        span = doc.char_span(ent.start_char, ent.end_char, label=ent.label_)\n",
    "        ents.append(span)\n",
    "        \n",
    "    for token in doc:\n",
    "        start = token.idx\n",
    "        end = start + len(token)\n",
    "        tokens.append((token.text, start, end))\n",
    "        \n",
    "    return doc, tokens, ents\n",
    "\n",
    "\n",
    "\n",
    "# chunk.text, chunk.start, chunk.end, chunk.root.head.lemma_, chunk.root.dep_, chunk.doc\n",
    "def load_json(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "        \"TOTALNOTA\": \"linear-gradient(90deg, #2ADB5E, #1FA346)\", \n",
    "        \"TOTAL\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\",\n",
    "        \"ENTREGUE\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\",\n",
    "        \"SALDO\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\", \n",
    "        \"FAZENDA\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\", \n",
    "        \"SAFRA\": \"#FFEA7F\",\n",
    "        \"NR_SAF\": \"#CCA10C\", \n",
    "        \"CONTRATO\": \"#AB9BFC\",\n",
    "        \"NR_CONT\": \"#7AECEC\",\n",
    "        \"INTENT\": \"#EE8AF8\",\n",
    "        \"INTENT\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\",\n",
    "        \"PERSON\": \"linear-gradient(90deg, #FFA9FB, #BF7FBC)\",\n",
    "        \"CLIENTE\": \"linear-gradient(90deg, #FFA9FB, #BF7FBC)\"}\n",
    "\n",
    "\n",
    "\n",
    "pattern_valortotal = [{\n",
    "    \"label\": \"TOTALNOTA\",\n",
    "    \"pattern\": [\n",
    "        {\"LOWER\": \"valor\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"total\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"da\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"nota\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"TEXT\": {\"REGEX\": \"R\\$\"}},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"TEXT\": {\"REGEX\": \"[0-9]{1,3}(\\.[0-9]{3})*(,[0-9]{2})?\"}, \"OP\": \"?\"}\n",
    "    ]\n",
    "}]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "patterns = pattern_valortotal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
