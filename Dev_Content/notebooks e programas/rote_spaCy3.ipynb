{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## database - AgroBI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rote - spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SQLServerToPandasDataFrame import createConnection, runQuery\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import os\n",
    "import pyodbc\n",
    "from pandas import DataFrame\n",
    "import pandas.plotting\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "import pymssql\n",
    "from sqlalchemy.pool import NullPool\n",
    "import openai\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Define the desired column names\n",
    "new_columns = {\n",
    "    'ID_FAZENDA':           '1-id fazenda',              \n",
    "    'NM_FAZENDA':           '2-nome fazenda',                   \n",
    "    'ID_CULTURA':           '3-id cultura',                     \n",
    "    'NM_CULTURA':           '4-nome cultura',                   \n",
    "    'ID_CLIENTE':           '5-id cliente',                     \n",
    "    'NM_CLIENTE':           '6-nome cliente',                   \n",
    "    'ID_SAFRA':             '7-id safra',                         \n",
    "    'NR_CONTRATO':          '8-nro contrato',                  \n",
    "    'QT_TOTAL_CONTRATO':    '9-qt total contrato',       \n",
    "    'QT_ENTREGUE_CONTRATO': '10-qt entregue contrato', \n",
    "    'QT_SALDO_CONTRATO':    '11-qt saldo contrato'        \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQL Server using pyodbc\n",
    "cnxn = pyodbc.connect('DSN=agrobi-portal;UID=admin-agrobi-portal', password=\"A@S6ZqB$5E#03B\")\n",
    "\n",
    "# Create the SQLAlchemy engine using the pyodbc dialect and the existing pyodbc connection\n",
    "engine = create_engine(\"mssql+pyodbc://\", creator=lambda: cnxn)\n",
    "\n",
    "# Define the SQL query with the original column names\n",
    "\n",
    "sql = 'SELECT ID_FAZENDA, NM_FAZENDA, ID_CULTURA, NM_CULTURA, ID_CLIENTE, NM_CLIENTE, ID_SAFRA, NR_CONTRATO, QT_TOTAL_CONTRATO, QT_ENTREGUE_CONTRATO, QT_SALDO_CONTRATO FROM [agrobi-portal].[ia].[SALDO_CONTRATO_GRAOS]'\n",
    "\n",
    "#sql = 'SELECT ID_FAZENDA, NM_FAZENDA, ID_CULTURA, NM_CULTURA, ID_CLIENTE, NM_CLIENTE, ID_SAFRA, NR_CONTRATO, QT_TOTAL_CONTRATO, QT_ENTREGUE_CONTRATO, QT_SALDO_CONTRATO FROM [agrobi-portal].[ia].[SALDO_CONTRATO_GRAOS]'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SQL query results into a pandas dataframe, with renamed columns\n",
    "df = pd.read_sql(sql, engine).rename(columns=new_columns)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## openai - criaçao de index e funcao ask_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import openai\n",
    "from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "def construct_index(directory_path):\n",
    "    # set maximum input size\n",
    "    max_input_size = 4096\n",
    "    # set number of output tokens\n",
    "    num_outputs = 2000\n",
    "    # set maximum chunk overlap\n",
    "    max_chunk_overlap = 20\n",
    "    # set chunk size limit\n",
    "    chunk_size_limit = 600 \n",
    "\n",
    "    # define LLM\n",
    "    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=\"text-davinci-003\", max_tokens=num_outputs))\n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
    " \n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "  \n",
    "\n",
    "\n",
    "    \n",
    "    index = GPTSimpleVectorIndex.from_documents(documents)\n",
    "\n",
    "\n",
    "\n",
    "   # index = GPTSimpleVectorIndex(\n",
    "   #     documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper\n",
    "   # )\n",
    "\n",
    "    index.save_to_disk('index.json')\n",
    "\n",
    "    return index\n",
    "\n",
    "def ask_ai():\n",
    "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
    "    while True: \n",
    "        query = input(\"o que posso lhe ajudar? \")\n",
    "        response = index.query(query, response_mode=\"compact\")\n",
    "        display(Markdown(f\"Response: <b>{response.response}</b>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy - Geral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy import displacy\n",
    "import re\n",
    "from spacy.lang.pt import Portuguese\n",
    "\n",
    "\n",
    "from langchain import OpenAI\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define method with arguments\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Set extension on the Doc with method\n",
    "Doc.set_extension(\"has_token\", method=has_token, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagem1 = \"Quanto entreguei nesta safra e nas safras anteriores\"\n",
    "\n",
    "mensagem2 = \"Quanto entreguei nesta safra e nas safras anteriores de soja\"\n",
    "\n",
    "mensagem3 = \"Eu quero saber o total de milho que fazenda AGROBIA tem para me entregar na safra 2023\"\n",
    "\n",
    "mensagem4 = \"Quantos quilos de milho faltam ser entregues pela fazenda X no contrato atual para a safra de 2021?\"\n",
    "\n",
    "mensagem5 = \"Como posso saber a quantidade total de milho que eu devo receber de todas as fazendas com as quais eu tenho contratos?\"\n",
    "\n",
    "mensagem6 = \"Como posso saber a quantidade total de milho que eu devo receber de todas as fazendas com as quais eu tenho contratos?\"\n",
    "\n",
    "mensagem7 = \"Qual e a quantidade total de soja do contrato 448S a ser entregue?\"\n",
    "\n",
    "mensagem8 = \"Qual e a quantidade total do contrato soja a ser entregue?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soluçao"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Busca entidade no db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remoteJid = \"5511994954119@s.whatsapp.net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pymongo\n",
    "import pprint\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb+srv://danielAgro:Dash012345@cluster0.qmjsaat.mongodb.net/?retryWrites=true&w=majority\")\n",
    "db = client.assistant_db #verificar\n",
    "entidades = db.entidades #aqui esta a coleÇao\n",
    "\n",
    "\n",
    "for entidade in entidades.find({\"remoteJid\": remoteJid}):\n",
    "    nome_entidade = entidade['Nome']\n",
    "    sobrenome_entidade = entidade['Sobrenome']\n",
    "    tipo_entidade = entidade['Tipo']\n",
    "    razao_entidade = entidade['razao']\n",
    "    \n",
    "    \n",
    "print(f'\\nNome: {nome_entidade} {sobrenome_entidade} | tipo: {tipo_entidade} | Razao Social: {razao_entidade}\\n')\n",
    "\n",
    "\n",
    "if nome_entidade:\n",
    "    entidade_cadastrada = True\n",
    "else:\n",
    "    entidade_cadastrada = False\n",
    "    categoria_atendimento = 'geral' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Verificar assunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso 1 - Proprietario quer saber sobre contrato de sua fazenda    o saldo do contrato de café \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mensagem  = 'Gostaria de saber o total do meu contrato gria'\n",
    "\n",
    "#mensagem = \"Gostaria de saber sobr minha fazenda AGROBI e do seu contrato 532S.\"\n",
    "\n",
    "doc = nlp(mensagem)\n",
    "\n",
    "Fase = \"2. Qualificacao\"\n",
    "resultados = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tipo_entidade == 'Produtor':\n",
    "    entidade_produtor = True\n",
    "else:\n",
    "    entidade_produtor = False\n",
    "\n",
    "entidade_produtor      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tipo_entidade == 'Cliente':\n",
    "    entidade_cliente = True\n",
    "else:\n",
    "    entidade_cliente = False\n",
    "    \n",
    "entidade_cliente    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condicoes = [\n",
    "        doc._.has_token(\"contrato\"), \n",
    "        doc._.has_token(\"contratos\"), \n",
    "        doc._.has_token(\"saldo\"),\n",
    "        doc._.has_token(\"quantidade\"),\n",
    "        doc._.has_token(\"quantidade total\"),\n",
    "        doc._.has_token(\"total\"),\n",
    "        doc._.has_token(\"Fazenda\"),\n",
    "        doc._.has_token(\"fazenda\"),\n",
    "        doc._.has_token(\"fazendas\"),\n",
    "        doc._.has_token(\"safra\"),\n",
    "        doc._.has_token(\"safras\"),\n",
    "        doc._.has_token(\"milho\"),\n",
    "        doc._.has_token(\"soja\"),\n",
    "        doc._.has_token(\"sorgo\"),\n",
    "        doc._.has_token(\"café\")\n",
    "]\n",
    "\n",
    "\n",
    "indices_true = [i for i, x in enumerate(condicoes) if x]\n",
    "\n",
    "if len(indices_true) >= 1 and entidade_cadastrada:\n",
    "        categoria_atendimento = 'agroIA'\n",
    "        print(f'Itens =  {len(indices_true)} portanto categoria de atendimento = {categoria_atendimento}\\n')\n",
    "  \n",
    "else:\n",
    "        categoria_atendimento = 'Geral'     \n",
    "        print(\"Não há pelo menos 3 condições verdadeiras\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []  \n",
    "\n",
    "\n",
    "if nome_entidade:\n",
    "    entidade_cadastrada = True\n",
    "else:\n",
    "    entidade_cadastrada = False\n",
    "    \n",
    "# 1. Entidade = Produtor\n",
    "if tipo_entidade == 'Produtor':\n",
    "    entidade_produtor = True\n",
    "else:\n",
    "    entidade_produtor = False\n",
    "    \n",
    "resultados.append(entidade_produtor)    \n",
    "\n",
    "\n",
    "# 2. Entidade = Cliente\n",
    "if tipo_entidade == 'Cliente':\n",
    "    entidade_cliente = True\n",
    "else:\n",
    "    entidade_cliente = False\n",
    "\n",
    "resultados.append(entidade_cliente)    \n",
    "     \n",
    "# 3. Nome da razao da Entidade     \n",
    "if razao_entidade:\n",
    "    nome_empresa_entidade = True     \n",
    "else:\n",
    "    nome_empresa_entidade = False        \n",
    "\n",
    "resultados.append(nome_empresa_entidade) \n",
    "\n",
    "# 4. Contrato\n",
    "ck_tema_contrato = [\n",
    "        doc._.has_token(\"contrato\"), \n",
    "        doc._.has_token(\"contratos\")]\n",
    "\n",
    "\n",
    "if any(ck_tema_contrato):\n",
    "    item_contrato = True\n",
    "else:\n",
    "    item_contrato = False\n",
    "    \n",
    "resultados.append(item_contrato)\n",
    "\n",
    "\n",
    "# 5. Cultura\n",
    "ck_tema_cultura = [\n",
    "        doc._.has_token(\"milho\"),\n",
    "        doc._.has_token(\"soja\"),\n",
    "        doc._.has_token(\"sorgo\"),\n",
    "        doc._.has_token(\"café\")]\n",
    "\n",
    "\n",
    "if any(ck_tema_cultura ):\n",
    "    item_cultura = True\n",
    "else:\n",
    "    item_cultura = False\n",
    "\n",
    "resultados.append(item_cultura)\n",
    "\n",
    "\n",
    "# 6. Quantidade\n",
    "ck_tema_quantidade = [\n",
    "        doc._.has_token(\"entregar\"),\n",
    "        doc._.has_token(\"saldo\"),\n",
    "        doc._.has_token(\"quantidade\"),\n",
    "        doc._.has_token(\"quantidade total\"),\n",
    "        doc._.has_token(\"total\")]\n",
    "\n",
    "\n",
    "if any(ck_tema_quantidade):\n",
    "    item_quantidade = True\n",
    "else:\n",
    "    item_quantidade = False\n",
    "\n",
    "resultados.append(item_quantidade)\n",
    "\n",
    "\n",
    "# 7. Fazenda\n",
    "ck_tema_fazenda = [\n",
    "        doc._.has_token(\"Fazenda\"),\n",
    "        doc._.has_token(\"fazenda\"),\n",
    "        doc._.has_token(\"fazendas\"),\n",
    "        doc._.has_token(\"terra\")]\n",
    "        \n",
    "\n",
    "if any(ck_tema_fazenda):\n",
    "    item_fazenda = True\n",
    "else:\n",
    "    item_fazenda = False\n",
    "    \n",
    "resultados.append(item_fazenda)  \n",
    "\n",
    "\n",
    "# 8. Safra\n",
    "ck_tema_safra = [\n",
    "        doc._.has_token(\"safra\"),\n",
    "        doc._.has_token(\"safras\")]\n",
    "        \n",
    "\n",
    "if any(ck_tema_safra):\n",
    "    item_safra = True\n",
    "else:\n",
    "    item_safra = False\n",
    "    \n",
    "resultados.append(item_safra)\n",
    "\n",
    "\n",
    "resultados  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomando a ação apropriada com base nos resultados:\n",
    "if resultados == [True, False, True, True, False, True, False, False]:\n",
    "    print(\"provavemente, quer saber quantidade a ser entregue do contrato todo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'token.text   token.lemma_  token.pos_:    pos_explained:             token.tag_: token.dep_:  token.shape_ token.is_alpha ,  token.is_stop')\n",
    "for token in doc:\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    pos_explained = spacy.explain(token_pos)\n",
    "    print(f'{token.text:<10} | {token.lemma_:<12} | {token.pos_:<10} | {pos_explained:<25} | {token.tag_:<10} | {token.dep_:<10} | {token.shape_:<10} | {token.is_alpha:<15} | {token.is_stop:<5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n{tipo_entidade} razao_entidade {(doc.text).lower()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Achando o nro do contrato + estrutura do prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cultura = ['milho', 'soja', 'sorgo', 'arroz', 'café', 'feijão']\n",
    "\n",
    "contrato = ['contrato', 'contratos']\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns_cultura = list(nlp.pipe(cultura))\n",
    "patterns_contrato = list(nlp.pipe(contrato))\n",
    "\n",
    "matcher.add(\"CULTURA\", patterns_cultura)\n",
    "matcher.add(\"CONTRATO\", patterns_contrato)\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    if matcher.vocab.strings[match_id] == \"CONTRATO\":\n",
    "        nro_contrato = doc[start+1:end+1]\n",
    "        print(f'inicio: {start} fim: {end} nro contrato: {nro_contrato.text}')\n",
    "    elif matcher.vocab.strings[match_id] == \"FAZENDA\":\n",
    "        nome_fazenda = doc[start:end + 1]\n",
    "        print(f'Fazenda: {nome_fazenda.text}')\n",
    "        \n",
    "for token in doc:\n",
    "    if token.text == token_text:\n",
    "        token_text = '<valor>'\n",
    "        \n",
    "print(token.text, token_text)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contratoUpper = nro_contrato.text.upper()\n",
    "razaoUpper = razao_entidade.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entregar = df.loc[(df['2-nome fazenda'] == razaoUpper) & (df['8-nro contrato'] == contratoUpper), '9-qt total contrato'].values[0]\n",
    "\n",
    "total_entregar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import openai\n",
    "from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper\n",
    "\n",
    "#MODEL = \"gpt-3.5-turbo\"\n",
    "def ask_ai_once(valor):\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\") \n",
    "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
    "    query = valor\n",
    "    response = index.query(query, response_mode=\"compact\")\n",
    "    return {response.response}\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_ai_once(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['2-nome fazenda'] == razao_entidade) & (df['8-nro contrato'] == nro_contrato), '9-qt total contrato'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['2-nome fazenda'] == nome_entidade) & (df['6-nome cliente'] == 'CLIENTE 000001') & (df['8-nro contrato'] == '448S'), '9-qt total contrato'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked as Apple reveals pre-orders\")\n",
    "\n",
    "# Initialize the Matcher with the shared vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Create a pattern matching two tokens: \"iPhone\" and \"X\"\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add(\"IPHONE_X_PATTERN\", [pattern])\n",
    "\n",
    "# Use the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"spaCy is cool!\"\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go, get started!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Go, get started!\"\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, really?!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Oh, really?!\"\n",
    "words = [\"Oh\", \",\", \"really\", \"?\", \"!\"]\n",
    "spaces = [False, True, False, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like David Bowie\n",
      "David Bowie PERSON\n",
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####   ERRADO #######\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "# Get all tokens and part-of-speech tags\n",
    "token_texts = [token.text for token in doc]\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Found proper noun before a verb:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "### CERTO!!!! #####\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"Found proper noun before a verb:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6638734340667725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10267/1458547384.py:12: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = span1.similarity(span2)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[12:15]\n",
    "\n",
    "# Get the similarity of the spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"}, {\"TEXT\": \"-\"}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "# matcher.add(\"CONTRATO\", [[{\"LOWER\": \"contrato\"}, {\"LOWER\": \"contratos\"}]])\n",
    "\n",
    "pattern = [{\"LOWER\": \"contrato\"}]\n",
    "matcher.add(\"contrato\", [pattern])\n",
    "\n",
    "\n",
    "expression = r\"(contrato)\"\n",
    "for match in re.finditer(expression, doc.text):\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start, end)\n",
    "    # This is a Span object or None if match doesn't map to valid token sequence\n",
    "    if span is not None:\n",
    "        print(\"Found match:\", span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"[Cc](ontrato$)\"\n",
    "sequence = \"Contrato\"\n",
    "if re.match(pattern, sequence):\n",
    "    print(\"Match!\")\n",
    "else: print(\"Not a match!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = 'Please contact us at: support@datacamp.com'\n",
    "match = re.search(r'([\\w\\.-]+)@([\\w\\.-]+)', statement)\n",
    "if statement:\n",
    "  print(\"Email address:\", match.group()) # The whole matched text\n",
    "  print(\"Username:\", match.group(1)) # The username (group 1)\n",
    "  print(\"Host:\", match.group(2)) # The host (group 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"CASEINSENSITIVE\": \"world\"}]\n",
    "matcher.add(\"HelloWorld\", [pattern])\n",
    "\n",
    "\n",
    "pattern = [{\"TEXT\": {\"REGEX\": \"^[Uu](\\\\.?|nited)$\"}},\n",
    "           {\"TEXT\": {\"REGEX\": \"^[Ss](\\\\.?|tates)$\"}},\n",
    "           {\"LOWER\": \"president\"}]\n",
    "\n",
    "\n",
    "# Match different spellings of token texts\n",
    "pattern = [{\"TEXT\": {\"REGEX\": \"deff?in[ia]tely\"}}]\n",
    "\n",
    "# Match tokens with fine-grained POS tags starting with 'V'\n",
    "pattern = [{\"TAG\": {\"REGEX\": \"^V\"}}]\n",
    "\n",
    "# Match custom attribute values with regular expressions\n",
    "pattern = [{\"_\": {\"country\": {\"REGEX\": \"^[Uu](nited|\\\\.?) ?[Ss](tates|\\\\.?)$\"}}}]\n",
    "\n",
    "\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The United States of America (USA) are commonly known as the United States (U.S. or US) or America.\")\n",
    "\n",
    "expression = r\"[Uu](nited|\\\\.?) ?[Ss](tates|\\\\.?)\"\n",
    "for match in re.finditer(expression, doc.text):\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start, end)\n",
    "    # This is a Span object or None if match doesn't map to valid token sequence\n",
    "    if span is not None:\n",
    "        print(\"Found match:\", span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"HellWorld\", [pattern])\n",
    "\n",
    "doc = nlp(\"Hello. world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contratos = ['contrato', 'contratos']\n",
    "# Only run nlp.make_doc to speed things up\n",
    "contrato_pattern = [nlp.make_doc(contrato) for contrato in contratos]\n",
    "matcher.add(\"CONTRATO\", contrato_pattern)\n",
    "\n",
    "fazendas = ['fazenda', 'fazendas']\n",
    "# Only run nlp.make_doc to speed things up\n",
    "fazendas_pattern = [nlp.make_doc(fazenda) for fazenda in fazendas]\n",
    "matcher.add(\"FAZENDA\", fazendas_pattern)\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    if matcher.vocab.strings[match_id] == \"CONTRATO\":\n",
    "        nome_contrato = doc[start:end + 1]\n",
    "        print(f'Contrato: {nome_contrato.text}')\n",
    "    elif matcher.vocab.strings[match_id] == \"FAZENDA\":\n",
    "        nome_fazenda = doc[start:end + 1]\n",
    "        print(f'Fazenda: {nome_fazenda.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomando a ação apropriada com base nos resultados:\n",
    "if resultados == [True, True, True, True, True, False]:\n",
    "    print(\"Falta apenas um item\")\n",
    "    \n",
    "elif resultados == [True, False, False, True, False]:\n",
    "    # Fazer algo se as condições forem verdadeiras nesta ordem\n",
    "    print(\"Ele tem contrato e quer saber sobre a fazenda????\")    \n",
    "    \n",
    "elif resultados == [True, True, True, True, False, False]:\n",
    "    # Fazer algo se as condições forem verdadeiras nesta ordem\n",
    "    print(\"Falta apenas dois itens\")\n",
    "    \n",
    "elif resultados == [False, False, False, True, False]:\n",
    "    # Fazer algo se as condições forem verdadeiras nesta ordem\n",
    "    print(\"Apenas tem fazenda\")    \n",
    "    \n",
    "else:\n",
    "    print(\"else\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(token.text, token.dep_, token.pos_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index:   \", [token.i for token in doc])\n",
    "print(\"Text:    \", [token.text for token in doc])\n",
    "\n",
    "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
    "print(\"is_punct:\", [token.is_punct for token in doc])\n",
    "print(\"like_num:\", [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Gostaria de saber sobre minha Fazenda AGROBI e do seu contrato 532S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(ent.text, ent.label_) for ent in doc.ents]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Nesta solucao que vc propos, deu certo, porem quero uma lista apenas de palavras_chave, e usar : doc._.has_token(\"palavra_chave) para iterar sobre ela e definir a resposta) O item de comparação correspondente ao índice 0 é: True = 'contrato' \n",
    "\n",
    "palavras_chave = [\n",
    "        \"contrato\", \n",
    "        \"contratos\", \n",
    "        \"saldo\",\n",
    "        \"quantidade\",\n",
    "        \"quantidade total\",\n",
    "        \"total\",\n",
    "        \"Fazenda\",\n",
    "        \"fazenda\",\n",
    "        \"fazendas\",\n",
    "        \"safra\",\n",
    "        \"safras\",\n",
    "        \"milho\",\n",
    "        \"soja\",\n",
    "        \"sorgo\",\n",
    "        \"café\"\n",
    "]\n",
    "\n",
    "condicoes = [\n",
    "        doc._.has_token(\"contrato\"), \n",
    "        doc._.has_token(\"contratos\"), \n",
    "        doc._.has_token(\"saldo\"),\n",
    "        doc._.has_token(\"quantidade\"),\n",
    "        doc._.has_token(\"quantidade total\"),\n",
    "        doc._.has_token(\"total\"),\n",
    "        doc._.has_token(\"Fazenda\"),\n",
    "        doc._.has_token(\"fazenda\"),\n",
    "        doc._.has_token(\"fazendas\"),\n",
    "        doc._.has_token(\"safra\"),\n",
    "        doc._.has_token(\"safras\"),\n",
    "        doc._.has_token(\"milho\"),\n",
    "        doc._.has_token(\"soja\"),\n",
    "        doc._.has_token(\"sorgo\"),\n",
    "        doc._.has_token(\"café\")\n",
    "]\n",
    "\n",
    "\n",
    "indices_true = [i for i, x in enumerate(condicoes) if x]\n",
    "\n",
    "if len(indices_true) >= 1:\n",
    "        print(f'Estamos falando sobre Agro pois temos: {len(indices_true)} itens de agro no total\\n')\n",
    "        for i in indices_true:\n",
    "                print(f\"O item de comparação correspondente ao índice {i} é: {condicoes[i]} = '{palavras_chave[i]}'\")\n",
    "else:\n",
    "    print(\"Não há pelo menos 3 condições verdadeiras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ENTENDI   pontuacao\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"HellWorld\", [pattern])\n",
    "\n",
    "doc = nlp(\"Hello. world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}], \"id\": \"san-francisco\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"fran\"}], \"id\": \"san-francisco\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc1 = nlp(\"Apple is opening its first big office in San Francisco.\")\n",
    "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc1.ents])\n",
    "\n",
    "doc2 = nlp(\"Apple is opening its first big office in San Fran.\")\n",
    "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc2.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DOG\", [[{\"LOWER\": \"golden\"}, {\"LOWER\": \"retriever\"}]])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"MISC\") #'Miscellaneous entities, e.g. events, nationalities, products or works of art'\n",
    "spacy.explain(\"NNP\")\n",
    "spacy.explain(\"dobj\")\n",
    "spacy.explain(\"ORG\") #'Companies, agencies, institutions, etc.'\n",
    "spacy.explain(\"LOC\") # 'Non-GPE locations, mountain ranges, bodies of water'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cultura = ['milho', 'soja', 'sorgo', 'arroz', 'café', 'feijão']\n",
    "\n",
    "contrato = ['contrato', 'contratos']\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns_cultura = list(nlp.pipe(cultura))\n",
    "patterns_contrato = list(nlp.pipe(contrato))\n",
    "\n",
    "matcher.add(\"CULTURA\", patterns_cultura)\n",
    "matcher.add(\"CONTRATO\", patterns_contrato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for match_id, start, end in matcher(doc):\n",
    "#     span = doc[start:end]\n",
    "#     print(\"Matched span:\", span.text)\n",
    "#     # Get the span's root token and root head token\n",
    "#     print(\"Root token:\", span.root.text)\n",
    "#     print(\"Root head token:\", span.root.head.text)\n",
    "#     # Get the previous token and its POS tag\n",
    "#     print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)\n",
    "        \n",
    "\n",
    "#print(matcher.__getattribute__)\n",
    "print(doc)\n",
    "print()    \n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(start, end, span.text)\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)\n",
    "\n",
    "    #print(f'span_with_label {span_with_label}, label {span_with_label.label_} | Matched span:{span.text}  Root token: {span.root.text} span.root.head.text {span.root.head.text} Previous token: {doc[start - 1].text, doc[start - 1].pos_  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "#nlp = spacy.blank(\"pt\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PERSON\", [[{\"lower\": \"daniel\"}, {\"lower\": \"nascimento\"}]])\n",
    "doc = nlp(\"Este contrato tem como parte Daniel Nascimento RG 22.571.820 trabalha na empresa AgroBI\")\n",
    "\n",
    "# 1. Return (match_id, start, end) tuples\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    # Create the matched span and assign the match_id as a label\n",
    "    span = Span(doc, start, end, label=match_id)\n",
    "    print(start, end, span.text, span.label_)\n",
    "\n",
    "# 2. Return Span objects directly\n",
    "matches = matcher(doc, as_spans=True)\n",
    "for span in matches:\n",
    "    print(span.text, span.label_)\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.pt import Portuguese\n",
    "nlp = Portuguese()\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"AgroBI\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"dois\"}, {\"LOWER\": \"irmãos\"}]}]\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "doc = nlp(\"A empresa AgroBI esta atendendo agora em Dois Irmãos.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.pt import Portuguese\n",
    "nlp = Portuguese()\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"fazenda\"}, {\"LOWER\": \"passo\"}, {\"LOWER\": \"fundo\"}]},\n",
    "            {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"fazenda\"}, {\"LOWER\": \"AgroBI\"}]},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"dois\"}, {\"LOWER\": \"irmãos\"}]}]\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "doc = nlp(\"A empresa AgroBI esta atendendo agora em Dois Irmãos.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"vamos conhecer a Fazenda Passo Fundo e a Fazenda AgroBI.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = Portuguese()\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "patterns = [{\"label\": \"MISC\", \"pattern\": \"FAZENDA PASSO FUNDO\"},\n",
    "            {\"label\": \"ORG\", \"pattern\": \"AgroBI\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"dois\"}, {\"LOWER\": \"irmãos\"}]}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"A empresa AgroBI esta atendendo agora em Dois Irmãos.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"A empresa AgroBI e a FAZENDA PASSO FUNDO estao localizadas na cidade de Dois Irmãos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REsolvido o MAtch para cultura\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "culturas = ['milho', 'soja', 'sorgo', 'arroz', 'café']\n",
    "# Only run nlp.make_doc to speed things up\n",
    "cult_patterns = [nlp.make_doc(cultura) for cultura in culturas]\n",
    "matcher.add(\"CULTURA\", cult_patterns)\n",
    "\n",
    "doc = nlp(\"Minhas culturas de milhor, soja e sorgo - todas em Novo Hamburgo\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(f'\\ncultura: {span.text} inicio: {start}, fim {end}')\n",
    "print()\n",
    "matches = matcher(doc, as_spans=True)\n",
    "for span in matches:\n",
    "    print(span.text, span.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fazendas_pattern = [[{\"LOWER\": \"fazenda\"}, {\"TITLE\": True, \"POS\": \"PROPN\"}, {\"TITLE\": True, \"POS\": \"PROPN\"}]]\n",
    "fazenda_pattern = [{'LOWER': 'fazenda'}, {'IS_TITLE': True, 'OP': '*'}, {'LOWER': {'IN': ['passo', 'fundo']}, 'OP': '?'}]\n",
    "fazenda_pattern = [{\"LOWER\": \"fazenda\"}, {\"IS_TITLE\": True, \"OP\": \"*\"}, {\"LOWER\": {\"IN\": [\"passo\", \"fundo\"]}, \"OP\": \"?\"}]\n",
    "\n",
    "fazendas = [{'LOWER': 'fazenda'}, {'IS_TITLE': True, 'OP': '*'}, {'LOWER': {'IN': ['passo', 'fundo']}, 'OP': '?'}]\n",
    "fazendas_pattern = [nlp.make_doc(fazenda) for fazenda in fazendas]\n",
    "matcher.add(\"FAZENDA\", fazendas_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.pt import Portuguese\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = Portuguese()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def add_fazenda_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"FAZENDA\")\n",
    "    doc.ents += (entity,)\n",
    "    print(entity.text)\n",
    "    \n",
    "def add_cultura_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"CULTURA\")\n",
    "    doc.ents += (entity,)\n",
    "    print(entity.text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"ORTH\": \"milho\"}]\n",
    "matcher.add(\"Culturas\", [pattern], on_match=add_cultura_ent)\n",
    "\n",
    "pattern = [{\"ORTH\": \"soja\"}]\n",
    "matcher.add(\"Culturas\", [pattern], on_match=add_cultura_ent)\n",
    "\n",
    "pattern = [{\"ORTH\": \"Fazenda\"}, {\"ORTH\": \"AgroBI\"}]\n",
    "matcher.add(\"Fazendas\", [pattern], on_match=add_fazenda_ent)\n",
    "\n",
    "pattern = [{\"ORTH\": \"Fazenda\"}, {\"ORTH\": \"Maraisa\"}]\n",
    "matcher.add(\"Fazendas\", [pattern], on_match=add_fazenda_ent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Estamos falando sobre soja\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Estamos falando sobre milho\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Estamos falando a Fazenda Maraisa que produz soja e milho. Falamos tambem sobre a Fazenda AgroBI - que produz somente soja\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(ent.text, ent.label_) for ent in doc.ents]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'start: {ent.start} end: {ent.end} | texto: {ent.text} , label: {ent.label_}')\n",
    "    # print(ent.text)\n",
    "    # print(ent.label_)\n",
    "    # print(ent.start)\n",
    "    # print(ent.end)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "#nlp = spacy.blank(\"pt\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PERSON\", [[{\"lower\": \"daniel\"}, {\"lower\": \"nascimento\"}]])\n",
    "doc = nlp(\"Este contrato tem como parte Daniel Nascimento RG 22.571.820 trabalha na empresa AgroBI\")\n",
    "\n",
    "# 1. Return (match_id, start, end) tuples\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    # Create the matched span and assign the match_id as a label\n",
    "    span = Span(doc, start, end, label=match_id)\n",
    "    print(start, end, span.text, span.label_)\n",
    "\n",
    "# 2. Return Span objects directly\n",
    "matches = matcher(doc, as_spans=True)\n",
    "for span in matches:\n",
    "    print(span.text, span.label_)\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tema_cultura = [\n",
    "        doc._.has_token(\"milho\"),\n",
    "        doc._.has_token(\"soja\"),\n",
    "        doc._.has_token(\"sorgo\"),\n",
    "        doc._.has_token(\"café\")]\n",
    "\n",
    "tema_cultura\n",
    "\n",
    "\n",
    "if any(tema_cultura):\n",
    "    print(\"Estamos falando sobre alguma cultura\")\n",
    "else:\n",
    "    print(\"O assunto seja outro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mensagem = \"Gostaria de saber sobr minha fazenda AGROBI e do seu contrato 532S.\"\n",
    "# doc = nlp(mensagem)\n",
    "\n",
    "contratos = ['contrato', 'contratos']\n",
    "# Only run nlp.make_doc to speed things up\n",
    "contrato_pattern = [nlp.make_doc(contrato) for contrato in contratos]\n",
    "matcher.add(\"CONTRATO\", contrato_pattern)\n",
    "\n",
    "fazendas = ['fazenda', 'fazendas']\n",
    "# Only run nlp.make_doc to speed things up\n",
    "fazendas_pattern = [nlp.make_doc(fazenda) for fazenda in fazendas]\n",
    "matcher.add(\"FAZENDA\", fazendas_pattern)\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    if matcher.vocab.strings[match_id] == \"CONTRATO\":\n",
    "        nome_contrato = doc[start:end + 1]\n",
    "        print(f'Contrato: {nome_contrato.text}')\n",
    "    elif matcher.vocab.strings[match_id] == \"FAZENDA\":\n",
    "        nome_fazenda = doc[start:end + 1]\n",
    "        print(f'Fazenda: {nome_fazenda.text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_true = [i for i, x in enumerate(tema_cultura) if x]\n",
    "for i in indices_true:\n",
    "    print(f\"O item de comparação correspondente ao índice {i} é: {condicoes[i]} = '{palavras_chave[i]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tema_cultura = [\n",
    "        doc._.has_token(\"milho\"),\n",
    "        doc._.has_token(\"soja\"),\n",
    "        doc._.has_token(\"sorgo\"),\n",
    "        doc._.has_token(\"café\")]\n",
    "\n",
    "tema_cultura\n",
    "\n",
    "\n",
    "if any(tema_cultura):\n",
    "    print(\"Estamos falando sobre alguma cultura\")\n",
    "else:\n",
    "    print(\"O assunto seja outro\")\n",
    "\n",
    "indices_true = [i for i, x in enumerate(tema_cultura) if x]\n",
    "for i in indices_true:\n",
    "        print(f\"O item de comparação correspondente ao índice {i} é: {condicoes[i]} = '{palavras_chave[i]}'\")\n",
    "else:\n",
    "    print(\"Não há pelo menos 3 condições verdadeiras\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(tema_cultura):\n",
    "    cultura = True\n",
    "else:\n",
    "    cultura = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomando a ação apropriada com base nos resultados:\n",
    "if resultados == [True, True, False, True]:\n",
    "    print(\"Primeiro if\")\n",
    "elif resultados == [False, True, True, False]:\n",
    "    # Fazer algo se as condições forem verdadeiras nesta ordem\n",
    "    print(\"Primeiro elif\")\n",
    "else:\n",
    "    print(\"else\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respostas = [True, False, True, False, False, False, True, False, False, False, False, False, False, True]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condicoes = [\n",
    "        doc._.has_token(\"contrato\"), \n",
    "        doc._.has_token(\"contratos\"), \n",
    "        doc._.has_token(\"saldo\"),\n",
    "        doc._.has_token(\"quantidade\"),\n",
    "        doc._.has_token(\"quantidade total\"),\n",
    "        doc._.has_token(\"total\"),\n",
    "        doc._.has_token(\"fazenda\"),\n",
    "        doc._.has_token(\"fazendas\"),\n",
    "        doc._.has_token(\"safra\"),\n",
    "        doc._.has_token(\"safras\"),\n",
    "        doc._.has_token(\"milho\"),\n",
    "        doc._.has_token(\"soja\"),\n",
    "        doc._.has_token(\"sorgo\"),\n",
    "        doc._.has_token(\"café\")]\n",
    "\n",
    "resultados = [True if c else False for c in condicoes]\n",
    "\n",
    "\n",
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de lista de condições\n",
    "condicoes = [\n",
    "    doc._.has_token(\"contrato\"), \n",
    "    doc._.has_token(\"contratos\"), \n",
    "    doc._.has_token(\"saldo\"),\n",
    "    doc._.has_token(\"quantidade\"),\n",
    "    doc._.has_token(\"quantidade total\"),\n",
    "    doc._.has_token(\"total\"),\n",
    "    doc._.has_token(\"fazenda\"),\n",
    "    doc._.has_token(\"fazendas\"),\n",
    "    doc._.has_token(\"safra\"),\n",
    "    doc._.has_token(\"safras\"),\n",
    "    doc._.has_token(\"milho\"),\n",
    "    doc._.has_token(\"soja\"),\n",
    "    doc._.has_token(\"sorgo\"),\n",
    "    doc._.has_token(\"café\")]\n",
    "\n",
    "# Conta quantas respostas são True\n",
    "qtd_true = sum(respostas)\n",
    "\n",
    "# Cria lista com os índices das respostas True\n",
    "indices_true = [i for i, x in enumerate(respostas) if x]\n",
    "\n",
    "# Verifica se pelo menos 3 respostas são True\n",
    "if qtd_true >= 3:\n",
    "    print(\"Pelo menos 3 condições são verdadeiras\")\n",
    "    # Imprime as condições verdadeiras e seus índices\n",
    "    for i in indices_true:\n",
    "        item = condicoes[i]\n",
    "        print(f\"O item de comparação correspondente ao índice {i} é: {item}\")\n",
    "else:\n",
    "    print(\"Menos de 3 condições são verdadeiras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagem = \"Meu contrato de café da fazenda tem um saldo.\"\n",
    "\n",
    "doc = nlp(mensagem)\n",
    "\n",
    "palavras_chave = [\n",
    "        \"contrato\", \n",
    "        \"contratos\", \n",
    "        \"saldo\",\n",
    "        \"quantidade\",\n",
    "        \"quantidade total\",\n",
    "        \"total\",\n",
    "        \"fazenda\",\n",
    "        \"fazendas\",\n",
    "        \"safra\",\n",
    "        \"safras\",\n",
    "        \"milho\",\n",
    "        \"soja\",\n",
    "        \"sorgo\",\n",
    "        \"café\"\n",
    "]\n",
    "\n",
    "indices_true = [i for i, palavra in enumerate(palavras_chave) if doc._.has_token(palavra)]\n",
    "if len(indices_true) >= 3:\n",
    "    for i in indices_true:\n",
    "        print(f\"O item de comparação correspondente ao índice {i} é: {doc._.has_token(palavras_chave[i])} = '{palavras_chave[i]}' - start: {doc[palavras_chave[i]].idx} end: {doc[palavras_chave[i]].idx + len(doc[palavras_chave[i]])}\")\n",
    "else:\n",
    "    print(\"Não há pelo menos 3 condições verdadeiras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_true = [i for i, x in enumerate(condicoes) if x]\n",
    "if len(indices_true) >= 3:\n",
    "    for i in indices_true:\n",
    "        print(f\"O item de comparação correspondente ao índice {i} é: {condicoes[i]} = '{palavras_chave[i]}'\")\n",
    "else:\n",
    "    print(\"Não há pelo menos 3 condições verdadeiras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tema_cultura = [\n",
    "        doc._.has_token(\"milho\"),\n",
    "        doc._.has_token(\"soja\"),\n",
    "        doc._.has_token(\"sorgo\"),\n",
    "        doc._.has_token(\"café\")]\n",
    "\n",
    "tema_cultura\n",
    "\n",
    "\n",
    "if any(tema_cultura):\n",
    "    print(\"Estamos falando sobre alguma cultura\")\n",
    "else:\n",
    "    print(\"O assunto seja outro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tema_contrato = [\n",
    "        doc._.has_token(\"contrato\"), \n",
    "        doc._.has_token(\"contratos\")]\n",
    "\n",
    "tema_contrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagem = \"Meu contrato de café da fazenda tem um saldo.\"\n",
    "\n",
    "doc = nlp(mensagem)\n",
    "\n",
    "palavras_chave = [\n",
    "        \"contrato\", \n",
    "        \"contratos\", \n",
    "        \"saldo\",\n",
    "        \"quantidade\",\n",
    "        \"quantidade total\",\n",
    "        \"total\",\n",
    "        \"fazenda\",\n",
    "        \"fazendas\",\n",
    "        \"safra\",\n",
    "        \"safras\",\n",
    "        \"milho\",\n",
    "        \"soja\",\n",
    "        \"sorgo\",\n",
    "        \"café\"\n",
    "]\n",
    "\n",
    "condicoes = [\n",
    "        doc._.has_token(\"contrato\"), \n",
    "        doc._.has_token(\"contratos\"), \n",
    "        doc._.has_token(\"saldo\"),\n",
    "        doc._.has_token(\"quantidade\"),\n",
    "        doc._.has_token(\"quantidade total\"),\n",
    "        doc._.has_token(\"total\"),\n",
    "        doc._.has_token(\"fazenda\"),\n",
    "        doc._.has_token(\"fazendas\"),\n",
    "        doc._.has_token(\"safra\"),\n",
    "        doc._.has_token(\"safras\"),\n",
    "        doc._.has_token(\"milho\"),\n",
    "        doc._.has_token(\"soja\"),\n",
    "        doc._.has_token(\"sorgo\"),\n",
    "        doc._.has_token(\"café\")\n",
    "]\n",
    "\n",
    "indices_true = [i for i, x in enumerate(condicoes) if x]\n",
    "if len(indices_true) >= 3:\n",
    "    for i in indices_true:\n",
    "        token = doc[palavras_chave[i]]\n",
    "        print(f\"O item de comparação correspondente ao índice {i} é: {condicoes[i]} = '{palavras_chave[i]}' - start: {token.idx} end: {token.idx + len(token)}\")\n",
    "else:\n",
    "    print(\"Não há pelo menos 3 condições verdadeiras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_chave = [\n",
    "        \"contrato\", \n",
    "        \"contratos\", \n",
    "        \"saldo\",\n",
    "        \"quantidade\",\n",
    "        \"quantidade total\",\n",
    "        \"total\",\n",
    "        \"fazenda\",\n",
    "        \"fazendas\",\n",
    "        \"safra\",\n",
    "        \"safras\",\n",
    "        \"milho\",\n",
    "        \"soja\",\n",
    "        \"sorgo\",\n",
    "        \"café\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condicoes = [\n",
    "        doc._.has_token(\"contrato\"), \n",
    "        doc._.has_token(\"contratos\"), \n",
    "        doc._.has_token(\"saldo\"),\n",
    "        doc._.has_token(\"quantidade\"),\n",
    "        doc._.has_token(\"quantidade total\"),\n",
    "        doc._.has_token(\"total\"),\n",
    "        doc._.has_token(\"fazenda\"),\n",
    "        doc._.has_token(\"fazendas\"),\n",
    "        doc._.has_token(\"safra\"),\n",
    "        doc._.has_token(\"safras\"),\n",
    "        doc._.has_token(\"milho\"),\n",
    "        doc._.has_token(\"soja\"),\n",
    "        doc._.has_token(\"sorgo\"),\n",
    "        doc._.has_token(\"café\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_true = [i for i, x in enumerate(condicoes) if x]\n",
    "if len(indices_true) >= 3:\n",
    "    for i in indices_true:\n",
    "        print(f\"O item de comparação correspondente ao índice {i} é: {condicoes[i]} = '{palavras_chave[i]}'\")\n",
    "        for i, condicao in enumerate(condicoes):\n",
    "            if condicao:\n",
    "                print(f\"O item de comparação correspondente ao índice {i} é: {condicao} = '{doc[i].text}' - start: {doc[i].idx} end: {doc[i].idx + len(doc[i])}\")\n",
    "else:\n",
    "    print(\"Não há pelo menos 3 condições verdadeiras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, condicao in enumerate(condicoes):\n",
    "    if condicao:\n",
    "        print(f\"O item de comparação correspondente ao índice {i} é: {condicao} = '{doc[i].text}' - start: {doc[i].idx} end: {doc[i].idx + len(doc[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, condicao in enumerate(condicoes):\n",
    "    if condicao:\n",
    "        print(f\"O item de comparação correspondente ao índice {i} é: {condicao} = '{doc[i].text}' - start: {doc[i].idx} end: {doc[i].idx + len(doc[i])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagem = \"Meu contrato de café da fazenda tem um saldo.\"\n",
    "doc = nlp(mensagem)\n",
    "\n",
    "if doc.has_annotation(\"TOKEN\"):\n",
    "    condicoes = [\n",
    "        doc._.has_token(\"contrato\"), \n",
    "        doc._.has_token(\"contratos\"), \n",
    "        doc._.has_token(\"saldo\"),\n",
    "        doc._.has_token(\"quantidade\"),\n",
    "        doc._.has_token(\"quantidade total\"),\n",
    "        doc._.has_token(\"total\"),\n",
    "        doc._.has_token(\"fazenda\"),\n",
    "        doc._.has_token(\"fazendas\"),\n",
    "        doc._.has_token(\"safra\"),\n",
    "        doc._.has_token(\"safras\"),\n",
    "        doc._.has_token(\"milho\"),\n",
    "        doc._.has_token(\"soja\"),\n",
    "        doc._.has_token(\"sorgo\"),\n",
    "        doc._.has_token(\"café\")\n",
    "    ]\n",
    "\n",
    "    true_count = sum(condicoes)\n",
    "    if true_count >= 3:\n",
    "        print(\"Pelo menos 3 condições são verdadeiras\")\n",
    "        for i, condicao in enumerate(condicoes):\n",
    "            if condicao:\n",
    "                token = doc[i]\n",
    "                print(f\"O item de comparação correspondente ao índice {i} é: {condicao} = '{token.text}' - start: {token.idx} end: {token.idx + len(token)}\")\n",
    "else:\n",
    "    print(\"O Doc não possui anotações de token\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condicoes = [\n",
    "    doc._.has_token(\"contrato\"), \n",
    "    doc._.has_token(\"contratos\"), \n",
    "    doc._.has_token(\"saldo\"),\n",
    "    doc._.has_token(\"quantidade\"),\n",
    "    doc._.has_token(\"quantidade total\"),\n",
    "    doc._.has_token(\"total\"),\n",
    "    doc._.has_token(\"fazenda\"),\n",
    "    doc._.has_token(\"fazendas\"),\n",
    "    doc._.has_token(\"safra\"),\n",
    "    doc._.has_token(\"safras\"),\n",
    "    doc._.has_token(\"milho\"),\n",
    "    doc._.has_token(\"soja\"),\n",
    "    doc._.has_token(\"sorgo\"),\n",
    "    doc._.has_token(\"café\")]\n",
    "\n",
    "respostas = [True, False, True, False, False, False, True, False, False, False, False, False, False, True]\n",
    "\n",
    "indices_true = [i for i, x in enumerate(respostas) if x]\n",
    "\n",
    "if len(indices_true) >= 3:\n",
    "    # fazer algo se houver pelo menos três True\n",
    "    for i in indices_true:\n",
    "        item = condicoes[i]\n",
    "        itens_verdadeiros = []\n",
    "        for i in indices_true:\n",
    "            item = condicoes[i]\n",
    "            itens_verdadeiros.append(item)\n",
    "            print(f\"O item de comparação correspondente ao índice {i} é: {item}\")\n",
    "\n",
    "        print(f\"O item de comparação correspondente ao índice {i} é: {item}\")\n",
    "else:\n",
    "    print(\"Talvez o assunto seja outro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tema_cultura = [\n",
    "        doc._.has_token(\"milho\"),\n",
    "        doc._.has_token(\"soja\"),\n",
    "        doc._.has_token(\"sorgo\"),\n",
    "        doc._.has_token(\"café\")]\n",
    "\n",
    "tema_cultura\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respostas = [True, False, True, False, False, False, True, False, False, False, False, False, False, True]\n",
    "\n",
    "indices_true = [i for i, x in enumerate(respostas) if x]\n",
    "\n",
    "if len(indices_true) >= 3:\n",
    "    print(\"Acho que estamos falando de Agro\")\n",
    "    print(f'Pois voce falou sobre: {indices_true}')\n",
    "else:\n",
    "    print(\"O assunto seja outro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if any(lista):\n",
    "    # fazer algo se pelo menos um item da lista for True\n",
    "else:\n",
    "    # fazer algo se todos os itens da lista forem False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respostas = [True, False, True, False, False, False, True, False, False, False, False, False, False, True]\n",
    "\n",
    "indices_true = [i for i, x in enumerate(respostas) if x]\n",
    "\n",
    "if len(indices_true) >= 3:\n",
    "    # fazer algo se houver pelo menos três True\n",
    "    # Você pode acessar os índices dos elementos True usando a lista 'indices_true'\n",
    "else:\n",
    "    # fazer algo se houver menos de três True\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscando o tema da conversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tema_quantidade = [\n",
    "        doc._.has_token(\"saldo\"),\n",
    "        doc._.has_token(\"quantidade\"),\n",
    "        doc._.has_token(\"quantidade total\"),\n",
    "        doc._.has_token(\"total\")]\n",
    "\n",
    "tema_quantidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tema_fazenda = [\n",
    "        doc._.has_token(\"fazenda\"),\n",
    "        doc._.has_token(\"fazendas\"),\n",
    "        doc._.has_token(\"terra\")]\n",
    "        \n",
    "\n",
    "tema_fazenda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tema_safra = [\n",
    "        doc._.has_token(\"safra\"),\n",
    "        doc._.has_token(\"safras\")]\n",
    "        \n",
    "\n",
    "tema_safra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respostas = [True, False, True, False, False, False, True, False, False, False, False, False, False, True]\n",
    "\n",
    "if respostas.count(True) >= 3:\n",
    "    # fazer algo se houver pelo menos três True\n",
    "else:\n",
    "    # fazer algo se houver menos de três True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = [True, False, False]\n",
    "\n",
    "if any(lista):\n",
    "    # fazer algo se pelo menos um item da lista for True\n",
    "else:\n",
    "    # fazer algo se todos os itens da lista forem False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mensagem = \"Entregamos 6 sacas de milho pelo contrato 67C4 na safra 2021.\"\n",
    "\n",
    "\n",
    "Fase = \"2. Qualificacao\"\n",
    "\n",
    "pushName = \"Antonio\"\n",
    "\n",
    "persona = pushName\n",
    "\n",
    "persona_type = 'proprietario'\n",
    "\n",
    "\n",
    "#texto = mensagem\n",
    "\n",
    "doc = nlp(texto)\n",
    "\n",
    "# print(f'\\n{pushName} quer saber sobre: fazenda - Mensagem: {mensagem}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificando se trata-se de uma demanda Agro.\n",
    "\n",
    "fazenda = None\n",
    "saldo = None\n",
    "contrato = None\n",
    "\n",
    "mensagem = \"Entregamos 6 sacas de milho pelo contrato 67C4 na safra 2021.\"\n",
    "\n",
    "doc = nlp(mensagem)\n",
    "\n",
    "\n",
    "\n",
    "# 1. Saldo / Quantidade / quantidade total\n",
    "if doc._.has_token(\"saldo\") or doc._.has_token(\"quantidade\") or doc._.has_token(\"quantidade total\") or doc._.has_token(\"total\"):\n",
    "    print('Saldo / Quantidade / quantidade total')\n",
    "    \n",
    "    # 2. fazenda / fazendas\n",
    "    if doc._.has_token(\"fazenda\") or doc._.has_token(\"fazendas\"):\n",
    "        print('fazenda / fazendasl')\n",
    "\n",
    "        \n",
    "     \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "if doc._.has_token(\"contrato\") or doc._.has_token(\"contratos\"):\n",
    "    contrato = True\n",
    "    print(f'\\n{pushName} quer saber sobre tema: contrato(s) - Mensagem: {mensagem}\\n')\n",
    "    \n",
    "    \n",
    "\n",
    "print(fazenda, saldo, contrato)\n",
    "    \n",
    "if doc._.has_token(\"safra\") or doc._.has_token(\"safras\"):\n",
    "    print(f'\\n{pushName} quer saber sobre: safra - Mensagem: {mensagem}\\n')\n",
    "    \n",
    "    \n",
    "if doc._.has_token(\"milho\") or doc._.has_token(\"soja\") or doc._.has_token(\"sorgo\"):\n",
    "    print(f'\\n{pushName} quer saber sobre: culturas - Mensagem: {mensagem}\\n')\n",
    "    \n",
    "\n",
    "if doc._.has_token(\"saldo\") or doc._.has_token(\"quantidade\") or doc._.has_token(\"quantidade total\") or doc._.has_token(\"total\"):\n",
    "    print(f'\\n{pushName} Destaca: quantidade, quantidade total ou total | Mensagem: {mensagem}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if condicao_1:\n",
    "    resultado_1 = True\n",
    "else:\n",
    "    resultado_1 = False\n",
    "\n",
    "if condicao_2:\n",
    "    resultado_2 = True\n",
    "else:\n",
    "    resultado_2 = False\n",
    "\n",
    "# E assim por diante para outras condições que você quiser testar...\n",
    "\n",
    "# Depois, você pode tomar a ação apropriada com base nos resultados:\n",
    "if resultado_1 and resultado_2:\n",
    "    # Fazer algo se ambas as condições forem verdadeiras\n",
    "elif resultado_1 or resultado_2:\n",
    "    # Fazer algo se pelo menos uma das condições for verdadeira\n",
    "else:\n",
    "    # Fazer algo se nenhuma das condições for verdadeira\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Material de Pesquisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index:   \", [token.i for token in doc])\n",
    "print(\"Text:    \", [token.text for token in doc])\n",
    "\n",
    "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
    "print(\"is_punct:\", [token.is_punct for token in doc])\n",
    "print(\"like_num:\", [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"sacas\":\n",
    "            print(\"Sacas encontradas:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    pos_explained = spacy.explain(token_pos)\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print(f\"{token_text:<12},{token_pos:<10},{pos_explained:<25},{token_dep:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# Apple ORG\n",
    "# first ORDINAL\n",
    "# U.S. GPE\n",
    "# $1 trillion MONEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pt_core_news_lg')\n",
    "\n",
    "text = \"É oficial: a Apple é o primeiro empresa pública dos EUA a atingir um valor de mercado de US$ 1 trilhão equivale a R$ 1,00\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "# Apple ORG\n",
    "# EUA LOC     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"MISC\") #'Miscellaneous entities, e.g. events, nationalities, products or works of art'\n",
    "spacy.explain(\"NNP\")\n",
    "spacy.explain(\"dobj\")\n",
    "spacy.explain(\"ORG\") #'Companies, agencies, institutions, etc.'\n",
    "spacy.explain(\"LOC\") # 'Non-GPE locations, mountain ranges, bodies of water'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cultura = ['milho', 'soja', 'sorgo', 'arroz', 'café', 'feijão']\n",
    "\n",
    "contrato = ['contrato', 'contratos']\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns_cultura = list(nlp.pipe(cultura))\n",
    "patterns_contrato = list(nlp.pipe(contrato))\n",
    "\n",
    "matcher.add(\"CULTURA\", patterns_cultura)\n",
    "matcher.add(\"CONTRATO\", patterns_contrato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for match_id, start, end in matcher(doc):\n",
    "#     span = doc[start:end]\n",
    "#     print(\"Matched span:\", span.text)\n",
    "#     # Get the span's root token and root head token\n",
    "#     print(\"Root token:\", span.root.text)\n",
    "#     print(\"Root head token:\", span.root.head.text)\n",
    "#     # Get the previous token and its POS tag\n",
    "#     print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)\n",
    "        \n",
    "\n",
    "#print(matcher.__getattribute__)\n",
    "print(doc)\n",
    "print()    \n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(start, end, span.text)\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)\n",
    "\n",
    "    #print(f'span_with_label {span_with_label}, label {span_with_label.label_} | Matched span:{span.text}  Root token: {span.root.text} span.root.head.text {span.root.head.text} Previous token: {doc[start - 1].text, doc[start - 1].pos_  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"CONTRATO\", [[{\"LOWER\": \"contrato\"}, {\"LOWER\": \"soja\"}]])\n",
    "#doc = nlp(\"Eu tenho um golden Retriever muito bonito, ele se chama Bud!\")\n",
    "doc = nlp('Eu tenho um contrato soja')\n",
    "\n",
    "\n",
    "token_texts = [token.text for token in doc]\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    #span = doc[start:end] # HAHAMMMM.....\n",
    "    span_with_label = Span(doc, start, end, label=\"CONTRATO\")\n",
    "    print(\"Matched span:\", span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"compact\": False, \"color\": \"rgb(0, 255, 0)\"}\n",
    "\n",
    "displacy.render(doc, style=\"ent\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detalhes_entidade = [(entidade, entidade.label_)for entidade in doc.ents]\n",
    "\n",
    "detalhes_entidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DOG\", [[{\"LOWER\": \"golden\"}, {\"LOWER\": \"retriever\"}]])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detalhes_entidade = [(entidade, entidade.label_)for entidade in doc.ents]\n",
    "\n",
    "detalhes_entidade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Qual e a quantidade total de soja do contrato 448S a ser entregue?')\n",
    "lexeme = nlp.vocab[\"soja\"]\n",
    "\n",
    "# Imprimir os atributos léxicos\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(\"But Google is starting from behind.\")\n",
    "ents_parse = displacy.parse_ents(doc)\n",
    "html = displacy.render(ents_parse, style=\"ent\", manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detalhes_entidade = [(entidade, entidade.label_)for entidade in doc.ents]\n",
    "\n",
    "detalhes_entidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"compact\": False, \"color\": \"rgb(0, 255, 0)\"}\n",
    "\n",
    "displacy.render(doc, style=\"ent\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos ver os nomes próprios do texto. Essa função é muito interessante para pesquisar pessoas e lugares citados no corpo do texto.\n",
    "entidades_nomeadas = list(doc.ents)\n",
    "\n",
    "entidades_nomeadas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O caso do Golden Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrato = ['contrato', 'contratos']\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns_contrato = list(nlp.pipe(contrato))\n",
    "matcher.add(\"CONTRATO\", patterns_contrato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DOG\", [[{\"LOWER\": \"golden\"}, {\"LOWER\": \"retriever\"}]])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detalhes_entidade = [(entidade, entidade.label_)for entidade in doc.ents]\n",
    "\n",
    "detalhes_entidade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    pos_explained = spacy.explain(token_pos)\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print(f\"{token_text:<12},{token_pos:<10},{pos_explained:<25},{token_dep:<10}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc = nlp(\"Meu contrato 567C\")\n",
    "lexeme = nlp.vocab[\"fazenda\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "\n",
    "# Imprimir os atributos léxicos\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detalhes_entidade = [(entidade, entidade.label_)for entidade in doc.ents]\n",
    "\n",
    "detalhes_entidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muito importante, extrair os dados de uma lista\n",
    "\n",
    "import json\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "\n",
    "\n",
    "#doc = nlp(\"Eu tenho um contrato soja e outro de milho com a fazenda Porto Seguro, ele vai vencer em julho deste ano!\")\n",
    "\n",
    "cultura = ['milho', 'soja', 'sorgo', 'arroz', 'café', 'culturas']\n",
    "\n",
    "contrato = ['contrato', 'contratos', 'meu contrato', 'contrato da fazenda', 'contrato do cliente']\n",
    "\n",
    "cliente = ['meu cliente', 'cliente', 'clientes', 'os clientes', 'o cliente' ]\n",
    "\n",
    "geralagro = ['safra', 'safras', 'fazenda', 'fazendas']\n",
    "\n",
    "saldo = ['saldo', 'meu saldo', 'saldo entregue', 'saldo a entregar']\n",
    "\n",
    "\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns_culturas = list(nlp.pipe(cultura))\n",
    "patterns_contratos = list(nlp.pipe(contrato))\n",
    "patterns_clientes = list(nlp.pipe(cliente))\n",
    "\n",
    "\n",
    "matcher.add(\"CULTURA\", patterns_culturas)\n",
    "matcher.add(\"CONTRATO\", patterns_contratos)\n",
    "matcher.add(\"CLIENTE\", patterns_clientes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_md pipeline\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "# Get the vector for the token \"bananas\"\n",
    "bananas_vector = doc[1].vector\n",
    "print(bananas_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.strings.add(\"contrato\")\n",
    "contrato_hash = nlp.vocab.strings[\"contrato\"]\n",
    "contrato_string = nlp.vocab.strings[contrato_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrato_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension('extensions', default=None, force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc._._extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "Doc.set_extension('hashes', default=None, force=True)\n",
    "Span.set_extension('hashes', default=None, force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_components = ['tok2vec', 'morphologizer', 'parser', 'lemmatizer', 'attribute_ruler', 'ner']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline_output(nlp, text):\n",
    "    pipeline_components = nlp.pipe_names\n",
    "    r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "    if r.exists(text):\n",
    "        cached_output_dict = r.hgetall(text)\n",
    "        deserialized_output_dict = {}\n",
    "        for comp in pipeline_components:\n",
    "            if r.hexists(text, comp):\n",
    "                output_bytes = cached_output_dict[comp].encode('latin1')\n",
    "                deserialized_output_dict[comp] = nlp.vocab.from_bytes(output_bytes)\n",
    "        return deserialized_output_dict\n",
    "    else:\n",
    "        doc = nlp(text)\n",
    "        output_dict = {}\n",
    "        for comp in pipeline_components:\n",
    "            if doc.has_extension(comp) and hasattr(doc._.extensions[comp], 'hashes'):\n",
    "                output_dict[comp] = doc.get_extension(comp).hashes[0].hexdigest()\n",
    "                print(output_dict)\n",
    "        output_str = json.dumps(output_dict)\n",
    "        r.hset(text, 'output', output_str)\n",
    "        return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Quero saber o saldo de milho do meu contrato que a fazenda Santa Cruz en Novo Hamburgo tem para me entregar soja na safra de 2024, tambem poderia me dizer a quantidade de  atual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = get_pipeline_output(nlp, text)\n",
    "print(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Exemplo de sentença.\")\n",
    "for comp in nlp.pipe_names:\n",
    "    print(f\"Pipeline component: {comp}, Extensions: {doc._.extensions.keys()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = nlp.config\n",
    "bytes_data = nlp.to_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline_output(nlp, text, r):\n",
    "    # Check if the output is already cached in Redis\n",
    "    cached_output_dict = r.hgetall(text)\n",
    "    if cached_output_dict:\n",
    "        # Deserialize the cached output dict\n",
    "        deserialized_output_dict = {}\n",
    "        for comp in pipeline_components:\n",
    "            if comp in cached_output_dict:\n",
    "                output_bytes = cached_output_dict[comp].encode('latin1')\n",
    "                if output_bytes:\n",
    "                    deserialized_output_dict[comp] = nlp.vocab.from_bytes(output_bytes)\n",
    "                else:\n",
    "                    deserialized_output_dict[comp] = None\n",
    "            else:\n",
    "                deserialized_output_dict[comp] = None\n",
    "        return deserialized_output_dict\n",
    "    else:\n",
    "        doc = nlp(text)\n",
    "        output_dict = {}\n",
    "        for comp in pipeline_components:\n",
    "            if doc._.has_extension(comp):\n",
    "                output_dict[comp] = doc.get_extension(comp).hashes[0].hexdigest()\n",
    "            else:\n",
    "                output_dict[comp] = None\n",
    "        r.hmset(text, output_dict)\n",
    "        return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline_output(nlp, text):\n",
    "    pipeline_components = nlp.pipe_names\n",
    "    cached_output_dict = {}\n",
    "    deserialized_output_dict = {}\n",
    "\n",
    "    # Check if key exists in Redis cache\n",
    "    if r.exists(text):\n",
    "        cached_output_dict = r.hgetall(text)\n",
    "    else:\n",
    "        doc = nlp(text)\n",
    "        output_dict = {comp: doc.get_extension(comp).hashes[0].hexdigest() for comp in pipeline_components}\n",
    "        r.hmset(text, output_dict)\n",
    "        cached_output_dict = output_dict\n",
    "\n",
    "    for comp in pipeline_components:\n",
    "        if comp in cached_output_dict:\n",
    "            output_bytes = cached_output_dict[comp].encode('latin1')\n",
    "            if comp == 'tok2vec' and 'vectors' not in nlp.vocab:\n",
    "                deserialized_output_dict[comp] = None\n",
    "            else:\n",
    "                deserialized_output_dict[comp] = nlp.vocab.from_bytes(output_bytes)\n",
    "        else:\n",
    "            deserialized_output_dict[comp] = None\n",
    "\n",
    "    return deserialized_output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load('pt_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"Tudo ótimo, gostaria de saber o saldo do meu contrato de soja\"\n",
    "doc = nlp(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    pos_explained = spacy.explain(token_pos)\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print(f\"| {token_text:<12} | {token_pos:<10} {pos_explained:<25} |  {token_dep:<10} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"CONTRATO\", [[{\"LOWER\": \"contrato\"}]])\n",
    "#doc = nlp(\"Eu tenho um golden Retriever muito bonito, ele se chama Bud!\")\n",
    "#doc = nlp(\"Eu tenho um contrato soja com a fazenda Porto Seguro, ele vai vencer em julho deste ano!\")\n",
    "\n",
    "\n",
    "token_texts = [token.text for token in doc]\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end] # HAHAMMMM.....\n",
    "    print(\"Matched span:\", span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp.pipe_names)\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mudando o pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method extensions <mark><b>top demais\n",
    "# - Assign a function that becomes available as an object method\n",
    "# - Lets you pass arguments to the extension function\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define method with arguments\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Set extension on the Doc with method\n",
    "Doc.set_extension(\"has_token\", method=has_token, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"compact\": True, \"color\": \"blue\"}\n",
    "displacy.render(doc, style=\"ent\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc = nlp(\"Eu tenho um contrato soja e outro de milho com a fazenda Porto Seguro, ele vai vencer em julho deste ano!\")\n",
    "\n",
    "cultura = ['milho', 'soja', 'sorgo', 'arroz', 'café', 'culturas']\n",
    "\n",
    "contrato = ['contrato', 'contratos', 'meu contrato', 'contrato da fazenda', 'contrato do cliente']\n",
    "\n",
    "cliente = ['meu cliente', 'cliente', 'clientes', 'os clientes', 'o cliente' ]\n",
    "\n",
    "geralagro = ['safra', 'safras', 'fazenda', 'fazendas']\n",
    "\n",
    "saldo = ['saldo', 'meu saldo', 'saldo entregue', 'saldo a entregar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mudanca do pipeline - add CULTURA\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "cultura_patterns = list(nlp.pipe(cultura))\n",
    "contrato_patterns = list(nlp.pipe(contrato))\n",
    "cliente_patterns = list(nlp.pipe(cliente))\n",
    "saldo_patterns = list(nlp.pipe(saldo))\n",
    "geralagro_patterns = list(nlp.pipe(geralagro))\n",
    "\n",
    "#print(\"cultura_patterns:\", cultura_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"CULTURA\", cultura_patterns)\n",
    "matcher.add(\"CONTRATO\", contrato_patterns)\n",
    "matcher.add(\"CLIENTE\", cliente_patterns)\n",
    "matcher.add(\"SALDO\", saldo_patterns)\n",
    "matcher.add(\"GERALAGRO\", geralagro_patterns)\n",
    "\n",
    "\n",
    "@Language.component(\"cultura_component\")\n",
    "def cultura_component_function(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"CULTURA\"\n",
    "    spans = [Span(doc, start, end, label=\"CULTURA\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# @Language.component(\"contrato_component\")\n",
    "# def contrato_component_function(doc):\n",
    "#     # Apply the matcher to the doc\n",
    "#     matches = matcher(doc)\n",
    "#     # Create a Span for each match and assign the label \"CULTURA\"\n",
    "#     spans = [Span(doc, start, end, label=\"CONTRATO\") for match_id, start, end in matches]\n",
    "#     # Overwrite the doc.ents with the matched spans\n",
    "#     doc.ents = spans\n",
    "#     return doc\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(\"cultura_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add the component to the pipeline after the \"ner\" component\n",
    "# nlp.add_pipe(\"contrato_component\", after=\"ner\", validate=False)\n",
    "# print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"Tudo ótimo, gostaria de saber o saldo do meu contrato de soja\"\n",
    "doc = nlp(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos ver os nomes próprios do texto. Essa função é muito interessante para pesquisar pessoas e lugares citados no corpo do texto.\n",
    "entidades_nomeadas = list(doc.ents)\n",
    "\n",
    "entidades_nomeadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"compact\": True, \"color\": \"blue\"}\n",
    "displacy.render(doc, style=\"ent\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "     print(f\"Match encontrado: {doc[start:end].text}\")\n",
    "    \n",
    "print(f\"\\nTotal de matches encontrados: {len(matches)}\\n\") \n",
    "\n",
    "\n",
    "options = {\"compact\": True, \"color\": \"blue\"}\n",
    "\n",
    "displacy.render(doc, style=\"ent\", options=options)\n",
    "\n",
    "\n",
    "\n",
    "# doc.spans[\"sc\"] = [\n",
    "#     Span(doc, 22, 25, \"SAFRA\")\n",
    "# ]\n",
    "\n",
    "\n",
    "options = {\"compact\": True, \"color\": \"blue\"}\n",
    "\n",
    "displacy.render(doc, style=\"span\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"compact\": True, \"color\": \"blue\"}\n",
    "\n",
    "displacy.render(doc, style=\"span\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos ver os nomes próprios do texto. Essa função é muito interessante para pesquisar pessoas e lugares citados no corpo do texto.\n",
    "entidades_nomeadas = list(doc.ents)\n",
    "\n",
    "entidades_nomeadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "     print(f\"Match encontrado: {doc[start:end].text}\")\n",
    "    \n",
    "print(f\"\\nTotal de matches encontrados: {len(matches)}\\n\") \n",
    "\n",
    "\n",
    "options = {\"compact\": True, \"color\": \"blue\"}\n",
    "\n",
    "displacy.render(doc, style=\"ent\", options=options)\n",
    "\n",
    "\n",
    "\n",
    "doc.spans[\"sc\"] = [\n",
    "    Span(doc, 22, 25, \"SAFRA\")\n",
    "]\n",
    "\n",
    "\n",
    "options = {\"compact\": True, \"color\": \"blue\"}\n",
    "\n",
    "displacy.render(doc, style=\"span\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe o comparador - Matcher\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "doc = nlp(\"Vazou a data de lançamento do novo iPhone X após a Apple revelar a existência de compras antecipadas.\")\n",
    "\n",
    "# Inicialize o comparador com o vocabulário compartilhado \n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Crie uma expressão que faça a correspondência dos tokens: \"iPhone\" and \"X\"\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "\n",
    "# Adicione uma expressão ao comparador\n",
    "matcher.add(\"IPHONE_X_PATTERN\", [pattern])\n",
    "\n",
    "# Use o comparador no doc\n",
    "matches = matcher(doc)\n",
    "print(\"Correspondências:\", [doc[start:end].text for match_id, start, end in matches])\n",
    "\n",
    "\n",
    "\n",
    "#Escreva uma expressão que corresponda às menções da versão IOS completa: “iOS 7”, “iOS 11” e “iOS 10”.\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Após fazer a atualização do iOS você não irá perceber uma mudança radical \"\n",
    "    \"na sua interface: nada parecido com a reviravolta estética que foi feita com o iOS 7. A \"\n",
    "    \"maioria da roupagem do iOS 11 permanece a mesma que o iOS 10. Mas você irá descobrir \"\n",
    "    \"alguns ajustes se você procurar nos detalhes.\"\n",
    ")\n",
    "\n",
    "# Escreva uma expressão que corresponda às versões completas do IOS (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Adicione a expressão ao comparador matcher e aplique o matcher ao doc\n",
    "matcher.add(\"IOS_VERSION_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total de correspondências encontradas:\", len(matches))\n",
    "\n",
    "# Faça a iteração sobre as correspondencias e imprima a partição do texto\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Correspondência encontrada:\", doc[start:end].text)\n",
    "    \n",
    "    \n",
    "\n",
    "# Chamar um comparador e passar um texto\n",
    "\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    #selecionar a particao que houve a correspondencia\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n",
    "    \n",
    "    \n",
    "\n",
    "# start: índice inicial da partição em que houve correspondência\n",
    "# match_id: código hash da expressão\n",
    "# end: índice final da partição em que houve correspondência\n",
    "\n",
    "# Quando você usa o comparador em um documento (doc), ele retorna uma lista de tuplas.\n",
    "\n",
    "# Cada tupla consiste em três valores: o ID a expressão, o índice inicial e o índice final da partição em que houve correspondência.\n",
    "\n",
    "# Desta forma é possível iterar nas correspondências e criar um objeto partição Span : a parte do texto correspondente (do índice inicial até o índice final).\n",
    "\n",
    "                                            \n",
    "                                            \n",
    "                                            \n",
    "# Expressões com atributos léxicos  \n",
    "\n",
    "pattern = [\n",
    "    {'IS_DIGIT': True},\n",
    "    {'LOWER': \"fifa\"},\n",
    "    {'LOWER':\"world\"},\n",
    "    {'LOWER':\"cup\"},\n",
    "    {'IS_PUNCT': True}\n",
    "]\n",
    "\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "#Expressões com outros atributos dos tokens\n",
    "\n",
    "pattern = [\n",
    "    {\"LEMMA\": \"love\"}, \n",
    "    {\"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"} \n",
    "]\n",
    "\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "\n",
    "# Neste exemplo, estamos procurando por dois tokens:\n",
    "\n",
    "# Um verbo com o lema \"love\", seguido de um substantivo. {\"POS\": \"NOUN\"}\n",
    "\n",
    "# Esta expressão terá correspondência com \"loved dogs\" e \"love cats\".\n",
    "\n",
    "\n",
    "\n",
    "# Utilizando operadores e quantificadores (1)\n",
    "pattern = [\n",
    "    {\"LEMMA\": \"buy\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"}, ## opcional: corresponde a 0 ou 1 vez\n",
    "    {\"POS\": \"NOUN\"}\n",
    "    \n",
    "]\n",
    "\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "\n",
    "# bought a smartphone\n",
    "# buying apps\n",
    "\n",
    "# Operadores e quantificadores permitem definir quantas \n",
    "# vezes deverá haver correspondência com a expressão. \n",
    "# Eles podem ser usados com a chave \"OP\".\n",
    "\n",
    "# Neste exemplo, o operador \"?\" faz com que a ocorrência seja \n",
    "# opcional, então a expressão corresponderá a um token com o \n",
    "# lema \"buy\", um artigo (opcional) e um substantivo.\n",
    "\n",
    "{\"OP\": \"!\"} #Negacao: corresponde 1 vez\n",
    "{\"OP\": \"?\"} #Opcional: corresponde a 0 ou 1 vez\n",
    "{\"OP\": \"+\"} #Corresponde 1 ou + vezes\n",
    "{\"OP\": \"*\"} #Corresponde 1 ou + vezes\n",
    "\n",
    "# \"OP\" pode ter um dos quatro valores abaixo:\n",
    "\n",
    "# \"!\" nega o valor do token, então corresponde a nenhuma ocorrência.\n",
    "\n",
    "# \"?\" faz o token opcional, corresponde a 0 ou 1 ocorrência.\n",
    "\n",
    "# \"+\" corresponde ao token uma ou mais ocorrências do token.\n",
    "\n",
    "# E \"*\" corresponde a zero ou mais ocorrências do token.\n",
    "\n",
    "# Os operadores dão poder às suas expressões, mas por outro lado são mais complexos, use-os com sabedoria.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Eu baixei o Fortnite em meu computador e não consigo abrir o jogo de jeito algum. Me ajudem? \"\n",
    "    \"Mas quando eu estava baixando o Minecraft, eu tinha a versão do Windows que é \"\n",
    "    \"uma pasta .zip e eu usei o aplicativo padrão para descompactá-lo. Será que \"\n",
    "    \"eu preciso baixar o Winzip também? \"\n",
    ")\n",
    "\n",
    "# Escreva uma expressão que corresponda às variações de \"baixar\" seguido de um\n",
    "# artigo e um substatantivo próprio\n",
    "pattern = [{\"LEMMA\": \"baixar\"},{\"POS\": \"DET\"}, {\"POS\": \"PROPN\"}]\n",
    "\n",
    "# Adicione a expressão ao comparador matcher e aplique o matcher ao doc\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total de correspondências encontradas:\", len(matches))\n",
    "\n",
    "# Faça a iteração nas correspondências e imprima a partição do texto\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Correspondência encontrada:\", doc[start:end].text)\n",
    "    \n",
    "    \n",
    "# Total de correspondências encontradas: 3\n",
    "# Correspondência encontrada: baixei o Fortnite\n",
    "# Correspondência encontrada: baixando o Minecraft\n",
    "# Correspondência encontrada: baixar o Winzip    \n",
    "    \n",
    "\n",
    "# Escreva uma expressão que corresponda a adjetivos (\"ADJ\") seguidos por um ou dois substantivos. \n",
    "# (um substantivo obrigatório e um seguinte opcional).\n",
    "                                             \n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Os recursos do aplicativo incluem um design bonito e moderno, busca inteligente, \"\n",
    "    \" rótulos automáticos e resposta de voz opcional.\"\n",
    ")\n",
    "\n",
    "\n",
    "                                             \n",
    "# Escreva uma expressão que corresponda a um substantivo seguido de um ou dois adjetivos\n",
    "pattern = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADJ\", \"OP\": \"?\"}, {\"TEXT\": \"e\", \"OP\": \"?\"}, {\"POS\": \"ADJ\"}] \n",
    "\n",
    "# Adicione uma expressão ao comparador matcher e aplique o matcher ao doc\n",
    "matcher.add(\"ADJ_NOUN_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total de correspondências encontradas:\", len(matches)) \n",
    "\n",
    "# Faça a iteração sobre as correspondencias e imprima a partição do texto\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Correspondências encontradas:\", doc[start:end].text)\n",
    "\n",
    "# Total de correspondências encontradas: 5\n",
    "# Correspondências encontradas: design bonito\n",
    "# Correspondências encontradas: design bonito e moderno\n",
    "# Correspondências encontradas: busca inteligente\n",
    "# Correspondências encontradas: rótulos automáticos\n",
    "# Correspondências encontradas: voz opcional                                           \n",
    "                                             "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial do Gabriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_lg')\n",
    "\n",
    "texto = (\"Quatro romeiros compraram R$ 10 reais em camisetas, na rodovia Presidente Dutra enquanto seguiam para Aparecida (SP)\" \n",
    "        \" entre a noite de ontem e madrugada de hoje. O dia 12, de Nossa Senhora Aparecida, padroeira do Brasil, é celebrado na próxima terça-feira\")\n",
    "\n",
    "doc = nlp(texto)\n",
    "texto "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos proceder com a tokenização do texto, ou seja, transformar todos os componentes em segmentos, chamados tokens. Entre eles estão as palavras, pontos e números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token.orth_ for token in doc]\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma função muito importante é fazer a seleção apenas de palavras dentro do nosso texto. Aqui não aparecem os pontos nem números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_tokens = [token.orth_ for token in doc if token.is_alpha]\n",
    "\n",
    "print(alpha_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E se quiséssemos mostrar apenas os números que aparecem na nossa amostra?\n",
    "# Simples, basta aplicar o seguinte algoritmo:\n",
    "\n",
    "alpha_tokens = [token.orth_ for token in doc if token.is_digit]\n",
    "\n",
    "print(alpha_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digamos que agora queremos analisar quantos sinais de pontuação existem em nosso texto, além de ver quais são:\n",
    "alpha_tokens = [token.orth_ for token in doc if token.is_punct]\n",
    "\n",
    "print(alpha_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digamos que agora queremos analisar quantos sinais de pontuação existem em nosso texto, além de ver quais são:\n",
    "alpha_tokens = [token.orth_ for token in doc if token.is_currency]\n",
    "\n",
    "print(alpha_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    A lematização é a determinação do significado principal da palavra, \n",
    "    através da determinação da forma de dicionário da palavra (chamada de lema). \n",
    "    \n",
    "    A lematização também leva em consideração o contexto da palavra para resolver outros problemas, \n",
    "    como desambiguação, o que significa que ela pode discriminar entre palavras idênticas \n",
    "    que têm significados diferentes, dependendo do contexto.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = [token.lemma_ for token in doc if token.pos_ == 'VERB']\n",
    "\n",
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramat = [(token.orth_,token.pos_) for token in doc]\n",
    "\n",
    "gramat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morfologia = [(token.orth_, token.morph) for token in doc]\n",
    "\n",
    "morfologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos ver os nomes próprios do texto. Essa função é muito interessante para pesquisar pessoas e lugares citados no corpo do texto.\n",
    "entidades_nomeadas = list(doc.ents)\n",
    "\n",
    "entidades_nomeadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detalhes_entidade = [(entidade, entidade.label_)for entidade in doc.ents]\n",
    "\n",
    "detalhes_entidade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "#text = \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\"\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "#doc = nlp(text)\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_lg')\n",
    "\n",
    "texto = (\"Eles comprarão 10 kilos de arroz dia 10/05/2023 pois precisariam repor seus estoques que ficaram quebrados.\")\n",
    "\n",
    "doc = nlp(texto)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    pos_explained = spacy.explain(token_pos)\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print(f\"| {token_text:<12} | {token_pos:<10} {pos_explained:<15} |  {token_dep:<10} | {spacy.explain(token_dep)}\")\n",
    "\n",
    "doc = nlp(texto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morfologia = [(token.orth_, token.morph) for token in doc]\n",
    "\n",
    "morfologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    pos_explained = spacy.explain(token_pos)\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print(f\"| {token_text:<12} | {token_pos:<10} {pos_explained:<15} |  {token_dep:<10} | {spacy.explain(token_dep)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando novas entidades"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/usage/rule-based-matching#pattern-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.pt import Portuguese\n",
    "nlp = Portuguese()\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"AgroBI\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"dois\"}, {\"LOWER\": \"irmãos\"}]}]\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "doc = nlp(\"A empresa AgroBI esta atendendo agora em Dois Irmãos.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"MyCorp Inc.\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"MyCorp Inc. is a company in the U.S.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'ORG', 'apple'), ('San Francisco', 'GPE', 'san-francisco')]\n",
      "[('Apple', 'ORG', 'apple'), ('San Fran', 'GPE', 'san-francisco')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}], \"id\": \"san-francisco\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"fran\"}], \"id\": \"san-francisco\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc1 = nlp(\"Apple is opening its first big office in San Francisco.\")\n",
    "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc1.ents])\n",
    "\n",
    "doc2 = nlp(\"Apple is opening its first big office in San Fran.\")\n",
    "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc2.ents])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": {\"REGEX\": \"^[Uu](\\\\.?|nited)$\"}},\n",
    "           {\"TEXT\": {\"REGEX\": \"^[Ss](\\\\.?|tates)$\"}},\n",
    "           {\"LOWER\": \"president\"}]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The REGEX operator allows defining rules for any attribute string value, including custom attributes. It always needs to be applied to an attribute like TEXT, LOWER or TAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match different spellings of token texts\n",
    "pattern = [{\"TEXT\": {\"REGEX\": \"deff?in[ia]tely\"}}]\n",
    "\n",
    "# Match tokens with fine-grained POS tags starting with 'V'\n",
    "pattern = [{\"TAG\": {\"REGEX\": \"^V\"}}]\n",
    "\n",
    "# Match custom attribute values with regular expressions\n",
    "pattern = [{\"_\": {\"country\": {\"REGEX\": \"^[Uu](nited|\\\\.?) ?[Ss](tates|\\\\.?)$\"}}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The United States of America (USA) are commonly known as the United States (U.S. or US) or America.\")\n",
    "\n",
    "expression = r\"[Uu](nited|\\\\.?) ?[Ss](tates|\\\\.?)\"\n",
    "for match in re.finditer(expression, doc.text):\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start, end)\n",
    "    # This is a Span object or None if match doesn't map to valid token sequence\n",
    "    if span is not None:\n",
    "        print(\"Found match:\", span.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, you might want to expand the match to the closest token boundaries, so you can create a Span for \"USA\", even though only the substring \"US\" is matched. You can calculate this using the character offsets of the tokens in the document, available as Token.idx. This lets you create a list of valid token start and end boundaries and leaves you with a rather basic algorithmic problem: Given a number, find the next lowest (start token) or the next highest (end token) number that’s part of a given list of numbers. This will be the closest valid token boundary.\n",
    "\n",
    "There are many ways to do this and the most straightforward one is to create a dict keyed by characters in the Doc, mapped to the token they’re part of. It’s easy to write and less error-prone, and gives you a constant lookup time: you only ever need to create the dict once per Doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_tokens = {}\n",
    "for token in doc:\n",
    "    for i in range(token.idx, token.idx + len(token.text)):\n",
    "        chars_to_tokens[i] = token.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span = doc.char_span(start, end)\n",
    "if span is not None:\n",
    "    print(\"Found match:\", span.text)\n",
    "else:\n",
    "    start_token = chars_to_tokens.get(start)\n",
    "    end_token = chars_to_tokens.get(end)\n",
    "    if start_token is not None and end_token is not None:\n",
    "        span = doc[start_token:end_token + 1]\n",
    "        print(\"Found closest match:\", span.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy matching V3.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy matching allows you to match tokens with alternate spellings, typos, etc. without specifying every possible variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matches \"favourite\", \"favorites\", \"gavorite\", \"theatre\", \"theatr\", ...\n",
    "pattern = [{\"TEXT\": {\"FUZZY\": \"favorite\"}},\n",
    "           {\"TEXT\": {\"FUZZY\": \"theater\"}}]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FUZZY attribute allows fuzzy matches for any attribute string value, including custom attributes. Just like REGEX, it always needs to be applied to an attribute like TEXT or LOWER. By default FUZZY allows a Levenshtein edit distance of at least 2 and up to 30% of the pattern string length. Using the more specific attributes FUZZY1..FUZZY9 you can specify the maximum allowed edit distance directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match lowercase with fuzzy matching (allows 3 edits)\n",
    "pattern = [{\"LOWER\": {\"FUZZY\": \"definitely\"}}]\n",
    "\n",
    "# Match custom attribute values with fuzzy matching (allows 3 edits)\n",
    "pattern = [{\"_\": {\"country\": {\"FUZZY\": \"Kyrgyzstan\"}}}]\n",
    "\n",
    "# Match with exact Levenshtein edit distance limits (allows 4 edits)\n",
    "pattern = [{\"_\": {\"country\": {\"FUZZY4\": \"Kyrgyzstan\"}}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"awesome\", \"cool\", \"wonderful\"]}}}]\n",
    "\n",
    "pattern = [{\"TEXT\": {\"REGEX\": {\"NOT_IN\": [\"^awe(some)?$\", \"^wonder(ful)?\"]}}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab, validate=True)\n",
    "# Add match ID \"HelloWorld\" with unsupported attribute CASEINSENSITIVE\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"CASEINSENSITIVE\": \"world\"}]\n",
    "matcher.add(\"HelloWorld\", [pattern])\n",
    "# 🚨 Raises an error:\n",
    "# MatchPatternError: Invalid token patterns for matcher rule 'HelloWorld'\n",
    "# Pattern 0:\n",
    "# - [pattern -> 2 -> CASEINSENSITIVE] extra fields not permitted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding on_match rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab, validate=True)\n",
    "# Add match ID \"HelloWorld\" with unsupported attribute CASEINSENSITIVE\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"CASEINSENSITIVE\": \"world\"}]\n",
    "matcher.add(\"HelloWorld\", [pattern])\n",
    "# 🚨 Raises an error:\n",
    "# MatchPatternError: Invalid token patterns for matcher rule 'HelloWorld'\n",
    "# Pattern 0:\n",
    "# - [pattern -> 2 -> CASEINSENSITIVE] extra fields not permitted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
    "# Only run nlp.make_doc to speed things up\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", patterns)\n",
    "\n",
    "doc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "          \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}], \"id\": \"san-francisco\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"fran\"}], \"id\": \"san-francisco\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc1 = nlp(\"Apple is opening its first big office in San Francisco.\")\n",
    "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc1.ents])\n",
    "\n",
    "doc2 = nlp(\"Apple is opening its first big office in San Fran.\")\n",
    "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc2.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding on_match rules - Agro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.pt import Portuguese\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = Portuguese()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def add_fazenda_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"FAZENDA\")\n",
    "    doc.ents += (entity,)\n",
    "    print(entity.text)\n",
    "    \n",
    "def add_cultura_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"CULTURA\")\n",
    "    doc.ents += (entity,)\n",
    "    print(entity.text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"ORTH\": \"milho\"}]\n",
    "matcher.add(\"Culturas\", [pattern], on_match=add_cultura_ent)\n",
    "\n",
    "pattern = [{\"ORTH\": \"soja\"}]\n",
    "matcher.add(\"Culturas\", [pattern], on_match=add_cultura_ent)\n",
    "\n",
    "pattern = [{\"ORTH\": \"Fazenda\"}, {\"ORTH\": \"AgroBI\"}]\n",
    "matcher.add(\"Fazendas\", [pattern], on_match=add_fazenda_ent)\n",
    "\n",
    "pattern = [{\"ORTH\": \"Fazenda\"}, {\"ORTH\": \"Maraisa\"}]\n",
    "matcher.add(\"Fazendas\", [pattern], on_match=add_fazenda_ent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Estamos falando sobre soja\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Estamos falando sobre milho\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Estamos falando a Fazenda Maraisa que produz soja e milho. Falamos tambem sobre a Fazenda AgroBI - que produz somente soja\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(ent.text, ent.label_) for ent in doc.ents]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'start: {ent.start} end: {ent.end} | texto: {ent.text} , label: {ent.label_}')\n",
    "    # print(ent.text)\n",
    "    # print(ent.label_)\n",
    "    # print(ent.start)\n",
    "    # print(ent.end)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "#nlp = spacy.blank(\"pt\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PERSON\", [[{\"lower\": \"daniel\"}, {\"lower\": \"nascimento\"}]])\n",
    "doc = nlp(\"Este contrato tem como parte Daniel Nascimento RG 22.571.820 trabalha na empresa AgroBI\")\n",
    "\n",
    "# 1. Return (match_id, start, end) tuples\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    # Create the matched span and assign the match_id as a label\n",
    "    span = Span(doc, start, end, label=match_id)\n",
    "    print(start, end, span.text, span.label_)\n",
    "\n",
    "# 2. Return Span objects directly\n",
    "matches = matcher(doc, as_spans=True)\n",
    "for span in matches:\n",
    "    print(span.text, span.label_)\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"fb is hiring a new vice president of global policy\")\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print('Before', ents)\n",
    "# The model didn't recognize \"fb\" as an entity :(\n",
    "\n",
    "# Create a span for the new entity\n",
    "fb_ent = Span(doc, 0, 1, label=\"ORG\")\n",
    "orig_ents = list(doc.ents)\n",
    "\n",
    "# Option 1: Modify the provided entity spans, leaving the rest unmodified\n",
    "doc.set_ents([fb_ent], default=\"unmodified\")\n",
    "\n",
    "# Option 2: Assign a complete list of ents to doc.ents\n",
    "doc.ents = orig_ents + [fb_ent]\n",
    "\n",
    "ents = [(e.text, e.start, e.end, e.label_) for e in doc.ents]\n",
    "print('After', ents)\n",
    "# [('fb', 0, 1, 'ORG')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def add_event_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"EVENT\")\n",
    "    doc.ents += (entity,)\n",
    "    print(entity.text)\n",
    "\n",
    "pattern = [{\"ORTH\": \"Google\"}, {\"ORTH\": \"I\"}, {\"ORTH\": \"/\"}, {\"ORTH\": \"O\"}]\n",
    "matcher.add(\"GoogleIO\", [pattern], on_match=add_event_ent)\n",
    "doc = nlp(\"This is a text about Google I/O\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I am goint to work at Google I/O\")\n",
    "matcher(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating spans from matches</h3>\n",
    "\n",
    "Creating Span objects from the returned matches is a very common use case. spaCy makes this easy by giving you access to the start and end token of each match, which you can use to construct a new span with an optional label. As of spaCy v3.0, you can also set as_spans=True when calling the matcher on a Doc, which will return a list of Span objects using the match_id as the span label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PERSON\", [[{\"lower\": \"barack\"}, {\"lower\": \"obama\"}]])\n",
    "doc = nlp(\"Barack Obama was the 44th president of the United States\")\n",
    "\n",
    "# 1. Return (match_id, start, end) tuples\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    # Create the matched span and assign the match_id as a label\n",
    "    span = Span(doc, start, end, label=match_id)\n",
    "    print(span.text, span.label_)\n",
    "\n",
    "# 2. Return Span objects directly\n",
    "matches = matcher(doc, as_spans=True)\n",
    "for span in matches:\n",
    "    print(span.text, span.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "#nlp = spacy.blank(\"pt\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PERSON\", [[{\"lower\": \"daniel\"}, {\"lower\": \"nascimento\"}]])\n",
    "doc = nlp(\"Este contrato tem como parte Daniel Nascimento RG 22.571.820 trabalha na empresa AgroBI\")\n",
    "\n",
    "# 1. Return (match_id, start, end) tuples\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    # Create the matched span and assign the match_id as a label\n",
    "    span = Span(doc, start, end, label=match_id)\n",
    "    print(start, end, span.text, span.label_)\n",
    "\n",
    "# 2. Return Span objects directly\n",
    "matches = matcher(doc, as_spans=True)\n",
    "for span in matches:\n",
    "    print(span.text, span.label_)\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.pt import Portuguese\n",
    "nlp = Portuguese()\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"AgroBI\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"dois\"}, {\"LOWER\": \"irmãos\"}]}]\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "doc = nlp(\"A empresa AgroBI esta atendendo agora em Dois Irmãos.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"fb is hiring a new vice president of global policy\")\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print('Before', ents)\n",
    "# The model didn't recognize \"fb\" as an entity :(\n",
    "\n",
    "# Create a span for the new entity\n",
    "fb_ent = Span(doc, 0, 1, label=\"ORG\")\n",
    "orig_ents = list(doc.ents)\n",
    "\n",
    "# Option 1: Modify the provided entity spans, leaving the rest unmodified\n",
    "doc.set_ents([fb_ent], default=\"unmodified\")\n",
    "\n",
    "# Option 2: Assign a complete list of ents to doc.ents\n",
    "doc.ents = orig_ents + [fb_ent]\n",
    "\n",
    "ents = [(e.text, e.start, e.end, e.label_) for e in doc.ents]\n",
    "print('After', ents)\n",
    "# [('fb', 0, 1, 'ORG')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fb_ent = doc.char_span(0, 2, label=\"ORG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import spacy\n",
    "from spacy.attrs import ENT_IOB, ENT_TYPE\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp.make_doc(\"London is a big city in the United Kingdom.\")\n",
    "print(\"Before\", doc.ents)  # []\n",
    "\n",
    "header = [ENT_IOB, ENT_TYPE]\n",
    "attr_array = numpy.zeros((len(doc), len(header)), dtype=\"uint64\")\n",
    "attr_array[0, 0] = 3  # B\n",
    "attr_array[0, 1] = doc.vocab.strings[\"GPE\"]\n",
    "doc.from_array(header, attr_array)\n",
    "print(doc.ents)  # [London]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "text = '''\"Let's go!\"'''\n",
    "doc = nlp(text)\n",
    "tok_exp = nlp.tokenizer.explain(text)\n",
    "assert [t.text for t in doc if not t.is_space] == [t[1] for t in tok_exp]\n",
    "for t in tok_exp:\n",
    "    print(t[1], \"\\\\t\", t[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "special_cases = {\":)\": [{\"ORTH\": \":)\"}]}\n",
    "prefix_re = re.compile(r'''^[\\\\[\\\\(\"']''')\n",
    "suffix_re = re.compile(r'''[\\\\]\\\\)\"']$''')\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "simple_url_re = re.compile(r'''^https?://''')\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, rules=special_cases,\n",
    "                                prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                url_match=simple_url_re.match)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "doc = nlp(\"hello-world. :)\")\n",
    "print([t.text for t in doc]) # ['hello', '-', 'world.', ':)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KnowledgeBase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/api/kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.kb import KnowledgeBase\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "class FullyImplementedKB(KnowledgeBase):\n",
    "  def __init__(self, vocab: Vocab, entity_vector_length: int):\n",
    "      super().__init__(vocab, entity_vector_length)\n",
    "      ...\n",
    "vocab = nlp.vocab\n",
    "kb = FullyImplementedKB(vocab=vocab, entity_vector_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "doc = nlp(\"Douglas Adams wrote 'The Hitchhiker's Guide to the Galaxy'.\")\n",
    "candidates = kb.get_candidates((doc[0:2], doc[3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "doc = nlp(\"Douglas Adams wrote 'The Hitchhiker's Guide to the Galaxy'.\")\n",
    "candidates = kb.get_candidates(doc[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule-based matching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/usage/rule-based-matching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![]https://spacy.io/images/architecture.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ENTENDI   pontuacao\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"HellWorld\", [pattern])\n",
    "\n",
    "doc = nlp(\"Hello. world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # 'HelloWorld'\n",
    "    span = doc[start:end]\n",
    "    print(span)# The matched span\n",
    "    print(string_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = English()  # We only want the tokenizer, so no need to load a pipeline\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pos_emoji = [\"😀\", \"😃\", \"😂\", \"🤣\", \"😊\", \"😍\"]  # Positive emoji\n",
    "neg_emoji = [\"😞\", \"😠\", \"😩\", \"😢\", \"😭\", \"😒\"]  # Negative emoji\n",
    "\n",
    "# Add patterns to match one or more emoji tokens\n",
    "pos_patterns = [[{\"ORTH\": emoji}] for emoji in pos_emoji]\n",
    "neg_patterns = [[{\"ORTH\": emoji}] for emoji in neg_emoji]\n",
    "\n",
    "# Function to label the sentiment\n",
    "def label_sentiment(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    if doc.vocab.strings[match_id] == \"HAPPY\":  # Don't forget to get string!\n",
    "        doc.sentiment += 0.1  # Add 0.1 for positive sentiment\n",
    "    elif doc.vocab.strings[match_id] == \"SAD\":\n",
    "        doc.sentiment -= 0.1  # Subtract 0.1 for negative sentiment\n",
    "\n",
    "matcher.add(\"HAPPY\", pos_patterns, on_match=label_sentiment)  # Add positive pattern\n",
    "matcher.add(\"SAD\", neg_patterns, on_match=label_sentiment)  # Add negative pattern\n",
    "\n",
    "# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\n",
    "matcher.add(\"HASHTAG\", [[{\"ORTH\": \"#\"}, {\"IS_ASCII\": True}]])\n",
    "\n",
    "doc = nlp(\"Hello world 😀 #MondayMotivation\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = doc.vocab.strings[match_id]  # Look up string ID\n",
    "    span = doc[start:end]\n",
    "    print(string_id, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para a vida real do Agro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cultura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REsolvido o MAtch para cultura\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "culturas = ['milho', 'soja', 'sorgo', 'arroz', 'café']\n",
    "# Only run nlp.make_doc to speed things up\n",
    "cult_patterns = [nlp.make_doc(cultura) for cultura in culturas]\n",
    "matcher.add(\"CULTURA\", cult_patterns)\n",
    "\n",
    "doc = nlp(\"Minhas culturas de milhor, soja e sorgo - todas em Novo Hamburgo\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(f'\\ncultura: {span.text} inicio: {start}, fim {end}')\n",
    "print()\n",
    "matches = matcher(doc, as_spans=True)\n",
    "for span in matches:\n",
    "    print(span.text, span.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolvido - Telefone celular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "cel_pattern = [{\"ORTH\": \"(\"}, {\"SHAPE\": \"dd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"ddddd\"},\n",
    "           {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]\n",
    "matcher.add(\"CELULAR\", [cel_pattern])\n",
    "\n",
    "doc = nlp(\"me ligue no 51 99495-4119 ou no (51) 99495-4119 ou  ainda no 51994954119!\")\n",
    "print([t.text for t in doc])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicionando entidades e localidades locais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.pt import Portuguese\n",
    "nlp = Portuguese()\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"AgroBI\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"dois\"}, {\"LOWER\": \"irmãos\"}]}]\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "doc = nlp(\"A empresa AgroBI esta atendendo agora em Dois Irmãos.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojis - sentimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.pt import Portuguese\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = Portuguese()  # We only want the tokenizer, so no need to load a pipeline\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pos_emoji = [\"😀\", \"😃\", \"😂\", \"🤣\", \"😊\", \"😍\"]  # Positive emoji\n",
    "neg_emoji = [\"😞\", \"😠\", \"😩\", \"😢\", \"😭\", \"😒\"]  # Negative emoji\n",
    "\n",
    "# Add patterns to match one or more emoji tokens\n",
    "pos_patterns = [[{\"ORTH\": emoji}] for emoji in pos_emoji]\n",
    "neg_patterns = [[{\"ORTH\": emoji}] for emoji in neg_emoji]\n",
    "\n",
    "# Function to label the sentiment\n",
    "def label_sentiment(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    if doc.vocab.strings[match_id] == \"FELIZ\":\n",
    "        doc.sentiment += 0.1  \n",
    "    elif doc.vocab.strings[match_id] == \"TRISTE\":\n",
    "        doc.sentiment -= 0.1  \n",
    "\n",
    "matcher.add(\"FELIZ\", pos_patterns, on_match=label_sentiment) \n",
    "matcher.add(\"TRISTE\", neg_patterns, on_match=label_sentiment) \n",
    "\n",
    "# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\n",
    "matcher.add(\"HASHTAG\", [[{\"ORTH\": \"#\"}, {\"IS_ASCII\": True}]])\n",
    "\n",
    "doc = nlp(\"Bom dia 😀 #AtendimentoTop\")\n",
    "\n",
    "#doc = nlp(\"Nao funcionou 😞\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = doc.vocab.strings[match_id]  \n",
    "    span = doc[start:end]\n",
    "    print(string_id, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "cel_pattern = [{\"ORTH\": \"(\"}, {\"SHAPE\": \"dd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"ddddd\"},\n",
    "           {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]\n",
    "matcher.add(\"CELULAR\", [cel_pattern])\n",
    "\n",
    "doc = nlp(\"me ligue no 51 99495-4119 ou no (51) 99495-4119 ou  ainda no 51994954119!\")\n",
    "print([t.text for t in doc])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nro do contrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "contratos = ['contrato', 'contratos']\n",
    "# Only run nlp.make_doc to speed things up\n",
    "contrato_patterns = [nlp.make_doc(contrato) for contrato in contratos]\n",
    "matcher.add(\"CONTRATO\", contrato_patterns)\n",
    "\n",
    "nro_pattern = [{\"TEXT\": \"contract\"}, {\"IS_PUNCT\": True, \"OP\": \"?\"}, {\"TEXT\": {\"REGEX\": \"[A-Za-z]{4}\"}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REsolvido o MAtch para cultura\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "contratos = ['contrato', 'contratos']\n",
    "# Only run nlp.make_doc to speed things up\n",
    "contrato_patterns = [nlp.make_doc(contrato) for contrato in contratos]\n",
    "matcher.add(\"CONTRATO\", contrato_patterns)\n",
    "\n",
    "doc = nlp(\"Meu contrato 532S da fazenda FAZENDA PASSO FUNDO da safra 2023\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    contract_number = None\n",
    "    print(f'\\nAchei aqui: {span.text} | inicio: {start}, fim {end}')\n",
    "    if end < len(doc):\n",
    "        next_token = doc[end]\n",
    "        print(f'next_token {next_token}')\n",
    "        if next_token.is_digit and len(next_token.is_digit) == 3 and end+1 < len(doc) and doc[end+1].is_alpha:\n",
    "            contract_number = next_token.text + doc[end+1].text\n",
    "            print(contract_number)\n",
    "    nro_contrato = doc[start:end + 1]\n",
    "    print(f'seria o nro do contrato esse: {nro_contrato}')\n",
    "print()\n",
    "matches = matcher(doc, as_spans=True)\n",
    "for span in matches:\n",
    "    print(f'Mostrando o span: {span.text}, e o label: {span.label} \\n')\n",
    "    # nro_contrato = span.lefts + 1\n",
    "    # print(f'Achei isso, sera que nro: {nro_contrato}')\n",
    "    \n",
    "# span = doc[start_token:end_token + 1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span = doc.char_span(start, end)\n",
    "if span is not None:\n",
    "    print(\"Found match:\", span.text)\n",
    "else:\n",
    "    start_token = chars_to_tokens.get(start)\n",
    "    end_token = chars_to_tokens.get(end)\n",
    "    if start_token is not None and end_token is not None:\n",
    "        span = doc[start_token:end_token + 1]\n",
    "        print(\"Found closest match:\", span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nro_contrato_pattern = [{\"ORTH\": \"(\"}, {\"SHAPE\": \"dd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"ddddd\"},\n",
    "           {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]\n",
    "matcher.add(\"CELULAR\", [cel_pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"TEXT\": \"contract\"}, {\"IS_PUNCT\": True, \"OP\": \"?\"}, {\"TEXT\": {\"REGEX\": \"[A-Za-z]{4}\"}}]\n",
    "matcher.add(\"ContractNumber\", pattern)\n",
    "\n",
    "doc = nlp(\"Meu contrato 532S da fazenda FAZENDA PASSO FUNDO da safra 2023\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    contract_number = None\n",
    "    if end < len(doc):\n",
    "        # Look for a 4-letter code followed by a single digit\n",
    "        next_token = doc[end]\n",
    "        if next_token.is_alpha and len(next_token.text) == 4 and end+1 < len(doc) and doc[end+1].is_digit:\n",
    "            contract_number = next_token.text + doc[end+1].text\n",
    "    print(span.text, contract_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"TEXT\": \"contract\"}, {\"IS_PUNCT\": True, \"OP\": \"?\"}, {\"TEXT\": {\"REGEX\": \"[A-Za-z]{4}\"}}]\n",
    "matcher.add(\"ContractNumber\", None, pattern)\n",
    "\n",
    "doc = nlp(\"We hereby enter into this contract, with contract ABCD1 as the reference.\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    contract_number = None\n",
    "    if end < len(doc):\n",
    "        # Look for a 4-letter code followed by a single digit\n",
    "        next_token = doc[end]\n",
    "        if next_token.is_alpha and len(next_token.text) == 4 and end+1 < len(doc) and doc[end+1].is_digit:\n",
    "            contract_number = next_token.text + doc[end+1].text\n",
    "    print(span.text, contract_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "# 2018 FIFA World Cup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "\n",
    "#loved dogs\n",
    "#love cats\n",
    "\n",
    "\n",
    "# In this example, we're looking for two tokens:\n",
    "\n",
    "# A verb with the lemma \"love\", followed by a noun.\n",
    "\n",
    "# This pattern will match \"loved dogs\" and \"love cats\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"buy\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},  # optional: match 0 or 1 times\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "\n",
    "# bought a smartphone\n",
    "# buying apps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Operators and quantifiers let you define how often a token should be matched. They can be added using the \"OP\" key.\n",
    "\n",
    "# Here, the \"?\" operator makes the determiner token optional, so it will match a token with the lemma \"buy\", an optional article and a noun."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"OP\" can have one of four values:\n",
    "\n",
    "An \"!\" negates the token, so it's matched 0 times.\n",
    "\n",
    "A \"?\" makes the token optional, and matches it 0 or 1 times.\n",
    "\n",
    "A \"+\" matches a token 1 or more times.\n",
    "\n",
    "And finally, an \"*\" matches 0 or more times.\n",
    "\n",
    "Operators can make your patterns a lot more powerful, but they also add more complexity – so use them wisely.\n",
    "\n",
    "\n",
    "\n",
    "{\"OP\": \"!\"}\tNegation: match 0 times\n",
    "\n",
    "{\"OP\": \"?\"}\tOptional: match 0 or 1 times\n",
    "\n",
    "{\"OP\": \"+\"}\tMatch 1 or more times\n",
    "\n",
    "{\"OP\": \"*\"}\tMatch 0 or more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked as Apple reveals pre-orders\")\n",
    "\n",
    "# Initialize the Matcher with the shared vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Create a pattern matching two tokens: \"iPhone\" and \"X\"\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add(\"IPHONE_X_PATTERN\", [pattern])\n",
    "\n",
    "# Use the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"After making the iOS update you won't notice a radical system-wide \"\n",
    "    \"redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of \"\n",
    "    \"iOS 11's furniture remains the same as in iOS 10. But you will discover \"\n",
    "    \"some tweaks once you delve a little deeper.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"IOS_VERSION_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"i downloaded Fortnite on my laptop and can't open the game at all. Help? \"\n",
    "    \"so when I was downloading Minecraft, I got the Windows version where it \"\n",
    "    \"is the '.zip' folder and I used the default program to unpack it... do \"\n",
    "    \"I also need to download Winzip?\"\n",
    ")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{\"LEMMA\": \"download\"}, {\"POS\": \"PROPN\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)\n",
    "    \n",
    "    \n",
    "# Total matches found: 3\n",
    "# Match found: downloaded Fortnite\n",
    "# Match found: downloading Minecraft\n",
    "# Match found: download Winzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
    "    \"labels and optional voice responses.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"ADJ_NOUN_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)\n",
    "    \n",
    "# Total matches found: 5\n",
    "# Match found: beautiful design\n",
    "# Match found: smart search\n",
    "# Match found: automatic labels\n",
    "# Match found: optional voice\n",
    "# Match found: optional voice responses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not just regular expressions?\n",
    "\n",
    "    Match on Doc objects, not just strings\n",
    "    Match on tokens and token attributes\n",
    "    Use a model's predictions\n",
    "    Example: \"duck\" (verb) vs. \"duck\" (noun)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to regular expressions, the matcher works with Doc and Token objects instead of only strings.\n",
    "\n",
    "It's also more flexible: you can search for texts but also other lexical attributes.\n",
    "\n",
    "You can even write rules that use a model's predictions.\n",
    "\n",
    "For example, find the word \"duck\" only if it's a verb, not a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the matcher on the doc\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n",
    "    \n",
    "# iPhone X    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- match_id: hash value of the pattern name\n",
    "- start: start index of matched span\n",
    "- end: end index of matched span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PERSON\", [[{\"lower\": \"barack\"}, {\"lower\": \"obama\"}]])\n",
    "doc = nlp(\"Barack Obama was the 44th president of the United States\")\n",
    "\n",
    "# 1. Return (match_id, start, end) tuples\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    # Create the matched span and assign the match_id as a label\n",
    "    span = Span(doc, start, end, label=match_id)\n",
    "    print(span.text, span.label_)\n",
    "\n",
    "# 2. Return Span objects directly\n",
    "matches = matcher(doc, as_spans=True)\n",
    "for span in matches:\n",
    "    print(span.text, span.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"contratos\", \"fazenda\", \"curtura\"]}}}]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/usage/rule-based-matching#phrasematcher-attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"SHAPE\")\n",
    "matcher.add(\"IP\", [nlp(\"127.0.0.1\"), nlp(\"127.127.0.0\")])\n",
    "\n",
    "doc = nlp(\"Often the router will have an IP address such as 192.168.1.1 or 192.168.2.1.\")\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(\"Matched based on token shape:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}],\n",
    "    [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}]\n",
    "]\n",
    "matcher.add(\"HelloWorld\", patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/api/entityruler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matches \"love cats\" or \"likes flowers\"\n",
    "pattern1 = [{\"LEMMA\": {\"IN\": [\"like\", \"love\"]}},\n",
    "            {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Matches tokens of length >= 10\n",
    "pattern2 = [{\"LENGTH\": {\">=\": 10}}]\n",
    "\n",
    "# Match based on morph attributes\n",
    "pattern3 = [{\"MORPH\": {\"IS_SUBSET\": [\"Number=Sing\", \"Gender=Neut\"]}}]\n",
    "# \"\", \"Number=Sing\" and \"Number=Sing|Gender=Neut\" will match as subsets\n",
    "# \"Number=Plur|Gender=Neut\" will not match\n",
    "# \"Number=Sing|Gender=Neut|Polite=Infm\" will not match because it's a superset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
