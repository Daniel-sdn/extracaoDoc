{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokens e funçoes especificas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Quantos quilos de milho já foram entregues pela fazenda Santa Rita no contrato atual?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tokens_id = [token.i for token in doc]\n",
    "seq_tokens_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:    Quantos | pos_:     DET | chil: []\n",
      "token:     quilos | pos_:    NOUN | chil: ['Quantos', 'milho']\n",
      "token:         de | pos_:     ADP | chil: []\n",
      "token:      milho | pos_:    NOUN | chil: ['de']\n",
      "token:         já | pos_:     ADV | chil: []\n",
      "token:      foram | pos_:     AUX | chil: []\n",
      "token:  entregues | pos_:    VERB | chil: ['quilos', 'já', 'foram', 'fazenda', 'contrato', '?']\n",
      "token:       pela | pos_:     ADP | chil: []\n",
      "token:    fazenda | pos_:    NOUN | chil: ['pela', 'Santa']\n",
      "token:      Santa | pos_:   PROPN | chil: ['Rita']\n",
      "token:       Rita | pos_:   PROPN | chil: []\n",
      "token:         no | pos_:     ADP | chil: []\n",
      "token:   contrato | pos_:    NOUN | chil: ['no', 'atual']\n",
      "token:      atual | pos_:     ADJ | chil: []\n",
      "token:          ? | pos_:   PUNCT | chil: []\n"
     ]
    }
   ],
   "source": [
    "# Identificando os filhos\n",
    "\n",
    "seq_tokens_id = [token.i for token in doc]\n",
    "\n",
    "for idx in seq_tokens_id:\n",
    "    give_children = doc[idx].children\n",
    "    # print(doc[idx], doc[idx].pos_)\n",
    "    texto = [t.text for t in give_children]\n",
    "    print(f'token: {doc[idx].text:>10} | pos_: {doc[idx].pos_:>7} | chil: {texto}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:    Quantos | pos_:     DET | chil: ['Quantos']\n",
      "token:     quilos | pos_:    NOUN | chil: ['Quantos', 'quilos', 'de', 'milho']\n",
      "token:         de | pos_:     ADP | chil: ['de']\n",
      "token:      milho | pos_:    NOUN | chil: ['de', 'milho']\n",
      "token:         já | pos_:     ADV | chil: ['já']\n",
      "token:      foram | pos_:     AUX | chil: ['foram']\n",
      "token:  entregues | pos_:    VERB | chil: ['Quantos', 'quilos', 'de', 'milho', 'já', 'foram', 'entregues', 'pela', 'fazenda', 'Santa', 'Rita', 'no', 'contrato', 'atual', '?']\n",
      "token:       pela | pos_:     ADP | chil: ['pela']\n",
      "token:    fazenda | pos_:    NOUN | chil: ['pela', 'fazenda', 'Santa', 'Rita']\n",
      "token:      Santa | pos_:   PROPN | chil: ['Santa', 'Rita']\n",
      "token:       Rita | pos_:   PROPN | chil: ['Rita']\n",
      "token:         no | pos_:     ADP | chil: ['no']\n",
      "token:   contrato | pos_:    NOUN | chil: ['no', 'contrato', 'atual']\n",
      "token:      atual | pos_:     ADJ | chil: ['atual']\n",
      "token:          ? | pos_:   PUNCT | chil: ['?']\n"
     ]
    }
   ],
   "source": [
    "# Identificando Subtree\n",
    "\n",
    "seq_tokens_id = [token.i for token in doc]\n",
    "\n",
    "for idx in seq_tokens_id:\n",
    "    subtree = doc[idx].subtree\n",
    "    # print(doc[idx], doc[idx].pos_)\n",
    "    texto = [t.text for t in subtree]\n",
    "    print(f'token: {doc[idx].text:>10} | pos_: {doc[idx].pos_:>7} | chil: {texto}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:    Quantos | pos_:     DET | n_rights: 0\n",
      "token:     quilos | pos_:    NOUN | n_rights: 1\n",
      "token:         de | pos_:     ADP | n_rights: 0\n",
      "token:      milho | pos_:    NOUN | n_rights: 0\n",
      "token:         já | pos_:     ADV | n_rights: 0\n",
      "token:      foram | pos_:     AUX | n_rights: 0\n",
      "token:  entregues | pos_:    VERB | n_rights: 3\n",
      "token:       pela | pos_:     ADP | n_rights: 0\n",
      "token:    fazenda | pos_:    NOUN | n_rights: 1\n",
      "token:      Santa | pos_:   PROPN | n_rights: 1\n",
      "token:       Rita | pos_:   PROPN | n_rights: 0\n",
      "token:         no | pos_:     ADP | n_rights: 0\n",
      "token:   contrato | pos_:    NOUN | n_rights: 1\n",
      "token:      atual | pos_:     ADJ | n_rights: 0\n",
      "token:          ? | pos_:   PUNCT | n_rights: 0\n"
     ]
    }
   ],
   "source": [
    "# n_rights: O número de filhos imediatos à direita da palavra na análise de dependência sintática.\n",
    "\n",
    "seq_tokens_id = [token.i for token in doc]\n",
    "\n",
    "for idx in seq_tokens_id:\n",
    "    n_rights = doc[idx].n_rights\n",
    "    #print(n_rights)\n",
    "    #texto = [t.i for t in n_rights]\n",
    "    print(f'token: {doc[idx].text:>10} | pos_: {doc[idx].pos_:>7} | n_rights: {n_rights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:    Quantos | pos_:     DET | rights: []\n",
      "token:     quilos | pos_:    NOUN | rights: ['milho']\n",
      "token:         de | pos_:     ADP | rights: []\n",
      "token:      milho | pos_:    NOUN | rights: []\n",
      "token:         já | pos_:     ADV | rights: []\n",
      "token:      foram | pos_:     AUX | rights: []\n",
      "token:  entregues | pos_:    VERB | rights: ['fazenda', 'contrato', '?']\n",
      "token:       pela | pos_:     ADP | rights: []\n",
      "token:    fazenda | pos_:    NOUN | rights: ['Santa']\n",
      "token:      Santa | pos_:   PROPN | rights: ['Rita']\n",
      "token:       Rita | pos_:   PROPN | rights: []\n",
      "token:         no | pos_:     ADP | rights: []\n",
      "token:   contrato | pos_:    NOUN | rights: ['atual']\n",
      "token:      atual | pos_:     ADJ | rights: []\n",
      "token:          ? | pos_:   PUNCT | rights: []\n"
     ]
    }
   ],
   "source": [
    "# Token.rights: Os filhos imediatos à direita da palavra na análise de dependência sintática.\n",
    "\n",
    "seq_tokens_id = [token.i for token in doc]\n",
    "\n",
    "for idx in seq_tokens_id:\n",
    "    rights = doc[idx].rights\n",
    "    #print(n_rights)\n",
    "    texto = [t.text for t in rights]\n",
    "    print(f'token: {doc[idx].text:>10} | pos_: {doc[idx].pos_:>7} | rights: {texto}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:    Quantos | pos_:     DET | n_rights: 0\n",
      "token:     quilos | pos_:    NOUN | n_rights: 1\n",
      "token:         de | pos_:     ADP | n_rights: 0\n",
      "token:      milho | pos_:    NOUN | n_rights: 1\n",
      "token:         já | pos_:     ADV | n_rights: 0\n",
      "token:      foram | pos_:     AUX | n_rights: 0\n",
      "token:  entregues | pos_:    VERB | n_rights: 3\n",
      "token:       pela | pos_:     ADP | n_rights: 0\n",
      "token:    fazenda | pos_:    NOUN | n_rights: 1\n",
      "token:      Santa | pos_:   PROPN | n_rights: 0\n",
      "token:       Rita | pos_:   PROPN | n_rights: 0\n",
      "token:         no | pos_:     ADP | n_rights: 0\n",
      "token:   contrato | pos_:    NOUN | n_rights: 1\n",
      "token:      atual | pos_:     ADJ | n_rights: 0\n",
      "token:          ? | pos_:   PUNCT | n_rights: 0\n"
     ]
    }
   ],
   "source": [
    "# n_lefts: O número de filhos imediatos à esquerda da palavra na análise de dependência sintática.\n",
    "\n",
    "seq_tokens_id = [token.i for token in doc]\n",
    "\n",
    "for idx in seq_tokens_id:\n",
    "    n_lefts = doc[idx].n_lefts\n",
    "    #print(n_rights)\n",
    "    #texto = [t.i for t in n_rights]\n",
    "    print(f'token: {doc[idx].text:>10} | pos_: {doc[idx].pos_:>7} | n_rights: {n_lefts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:    Quantos | pos_:     DET | lefts: []\n",
      "token:     quilos | pos_:    NOUN | lefts: ['Quantos']\n",
      "token:         de | pos_:     ADP | lefts: []\n",
      "token:      milho | pos_:    NOUN | lefts: ['de']\n",
      "token:         já | pos_:     ADV | lefts: []\n",
      "token:      foram | pos_:     AUX | lefts: []\n",
      "token:  entregues | pos_:    VERB | lefts: ['quilos', 'já', 'foram']\n",
      "token:       pela | pos_:     ADP | lefts: []\n",
      "token:    fazenda | pos_:    NOUN | lefts: ['pela']\n",
      "token:      Santa | pos_:   PROPN | lefts: []\n",
      "token:       Rita | pos_:   PROPN | lefts: []\n",
      "token:         no | pos_:     ADP | lefts: []\n",
      "token:   contrato | pos_:    NOUN | lefts: ['no']\n",
      "token:      atual | pos_:     ADJ | lefts: []\n",
      "token:          ? | pos_:   PUNCT | lefts: []\n"
     ]
    }
   ],
   "source": [
    "# Token.lefts: Os filhos imediatos à esquerda da palavra na análise de dependência sintática.\n",
    "\n",
    "seq_tokens_id = [token.i for token in doc]\n",
    "\n",
    "for idx in seq_tokens_id:\n",
    "    lefts = doc[idx].lefts\n",
    "    #print(n_rights)\n",
    "    texto = [t.text for t in lefts]\n",
    "    print(f'token: {doc[idx].text:>10} | pos_: {doc[idx].pos_:>7} | lefts: {texto}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:    Quantos | pos_:     DET | ancestors: ['quilos', 'entregues']\n",
      "token:     quilos | pos_:    NOUN | ancestors: ['entregues']\n",
      "token:         de | pos_:     ADP | ancestors: ['milho', 'quilos', 'entregues']\n",
      "token:      milho | pos_:    NOUN | ancestors: ['quilos', 'entregues']\n",
      "token:         já | pos_:     ADV | ancestors: ['entregues']\n",
      "token:      foram | pos_:     AUX | ancestors: ['entregues']\n",
      "token:  entregues | pos_:    VERB | ancestors: []\n",
      "token:       pela | pos_:     ADP | ancestors: ['fazenda', 'entregues']\n",
      "token:    fazenda | pos_:    NOUN | ancestors: ['entregues']\n",
      "token:      Santa | pos_:   PROPN | ancestors: ['fazenda', 'entregues']\n",
      "token:       Rita | pos_:   PROPN | ancestors: ['Santa', 'fazenda', 'entregues']\n",
      "token:         no | pos_:     ADP | ancestors: ['contrato', 'entregues']\n",
      "token:   contrato | pos_:    NOUN | ancestors: ['entregues']\n",
      "token:      atual | pos_:     ADJ | ancestors: ['contrato', 'entregues']\n",
      "token:          ? | pos_:   PUNCT | ancestors: ['entregues']\n"
     ]
    }
   ],
   "source": [
    "# Token.ancestors: Uma sequência dos ancestrais sintáticos do token (pais, avós, etc).\n",
    "\n",
    "seq_tokens_id = [token.i for token in doc]\n",
    "\n",
    "for idx in seq_tokens_id:\n",
    "    ancestors = doc[idx].ancestors\n",
    "    #print(n_rights)\n",
    "    texto = [t.text for t in ancestors]\n",
    "    print(f'token: {doc[idx].text:>10} | pos_: {doc[idx].pos_:>7} | ancestors: {texto}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token.is_ancestorVerifique se este token é pai, avô etc. de outro na árvore de dependências.\n",
    "\n",
    "quilos = doc[1]\n",
    "quantos = doc[0]\n",
    "quilos.is_ancestor(quantos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token.nbor: Uma sequência nbor\n",
    "print(doc)\n",
    "print()\n",
    "\n",
    "seq_tokens_id = [token.i for token in doc]\n",
    "\n",
    "for idx in seq_tokens_id:\n",
    "    nbor = doc[idx].nbor()\n",
    "    #print(n_rights)\n",
    "    # texto = [t.text for t in ancestors]\n",
    "    print(f'token: {doc[idx].text:>10} | pos_: {doc[idx].pos_:>7} | nbor: {nbor.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token.conjuncts: Uma tupla de tokens coordenados, não incluindo o próprio token.\n",
    "\n",
    "\n",
    "doc = nlp(\"Eu gosto de maças e laranjas\")\n",
    "\n",
    "\n",
    "seq_tokens_id = [token.i for token in doc]\n",
    "\n",
    "for idx in seq_tokens_id:\n",
    "    conjuncts = doc[idx].conjuncts\n",
    "    #print(n_rights)\n",
    "    texto = [t.text for t in conjuncts]\n",
    "    print(f'token: {doc[idx].text:>10} | pos_: {doc[idx].pos_:>7} | conjuncts: {texto}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.morphology import Morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.strings import StringStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringstore = StringStore([\"apple\", \"orange\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morphology = Morphology(stringstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = morphology.feats_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. noun_chunks e similaridade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Quantos quilos de milho já foram entregues pela fazenda Santa Rita no contrato atual?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_tokens_id = [token.i for token in doc]\n",
    "seq_tokens_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_tokens_id = [token.i for token in doc]\n",
    "\n",
    "significative_tokens = pd.DataFrame(data=[], \\\n",
    "  columns=[\"idx\", \"T_texto\",\"T_shape_\", \"T_ent_type_\", \"T_ent_id_\", \"T_ent_iob_\", \"T_pos_\", \"T_lemma_\", \"T_dep_\", \"T_head\"])\n",
    "i = 0\n",
    "for idx in seq_tokens_id:\n",
    "    significative_tokens.loc[i,\"idx\"] = doc[idx].i\n",
    "    significative_tokens.loc[i,\"T_texto\"] = doc[idx].text\n",
    "    significative_tokens.loc[i,\"T_shape_\"] = doc[idx].shape_\n",
    "    significative_tokens.loc[i,\"T_ent_type_\"] = doc[idx].ent_type_\n",
    "    significative_tokens.loc[i,\"T_ent_id_\"] = doc[idx].ent_id_\n",
    "    significative_tokens.loc[i,\"T_ent_iob_\"] = doc[idx].ent_iob_\n",
    "    significative_tokens.loc[i,\"T_pos_\"] = doc[idx].pos_\n",
    "    significative_tokens.loc[i,\"T_lemma_\"] = doc[idx].lemma_\n",
    "    significative_tokens.loc[i,\"T_dep_\"] = doc[idx].dep_\n",
    "    significative_tokens.loc[i,\"T_head\"] = doc[idx].head\n",
    "\n",
    "    \n",
    "\n",
    "    i = i+1\n",
    "\n",
    "significative_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Gostaria de saber o saldo do meu contrato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_chunk(doc, param):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_root_head_lemma_ = (chunk.root.head.lemma_).lower()\n",
    "        if chunk_root_head_lemma_ in [\"gostaria\", \"qual\"]:\n",
    "            if chunk.root.dep_ == param:\n",
    "                return chunk.text\n",
    "\n",
    "# chunk.text, chunk.start, chunk.end, chunk.root.head.lemma_, chunk.root.dep_, chunk.doc\n",
    "def load_json(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/home/wklinux/spaCy/query_utter.json\"  \n",
    "\n",
    "data = load_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIAZACAO ESTRUTURA DEP, HEAD, ROOT, LEMMA\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "  chunk_text = chunk.text\n",
    "  chunk_root = chunk.root.text\n",
    "  chunk_root_dep = chunk.root.dep_\n",
    "  chunk_root_head = chunk.root.head.text\n",
    "  chunk_root_head_dep = chunk.root.head.dep_\n",
    "  chunk_root_head_lemma = chunk.root.head.lemma_\n",
    "  \n",
    "  chunk_ents = chunk.ents\n",
    "  chunk_root_ent_type = chunk.root.ent_type_\n",
    "  \n",
    "  print(f'1.chunk.text: {chunk_text:>15} | 2.ch.root: {chunk_root:>10} | 3.chunk.root.dep_: {chunk_root_dep:>6} | 4.ch.root.head: {chunk_root_head:>12} | 5.ch.root.head.dep_: {chunk_root_head_dep:>6} |  6.chunk.root.head.lemma_: {chunk_root_head_lemma:>9}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_chunks = []\n",
    "for chunk in doc.noun_chunks:\n",
    "  #valor = chunk.text.similarity(chunk.root.head.lemma_)\n",
    "  #print(valor)\n",
    "  chunk_text = nlp(chunk.text)\n",
    "  print(chunk_text)\n",
    "  param_chunk_root_dep_ = chunk.root.dep_\n",
    "  chunk_root_head = chunk.root.head.text\n",
    "  chunk_root_head_lemma = nlp(chunk.root.head.lemma_)\n",
    "  print(chunk_root_head_lemma)\n",
    "  # if chunk_text != chunk_root_head_lemma:\n",
    "  #     relevant_chunks.append(chunk_root_head_lemma)\n",
    "  #     relevant_chunks.append(chunk_text)\n",
    "  # elif   chunk_text  \n",
    "      \n",
    "print(chunk_text.similarity(chunk_root_head_lemma))\n",
    "print()\n",
    "print(chunk_root_head_lemma)\n",
    "print()\n",
    "print(relevant_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_chunk(doc, param):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_root_head_lemma_ = (chunk.root.head.lemma_).lower()\n",
    "        if chunk_root_head_lemma_ in [\"gostaria\", \"qual\"]:\n",
    "            if chunk.root.dep_ == param:\n",
    "                return chunk.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc)\n",
    "print()\n",
    "print(\"==============================================================\")\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "  chunk_text = chunk.text \n",
    "  param_chunk_root_dep_ = chunk.root.dep_\n",
    "  chunk_root_head = chunk.root.head.text\n",
    "  chunk_root_head_lemma = chunk.root.head.lemma_\n",
    "  \n",
    "  #print(f'\\n1. dentro primeiro loop - chunk: {chunk.text} | param_chunk_root_dep_(chunk): {param_chunk_root_dep_} | {chunk.doc} \\n')\n",
    "  # print(param_chunk_root_dep_)\n",
    "  i = 0\n",
    "  for i in range(len(data)):\n",
    "     doc_query = nlp(data[i][\"user_utter\"])\n",
    "     \n",
    "     print(f'\\n2. indice: {i} | doc_query: {doc_query} ===================\\n')\n",
    "     \n",
    "     for chunk_query in doc_query.noun_chunks:\n",
    "         chunk_query_root_head_lemma = (chunk_query.root.head.lemma_).lower()\n",
    "         chunk_query_root_dep = chunk_query.root.dep_\n",
    "         print(f'\\n3. dentro segundo loop - chunk_query: {chunk_query.text} | chunk_query_root_head_lemma: {chunk_query_root_head_lemma} | chunk_query_root_dep: {chunk_query_root_dep}')\n",
    "         if chunk_query_root_head_lemma == chunk_root_head_lemma:\n",
    "            print(f'\\n4. primeira condicao:  True para chunk_root_head_lemma_: {chunk_query_root_head_lemma}')\n",
    "            if chunk_query_root_dep == param_chunk_root_dep_:\n",
    "               #print(f'\\n5. segunda cond:  True para chunk_query_root_dep: {chunk_query_root_dep}')\n",
    "               bot_utter = nlp(data[i][\"bot_utter\"])\n",
    "               user_keys = nlp(data[i][\"user_keys\"])\n",
    "               user_desire = nlp(data[i][\"user_des\"])\n",
    "               missed_keys = nlp(data[i][\"miss_key\"])\n",
    "               print(f'\\n1. Resposta do bot: {bot_utter}   | 2. user_keys: {user_keys}  |  3. user_desire: {user_desire} | 4. missed_keys: {missed_keys}')\n",
    "               #break\n",
    "            else:\n",
    "                print(f'\\n5. segunda cond:  False para chunk_query_root_dep') \n",
    "                print() \n",
    "                \n",
    "         else:\n",
    "           print(f'\\n4. deu False')\n",
    "           print()  \n",
    "         #break   \n",
    "      # i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"user_utter\":\"Qual seria o saldo do meu contrato?\",\"bot_utter\":\"Claro, por favor, informe o número do contrato.\", \"user_keys\":\"NM_FAZENDA,ID_SAFRA\", \"user_des\":\"QT_SALDO_CONTRATO\", \"miss_key\":\"NR_CONTRATO\"},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(len(data)):\n",
    "    print(data[i][\"bot_utter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "  param_chunk_root_dep_ = chunk.root.dep_\n",
    "  print(f'start: {chunk.start:>2} end: {chunk.end:>2} | chunk.text: {chunk.text:>12} || chunk.root: {chunk.root.text:>12} | chunk.root.head: {chunk.root.head.text:>12} | chunk.root.head.lemma_: {chunk.root.head.lemma_:>9} | chunk.root.dep_: {chunk.root.dep_:>6} || chunk.root.head.dep_: {chunk.root.head.dep_:>6}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
