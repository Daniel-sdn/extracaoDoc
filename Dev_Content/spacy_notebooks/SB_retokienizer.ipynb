{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Overwriting custom extension attributes</h3>\n",
    "\n",
    "Iterate over the base noun phrases in the span. Yields base noun-phrase Span objects, if the document has been syntactically parsed. A base noun phrase, or “NP chunk”, is a noun phrase that does not permit other NPs to be nested within it – so no NP-level coordination, no prepositional phrases, and no relative clauses.\n",
    "\n",
    "If the noun_chunk syntax iterator has not been implemeted for the given language, a NotImplementedError is raised.\n",
    "\n",
    "\n",
    "https://spacy.io/usage/linguistic-features#retokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "\n",
    "# Register a custom token attribute, token._.is_musician\n",
    "Token.set_extension(\"is_musician\", default=False)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I like David Bowie\")\n",
    "print(\"Before:\", [(token.text, token._.is_musician) for token in doc])\n",
    "\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[2:4], attrs={\"_\": {\"is_musician\": True}})\n",
    "print(\"After:\", [(token.text, token._.is_musician) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the \"yield\" keyword do in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_child_candidates(self, distance, min_dist, max_dist):\n",
    "    if self._leftchild and distance - max_dist < self._median:\n",
    "        yield self._leftchild\n",
    "    if self._rightchild and distance + max_dist >= self._median:\n",
    "        yield self._rightchild "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, candidates = [], [self]\n",
    "while candidates:\n",
    "    node = candidates.pop()\n",
    "    distance = node._get_dist(obj)\n",
    "    if distance <= max_dist and distance >= min_dist:\n",
    "        result.extend(node._values)\n",
    "    candidates.extend(node._get_child_candidates(distance, min_dist, max_dist))\n",
    "return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste substituiÇao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're using a component factory because the component needs to be\n",
    "# initialized with the shared vocab via the nlp object\n",
    "@Language.factory(\"html_merger\")\n",
    "def create_bad_html_merger(nlp, name):\n",
    "    return BadHTMLMerger(nlp.vocab)\n",
    "\n",
    "class BadHTMLMerger:\n",
    "    def __init__(self, vocab):\n",
    "        patterns = [\n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"r\"}, {\"ORTH\": \">\"}],\n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"/r\"}, {\"ORTH\": \">\"}],\n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"user_keys\"}, {\"ORTH\": \">\"}],\n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"/user_keys\"}, {\"ORTH\": \">\"}],\n",
    "            \n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"user_intent\"}, {\"ORTH\": \">\"}],\n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"/user_intent\"}, {\"ORTH\": \">\"}],\n",
    "            \n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"missing_keys\"}, {\"ORTH\": \">\"}],\n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"/missing_keys\"}, {\"ORTH\": \">\"}],           \n",
    "            \n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"/n\"}, {\"ORTH\": \">\"}],\n",
    "            {\"LOWER\": \"QT_SALDO_CONTRATO\"}, {\"ORTH\": \">\"},\n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"nro_safra\"}, {\"ORTH\": \">\"}],\n",
    "        ]\n",
    "        # Register a new token extension to flag bad HTML\n",
    "        Token.set_extension(\"bad_html\", default=False)\n",
    "        self.matcher = Matcher(vocab)\n",
    "        self.matcher.add(\"BAD_HTML\", patterns)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        # This method is invoked when the component is called on a Doc\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # Collect the matched spans here\n",
    "        for match_id, start, end in matches:\n",
    "            spans.append(doc[start:end])\n",
    "        with doc.retokenize() as retokenizer:\n",
    "            for span in spans:\n",
    "                retokenizer.merge(span)\n",
    "                for token in span:\n",
    "                    token._.bad_html = True  # Mark token as bad HTML\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.add_pipe(\"html_merger\", last=True)  # Add component to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pipeline:\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Gostaria de saber o saldo do meu contrato.<r>Claro, por favor, informe o número do contrato<\\r><user_keys>NM_FAZENDA</user_keys><user_intent>QT_SALDO_CONTRATO</user_intent><missing_keys>NR_CONTRATO</missing_keys>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Hello<br>world! <br/> This is a test </n> for the contract: <nro_contrato> and the safra: <nro_safra>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    token_bad_html = token._.bad_html\n",
    "    print(f'idx: {token.i:>3} | token.text: {token.text:>15} | token._.bad_html: {token_bad_html}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of hard-coding the patterns into the component, you could also make it take a path to a JSON file containing the patterns. This lets you reuse the component with different patterns, depending on your application. When adding the component to the pipeline with nlp.add_pipe, you can pass in the argument via the config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.factory(\"html_merger\", default_config={\"path\": None})\n",
    "def create_bad_html_merger(nlp, name, path):\n",
    "    return BadHTMLMerger(nlp, path=path)\n",
    "\n",
    "nlp.add_pipe(\"html_merger\", config={\"path\": \"/path/to/patterns.json\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I like New York in Autumn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, like, new, york, in_, autumn, dot = range(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[new].head.text == \"York\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[york].head.text == \"like\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_york = doc[new:york+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_york.root.text == \"York\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "assert doc[new].head.text == \"York\"\n",
    "assert doc[york].head.text == \"like\"\n",
    "new_york = doc[new:york+1]\n",
    "assert new_york.root.text == \"York\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Token\n",
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I like New York in Autumn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lefts = [t.text for t in doc[3:7].lefts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lefts == [\"New\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I like New York in Autumn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[3:7].n_lefts == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Give it back! He pleaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtree = [t.text for t in doc[:5].subtree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtree == [\"Give\", \"it\", \"back\", \"!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Give it back! He pleaded.\")\n",
    "span = doc[4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span.sent.text == \"He pleaded.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I like New York\")\n",
    "span = doc.char_span(7, 15, label=\"GPE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span.text == \"New York\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span.label_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retokenizer.merge\n",
    "\n",
    "Mark a span for merging. The attrs will be applied to the resulting token (if they’re context-dependent token attributes like LEMMA or DEP) or to the underlying lexeme (if they’re context-independent lexical attributes like LOWER or IS_STOP). Writable custom extension attributes can be provided using the \"_\" key and specifying a dictionary that maps attribute names to values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I like David Bowie\")\n",
    "with doc.retokenize() as retokenizer:\n",
    "    attrs = {\"LEMMA\": \"David Bowie\"}\n",
    "    retokenizer.merge(doc[2:4], attrs=attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retokenizer.split\n",
    "\n",
    "\n",
    "Mark a token for splitting, into the specified orths. The heads are required to specify how the new subtokens should be integrated into the dependency tree. The list of per-token heads can either be a token in the original document, e.g. doc[2], or a tuple consisting of the token in the original document and its subtoken index. For example, (doc[3], 1) will attach the subtoken to the second subtoken of doc[3].\n",
    "\n",
    "This mechanism allows attaching subtokens to other newly created subtokens, without having to keep track of the changing token indices. If the specified head token will be split within the retokenizer block and no subtoken index is specified, it will default to 0. Attributes to set on subtokens can be provided as a list of values. They’ll be applied to the resulting token (if they’re context-dependent token attributes like LEMMA or DEP) or to the underlying lexeme (if they’re context-independent lexical attributes like LOWER or IS_STOP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Eu moro em NovoHamburgo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Gostaria de saber o saldo do meu contrato.<&>Claro, por favor, informe o número do contrato<\\r><user_keys>NM_FAZENDA</user_keys><user_intent>QT_SALDO_CONTRATO</user_intent><missing_keys>NR_CONTRATO</missing_keys>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep',\n",
    "                jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "\n",
    "lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"r\"}, {\"ORTH\": \">\"}],\n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"/r\"}, {\"ORTH\": \">\"}],\n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"user_keys\"}, {\"ORTH\": \">\"}],\n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"/user_keys\"}, {\"ORTH\": \">\"}],\n",
    "            \n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"user_intent\"}, {\"ORTH\": \">\"}],\n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"/user_intent\"}, {\"ORTH\": \">\"}],\n",
    "            \n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"missing_keys\"}, {\"ORTH\": \">\"}],\n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"/missing_keys\"}, {\"ORTH\": \">\"}],           \n",
    "            \n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"/n\"}, {\"ORTH\": \">\"}],\n",
    "            {\"LOWER\": \"QT_SALDO_CONTRATO\"}, {\"ORTH\": \">\"},\n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"nro_safra\"}, {\"ORTH\": \">\"}],\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with doc.retokenize() as retokenizer:\n",
    "    heads = [(doc[3], 1), doc[2]]\n",
    "    attrs = {\"POS\": [\"PROPN\", \"PROPN\"],\n",
    "             \"DEP\": [\"pobj\", \"compound\"]}\n",
    "    retokenizer.split(doc[3], [\"Novo\", \"Hamburgo\"], heads=heads, attrs=attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "\n",
    "lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep',\n",
    "                jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '  string with spaces  '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '....string....'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.strip('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'thisthat'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.rstrip('hat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = '     Learn Python  '\n",
    "\n",
    "# remove leading and trailing whitespaces\n",
    "print('Message:', message.strip())\n",
    "\n",
    "# Output: Message: Learn Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '  xoxo love xoxo   '\n",
    "\n",
    "# Leading and trailing whitespaces are removed\n",
    "print(string.strip())\n",
    "\n",
    "# All <whitespace>,x,o,e characters in the left\n",
    "# and right of string are removed\n",
    "print(string.strip(' xoe'))\n",
    "\n",
    "# Argument doesn't contain space\n",
    "# No characters are removed.\n",
    "print(string.strip('stx'))\n",
    "\n",
    "string = 'android is awesome'\n",
    "print(string.strip('an'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_queries = []\n",
    "text = \"Gostaria de saber o saldo do meu contrato\\nClaro, por favor, informe o número do contrato.\\nNM_FAZENDA,ID_SAFRA\\nQT_SALDO_CONTRATO\\nNR_CONTRATO\"\n",
    "user_queries.append(text)\n",
    "text = \"Qual é o saldo do meu contrato?\\nClaro, por favor, informe o número do contrato.\\nNM_FAZENDA,ID_SAFRA\\nQT_SALDO_CONTRATO\\nNR_CONTRATO\"\n",
    "user_queries.append(text)\n",
    "user_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '\\n \\t \\v\\f Como é bom estudar MAC0110! \\n\\t\\n\\v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_limpa = s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_limpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ' linha 1 \\n   linha 2 \\n   linha3 ' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Gostaria de saber o sabor da minha pizza.\\nClaro, por favor, informe o número do contrato.\\nNM_FAZENDA,ID_SAFRA\\nQT_SALDO_CONTRATO\\nNR_CONTRATO\"\n",
    "textList = text.split('\\n')\n",
    "doc_list = nlp(textList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Gostaria de saber o saldo do meu contrato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.similarity(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apples = nlp(\"Gostaria de saber o saldo do meu contrato\")\n",
    "oranges = nlp(\"I like oranges\")\n",
    "apples_oranges = apples.similarity(oranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_utter = textList[0]\n",
    "user_utter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_utter = textList[1]\n",
    "bot_utter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_keys = textList[2].split(',')\n",
    "user_keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_keys = textList[3].split(',')\n",
    "intent_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_keys = textList[4]\n",
    "missing_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analise_similiar.sort()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similaridade e tals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Qual é o saldo do meu contrato?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/home/wklinux/spaCy/query_utter.json\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_chunk(doc, param):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_root_head_lemma_ = (chunk.root.head.lemma_).lower()\n",
    "        if chunk_root_head_lemma_ in [\"gostaria\", \"qual\"]:\n",
    "            if chunk.root.dep_ == param:\n",
    "                return chunk.text\n",
    "\n",
    "# chunk.text, chunk.start, chunk.end, chunk.root.head.lemma_, chunk.root.dep_, chunk.doc            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.chunk.text:            Qual | 2.ch.root:       Qual | 3.chunk.root.dep_:   ROOT | 4.ch.root.head:         Qual | 5.ch.root.head.dep_:   ROOT |  6.chunk.root.head.lemma_:      qual\n",
      "1.chunk.text:         o saldo | 2.ch.root:      saldo | 3.chunk.root.dep_:  nsubj | 4.ch.root.head:         Qual | 5.ch.root.head.dep_:   ROOT |  6.chunk.root.head.lemma_:      qual\n",
      "1.chunk.text:    meu contrato | 2.ch.root:   contrato | 3.chunk.root.dep_:   nmod | 4.ch.root.head:        saldo | 5.ch.root.head.dep_:  nsubj |  6.chunk.root.head.lemma_:     saldo\n"
     ]
    }
   ],
   "source": [
    "# VISUALIAZACAO ESTRUTURA DEP, HEAD, ROOT, LEMMA\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "  chunk_text = chunk.text\n",
    "  chunk_root = chunk.root.text\n",
    "  chunk_root_dep = chunk.root.dep_\n",
    "  chunk_root_head = chunk.root.head.text\n",
    "  chunk_root_head_dep = chunk.root.head.dep_\n",
    "  chunk_root_head_lemma = chunk.root.head.lemma_\n",
    "  \n",
    "  chunk_ents = chunk.ents\n",
    "  chunk_root_ent_type = chunk.root.ent_type_\n",
    "  \n",
    "  print(f'1.chunk.text: {chunk_text:>15} | 2.ch.root: {chunk_root:>10} | 3.chunk.root.dep_: {chunk_root_dep:>6} | 4.ch.root.head: {chunk_root_head:>12} | 5.ch.root.head.dep_: {chunk_root_head_dep:>6} |  6.chunk.root.head.lemma_: {chunk_root_head_lemma:>9}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_chunk(doc, param):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_root_head_lemma_ = (chunk.root.head.lemma_).lower()\n",
    "        if chunk_root_head_lemma_ in [\"gostaria\", \"qual\"]:\n",
    "            if chunk.root.dep_ == param:\n",
    "                return chunk.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. dentro primeiro loop - chunk: Qual | ROOT | Qual é o saldo do meu contrato? \n",
      "\n",
      "\n",
      "2. ============indice: 0 | doc_query: Qual seria o saldo do meu contrato? ===================\n",
      "\n",
      "\n",
      "3. dentro segundo loop - chunk_query: Qual | qual\n",
      "\n",
      "4. deu True para chunk_root_head_lemma_: qual\n",
      "\n",
      "3. dentro segundo loop - chunk_query: seria o saldo | qual\n",
      "\n",
      "4. deu True para chunk_root_head_lemma_: qual\n",
      "\n",
      "3. dentro segundo loop - chunk_query: meu contrato | saldo\n",
      "\n",
      "4. deu False\n",
      "\n",
      "2. ============indice: 1 | doc_query: Gostaria de saber o saldo do meu contrato? ===================\n",
      "\n",
      "\n",
      "3. dentro segundo loop - chunk_query: o saldo | saber\n",
      "\n",
      "4. deu False\n",
      "\n",
      "3. dentro segundo loop - chunk_query: meu contrato | saldo\n",
      "\n",
      "4. deu False\n",
      "\n",
      "2. ============indice: 2 | doc_query: Preciso saber o saldo do meu contrato? ===================\n",
      "\n",
      "\n",
      "3. dentro segundo loop - chunk_query: o saldo | saber\n",
      "\n",
      "4. deu False\n",
      "\n",
      "3. dentro segundo loop - chunk_query: meu contrato | saldo\n",
      "\n",
      "4. deu False\n",
      "\n",
      "1. dentro primeiro loop - chunk: o saldo | nsubj | Qual é o saldo do meu contrato? \n",
      "\n",
      "\n",
      "2. ============indice: 0 | doc_query: Qual seria o saldo do meu contrato? ===================\n",
      "\n",
      "\n",
      "3. dentro segundo loop - chunk_query: Qual | qual\n",
      "\n",
      "4. deu True para chunk_root_head_lemma_: qual\n",
      "\n",
      "3. dentro segundo loop - chunk_query: seria o saldo | qual\n",
      "\n",
      "4. deu True para chunk_root_head_lemma_: qual\n",
      "\n",
      "3. dentro segundo loop - chunk_query: meu contrato | saldo\n",
      "\n",
      "4. deu False\n",
      "\n",
      "2. ============indice: 1 | doc_query: Gostaria de saber o saldo do meu contrato? ===================\n",
      "\n",
      "\n",
      "3. dentro segundo loop - chunk_query: o saldo | saber\n",
      "\n",
      "4. deu False\n",
      "\n",
      "3. dentro segundo loop - chunk_query: meu contrato | saldo\n",
      "\n",
      "4. deu False\n",
      "\n",
      "2. ============indice: 2 | doc_query: Preciso saber o saldo do meu contrato? ===================\n",
      "\n",
      "\n",
      "3. dentro segundo loop - chunk_query: o saldo | saber\n",
      "\n",
      "4. deu False\n",
      "\n",
      "3. dentro segundo loop - chunk_query: meu contrato | saldo\n",
      "\n",
      "4. deu False\n",
      "\n",
      "1. dentro primeiro loop - chunk: meu contrato | nmod | Qual é o saldo do meu contrato? \n",
      "\n",
      "\n",
      "2. ============indice: 0 | doc_query: Qual seria o saldo do meu contrato? ===================\n",
      "\n",
      "\n",
      "3. dentro segundo loop - chunk_query: Qual | qual\n",
      "\n",
      "4. deu True para chunk_root_head_lemma_: qual\n",
      "\n",
      "3. dentro segundo loop - chunk_query: seria o saldo | qual\n",
      "\n",
      "4. deu True para chunk_root_head_lemma_: qual\n",
      "\n",
      "3. dentro segundo loop - chunk_query: meu contrato | saldo\n",
      "\n",
      "4. deu False\n",
      "\n",
      "2. ============indice: 1 | doc_query: Gostaria de saber o saldo do meu contrato? ===================\n",
      "\n",
      "\n",
      "3. dentro segundo loop - chunk_query: o saldo | saber\n",
      "\n",
      "4. deu False\n",
      "\n",
      "3. dentro segundo loop - chunk_query: meu contrato | saldo\n",
      "\n",
      "4. deu False\n",
      "\n",
      "2. ============indice: 2 | doc_query: Preciso saber o saldo do meu contrato? ===================\n",
      "\n",
      "\n",
      "3. dentro segundo loop - chunk_query: o saldo | saber\n",
      "\n",
      "4. deu False\n",
      "\n",
      "3. dentro segundo loop - chunk_query: meu contrato | saldo\n",
      "\n",
      "4. deu False\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "  param_chunk_root_dep_ = chunk.root.dep_\n",
    "  print(f'\\n1. dentro primeiro loop - chunk: {chunk.text} | {param_chunk_root_dep_} | {chunk.doc} \\n')\n",
    "  # print(param_chunk_root_dep_)\n",
    "  i = 0\n",
    "  for i in range(len(data)):\n",
    "    doc_query = nlp(data[i][\"user_utter\"])\n",
    "    print(f'\\n2. ============indice: {i} | doc_query: {doc_query} ===================\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "    for chunk_query in doc_query.noun_chunks:\n",
    "        chunk_root_head_lemma_ = (chunk_query.root.head.lemma_).lower()\n",
    "        print(f'\\n3. dentro segundo loop - chunk_query: {chunk_query.text} | {chunk_root_head_lemma_}')\n",
    "        if chunk_root_head_lemma_ in [\"gostaria\", \"qual\"]:\n",
    "           print(f'\\n4. deu True para chunk_root_head_lemma_: {chunk_root_head_lemma_}')\n",
    "        else:\n",
    "           print(f'\\n4. deu False')  \n",
    "            # if chunk.root.dep_ == \"obj\":\n",
    "            #   print()\n",
    "            #   print()\n",
    "            #   print(chunk.text)\n",
    "  \n",
    "  # print(f'start: {chunk.start:>2} end: {chunk.end:>2} | chunk.text: {chunk.text:>12} || chunk.root: {chunk.root.text:>12} | chunk.root.head: {chunk.root.head.text:>12} | chunk.root.head.lemma_: {chunk.root.head.lemma_:>9} | chunk.root.dep_: {chunk.root.dep_:>6} || chunk.root.head.dep_: {chunk.root.head.dep_:>6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(len(data)):\n",
    "    doc_query = nlp(data[i][\"user_utter\"])\n",
    "    for chunk in doc_query.noun_chunks:\n",
    "        relevant_chunk(doc_query)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[chunk for doc in nlp.pipe(user_queries) for chunk in relevant_chunk(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(len(data)):\n",
    "    print(data[i][\"bot_utter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analise_similiar = []\n",
    "for query in list(user_queries):\n",
    "    doc_query = nlp(query.split('\\n')[0])\n",
    "    indice = doc.similarity(doc_query)\n",
    "    analise_similiar.append((indice, doc_query))\n",
    "    # analise_similiar.append(query)\n",
    "    # print(indice, doc_query)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "  param_chunk_root_dep_ = chunk.root.dep_\n",
    "  print(f'start: {chunk.start:>2} end: {chunk.end:>2} | chunk.text: {chunk.text:>12} || chunk.root: {chunk.root.text:>12} | chunk.root.head: {chunk.root.head.text:>12} | chunk.root.head.lemma_: {chunk.root.head.lemma_:>9} | chunk.root.dep_: {chunk.root.dep_:>6} || chunk.root.head.dep_: {chunk.root.head.dep_:>6}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
