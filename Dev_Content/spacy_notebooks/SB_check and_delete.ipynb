{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "\n",
    "Doc.set_extension(\"pps\", default=False)\n",
    "Doc.set_extension(\"noum\", default=False)\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Quero saber qual é o total de milho que eu tenho para entregar para a Bunge?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Qual é o total de milho que eu vou receber na safra 22/23?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Quero saber quais são meus contrato com a fazenda Santa Rita?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Quero saber quais são meus contrato com a fazenda Passo Fundo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Texto</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Tag_explainned</th>\n",
       "      <th>token_POS</th>\n",
       "      <th>POS_explainned</th>\n",
       "      <th>dep</th>\n",
       "      <th>T. Head</th>\n",
       "      <th>dep explained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Quero</td>\n",
       "      <td>querer</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Quero</td>\n",
       "      <td>(Mood=Ind, Number=Sing, Person=1, Tense=Pres, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>saber</td>\n",
       "      <td>saber</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>Quero</td>\n",
       "      <td>(VerbForm=Inf)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>quais</td>\n",
       "      <td>qual</td>\n",
       "      <td>PRON</td>\n",
       "      <td>pronoun</td>\n",
       "      <td>PRON</td>\n",
       "      <td>pronoun</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>contrato</td>\n",
       "      <td>(Gender=Fem, Number=Plur, PronType=Int)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>são</td>\n",
       "      <td>ser</td>\n",
       "      <td>AUX</td>\n",
       "      <td>auxiliary</td>\n",
       "      <td>AUX</td>\n",
       "      <td>auxiliary</td>\n",
       "      <td>cop</td>\n",
       "      <td>contrato</td>\n",
       "      <td>(Mood=Ind, Number=Plur, Person=3, Tense=Pres, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>meus</td>\n",
       "      <td>meu</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "      <td>det</td>\n",
       "      <td>contrato</td>\n",
       "      <td>(Gender=Masc, Number=Plur, PronType=Prs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>contrato</td>\n",
       "      <td>contrato</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>ccomp</td>\n",
       "      <td>saber</td>\n",
       "      <td>(Gender=Masc, Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>ADP</td>\n",
       "      <td>adposition</td>\n",
       "      <td>ADP</td>\n",
       "      <td>adposition</td>\n",
       "      <td>case</td>\n",
       "      <td>fazenda</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>a</td>\n",
       "      <td>o</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "      <td>det</td>\n",
       "      <td>fazenda</td>\n",
       "      <td>(Definite=Def, Gender=Fem, Number=Sing, PronTy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>fazenda</td>\n",
       "      <td>fazenda</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>nmod</td>\n",
       "      <td>contrato</td>\n",
       "      <td>(Gender=Fem, Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Passo</td>\n",
       "      <td>Passo</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>appos</td>\n",
       "      <td>fazenda</td>\n",
       "      <td>(Gender=Masc, Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Fundo</td>\n",
       "      <td>Fundo</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>flat:name</td>\n",
       "      <td>Passo</td>\n",
       "      <td>(Number=Sing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>punct</td>\n",
       "      <td>Quero</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id     Texto     Lemma    Tag Tag_explainned token_POS POS_explainned   \n",
       "0    0     Quero    querer   VERB           verb      VERB           verb  \\\n",
       "1    1     saber     saber   VERB           verb      VERB           verb   \n",
       "2    2     quais      qual   PRON        pronoun      PRON        pronoun   \n",
       "3    3       são       ser    AUX      auxiliary       AUX      auxiliary   \n",
       "4    4      meus       meu    DET     determiner       DET     determiner   \n",
       "5    5  contrato  contrato   NOUN           noun      NOUN           noun   \n",
       "6    6       com       com    ADP     adposition       ADP     adposition   \n",
       "7    7         a         o    DET     determiner       DET     determiner   \n",
       "8    8   fazenda   fazenda   NOUN           noun      NOUN           noun   \n",
       "9    9     Passo     Passo  PROPN    proper noun     PROPN    proper noun   \n",
       "10  10     Fundo     Fundo  PROPN    proper noun     PROPN    proper noun   \n",
       "11  11         .         .  PUNCT    punctuation     PUNCT    punctuation   \n",
       "\n",
       "          dep   T. Head                                      dep explained  \n",
       "0        ROOT     Quero  (Mood=Ind, Number=Sing, Person=1, Tense=Pres, ...  \n",
       "1       xcomp     Quero                                     (VerbForm=Inf)  \n",
       "2       nsubj  contrato            (Gender=Fem, Number=Plur, PronType=Int)  \n",
       "3         cop  contrato  (Mood=Ind, Number=Plur, Person=3, Tense=Pres, ...  \n",
       "4         det  contrato           (Gender=Masc, Number=Plur, PronType=Prs)  \n",
       "5       ccomp     saber                         (Gender=Masc, Number=Sing)  \n",
       "6        case   fazenda                                                 ()  \n",
       "7         det   fazenda  (Definite=Def, Gender=Fem, Number=Sing, PronTy...  \n",
       "8        nmod  contrato                          (Gender=Fem, Number=Sing)  \n",
       "9       appos   fazenda                         (Gender=Masc, Number=Sing)  \n",
       "10  flat:name     Passo                                      (Number=Sing)  \n",
       "11      punct     Quero                                                 ()  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "token ADP: com | token.pos_: ADP | token.lemma_: com | token.dep_: case | token.head: fazenda\n",
      "\n",
      "[com]\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "token PRON: quais | token.pos_: PRON | token.lemma_: qual | token.dep_: nsubj | token.head: contrato\n",
      "\n",
      "[quais]\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "token NOUN: contrato | token.pos_: NOUN | token.lemma_: contrato | token.dep_: ccomp | token.head: saber\n",
      "\n",
      "[quais, são, meus, contrato, com, a, fazenda, Passo, Fundo]\n",
      "\n",
      "\n",
      "token NOUN: fazenda | token.pos_: NOUN | token.lemma_: fazenda | token.dep_: nmod | token.head: contrato\n",
      "\n",
      "[com, a, fazenda, Passo, Fundo]\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "token AUX: são | token.pos_: AUX | token.lemma_: ser | token.dep_: cop | token.head: contrato\n",
      "\n",
      "[são]\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "token DET: meus | token.pos_: DET | token.lemma_: meu | token.dep_: det | token.head: contrato\n",
      "\n",
      "[meus]\n",
      "\n",
      "\n",
      "token DET: a | token.pos_: DET | token.lemma_: o | token.dep_: det | token.head: fazenda\n",
      "\n",
      "[a]\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "token VERB: Quero | token.pos_: VERB | token.lemma_: querer | token.dep_: ROOT | token.head: Quero\n",
      "\n",
      "[Quero, saber, quais, são, meus, contrato, com, a, fazenda, Passo, Fundo, .]\n",
      "\n",
      "\n",
      "token VERB: saber | token.pos_: VERB | token.lemma_: saber | token.dep_: xcomp | token.head: Quero\n",
      "\n",
      "[saber, quais, são, meus, contrato, com, a, fazenda, Passo, Fundo]\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "token PROPN: Passo | token.pos_: PROPN | token.lemma_: Passo | token.dep_: appos | token.head: fazenda\n",
      "\n",
      "[Passo, Fundo]\n",
      "\n",
      "\n",
      "token PROPN: Fundo | token.pos_: PROPN | token.lemma_: Fundo | token.dep_: flat:name | token.head: Passo\n",
      "\n",
      "[Fundo]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------------------------------')   \n",
    "for token in doc:\n",
    "    if token.pos_ == \"ADP\":\n",
    "        print(f'token ADP: {token} | token.pos_: {token.pos_} | token.lemma_: {token.lemma_} | token.dep_: {token.dep_} | token.head: {token.head}\\n')\n",
    "        print(list(token.subtree))\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "print('---------------------------------------------------------------------------')           \n",
    "for token in doc:\n",
    "    if token.pos_ == \"PRON\":\n",
    "        print(f'token PRON: {token} | token.pos_: {token.pos_} | token.lemma_: {token.lemma_} | token.dep_: {token.dep_} | token.head: {token.head}\\n')\n",
    "        print(list(token.subtree))\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "print('---------------------------------------------------------------------------')   \n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        print(f'token NOUN: {token} | token.pos_: {token.pos_} | token.lemma_: {token.lemma_} | token.dep_: {token.dep_} | token.head: {token.head}\\n')\n",
    "        print(list(token.subtree))\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "print('---------------------------------------------------------------------------')           \n",
    "for token in doc:\n",
    "    if token.pos_ == \"AUX\":\n",
    "        print(f'token AUX: {token} | token.pos_: {token.pos_} | token.lemma_: {token.lemma_} | token.dep_: {token.dep_} | token.head: {token.head}\\n')\n",
    "        print(list(token.subtree))\n",
    "        print()\n",
    "        print()\n",
    "print('---------------------------------------------------------------------------')           \n",
    "for token in doc:\n",
    "    if token.pos_ == \"DET\":\n",
    "        print(f'token DET: {token} | token.pos_: {token.pos_} | token.lemma_: {token.lemma_} | token.dep_: {token.dep_} | token.head: {token.head}\\n')\n",
    "        print(list(token.subtree))\n",
    "        print()\n",
    "        print() \n",
    "print('---------------------------------------------------------------------------')   \n",
    "for token in doc:\n",
    "    if token.pos_ == \"VERB\":\n",
    "        print(f'token VERB: {token} | token.pos_: {token.pos_} | token.lemma_: {token.lemma_} | token.dep_: {token.dep_} | token.head: {token.head}\\n')\n",
    "        print(list(token.subtree))\n",
    "        print()\n",
    "        print() \n",
    "\n",
    "print('---------------------------------------------------------------------------')        \n",
    "for token in doc:\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        print(f'token PROPN: {token} | token.pos_: {token.pos_} | token.lemma_: {token.lemma_} | token.dep_: {token.dep_} | token.head: {token.head}\\n')\n",
    "        print(list(token.subtree))\n",
    "        print()\n",
    "        print()                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == \"PRON\":\n",
    "        print(f'token PRON: {token} | token.pos_: {token.pos_} | token.lemma_: {token.lemma_} | token.dep_: {token.dep_} | token.head: {token.head}\\n')\n",
    "        print(list(token.subtree))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        print(f'token NOUN: {token} | token.pos_: {token.pos_} | token.lemma_: {token.lemma_} | token.dep_: {token.dep_} | token.head: {token.head}\\n')\n",
    "        print(list(token.subtree))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == \"AUX\":\n",
    "        print(f'token AUX: {token} | token.pos_: {token.pos_} | token.lemma_: {token.lemma_} | token.dep_: {token.dep_} | token.head: {token.head}\\n')\n",
    "        print(list(token.subtree))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == \"DET\":\n",
    "        print(f'token DET: {token} | token.pos_: {token.pos_} | token.lemma_: {token.lemma_} | token.dep_: {token.dep_} | token.head: {token.head}\\n')\n",
    "        print(list(token.subtree))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == \"VERB\":\n",
    "        print(f'token VERB: {token} | token.pos_: {token.pos_} | token.lemma_: {token.lemma_} | token.dep_: {token.dep_} | token.head: {token.head}\\n')\n",
    "        print(list(token.subtree))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        print(f'token PROPN: {token} | token.pos_: {token.pos_} | token.lemma_: {token.lemma_} | token.dep_: {token.dep_} | token.head: {token.head}\\n')\n",
    "        print(list(token.subtree))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pps = []\n",
    "for i, token in enumerate(doc):\n",
    "    if token.pos_ == \"ADP\":\n",
    "        print(token, token.pos_)\n",
    "        print(list(token.subtree))\n",
    "        raw_tokens = []\n",
    "        for item in token.subtree:\n",
    "            raw_tokens.append(item.text)\n",
    "        raw_text = \" \".join(raw_tokens)    \n",
    "        print(raw_tokens)\n",
    "        print(raw_text)\n",
    "        pps.append(doc[i:i+len(raw_tokens)])\n",
    "         \n",
    "# print(list(doc.sents))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noum = []\n",
    "for i, token in enumerate(doc):\n",
    "    if token.pos_ == \"PRON\":\n",
    "        print(token, token.pos_)\n",
    "        print(list(token.subtree))\n",
    "        raw_tokens = []\n",
    "        for item in token.subtree:\n",
    "            raw_tokens.append(item.text)\n",
    "        raw_text = \" \".join(raw_tokens)    \n",
    "        print(raw_tokens)\n",
    "        print(raw_text)\n",
    "        noum.append(doc[i:i+len(raw_tokens)])\n",
    "         \n",
    "# print(list(doc.sents))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.pps = pps\n",
    "doc._.noum = noum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "for pp in doc._.pps:\n",
    "    print(pp[i], pp[i].lemma_, pp[i].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "for nm in doc._.noum:\n",
    "    print(pp[i], pp[i].lemma_, pp[i].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.pps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.noum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        print(token, token.pos_)\n",
    "        print(list(token.subtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == \"DET\":\n",
    "        print(token, token.pos_)\n",
    "        print(list(token.subtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.pps = pps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options={'fine_grained': True}\n",
    "displacy.render(doc, style='dep', jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "doc = nlp(\"I was reading the paper.\")\n",
    "token = doc[0]  # 'I'\n",
    "print(token.morph)  # 'Case=Nom|Number=Sing|Person=1|PronType=Prs'\n",
    "print(token.morph.get(\"PronType\"))  # ['Prs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Where are you?\")\n",
    "print(doc[2].morph)  # 'Case=Nom|Person=2|PronType=Prs'\n",
    "print(doc[2].pos_)  # 'PRON'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# English pipelines include a rule-based lemmatizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "print(lemmatizer.mode)  # 'rule'\n",
    "\n",
    "doc = nlp(\"I was reading the paper.\")\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline.edit_tree_lemmatizer import DEFAULT_EDIT_TREE_LEMMATIZER_MODEL\n",
    "config = {\"model\": DEFAULT_EDIT_TREE_LEMMATIZER_MODEL}\n",
    "nlp.add_pipe(\"trainable_lemmatizer\", config=config, name=\"lemmatizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U spacy[lookups]\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"sv\")\n",
    "nlp.add_pipe(\"lemmatizer\", config={\"mode\": \"lookup\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"de\")\n",
    "# Morphologizer (note: model is not yet trained!)\n",
    "nlp.add_pipe(\"morphologizer\")\n",
    "# Rule-based lemmatizer\n",
    "nlp.add_pipe(\"lemmatizer\", config={\"mode\": \"rule\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc)\n",
    "print()\n",
    "\n",
    "print(f\"         chunk.text   | chunk.root.text | chunk.root.dep_ | chunk.root.head.text\\n\")\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(f\"{chunk.text:>20}  | {chunk.root.text:>15} | {chunk.root.dep_:>15} | {chunk.root.head.text:>15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Carros autônomos transferem a responsabilidade do seguro para os fabricantes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc)\n",
    "print()\n",
    "\n",
    "print(f\"         chunk.text   | chunk.root.text | chunk.root.dep_ | chunk.root.head.text\\n\")\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(f\"{chunk.text:>20}  | {chunk.root.text:>15} | {chunk.root.dep_:>15} | {chunk.root.head.text:>15}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atencao, possivel solucao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"T. Head POS\", \"child\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"T. Head POS\"] = token.head.pos_\n",
    "    lemmatization.loc[i,\"child\"] = [child for child in token.children]\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f\"token.text: {token.text} | token.dep_: {token.dep_} | token.head.text: {token.head.text} token.head.pos_: {token.head.pos_} | child: {[child for child in token.children]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "\n",
    "# Finding a verb with a subject from below — good\n",
    "verbs = set()\n",
    "for possible_subject in doc:\n",
    "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
    "        verbs.add(possible_subject.head)\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from thinc.api import set_gpu_allocator, require_gpu\n",
    "\n",
    "# Use the GPU, with memory allocations directed via PyTorch.\n",
    "# This prevents out-of-memory errors that would otherwise occur from competing\n",
    "# memory pools.\n",
    "set_gpu_allocator(\"pytorch\")\n",
    "require_gpu(0)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "for doc in nlp.pipe([\"some text\", \"some other text\"]):\n",
    "    tokvecs = doc._.trf_data.tensors[-1]\n",
    "    print(doc.text, doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple shares rose on the news. Apple pie is delicious.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "from spacy.tokens import Span\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionamos um tokenizador\n",
    "nlp.add_pipe(\"tok2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"tok2vec\").initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "doc = nlp(\"Apple shares rose on the news. Apple pie is delicious.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple shares rose on the news. Apple pie is delicious.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionamos um tokenizador\n",
    "nlp.add_pipe(\"tok2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionamos um tagger\n",
    "nlp.add_pipe(\"tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionamos um parser\n",
    "nlp.add_pipe(\"parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionamos um ner\n",
    "nlp.add_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"entity_linker\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando o pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.analyze_pipes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
