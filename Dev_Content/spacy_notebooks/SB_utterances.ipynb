{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poc Utterances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User inttents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_utterance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Gostaria de saber o saldo do meu contrato\"\n",
    "user_utterance.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Quero saber o saldo do meu contrato\"\n",
    "user_utterance.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Qual é o saldo do meu contrato?\"\n",
    "user_utterance.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_utterance.index(\"Quero saber o saldo do meu contrato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_utterance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_queries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Gostaria de saber o saldo do meu contrato\\nClaro, por favor, informe o número do contrato.\\nNM_FAZENDA,ID_SAFRA\\nQT_SALDO_CONTRATO\\nNR_CONTRATO\"\n",
    "user_queries.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Qual é o saldo do meu contrato?\\nClaro, por favor, informe o número do contrato.\\nNM_FAZENDA,ID_SAFRA\\nQT_SALDO_CONTRATO\\nNR_CONTRATO\"\n",
    "user_queries.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_queries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Por favor, informar o saldo do contrato 657S para safra 22/23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Gostaria de pedir um contrato de soja e outro contrato de milho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "doc = nlp(user_utterance[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx, doc)\n",
    "print()\n",
    "\n",
    "displacy.render(doc, style='dep',\n",
    "                jupyter=True, options={'distance': 120})\n",
    "print()\n",
    "for chunk in doc.noun_chunks:\n",
    "  print(f'start: {chunk.start:>2} end: {chunk.end:>2} | chunk.text: {chunk.text:>12} || chunk.root: {chunk.root.text:>12} | chunk.root.head: {chunk.root.head.text:>12} | chunk.root.head.lemma_: {chunk.root.head.lemma_:>9} | chunk.root.dep_: {chunk.root.dep_:>6} || chunk.root.head.dep_: {chunk.root.head.dep_:>6}')\n",
    "\n",
    "print()\n",
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "\n",
    "lemmatization  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_chunk_yield(doc):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_root_head_lemma_ = (chunk.root.head.lemma_).lower()\n",
    "        if chunk_root_head_lemma_ in [\"gostaria\", \"qual\"]:\n",
    "            if chunk.root.dep_ == \"ROOT\":\n",
    "                yield chunk.text, chunk.start, chunk.end, chunk.root.head.lemma_, chunk.root.dep_, chunk.doc\n",
    "    \n",
    "    \n",
    "    \n",
    "    # if chunk.root.dep_ == \"obj\" or chunk.root.dep_ == \"nsubj\" or chunk.root.dep_ == \"ROOT\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_chunk(doc):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_root_head_lemma_ = (chunk.root.head.lemma_).lower()\n",
    "        if chunk_root_head_lemma_ in [\"gostaria\", \"qual\"]:\n",
    "            if chunk.root.dep_ == \"ROOT\":\n",
    "                return chunk.text, chunk.start, chunk.end, chunk.root.head.lemma_, chunk.root.dep_, chunk.doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[chunk for doc in nlp.pipe(user_queries) for chunk in relevant_chunk_yield(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_text, chunk_start, chunk_end, chunk_root_head_lemma_, chunk_root_dep_, chunk_doc = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = (chunk for doc in nlp.pipe(user_queries) for chunk in relevant_chunk_yield(doc))\n",
    "# print()\n",
    "# print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in g:\n",
    "    chunk_text, chunk_start, chunk_end, chunk_root_head_lemma_, chunk_root_dep_, chunk_doc = i\n",
    "    print(f'{chunk_text} | {chunk_start} | {chunk_end} | {chunk_root_head_lemma_} | {chunk_root_dep_} {chunk_doc} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{chunk_text} | {chunk_start} | {chunk_end} | {chunk_root_head_lemma_} | {chunk_root_dep_} {chunk_doc} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_amostra = nlp(texts[1])\n",
    "print(doc_amostra)\n",
    "\n",
    "displacy.render(doc_amostra, style=\"dep\")\n",
    "\n",
    "\n",
    "print(doc_amostra)\n",
    "print()\n",
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc_amostra:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Poderia me detalhar o saldo total do meu contrato de soja e total entregue de milho para as safras 22/23 e 23/24.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Por favor, informar o saldo do contrato 657S para safra 22/23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep',\n",
    "                jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc)\n",
    "print()\n",
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "\n",
    "lemmatization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prova dos 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(texts[2])\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc)\n",
    "print()\n",
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "  print(f'chunk.text: {chunk.text:>15} | start: {chunk.start:>3} end: {chunk.end:>3} | chunk.root.head.lemma_: {chunk.root.head.lemma_:>12} | chunk.root.dep_: {chunk.root.dep_:>8} | chunk.root: {chunk.root.text:>12} | chunk.root.head: {chunk.root.head.text:>15}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep',\n",
    "                jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "  print(f'chunk.text: {chunk.text:>15} | start: {chunk.start:>3} end: {chunk.end:>3} | chunk.root.head.lemma_: {chunk.root.head.lemma_:>12} | chunk.root.dep_: {chunk.root.dep_:>8} | chunk.root: {chunk.root.text:>12} | chunk.root.head: {chunk.root.head.text:>15}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep(doc):\n",
    "    for token in doc:\n",
    "        if token.lemma_ in [\"saber\", \"querer\", \"poder\"]:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = (doc for doc in nlp.pipe(texts) if keep(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pps = []\n",
    "for i, token in enumerate(doc):\n",
    "    print(i)\n",
    "    if token.pos_ == \"ADP\":\n",
    "        print(token, token.text)\n",
    "        print(list(token.subtree))\n",
    "        raw_tokens = []\n",
    "        for item in token.subtree:\n",
    "            raw_tokens.append(item.text)\n",
    "        raw_text = \" \".join(raw_tokens)    \n",
    "        print(raw_tokens)\n",
    "        print(raw_text) \n",
    "        pps.append(raw_text)  \n",
    "        pps.append(doc[i:i+len(raw_tokens)]) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.pps = pps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.pps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(doc.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "assert doc.has_annotation(\"SENT_START\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "import spacy\n",
    "\n",
    "text = \"this is a sentence...hello...and another sentence.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "print(\"Before:\", [sent.text for sent in doc.sents])\n",
    "\n",
    "@Language.component(\"set_custom_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \"...\":\n",
    "            doc[token.i + 1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
    "doc = nlp(text)\n",
    "print(\"After:\", [sent.text for sent in doc.sents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "class CustomEnglishDefaults(English.Defaults):\n",
    "    stop_words = set([\"custom\", \"stop\"])\n",
    "\n",
    "class CustomEnglish(English):\n",
    "    lang = \"custom_en\"\n",
    "    Defaults = CustomEnglishDefaults\n",
    "\n",
    "nlp1 = English()\n",
    "nlp2 = CustomEnglish()\n",
    "\n",
    "print(nlp1.lang, [token.is_stop for token in nlp1(\"custom stop\")])\n",
    "print(nlp2.lang, [token.is_stop for token in nlp2(\"custom stop\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp2(\"This is love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "class CustomEnglishDefaults(English.Defaults):\n",
    "    stop_words = set([\"custom\", \"stop\"])\n",
    "\n",
    "@spacy.registry.languages(\"custom_en\")\n",
    "class CustomEnglish(English):\n",
    "    lang = \"custom_en\"\n",
    "    Defaults = CustomEnglishDefaults\n",
    "\n",
    "# This now works! 🎉\n",
    "nlp = spacy.blank(\"custom_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "from spacy.attrs import IS_TITLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from spacy.lang.en import English\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "@Language.factory(\"rest_countries\")\n",
    "class RESTCountriesComponent:\n",
    "    def __init__(self, nlp, name, label=\"GPE\"):\n",
    "        r = requests.get(\"https://restcountries.com/v2/all\")\n",
    "        r.raise_for_status()  # make sure requests raises an error if it fails\n",
    "        countries = r.json()\n",
    "        # Convert API response to dict keyed by country name for easy lookup\n",
    "        self.countries = {c[\"name\"]: c for c in countries}\n",
    "        self.label = label\n",
    "        # Set up the PhraseMatcher with Doc patterns for each country name\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add(\"COUNTRIES\", [nlp.make_doc(c) for c in self.countries.keys()])\n",
    "        # Register attributes on the Span. We'll be overwriting this based on\n",
    "        # the matches, so we're only setting a default value, not a getter.\n",
    "        Span.set_extension(\"is_country\", default=None)\n",
    "        Span.set_extension(\"country_capital\", default=None)\n",
    "        Span.set_extension(\"country_latlng\", default=None)\n",
    "        Span.set_extension(\"country_flag\", default=None)\n",
    "        # Register attribute on Doc via a getter that checks if the Doc\n",
    "        # contains a country entity\n",
    "        Doc.set_extension(\"has_country\", getter=self.has_country)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        spans = []  # keep the spans for later so we can merge them afterwards\n",
    "        for _, start, end in self.matcher(doc):\n",
    "            # Generate Span representing the entity & set label\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            # Set custom attributes on entity. Can be extended with other data\n",
    "            # returned by the API, like currencies, country code, calling code etc.\n",
    "            entity._.set(\"is_country\", True)\n",
    "            entity._.set(\"country_capital\", self.countries[entity.text][\"capital\"])\n",
    "            entity._.set(\"country_latlng\", self.countries[entity.text][\"latlng\"])\n",
    "            entity._.set(\"country_flag\", self.countries[entity.text][\"flag\"])\n",
    "            spans.append(entity)\n",
    "        # Overwrite doc.ents and add entity – be careful not to replace!\n",
    "        doc.ents = list(doc.ents) + spans\n",
    "        return doc  # don't forget to return the Doc!\n",
    "\n",
    "    def has_country(self, doc):\n",
    "        \"\"\"Getter for Doc attributes. Since the getter is only called\n",
    "        when we access the attribute, we can refer to the Span's 'is_country'\n",
    "        attribute here, which is already set in the processing step.\"\"\"\n",
    "        return any([entity._.get(\"is_country\") for entity in doc.ents])\n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"rest_countries\", config={\"label\": \"GPE\"})\n",
    "doc = nlp(\"Some text about Colombia and the Czech Republic\")\n",
    "print(\"Pipeline\", nlp.pipe_names)  # pipeline contains component name\n",
    "print(\"Doc has countries\", doc._.has_country)  # Doc contains countries\n",
    "for ent in doc.ents:\n",
    "    if ent._.is_country:\n",
    "        print(ent.text, ent.label_, ent._.country_capital, ent._.country_latlng, ent._.country_flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Give it back! He pleaded.\")\n",
    "token = doc[0]\n",
    "assert token.check_flag(IS_TITLE) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction from subclass\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Construction from scratch\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.language import Language\n",
    "nlp = Language(Vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import your_custom_entity_recognizer\n",
    "from spacy.training import biluo_tags_to_spans\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"custom_ner_wrapper\")\n",
    "def custom_ner_wrapper(doc):\n",
    "    words = [token.text for token in doc]\n",
    "    custom_entities = your_custom_entity_recognizer(words)\n",
    "    doc.ents = biluo_tags_to_spans(doc, custom_entities)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_my_product(text):\n",
    "    products = [\"spaCy\", \"Thinc\", \"displaCy\"]\n",
    "    return text in products\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_PRODUCT = nlp.vocab.add_flag(is_my_product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I like spaCy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.i, token.text, token.left_edge, token.right_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[1].n_rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[1].head.pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[2].get_morph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[0].lang_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[2].check_flag(MY_PRODUCT) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[2]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
