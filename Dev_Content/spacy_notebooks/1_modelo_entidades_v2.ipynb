{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solucao para identificacao das entidades do documento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Importaçoes de modulos, pipeline e funcoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "from spacy.tokens import Span\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = nlp.remove_pipe('ner')\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principais funcoes\n",
    "\n",
    "def show_ent_new(text, patterns):\n",
    "    #nlp = spacy.blank(\"pt\")\n",
    "    #ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    ruler.add_patterns(patterns)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tokens = []\n",
    "    ents = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        span = doc.char_span(ent.start_char, ent.end_char, label=ent.label_)\n",
    "        ents.append(span)\n",
    "        \n",
    "    for token in doc:\n",
    "        start = token.idx\n",
    "        end = start + len(token)\n",
    "        tokens.append((token.text, start, end))\n",
    "        \n",
    "    return doc, tokens, ents\n",
    "\n",
    "\n",
    "def write_patterns_to_file(patterns, colors, filename):\n",
    "    data = {\"patterns\": patterns, \"colors\": colors}\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "def load_patterns_and_colors(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        patterns = data[\"patterns\"]\n",
    "        colors = data[\"colors\"]\n",
    "    return patterns, colors   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component         Assigns               Requires   Scores             Retokenizes\n",
      "-   ---------------   -------------------   --------   ----------------   -----------\n",
      "0   tok2vec           doc.tensor                                          False      \n",
      "                                                                                     \n",
      "1   morphologizer     token.morph                      pos_acc            False      \n",
      "                      token.pos                        morph_acc                     \n",
      "                                                       morph_per_feat                \n",
      "                                                                                     \n",
      "2   parser            token.dep                        dep_uas            False      \n",
      "                      token.head                       dep_las                       \n",
      "                      token.is_sent_start              dep_las_per_type              \n",
      "                      doc.sents                        sents_p                       \n",
      "                                                       sents_r                       \n",
      "                                                       sents_f                       \n",
      "                                                                                     \n",
      "3   lemmatizer        token.lemma                      lemma_acc          False      \n",
      "                                                                                     \n",
      "4   attribute_ruler                                                       False      \n",
      "                                                                                     \n",
      "5   entity_ruler      doc.ents                         ents_f             False      \n",
      "                      token.ent_type                   ents_p                        \n",
      "                      token.ent_iob                    ents_r                        \n",
      "                                                       ents_per_type                 \n",
      "\n",
      "\u001b[38;5;2m✔ No problems found.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'morphologizer': {'assigns': ['token.morph', 'token.pos'],\n",
       "   'requires': [],\n",
       "   'scores': ['pos_acc', 'morph_acc', 'morph_per_feat'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'morphologizer': [],\n",
       "  'parser': [],\n",
       "  'lemmatizer': [],\n",
       "  'attribute_ruler': [],\n",
       "  'entity_ruler': []},\n",
       " 'attrs': {'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.morph': {'assigns': ['morphologizer'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.pos': {'assigns': ['morphologizer'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['entity_ruler'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['entity_ruler'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['entity_ruler'], 'requires': []}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Execucao das configuraçoes e testes (neste caso, em processo de criacao)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ja temos o processo de transferir e carregar patterns de um json, mas estamos em processo de testes e ajustes do modelo de pattern, neste caso e melhor ter aqui o bloco de configuraçao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "        \"CULTURA\": \"linear-gradient(90deg, #2ADB5E, #1FA346)\", \n",
    "        \"TOTAL\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\",\n",
    "        \"ENTREGUE\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\",\n",
    "        \"SALDO\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\", \n",
    "        \"FAZENDA\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\", \n",
    "        \"SAFRA\": \"linear-gradient(90deg, #FFC90E, #BA930A)\", \n",
    "        \"CONTRATO\": \"linear-gradient(90deg, #B5B5B5, #8A8A8A)\"}\n",
    "\n",
    "\n",
    "patternsOthers = [{\"label\": \"PERSON\", \"pattern\": \"Daniel\", \"id\": \"daniel\"},\n",
    "                  {\"label\": \"PERSON\", \"pattern\": \"Antônio\", \"id\": \"antonio\"},\n",
    "                  {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"fast\"}, {\"LOWER\": \"innovation\"}], \"id\": \"fast-innovation\"},\n",
    "                  {\"label\": \"ORG\", \"pattern\": {\"LOWER\": \"agrobi\"}, \"id\": \"agrobi\"},\n",
    "                  {\"label\": \"ORG\", \"pattern\": {\"LOWER\": \"AgroBi\"}, \"id\": \"agrobi\"},\n",
    "                  {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"agro\"}, {\"LOWER\": \"bi\"}], \"id\": \"agrobi\"},\n",
    "                  ] \n",
    " \n",
    "\n",
    "patternsCult = [\n",
    "    {\n",
    "        \"label\":\"CULTURA\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"soja\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"milho\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"sorgo\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"trigo\", \"OP\":\"?\"},\n",
    "            \n",
    "        ]    \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "patternsQuant = [\n",
    "    {\n",
    "        \"label\":\"TOTAL\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"total\"},\n",
    "            {\"LOWER\": \"quantidade\", \"OP\":\"?\"},\n",
    "            \n",
    "        ]    \n",
    "    },\n",
    "    {\n",
    "        \"label\":\"ENTREGUE\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"quantidade\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"entregue\",\"OP\":\"?\"},\n",
    "            {\"LOWER\": \"entregues\",\"OP\":\"?\"},\n",
    "            {\"LOWER\": \"entregado\",\"OP\":\"?\"},\n",
    "            {\"LOWER\": \"entreguei\",\"OP\":\"?\"},\n",
    "        ]    \n",
    "    },\n",
    "    {\n",
    "        \"label\":\"SALDO\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"saldo\"},\n",
    "            {\"LOWER\": \"quantidade\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"total\", \"OP\":\"?\"},\n",
    "        ]    \n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "patternsSafra = [\n",
    "    {\n",
    "        \"label\":\"SAFRA\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"safra\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"safras\", \"OP\":\"?\"},\n",
    "            {\"SHAPE\": \"dd/dd\"},\n",
    "        ]    \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "patternsFazenda_antigo = [\n",
    "    {\n",
    "        \"label\":\"FAZENDA\",\n",
    "        \"pattern\":[\n",
    "            {\"ORTH\": \"Santa\"}, {\"ORTH\": \"Rita\"},\n",
    "            {\"ORTH\": \"Passo\"}, {\"ORTH\": \"Fundo\"},\n",
    "            {\"ORTH\": \"Bela\"}, {\"ORTH\": \"Vista\"},\n",
    "            [{\"LOWER\": \"bela\"}, {\"LOWER\": \"vista\"}],\n",
    "        ]\n",
    "        \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "patternsOthersFazenda = [{\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"santa\"}, {\"LOWER\": \"rita\"}], \"id\": \"faz-santa-rita\"},\n",
    "                         {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"bela\"}, {\"LOWER\": \"vista\"}], \"id\": \"faz-bela-vista\"},\n",
    "                         {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"passo\"}, {\"LOWER\": \"fundo\"}], \"id\": \"faz-passo-fundo\"},\n",
    "                         {\"label\": \"FAZENDA\", \"pattern\": [{\"LOWER\": \"minha\"}, {\"LOWER\": \"fazenda\"}], \"id\": \"faz-produtor\"}\n",
    "                        ]\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "patternsContrato = [\n",
    "    {\n",
    "        \"label\":\"CONTRATO\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"contrato\", \"OP\":\"*\"},\n",
    "            {\"SHAPE\": \"dddX\", \"OP\":\"*\"},\n",
    "            {\"LOWER\": \"contratos\", \"OP\":\"*\"},\n",
    "            \n",
    "        ]\n",
    "        \n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "patterns = patternsCult + patternsQuant + patternsOthersFazenda + patternsSafra + patternsContrato + patternsOthers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Quanto de \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #2ADB5E, #1FA346); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    milho\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CULTURA</span>\n",
       "</mark>\n",
       " eu já \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #09D6FF, #08A0D1); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    entreguei\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ENTREGUE</span>\n",
       "</mark>\n",
       " pelo \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #B5B5B5, #8A8A8A); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    contrato 501S\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CONTRATO</span>\n",
       "</mark>\n",
       " pela \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    minha fazenda\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAZENDA</span>\n",
       "</mark>\n",
       "?</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"Quanto de milho eu já entreguei pelo contrato 501S pela minha fazenda?\"\n",
    "\n",
    "\n",
    "\n",
    "doc, tokens, ents = show_ent_new(text, patterns=patterns)\n",
    "\n",
    "## GARANTIR que a ordem dos Tokens esteja correta (ascendente)\n",
    "seq_tokens_id = []\n",
    "\n",
    "seq_tokens_valor = []\n",
    "\n",
    "\n",
    "## tokens_ids + Lista de todos os Tokens da frase\n",
    "for token in doc:\n",
    "    seq_tokens_id.append(token.i)\n",
    "    \n",
    "    \n",
    "displacy.render(doc, style=\"ent\", options={\"colors\": colors})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 |                milho |  CULTURA |               |  3 || 10 | 15\n",
      " 5 |            entreguei | ENTREGUE |               |  6 || 22 | 31\n",
      " 7 |        contrato 501S | CONTRATO |               |  9 || 37 | 50\n",
      "10 |        minha fazenda |  FAZENDA | faz-produtor  | 12 || 56 | 69\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.start:>2} | {ent.text:>20} | {ent.label_:>8} | {ent.id_:>12}  | {ent.end:>2} || {ent.start_char:>2} | {ent.end_char:>2}')\n",
    "    seq_tokens_valor.append(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[milho, entreguei, contrato 501S, minha fazenda]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tokens_valor.append([ent for ent in doc.ents])\n",
    "seq_tokens_valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entreguei'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_root = [token.text for token in doc if token.dep_ == \"ROOT\"][0]\n",
    "token_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo de criaçao de uma lista com tokens de valor para o processo\n",
    "\n",
    "seq_tokens_valor.append(token_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Quanto de \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #2ADB5E, #1FA346); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    milho\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CULTURA</span>\n",
       "</mark>\n",
       " eu já \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #09D6FF, #08A0D1); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    entreguei\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ENTREGUE</span>\n",
       "</mark>\n",
       " pelo \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #B5B5B5, #8A8A8A); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    contrato 501S\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CONTRATO</span>\n",
       "</mark>\n",
       " pela fazenda \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Santa Rita\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAZENDA</span>\n",
       "</mark>\n",
       " que produz \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #2ADB5E, #1FA346); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    trigo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CULTURA</span>\n",
       "</mark>\n",
       "?</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\", options={\"colors\": colors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entreguei', [milho, entreguei, contrato 501S]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tokens_valor.append([ent for ent in doc.ents])\n",
    "seq_tokens_valor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listagem das entidades com posicao dos tokens e posicao de inicio e fim de caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 |                milho |  CULTURA |  3 || 10 | 15\n",
      " 5 |            entreguei | ENTREGUE |  6 || 22 | 31\n",
      " 7 |        contrato 501S | CONTRATO |  9 || 37 | 50\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.start:>2} | {ent.text:>20} | {ent.label_:>8} | {ent.end:>2} || {ent.start_char:>2} | {ent.end_char:>2}')\n",
    "    seq_tokens_valor.append(ent.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algumas rotinas de tratamento de algumas entidades como contratos e safra que podem ter mais dados a serem descobertos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ele esta falando somente de um contrato\n",
      "nao ha numero do contrato\n",
      "Ele esta falando somente de um contrato\n",
      "esse e o nro do contrato: 501S\n"
     ]
    }
   ],
   "source": [
    "# Tratamento para contratos\n",
    "for token in doc:\n",
    "    if token.ent_type_ == \"CONTRATO\":\n",
    "        # print(token.text, token.ent_type_, token.shape_)\n",
    "        qtd_contratops = token.morph.get(\"Number\")\n",
    "        # print(qtd_contratops)\n",
    "        if \"Plur\" in qtd_contratops:\n",
    "            print(\"Ele esta falando mais de um contrato\")\n",
    "        else:\n",
    "             print(\"Ele esta falando somente de um contrato\")   \n",
    "        if token.shape_ == \"dddX\":\n",
    "            nro_contrato = token.text\n",
    "            print(f'esse e o nro do contrato: {nro_contrato}')\n",
    "        else:\n",
    "            print(f'nao ha numero do contrato')\n",
    "    \n",
    "    # elif token.ent_type_ == \"CONTRATO\":    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entreguei',\n",
       " [milho, entreguei, contrato 501S],\n",
       " 'milho',\n",
       " 'entreguei',\n",
       " 'contrato 501S']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tokens_valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[milho, entreguei, contrato 501S]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ent for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[contrato 501S]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ent for ent in doc.ents if ent.label_ == \"CONTRATO\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analise de noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.chunk.text: Quanto de milho |   0   3 |   0 15 \n",
      "1.chunk.text:              eu |   3   4 |  16 18 \n",
      "1.chunk.text:   contrato 501S |   7   9 |  37 50 \n"
     ]
    }
   ],
   "source": [
    "# VISUALIAZACAO RESUMIDA _ ENTS\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "  chunk_text = chunk.text\n",
    "  chunk_root = chunk.root.text\n",
    "  chunk_root_dep = chunk.root.dep_\n",
    "  chunk_root_head = chunk.root.head.text\n",
    "  chunk_root_head_dep = chunk.root.head.dep_\n",
    "  chunk_root_head_lemma = chunk.root.head.lemma_\n",
    "  \n",
    "  chunk_ents = chunk.ents\n",
    "  chunk_root_ent_type = chunk.root.ent_type_\n",
    "  \n",
    "  print(f'1.chunk.text: {chunk_text:>15} |  {chunk.start:>2}  {chunk.end:>2} |  {chunk.start_char:>2} {chunk.end_char:>2} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.chunk.text: Quanto de milho | 2.ch.root:      milho | 3.chunk.root.dep_:        obl | 4.chunk_root_ent_type      CULTURA | 5.chunk_ents [milho] \n",
      "1.chunk.text:              eu | 2.ch.root:         eu | 3.chunk.root.dep_:      nsubj | 4.chunk_root_ent_type              | 5.chunk_ents [] \n",
      "1.chunk.text:   contrato 501S | 2.ch.root:   contrato | 3.chunk.root.dep_:        obl | 4.chunk_root_ent_type     CONTRATO | 5.chunk_ents [contrato 501S] \n"
     ]
    }
   ],
   "source": [
    "# VISUALIAZACAO RESUMIDA _ ENTS\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "  chunk_text = chunk.text\n",
    "  chunk_root = chunk.root.text\n",
    "  chunk_root_dep = chunk.root.dep_\n",
    "  chunk_root_head = chunk.root.head.text\n",
    "  chunk_root_head_dep = chunk.root.head.dep_\n",
    "  chunk_root_head_lemma = chunk.root.head.lemma_\n",
    "  \n",
    "  chunk_ents = chunk.ents\n",
    "  chunk_root_ent_type = chunk.root.ent_type_\n",
    "  \n",
    "  print(f'1.chunk.text: {chunk_text:>15} | 2.ch.root: {chunk_root:>10} | 3.chunk.root.dep_: {chunk_root_dep:>10} | 4.chunk_root_ent_type {chunk_root_ent_type:>12} | 5.chunk_ents {chunk_ents} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.chunk.text: Quanto de milho | 2.ch.root:      milho | 3.chunk.root.dep_:             obl | 4.ch.root.head:    entreguei | 5.ch.root.head.dep_:       ROOT |  6.chunk.root.head.lemma_: entregueir\n",
      "1.chunk.text:              eu | 2.ch.root:         eu | 3.chunk.root.dep_:           nsubj | 4.ch.root.head:    entreguei | 5.ch.root.head.dep_:       ROOT |  6.chunk.root.head.lemma_: entregueir\n",
      "1.chunk.text:   contrato 501S | 2.ch.root:   contrato | 3.chunk.root.dep_:             obl | 4.ch.root.head:    entreguei | 5.ch.root.head.dep_:       ROOT |  6.chunk.root.head.lemma_: entregueir\n"
     ]
    }
   ],
   "source": [
    "# VISUALIAZACAO ESTRUTURA DEP, HEAD, ROOT, LEMMA\n",
    "chunks_valor = []\n",
    "\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "  chunk_text = chunk.text\n",
    "  chunks_valor.append(chunk_text)\n",
    "  chunk_root = chunk.root.text\n",
    "  chunk_root_dep = chunk.root.dep_\n",
    "  chunk_root_head = chunk.root.head.text\n",
    "  chunk_root_head_dep = chunk.root.head.dep_\n",
    "  chunk_root_head_lemma = chunk.root.head.lemma_\n",
    "  chunk_ents = chunk.ents\n",
    "  chunk_root_ent_type = chunk.root.ent_type_\n",
    "  \n",
    "  print(f'1.chunk.text: {chunk_text:>15} | 2.ch.root: {chunk_root:>10} | 3.chunk.root.dep_: {chunk_root_dep:>15} | 4.ch.root.head: {chunk_root_head:>12} | 5.ch.root.head.dep_: {chunk_root_head_dep:>10} |  6.chunk.root.head.lemma_: {chunk_root_head_lemma:>9}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "pos_tagging = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"T_texto\",\"T_lemma_\", \"T_pos_\", \"T_tag_\", \"T_dep_\", \"T_shape_\", \"T_is_alpha\", \"T_is_stop\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    pos_tagging.loc[i,\"id\"] = token.i\n",
    "    pos_tagging.loc[i,\"T_texto\"] = token.text\n",
    "    pos_tagging.loc[i,\"T_lemma_\"] = token.lemma_\n",
    "    pos_tagging.loc[i,\"T_pos_\"] = token.pos_\n",
    "    pos_tagging.loc[i,\"T_tag_\"] = token.tag_\n",
    "    pos_tagging.loc[i,\"T_dep_\"] = token.dep_\n",
    "    pos_tagging.loc[i,\"T_shape_\"] = token.shape_\n",
    "    pos_tagging.loc[i,\"T_is_alpha\"] = token.is_alpha\n",
    "    pos_tagging.loc[i,\"T_is_stop\"] = token.is_stop\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "pos_tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)\n",
    "print(ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep',\n",
    "                jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safra_ents = [ent for ent in doc.ents if ent.label_ == \"SAFRA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.start:>2} | {ent.text:>20} | {ent.label_:>8} | {ent.end:>2} || {ent.start_char:>2} | {ent.end_char:>2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar char vs i do Token\n",
    "chars_to_tokens = {}\n",
    "for token in doc:\n",
    "    for i in range(token.idx, token.idx + len(token.text)):\n",
    "        chars_to_tokens[i] = token.i\n",
    "        #print(i, chars_to_tokens[i])\n",
    "        \n",
    "chars_to_tokens[24]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar i vs token.text\n",
    "tokens_id_text = {}\n",
    "for token in doc:\n",
    "    tokens_id_text[token.i] = token.text\n",
    "    \n",
    "for key, value in tokens_id_text.items():\n",
    "    if key == 4:\n",
    "        print(value)    # saldo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:    \n",
    "    token_i =  token.i\n",
    "    token_text = token.text\n",
    "    token_ent_id = token.ent_id\n",
    "    token_lemma = token.lemma_\n",
    "    token_pos = token.pos_\n",
    "\n",
    "    token_dep = token.dep_\n",
    "    token_shape = token.shape_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ent for ent in doc.ents if ent.label_ == \"CONTRATO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ent for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[i].i, doc[i].text, doc[i].ent_type_, doc[i].shape_, doc[i].ent_iob_, doc[i].idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXEMPLOS: Busca de informaçoes morph\n",
    "\n",
    "{doc[i].morph.get(\"VerbForm\")} | {doc[i].morph.get(\"Tense\")} | {doc[i].morph.get(\"Number\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significative_tokens = pd.DataFrame(data=[], \\\n",
    "  columns=[\"idx\", \"T_texto\",\"T_shape_\", \"T_ent_type_\", \"T_ent_id_\", \"T_ent_iob_\", \"T_pos_\", \"T_lemma_\", \"T_dep_\", \"T_head\"])\n",
    "i = 0\n",
    "for idx in seq_tokens_id:\n",
    "    significative_tokens.loc[i,\"idx\"] = doc[idx].i\n",
    "    significative_tokens.loc[i,\"T_texto\"] = doc[idx].text\n",
    "    significative_tokens.loc[i,\"T_shape_\"] = doc[idx].shape_\n",
    "    significative_tokens.loc[i,\"T_ent_type_\"] = doc[idx].ent_type_\n",
    "    significative_tokens.loc[i,\"T_ent_id_\"] = doc[idx].ent_id_\n",
    "    significative_tokens.loc[i,\"T_ent_iob_\"] = doc[idx].ent_iob_\n",
    "    significative_tokens.loc[i,\"T_pos_\"] = doc[idx].pos_\n",
    "    significative_tokens.loc[i,\"T_lemma_\"] = doc[idx].lemma_\n",
    "    significative_tokens.loc[i,\"T_dep_\"] = doc[idx].dep_\n",
    "    significative_tokens.loc[i,\"T_head\"] = doc[idx].head\n",
    "\n",
    "    \n",
    "\n",
    "    i = i+1\n",
    "\n",
    "significative_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "pos_tagging = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"T_texto\",\"T_lemma_\", \"T_pos_\", \"T_tag_\", \"T_dep_\", \"T_head\", \"T_is_sent_start\", \"T_shape_\", \"T_is_alpha\", \"T_is_stop\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    pos_tagging.loc[i,\"id\"] = token.i\n",
    "    pos_tagging.loc[i,\"T_texto\"] = token.text\n",
    "    pos_tagging.loc[i,\"T_lemma_\"] = token.lemma_\n",
    "    pos_tagging.loc[i,\"T_pos_\"] = token.pos_\n",
    "    pos_tagging.loc[i,\"T_tag_\"] = token.tag_\n",
    "    pos_tagging.loc[i,\"T_dep_\"] = token.dep_\n",
    "    pos_tagging.loc[i,\"T_head\"] = token.head\n",
    "    pos_tagging.loc[i,\"T_is_sent_start\"] = token.is_sent_start\n",
    "    pos_tagging.loc[i,\"T_shape_\"] = token.shape_\n",
    "    pos_tagging.loc[i,\"T_is_alpha\"] = token.is_alpha\n",
    "    pos_tagging.loc[i,\"T_is_stop\"] = token.is_stop\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "pos_tagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ruled-based-matching and spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Regular expressions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": {\"REGEX\": \"^[Cc](\\\\.?|omo)$\"}},\n",
    "           {\"TEXT\": {\"REGEX\": \"^[Pp](\\\\.?|osso)$\"}},\n",
    "           {\"LOWER\": \"saber\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"DESEJO\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Como posso saber o total de milho que tenho para entregar para meu cliente?\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    #span = doc[start:end]  # The matched span\n",
    "    span = Span(doc, start, end, label=\"PERGUNTA\")\n",
    "    print(f'match_id: {match_id:>20} | string_id: {string_id:>10} | start: {start} | end: {end} | span.text: {span.text:>12} | span.label_: {span.label_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Return Span objects directly\n",
    "matches = matcher(doc, as_spans=True)\n",
    "for span in matches:\n",
    "    print(span.text, span.label_)\n",
    "\n",
    "    colors = {\"PERGUNTA\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\"}\n",
    "    options = {\"span\": [\"PERGUNTA\"], \"colors\": colors}\n",
    "    doc.spans[\"sc\"] = [\n",
    "    Span(doc, start, end, \"PERGUNTA\"),\n",
    "    ]\n",
    "\n",
    "    displacy.render(doc, style=\"span\", options=options) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fuzzy matching</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add attribute ruler with exception for \"A Santa Rita\" as NNP/PROPN NNP/PROPN\n",
    "ruler = nlp.get_pipe(\"attribute_ruler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"contrato\", \"contratos\", \"contract\"]}}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern to match \"A Santa Rita\"\n",
    "patterns = [[{\"TEXT\": {\"FUZZY\": \"Fazenda\"}}, {\"LOWER\": \"Santa\"}, {\"TEXT\": {\"FUZZY\": \"Rita\"}}]]\n",
    "# The attributes to assign to the matched token\n",
    "attrs = {\"TAG\": \"NNP\", \"POS\": \"PROPN\"}\n",
    "# Add rules to the attribute ruler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rules to the attribute ruler\n",
    "ruler.add(patterns=patterns, attrs=attrs, index=0)  # \"A\" em \"A Santa Rita\"\n",
    "ruler.add(patterns=patterns, attrs=attrs, index=1)  # \"Santa\" em \"A Santa Rita\"\n",
    "ruler.add(patterns=patterns, attrs=attrs, index=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A Fazenda Santa Rita ira produzir 10 toneladas de arroz este ano.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(token.i, token.text) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_check = [0, 1, 2, 3]\n",
    "\n",
    "tokens_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = doc\n",
    "print(doc)\n",
    "text_ruler = pd.DataFrame(data=[], \\\n",
    "  columns=[\"T_idx\", \"T_texto\",\"T_Tag_\", \"T_Tag_explained\", \"T_pos_\", \"T_pos_explained\"])\n",
    "i = 0\n",
    "for idx in tokens_to_check:\n",
    "    text_ruler.loc[i,\"T_idx\"] = doc[idx].i\n",
    "    text_ruler.loc[i,\"T_texto\"] = doc[idx].text\n",
    "    text_ruler.loc[i,\"T_Tag_\"] = doc[idx].tag_\n",
    "    text_ruler.loc[i,\"T_Tag_explained\"] = spacy.explain(doc[idx].tag_)\n",
    "    text_ruler.loc[i,\"T_pos_\"] = doc[idx].pos_\n",
    "    text_ruler.loc[i,\"T_pos_explained\"] = spacy.explain(doc[idx].pos_)\n",
    " \n",
    "    \n",
    "    i = i+1\n",
    "text_ruler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Regex and fuzzy with lists</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"fantastico\", \"top\", \"maravilhosa\"]}}}]\n",
    "\n",
    "pattern = [{\"TEXT\": {\"REGEX\": {\"NOT_IN\": [\"^fan(tastico)?$\", \"^mara(vilhosa)?\"]}}}]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding on_match rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.pt import Portuguese\n",
    "\n",
    "nlp = Portuguese()\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_event_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"FAZENDA\")\n",
    "    doc.ents += (entity,)\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"ORTH\": \"Santa\"}, {\"ORTH\": \"Rita\"}]\n",
    "matcher.add(\"SantaRita\", [pattern], on_match=add_event_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Estamos nos aproximando da fazenda Santa Rita.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "html = displacy.render(doc, style=\"ent\", page=True,\n",
    "                       options={\"ents\": [\"FAZENDA\"]})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importante: using <mark>on_match</mark> event</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matched_sents = []  # Collect data of matched sentences to be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]  # Matched span\n",
    "    sent = span.sent  # Sentence containing matched span\n",
    "    # Append mock entity for match in displaCy style to matched_sents\n",
    "    # get the match span by ofsetting the start and end of the span with the\n",
    "    # start and end of the sentence in the doc\n",
    "    match_ents = [{\n",
    "        \"start\": span.start_char - sent.start_char,\n",
    "        \"end\": span.end_char - sent.start_char,\n",
    "        \"label\": \"MATCH\",\n",
    "    }]\n",
    "    matched_sents.append({\"text\": sent.text, \"ents\": match_ents}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"LOWER\": \"agrobi\"}, {\"LEMMA\": \"ser\"}, {\"POS\": \"ADV\", \"OP\": \"*\"},\n",
    "           {\"POS\": \"ADJ\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"AgrobiIs\", [pattern], on_match=collect_sents)  # add pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"eu diria que a Agrobi seria legal se ela desse panettone no natal. – Agrobi é muito legal, certo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(matched_sents, style=\"ent\", manual=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient phrase matching\n",
    "\n",
    "If you need to match large terminology lists, you can also use the PhraseMatcher and create Doc objects instead of token patterns, which is much more efficient overall. The Doc patterns can contain single or multiple tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\"Santa Rita\", \"Passo Fundo\", \"Trem Bom\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run nlp.make_doc to speed things up\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Fui passear na fazenda Santa Rita e descrobri que \"\n",
    "          \"as bebidas que eles produzem se equiparam com as produzidas pela fazenda Passo Fundo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text, span.ents, span.ent_id_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados e reserva de codigo\n",
    "\n",
    "Esta funcionando"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alguns exemplos e fragmentos que podem ser uteis no curto prazo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Processo de salvamento e recuperaçao de patterns e parametros das entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/home/wklinux/spaCy/configuracoes/patterns_padrao.json\"  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns, colors = load_patterns_and_colors(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character '\\xf4' in position 4: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m write_patterns_to_file(patterns\u001b[39m=\u001b[39;49mpatterns, colors\u001b[39m=\u001b[39;49mcolors, filename\u001b[39m=\u001b[39;49mfilename)\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mwrite_patterns_to_file\u001b[0;34m(patterns, colors, filename)\u001b[0m\n\u001b[1;32m     25\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpatterns\u001b[39m\u001b[39m\"\u001b[39m: patterns, \u001b[39m\"\u001b[39m\u001b[39mcolors\u001b[39m\u001b[39m\"\u001b[39m: colors}\n\u001b[1;32m     26\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 27\u001b[0m     json\u001b[39m.\u001b[39;49mdump(data, f, ensure_ascii\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, indent\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:180\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39m# a debuggability cost\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m--> 180\u001b[0m     fp\u001b[39m.\u001b[39;49mwrite(chunk)\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character '\\xf4' in position 4: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "write_patterns_to_file(patterns=patterns, colors=colors, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token for token in head.children if token.dep_ == \"prep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc:\n",
    "    # Because the entity is a span, we need to use its root token. The head\n",
    "    # is the syntactic governor of the person, e.g. the verb\n",
    "    head = ent.root.head\n",
    "    print(head)\n",
    "    if head.lemma_ == \"milho\":\n",
    "        # Check if the children contain a preposition\n",
    "        preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
    "        print(preps)\n",
    "        for prep in preps:\n",
    "            # Check if tokens part of ORG entities are in the preposition's\n",
    "            # children, e.g. at -> Acme Corp Inc.\n",
    "            orgs = [token for token in prep.children if token.ent_type_ == \"ORG\"]\n",
    "            # If the verb is in past tense, the company was a previous company\n",
    "            print({\"person\": ent, \"orgs\": orgs, \"past\": head.tag_ == \"VBD\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo superado - utilizado na versao 1\n",
    "\n",
    "colors = {\n",
    "        \"CULTURA\": \"linear-gradient(90deg, #2ADB5E, #1FA346)\", \n",
    "        \"QUANTIDADE\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\", \n",
    "        \"FAZENDA\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\", \n",
    "        \"SAFRA\": \"linear-gradient(90deg, #FFC90E, #BA930A)\", \n",
    "        \"CONTRATO\": \"linear-gradient(90deg, #B5B5B5, #8A8A8A)\"}\n",
    "\n",
    "\n",
    "patternsOthers = [{\"label\": \"PERSON\", \"pattern\": \"Daniel\", \"id\": \"daniel\"},\n",
    "                  {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"fast\"}, {\"LOWER\": \"innovation\"}], \"id\": \"fast-innovation\"}] \n",
    " \n",
    "\n",
    "patternsCult = [\n",
    "    {\n",
    "        \"label\":\"CULTURA\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"soja\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"milho\", \"OP\":\"?\"},\n",
    "        ]    \n",
    "    }\n",
    "]  \n",
    "\n",
    "patternsQuant = [\n",
    "    {\n",
    "        \"label\":\"QUANTIDADE\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"saldo\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"total\"},\n",
    "            {\"LOWER\": \"entregue\", \"OP\":\"?\"},\n",
    "        ]    \n",
    "    }\n",
    "]\n",
    "\n",
    "patternsSafra = [\n",
    "    {\n",
    "        \"label\":\"SAFRA\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"safra\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"safras\", \"OP\":\"?\"},\n",
    "            {\"SHAPE\": \"dd/dd\"},\n",
    "        ]    \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "patternsFazenda = [\n",
    "    {\n",
    "        \"label\":\"FAZENDA\",\n",
    "        \"pattern\":[\n",
    "            {\"LOWER\": \"fazenda\", \"OP\":\"?\"},\n",
    "            {\"ORTH\": \"Santa\"}, {\"ORTH\": \"Rita\"}\n",
    "        ]\n",
    "        \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "patternsContrato = [\n",
    "    {\n",
    "        \"label\":\"CONTRATO\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"contrato\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"contratos\", \"OP\":\"?\"},\n",
    "            {\"SHAPE\": \"dddX\", \"OP\":\"?\"},\n",
    "        ]\n",
    "        \n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "patterns = patternsCult + patternsQuant + patternsFazenda + patternsSafra + patternsContrato + patternsOthers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar um novo entity_ruler + patterns\n",
    "\n",
    "new_ruler = nlp.add_pipe(\"entity_ruler\").from_disk(\"/home/wklinux/spaCy/configuracoes/pattern_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ent for ent in doc.ents if ent.label_ == \"ENTREGUE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrega_entidade = [ent for ent in doc.ents if ent.label_ == \"ENTREGUE\"]\n",
    "for ent in entrega_entidade:\n",
    "    # Because the entity is a span, we need to use its root token. The head\n",
    "    # is the syntactic governor of the person, e.g. the verb\n",
    "    head = ent.root.head\n",
    "    print(head)\n",
    "    if head.lemma_ == \"entregar\":\n",
    "        # Check if the children contain a preposition\n",
    "        preps = [token for token in head.children if token.dep_ == \"aux:pass\"]\n",
    "        print(\"preps: \", preps)\n",
    "        for prep in preps:\n",
    "            # Check if tokens part of ORG entities are in the preposition's\n",
    "            # children, e.g. at -> Acme Corp Inc.\n",
    "            prep_children = [token for token in prep.children]\n",
    "            print(prep_children)\n",
    "            orgs = [token for token in prep.children if token.ent_type_ == \"ENTREGUE\"]\n",
    "            # If the verb is in past tense, the company was a previous company\n",
    "            print({\"acao\": ent, \"orgs\": orgs, \"past\": head.tag_ == \"VBD\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    head = ent.root.head\n",
    "    print(f'{ent.start:>2} | {ent.text:>20} | {ent.label_:>8} | head: {head.text:>10} |  lemma: {head.lemma_:>8}  | {ent.end:>2} || {ent.start_char:>2} | {ent.end_char:>2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Give it back! He pleaded.\")\n",
    "give_children = doc[0].children\n",
    "assert [t.text for t in give_children] == [\"it\", \"back\", \"!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_entities = [ent for ent in doc.ents]\n",
    "for ent in person_entities:\n",
    "    # Because the entity is a span, we need to use its root token. The head\n",
    "    # is the syntactic governor of the person, e.g. the verb\n",
    "    head = ent.root.head\n",
    "    print(head)\n",
    "    if head.lemma_ == \"entregar\":\n",
    "        # Check if the children contain a preposition\n",
    "        preps = [token for token in head.children if token.dep_ == \"aux:pass\"]\n",
    "        print(\"preps: \", preps)\n",
    "        for prep in preps:\n",
    "            # Check if tokens part of ORG entities are in the preposition's\n",
    "            # children, e.g. at -> Acme Corp Inc.\n",
    "            prep_children = [token for token in prep.children]\n",
    "            print(prep_children)\n",
    "            orgs = [token for token in prep.children if token.ent_type_ == \"ENTREGUE\"]\n",
    "            # If the verb is in past tense, the company was a previous company\n",
    "            print({\"acao\": ent, \"orgs\": orgs, \"past\": head.tag_ == \"VBD\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)\n",
    "print(ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.to_disk(\"/home/wklinux/spaCy/configuracoes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.from_disk(\"/path/to/patterns.jsonl\")  # loads patterns only\n",
    "ruler.from_disk(\"/path/to/entity_ruler\")    # loads patterns and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents = ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(ents):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(tokens):\n",
    "    print(i[0], i[1], i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "pos_tagging = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"T_texto\",\"T_lemma_\", \"T_pos_\", \"T_tag_\", \"T_dep_\", \"T_head\", \"T_is_sent_start\", \"T_shape_\", \"T_is_alpha\", \"T_is_stop\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    pos_tagging.loc[i,\"id\"] = token.i\n",
    "    pos_tagging.loc[i,\"T_texto\"] = token.text\n",
    "    pos_tagging.loc[i,\"T_lemma_\"] = token.lemma_\n",
    "    pos_tagging.loc[i,\"T_pos_\"] = token.pos_\n",
    "    pos_tagging.loc[i,\"T_tag_\"] = token.tag_\n",
    "    pos_tagging.loc[i,\"T_dep_\"] = token.dep_\n",
    "    pos_tagging.loc[i,\"T_head\"] = token.head\n",
    "    pos_tagging.loc[i,\"T_is_sent_start\"] = token.is_sent_start\n",
    "    pos_tagging.loc[i,\"T_shape_\"] = token.shape_\n",
    "    pos_tagging.loc[i,\"T_is_alpha\"] = token.is_alpha\n",
    "    pos_tagging.loc[i,\"T_is_stop\"] = token.is_stop\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "pos_tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"pt\")\n",
    "nlp.add_pipe(\"tagger\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"pt\")\n",
    "nlp.add_pipe(\"morphologizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"pt\")\n",
    "nlp.add_pipe(\"tagger\")\n",
    "nlp.add_pipe(\"morphologizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PS>: Novo modelo proposto - preservar as caracateristicas do doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "# text = \"Eu gostaria de saber o saldo total do meu contrato de soja e total entregue de milho para as safras 22/23 e 23/24.\"\n",
    "# doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attibute_ruler = nlp.get_pipe('attribute_ruler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attibute_ruler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = nlp.remove_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = nlp.add_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import EntityRecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = EntityRecognizer(nlp.vocab, \"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner.initialize(lambda: examples, nlp=nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "        \"CULTURA\": \"linear-gradient(90deg, #2ADB5E, #1FA346)\", \n",
    "        \"QUANTIDADE\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\", \n",
    "        \"FAZENDA\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\", \n",
    "        \"SAFRA\": \"linear-gradient(90deg, #FFC90E, #BA930A)\", \n",
    "        \"CONTRATO\": \"linear-gradient(90deg, #B5B5B5, #8A8A8A)\"}\n",
    "\n",
    "patternsCult = [\n",
    "    {\n",
    "        \"label\":\"CULTURA\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"soja\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"milho\", \"OP\":\"?\"},\n",
    "        ]    \n",
    "    }\n",
    "]  \n",
    "\n",
    "patternsQuant = [\n",
    "    {\n",
    "        \"label\":\"QUANTIDADE\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"saldo\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"total\"},\n",
    "            {\"LOWER\": \"entregue\", \"OP\":\"?\"},\n",
    "        ]    \n",
    "    }\n",
    "]\n",
    "\n",
    "patternsSafra = [\n",
    "    {\n",
    "        \"label\":\"SAFRA\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"safra\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"safras\", \"OP\":\"?\"},\n",
    "            {\"SHAPE\": \"dd/dd\"},\n",
    "        ]    \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "patternsFazenda = [\n",
    "    {\n",
    "        \"label\":\"FAZENDA\",\n",
    "        \"pattern\":[\n",
    "            {\"LOWER\": \"fazenda\", \"OP\":\"?\"},\n",
    "            {\"ORTH\": \"Santa\"}, {\"ORTH\": \"Rita\"}\n",
    "        ]\n",
    "        \n",
    "    }\n",
    "]\n",
    "\n",
    "patternsContrato = [\n",
    "    {\n",
    "        \"label\":\"CONTRATO\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"contrato\", \"OP\":\"?\"},\n",
    "            {\"LOWER\": \"contratos\", \"OP\":\"?\"},\n",
    "            {\"SHAPE\": \"dddX\", \"OP\":\"?\"},\n",
    "        ]\n",
    "        \n",
    "    }\n",
    "]\n",
    "\n",
    "patterns = patternsCult + patternsQuant + patternsFazenda + patternsSafra + patternsContrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Eu gostaria de saber o saldo total do meu contrato 658S de soja e total entregue de milho para as safras 22/23 e 23/24 pela fazenda Santa Rita.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.remove_pipe('entity_ruler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar novas entidades ao documento existente\n",
    "new_ents = []\n",
    "for ent in doc.ents:\n",
    "    new_ents.append({\n",
    "        \"start\": ent.start_char,\n",
    "        \"end\": ent.end_char,\n",
    "        \"label\": ent.label_\n",
    "    })\n",
    "new_ents    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in nlp(text).ents:\n",
    "    new_ents.append({\n",
    "        \"start\": ent.start_char,\n",
    "        \"end\": ent.end_char,\n",
    "        \"label\": ent.label_\n",
    "    })\n",
    "    \n",
    "new_ents    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents = new_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", options={\"colors\": colors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir as entidades identificadas e as informações de POS, TAG e lemma\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_, token.lemma_, token.ent_type_, token.ent_iob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "pos_tagging = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"T_texto\",\"T_lemma_\", \"T_pos_\", \"T_tag_\", \"T_dep_\", \"T_head\", \"T_is_sent_start\", \"T_shape_\", \"T_is_alpha\", \"T_is_stop\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    pos_tagging.loc[i,\"id\"] = token.i\n",
    "    pos_tagging.loc[i,\"T_texto\"] = token.text\n",
    "    pos_tagging.loc[i,\"T_lemma_\"] = token.lemma_\n",
    "    pos_tagging.loc[i,\"T_pos_\"] = token.pos_\n",
    "    pos_tagging.loc[i,\"T_tag_\"] = token.tag_\n",
    "    pos_tagging.loc[i,\"T_dep_\"] = token.dep_\n",
    "    pos_tagging.loc[i,\"T_head\"] = token.head\n",
    "    pos_tagging.loc[i,\"T_is_sent_start\"] = token.is_sent_start\n",
    "    pos_tagging.loc[i,\"T_shape_\"] = token.shape_\n",
    "    pos_tagging.loc[i,\"T_is_alpha\"] = token.is_alpha\n",
    "    pos_tagging.loc[i,\"T_is_stop\"] = token.is_stop\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "pos_tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(patternsCult)\n",
    "nlp.remove_pipe(ruler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"pt\")\n",
    "nlp.add_pipe(\"tagger\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"pt\")\n",
    "nlp.add_pipe(\"morphologizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"pt\")\n",
    "nlp.add_pipe(\"tagger\")\n",
    "nlp.add_pipe(\"morphologizer\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reserva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ent(text, patterns):\n",
    "    nlp = spacy.blank(\"pt\")\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    ruler.add_patterns(patterns)\n",
    "    doc = nlp(text)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_patterns_to_file(patterns, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(patterns, f, ensure_ascii=False)\n",
    "\n",
    "def load_patterns_from_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        patterns = json.load(f)\n",
    "    return patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ent_new(text, patterns):\n",
    "    nlp = spacy.blank(\"pt\")\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    ruler.add_patterns(patterns)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tokens = []\n",
    "    ents = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        span = doc.char_span(ent.start_char, ent.end_char, label=ent.label_)\n",
    "        ents.append(span)\n",
    "        \n",
    "    for token in doc:\n",
    "        start = token.idx\n",
    "        end = start + len(token)\n",
    "        tokens.append((token.text, start, end))\n",
    "        \n",
    "    return tokens, ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternsCult = [\n",
    "    {\n",
    "        \"label\":\"CULTURA\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"soja\", \"OP\":\"?\"},\n",
    "\n",
    "        ]    \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternsQuant = [\n",
    "    {\n",
    "        \"label\":\"QUANTIDADE\",\n",
    "        \"pattern\": [\n",
    "            {\"LOWER\": \"saldo\", \"OP\":\"?\"},\n",
    "            # {\"LOWER\": {\"IN\": [\"total\", \"entregue\"]}},\n",
    "            {\"LOWER\": \"total\"},\n",
    "            {\"LOWER\": \"entregue\", \"OP\":\"?\"},\n",
    "        ]    \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Eu gostaria de saber o saldo total do meu contrato de soja e total entregue de milho para as safras 22/23 e 23/24 pela fazenda Santa Rita.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Eu gostaria de saber o saldo total do meu contrato de soja e total entregue de milho para as safras 22/23 e 23/24.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = patternsCult + patternsQuant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = show_ent(text, patterns=patterns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\"CULTURA\": \"green\", \"QUANTIDADE\": \"orange\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", options={\"colors\": colors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternsOthers = [  \n",
    "                    {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"fast\"}, {\"lower\": \"innovation\"}], \"id\": \"fastinnovation\"},\n",
    "                    {\"label\": \"PERSON\", \"pattern\": \"LOWER\": \"daniel\", \"id\": \"daniel-nascimento\"},\n",
    "                    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"daniel\"}, {\"LOWER\": \"silva\"}, {\"LOWER\": \"do\"}, {\"LOWER\": \"nascimento\"}], \"id\": \"daniel-nascimento\"},\n",
    "                    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"daniel\"}, {\"LOWER\": \"nascimento\"}], \"id\": \"daniel-nascimento\"} ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
