{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solucao Extracao pdf v2\n",
    "\n",
    "0_solucao_extracao_pdf_v2.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modulos, config e dicionarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Modulos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "import fitz  # Módulo PyMuPDF\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "import PyPDF2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import ImageFont\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "\n",
    "import zipfile\n",
    "\n",
    "from outlook_msg import Message\n",
    "import extract_msg\n",
    "\n",
    "import locale\n",
    "from datetime import datetime, timezone, timedelta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. path para documentos PDF (omelhor se estiverem dentro de um unico diretorio)\n",
    "root_pdf_path = \"pipeline_extracao_documentos/2_documentos_para_extracao\"\n",
    "\n",
    "# 2. path para documentos PDF que podem estar aguardando para serem processados\n",
    "root_pdf_aguardando_path = \"pipeline_extracao_documentos/3_tratamento_excecoes/pdf_aguardando_processar\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# path para e-mails recebidos com documentos\n",
    "email_recebido_documento_path = \"pipeline_extracao_documentos/1_emails\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. path para documentos PDF externos para serem processados\n",
    "root_external_pdf_path = \"content_from_pdftool/data/data_pdf/NF_para_processamento/NFRJ_PDF_para _ocr\"\n",
    "# 4. path para documentos PDF PESQUISAVEIS externos para serem processados\n",
    "root_external_pdf_pesquisavel_path = \"content_from_pdftool/data/data_pdf/NF_processadas/NFRJ/fwdnotasfiscaisemitidaslmpadalegal\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5. path para imagem padrao\n",
    "image_resized_path = 'pipeline_extracao_documentos/6_geral_administacao/images/processadas'\n",
    "\n",
    "# 6. path para log\n",
    "log_path = 'pipeline_extracao_documentos/6_geral_administacao/logs'\n",
    "\n",
    "\n",
    "# 7. path para arquivos json\n",
    "json_path = \"pipeline_extracao_documentos/5_documentos_processados/jsons\"\n",
    "\n",
    "# 8. path para NFs processadas\n",
    "nf_processada_path = \"pipeline_extracao_documentos/5_documentos_processados\"\n",
    "\n",
    "\n",
    "\n",
    "#### paths de objetos para criacao/gestao (dicionarios/datasets)\n",
    "\n",
    "# 9. path para modelos\n",
    "nf_model_path = \"pipeline_extracao_documentos/6_geral_administacao/modelos/frames_nf_v5_mage1.xlsx\"\n",
    "\n",
    "# 10. path para dicionario de modelos\n",
    "model_dict_path = \"pipeline_extracao_documentos/6_geral_administacao/modelos/models.csv\"\n",
    "\n",
    "# 11. path para datasets CNAE e Itens de Serviço\n",
    "nf_datasets_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# VERIFICAR\n",
    "tgt_imagens = \"pipeline_extracao_documentos/images\"\n",
    "\n",
    "# 12. poppler path\n",
    "poppler_path = \"/home/dani-boy/miniconda3/envs/tables-detr/bin\"\n",
    "\n",
    "# 13. path para config Tesseract\n",
    "tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Caminho do arquivo uma mensagem especifica\n",
    "msg_dir_path = 'pipeline_extracao_documentos/1_emails'\n",
    "\n",
    "# 2. Path para arquivos atachados compactados\n",
    "msg_attachment_zip = 'pipeline_extracao_documentos/1_emails/attachments'\n",
    "\n",
    "# 3. Path para documentos atachados:\n",
    "documentos_extracao_path = 'pipeline_extracao_documentos/2_documentos_para_extracao'\n",
    "\n",
    "batch_name = \"Batch_1\"\n",
    "\n",
    "df_processamento = pd.read_csv('pipeline_extracao_documentos/6_geral_administacao/datasets/df_processamento.csv')\n",
    "\n",
    "df_processamento.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Consistencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tratamento dos e-mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(msg_dir_path):\n",
    "    #print(f'{root}  | {dirs} | {document} | {files}\\n')\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        if file.lower().endswith('.msg'):\n",
    "            email_message = file\n",
    "            email_path = os.path.join(root, file)\n",
    "            print(f'email: {email_message}, email_path: {email_path}')\n",
    "            \n",
    "            \n",
    "msg = extract_msg.Message(email_path)\n",
    "\n",
    "msg_raw_sender = msg.sender\n",
    "\n",
    "parts = msg_raw_sender.rsplit('<', 1)\n",
    "\n",
    "msg_email_address = parts[1].strip('<>')\n",
    "\n",
    "msg_sender = parts[0].strip(' ')\n",
    "\n",
    "msg_subject = msg.subject\n",
    "\n",
    "\n",
    "msg_body = msg.body\n",
    "\n",
    "# Defina a localização para pt_BR\n",
    "locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")\n",
    "\n",
    "\n",
    "# String original\n",
    "original_date_str = msg.date\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliza expressão regular para extrair os componentes\n",
    "match = re.search(r'(\\d{2}) (\\w{3}) (\\d{4}) (\\d{2}):(\\d{2}):(\\d{2}) ([+-]\\d{4})', original_date_str)\n",
    "if match:\n",
    "    day, month_str, year, hour, minute, second, tz_offset = match.groups()\n",
    "    \n",
    "    month_map_en = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}\n",
    "    month_map_pt = {'jan': 1, 'fev': 2, 'mar': 3, 'abr': 4, 'mai': 5, 'jun': 6, 'jul': 7, 'ago': 8, 'set': 9, 'out': 10, 'nov': 11, 'dez': 12}\n",
    "\n",
    "    # Detectar idioma do campo de data (um exemplo simples, você pode fazer algo mais robusto)\n",
    "    if 'Jan' in original_date_str or 'Feb' in original_date_str or 'Mar' in original_date_str:\n",
    "        month_map = month_map_en\n",
    "    else:\n",
    "        month_map = month_map_pt\n",
    "\n",
    "# Continua com o resto do seu código\n",
    "\n",
    "    month = month_map.get(month_str, 0)\n",
    "    \n",
    "    # Cria um objeto datetime\n",
    "    dt = datetime(year=int(year), month=month, day=int(day),\n",
    "                  hour=int(hour), minute=int(minute), second=int(second))\n",
    "    \n",
    "    # Ajusta o fuso horário\n",
    "    offset_minutes = int(tz_offset) // 100 * 60 + int(tz_offset) % 100\n",
    "    dt = dt.replace(tzinfo=timezone(timedelta(minutes=offset_minutes)))\n",
    "\n",
    "\n",
    "    # Formata a data para o formato desejado (pt_BR)\n",
    "    formatted_date_str = dt.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    #print(f\"Data e horário formatados: {formatted_date_str}\")\n",
    "\n",
    "    # Formata apenas a data\n",
    "    formatted_date = dt.strftime(\"%d/%m/%Y\")\n",
    "    #print(f\"Data: {formatted_date}\")\n",
    "\n",
    "    # Formata apenas o horário\n",
    "    formatted_time = dt.strftime(\"%H:%M:%S\")\n",
    "    #print(f\"Horário: {formatted_time}\")\n",
    "    \n",
    "    print(f'\\nmsg_sender: {msg_sender}, msg_email_address: {msg_email_address}, msg_subject: {msg_subject} data: {formatted_date}, horario: {formatted_time} data completa: {formatted_date_str}')\n",
    "else:\n",
    "    print(\"Não foi possível fazer o parse da data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 Salvando os attachments do e-mail </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Tratando os anexos, zips </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from outlook_msg import Message\n",
    "\n",
    "# Caminho da pasta onde você quer salvar os anexos\n",
    "pasta_destino = msg_attachment_zip\n",
    "\n",
    "# Verifica se a pasta existe; se não, cria ela\n",
    "if not os.path.exists(pasta_destino):\n",
    "    os.makedirs(pasta_destino)\n",
    "\n",
    "with open(email_path) as msg_file:\n",
    "    msg = Message(msg_file)\n",
    "\n",
    "# Contents are the plaintext body of the email\n",
    "#contents = msg.body\n",
    "\n",
    "\n",
    "total_attch = len(msg.attachments)\n",
    "\n",
    "print(total_attch)\n",
    "\n",
    "arquivos_zip = []\n",
    "i = 0\n",
    "# Loop para salvar cada anexo\n",
    "for i in range(total_attch):\n",
    "    attachment = msg.attachments[i]\n",
    "    caminho_completo_anexo = os.path.join(pasta_destino, attachment.filename)\n",
    "    arquivos_zip.append(attachment.filename)\n",
    "    print(caminho_completo_anexo)\n",
    "    with attachment.open() as attachment_fp, open(caminho_completo_anexo, 'wb') as output_fp:\n",
    "        output_fp.write(attachment_fp.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> Extraindo documentos do ZIP </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório onde você quer salvar os arquivos extraídos\n",
    "output_dir = os.path.join(documentos_extracao_path, batch_name)\n",
    "\n",
    "folder_file_dict = {}\n",
    "\n",
    "for root, dirs, files in os.walk(msg_attachment_zip):\n",
    "    #print(f'{root}  | {dirs} | {document} | {files}\\n')\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        if file.lower().endswith('.zip'):\n",
    "            zip_file = file\n",
    "            zip_file_path = os.path.join(root, file)\n",
    "            print(zip_file_path)\n",
    "            \n",
    "            # Obtém o nome base do arquivo ZIP para usar como subdiretório\n",
    "            zip_basename = os.path.splitext(os.path.basename(zip_file_path))[0]\n",
    "            \n",
    "            print(zip_basename)\n",
    "            # Obtém o nome base do arquivo ZIP para usar como subdiretório\n",
    "            zip_basename = os.path.splitext(os.path.basename(zip_file_path))[0]\n",
    "\n",
    "            # Cria o subdiretório com base no nome do arquivo ZIP\n",
    "            root_output_dir = os.path.join(output_dir, zip_basename)\n",
    "            if not os.path.exists(root_output_dir):\n",
    "                os.makedirs(root_output_dir) # estou criando o diretorio caso nao exista\n",
    "\n",
    "            # Dicionário para guardar o nome da pasta e os arquivos associados\n",
    "            \n",
    "\n",
    "            # Abre o arquivo ZIP\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                for member in zip_ref.namelist():\n",
    "                    # Separa o nome da pasta e o nome do arquivo usando barra invertida como delimitador\n",
    "                    parts = member.rsplit('\\\\', 1)\n",
    "                    folder_name = parts[0] if len(parts) > 1 else ''\n",
    "                    filename = parts[-1]\n",
    "\n",
    "                    if filename:  # ignora diretórios\n",
    "                        # Adiciona ao dicionário\n",
    "                        folder_file_dict.setdefault(folder_name, []).append(filename)\n",
    "\n",
    "                        # Cria um subdiretório se ele não existir\n",
    "                        sub_dir = os.path.join(root_output_dir, folder_name)\n",
    "                        if not os.path.exists(sub_dir):\n",
    "                            os.makedirs(sub_dir)\n",
    "\n",
    "                        # Salva o arquivo no subdiretório especificado\n",
    "                        source = zip_ref.open(member)\n",
    "                        target_path = os.path.join(sub_dir, filename)\n",
    "                        with open(target_path, \"wb\") as target:\n",
    "                            target.write(source.read())\n",
    "\n",
    "\n",
    "folder_file_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dicionarios e Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_names = []\n",
    "file_counts = []\n",
    "file_names = []\n",
    "\n",
    "# Iterar sobre o dicionário para coletar informações\n",
    "for folder, files in folder_file_dict.items():\n",
    "    folder_names.append(folder)\n",
    "    file_counts.append(len(files))\n",
    "    file_names.append(files)\n",
    "\n",
    "\n",
    "# Suponha que folder_file_dict é algo como {'pasta1': 'arquivo1.pdf', 'pasta2': 'arquivo2.pdf'}\n",
    "folder_names = list(folder_file_dict.keys())\n",
    "file_names = list(folder_file_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {} \n",
    "# Criar o DataFrame do batch\n",
    "dfs[batch_name] = pd.DataFrame({\n",
    "    \"Dt_hora\": [formatted_date_str],\n",
    "    \"Assunto\": [msg_subject],\n",
    "    \"Arquivos_zip\": [arquivos_zip],\n",
    "    \"Pasta\": folder_names,\n",
    "    \"Quantidade de Documentos\": [file_counts],\n",
    "    \"documentos\": file_names,\n",
    "    \"De\": [msg_sender],\n",
    "    \"batch\": [batch_name],\n",
    "    \"email\": [msg_email_address],\n",
    "    \"Data\": [formatted_date],\n",
    "    \"Hora\": [formatted_time],\n",
    "})\n",
    "\n",
    "df_batch = dfs[batch_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processamento = dfs[batch_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anexando o novo DataFrame ao original\n",
    "df_processamento = df_processamento.append(df_batch, ignore_index=True)\n",
    "\n",
    "df_processamento.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anexando o novo DataFrame ao original\n",
    "df_processamento = df_processamento.append(df_batch, ignore_index=True)\n",
    "\n",
    "df_processamento.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o DF para csv\n",
    "df_processamento.to_csv('pipeline_extracao_documentos/datasets/df_processamento.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gerar excel a partir do df\n",
    "\n",
    "df_main.to_excel(\"nome_do_arquivo.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analise dos PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertResizeAnalise_1page(doc2convert, document_path, image_resized_path):\n",
    "    \n",
    "    \"\"\"# 1. remocao do sufixo .pdf\n",
    "    if doc2convert.split(\".\")[1].islower():\n",
    "        nameImage= doc2convert.removesuffix(\".pdf\")\n",
    "    else:\n",
    "        nameImage= doc2convert.removesuffix(\".PDF\")\"\"\"\n",
    "    \n",
    "    # 2. construo um novo nome para o documento imagem\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(doc2convert)}.jpg')\n",
    "    \n",
    "    # 3. Conversao para imagem\n",
    "    pages = convert_from_path(document_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    resized_pages = []\n",
    "    for page in pages:\n",
    "        resized_page = page.resize((2067, 2923))\n",
    "        resized_pages.append(resized_page)\n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "    return resized_pages[0], image_resized_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Verifica se PDF e pesquisavel ou nao e grava metadados dele\n",
    "def is_pdf_searchable_analise(pdf_path):\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        pages = pdf_document.page_count\n",
    "        is_searchable = all(page.get_text(\"text\") != \"\" for page in pdf_document)\n",
    "        dados_pdf = pdf_document.metadata\n",
    "        pdf_document.close()\n",
    "        return is_searchable, dados_pdf, pages\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao verificar o PDF: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Efetua OCR no documento (area parao do texto da NF)\n",
    "def ocr_RasterPDF_free(image_name, vx0, vy0, vx1, vy1):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = vx0\n",
    "    y0 = vy0\n",
    "    x1= vx1\n",
    "    y1 = vy1\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "    return values, extracted_text_frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Efetua OCR no documento (area parao do texto da NF)\n",
    "def ocr_RasterPDF(image_name):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = 406\n",
    "    y0 = 0\n",
    "    x1= 1540\n",
    "    y1 = 380\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "    return values, extracted_text_frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Interacao para pesquisar prefeitura\n",
    "def pesquisa_texto(texto):\n",
    "    nome_prefeitura_match = re.search(r'PREFEITURA (.+)', texto)\n",
    "    if nome_prefeitura_match:\n",
    "        is_prefeitura = \"PREFEITURA \" + nome_prefeitura_match.group(1)\n",
    "        \n",
    "        return  is_prefeitura\n",
    "    else:\n",
    "        raise ValueError(\"Nao consegui pesquisar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao importante - process_line\n",
    "\n",
    "def process_line(value, reference, label):\n",
    "    name_match = re.search(fr'{reference} (.+)', value)\n",
    "    if name_match:\n",
    "        extracted_value = reference + \" \" + name_match.group(1)\n",
    "        return {label: extracted_value}\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. dicionario para ocorrencias dos documentos pesquisados\n",
    "analise_doc_nf = {}\n",
    "file_data = []\n",
    "# 2. caminho usado dinamicamente para funcao poder ser utizada em outras area da estrutura de diretorios\n",
    "root_doc_analise = os.path.join(documentos_extracao_path, batch_name)\n",
    "# Inicialização do DataFrame\n",
    "colunas = [\"documento\", \"pdf\", \"batch\", \"pesquisavel\", \"paginas\", \"metadata\"]\n",
    "analise_df = pd.DataFrame(columns=colunas)\n",
    "i = 0\n",
    "for root, dirs, files in os.walk(root_doc_analise):\n",
    "    dir_name = os.path.basename(root)\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        dados_pdf = None  # Inicialização fora do try/catch\n",
    "        \n",
    "        if file.lower().endswith('.pdf'):\n",
    "            documento_pdf = True\n",
    "\n",
    "            pesquisavel, metadados, paginas = is_pdf_searchable_analise(file_path)\n",
    "                \n",
    "            if not pesquisavel:\n",
    "                \n",
    "                if paginas == 1:\n",
    "                    #1. Converto para imagem\n",
    "                    image_2work, name_image_2work = convertResizeAnalise_1page(file, file_path, image_resized_path)\n",
    "\n",
    "\n",
    "                    #2_frame_dados_prestador:  0\t552\t2067\t785\n",
    "\n",
    "\n",
    "                    #2. Defino cordenandas, string_pesquisa e label  550\t380\t1357\t555\n",
    "                    v_x0 = 0#406\n",
    "                    v_y0 = 552#0\n",
    "                    v_x1 = 2067#1540\n",
    "                    v_y1 = 785#380\n",
    "                    string_pesquisa = \"Nome/Razão Social:\"\n",
    "                    label = \"testando pesquisas regex\"\n",
    "                    \n",
    "                    #3. Print do teste\n",
    "                    print(f'\\nTeste nro: {i} | doc: {file} | pdf?: {documento_pdf} | pesquisavel?: {pesquisavel} | paginas: {paginas} | string_pesquisa: \"{string_pesquisa}\"\\ncoordenadas ocr: v_x0 = {v_x0}, v_y0 = {v_y0}, v_x1 = {v_x1}, v_y1 = {v_y1}\\n')\n",
    "                    \n",
    "                    #4. Processo OCR\n",
    "                    text_splited, text_frame = ocr_RasterPDF_free(image_2work, v_x0, v_y0, v_x1, v_y1)\n",
    "                                        \n",
    "                    #5. Itero sobre text_splited (lista)\n",
    "                    a = 0\n",
    "                    for texto_spl in text_splited:\n",
    "                        print(f'texto_spl linha {a}: {texto_spl}')\n",
    "                        #6. chamo funcao process_line\n",
    "                        verificacao_texto = process_line(texto_spl, string_pesquisa, label)\n",
    "                        if verificacao_texto:\n",
    "                            print(f'\\n1. nro: {i} | file: {file}\\nverificacao_texto: {verificacao_texto}\\n')\n",
    "                        a +=1    \n",
    "                    \n",
    "                   \n",
    "                    #6. Imprimo valores de text_splited\n",
    "                    print(f'2. text_splited:\\n{text_splited}\\n')\n",
    "                    print(f'\\n\\n=============================================================================================')\n",
    "                    \n",
    "                    \n",
    "                    i +=1\n",
    "                    \n",
    "                    \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. dicionario para ocorrencias dos documentos pesquisados\n",
    "analise_doc_nf = {}\n",
    "file_data = []\n",
    "# 2. caminho usado dinamicamente para funcao poder ser utizada em outras area da estrutura de diretorios\n",
    "root_doc_analise = os.path.join(documentos_extracao_path, batch_name)\n",
    "\n",
    "\n",
    "# Inicialização do DataFrame\n",
    "colunas = [\"documento\", \"pdf\", \"batch\", \"pesquisavel\", \"paginas\", \"metadata\"]\n",
    "analise_df = pd.DataFrame(columns=colunas)\n",
    "\n",
    "for root, dirs, files in os.walk(root_doc_analise):\n",
    "    dir_name = os.path.basename(root)\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        dados_pdf = None  # Inicialização fora do try/catch\n",
    "        \n",
    "        if file.lower().endswith('.pdf'):\n",
    "            documento_pdf = True\n",
    "            try:\n",
    "                pesquisavel, metadados, paginas = is_pdf_searchable_analise(file_path)\n",
    "                \n",
    "                \n",
    "                                \n",
    "                if not pesquisavel:\n",
    "                    if paginas == 1:\n",
    "                        image_2work, name_image_2work = convertResizeAnalise_1page(file, file_path, image_resized_path)\n",
    "                        text_splited, text_frame  = ocr_RasterPDF(name_image_2work)\n",
    "                    \n",
    "                    \n",
    "                    print(f'\\ntext_splited: {text_splited}, text_frame: {text_frame}')\n",
    "                    \n",
    "                    #print(file, \"nao e pesquisavel\")\n",
    "                \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao verificar o PDF: {e}\")\n",
    "            # Adicionando uma nova linha ao DataFrame\n",
    "            nova_linha_df = pd.DataFrame([{\n",
    "                \"pdf\": documento_pdf,\n",
    "                \"documento\": file,\n",
    "                \"batch\": batch_name,\n",
    "                \"pesquisavel\": pesquisavel,\n",
    "                \"paginas\": paginas,\n",
    "                \"metadata\": metadados\n",
    "            }])\n",
    "            \n",
    "            analise_df['pdf'] = analise_df['pdf'].astype(bool)\n",
    "            analise_df['pesquisavel'] = analise_df['pesquisavel'].astype(bool)\n",
    "\n",
    "            analise_df = pd.concat([analise_df, nova_linha_df], ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            documento_pdf = False\n",
    "            # Adicione lógica para outros tipos de arquivo, se necessário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Funcao de conversao e resize do documento\n",
    "def convertResize_analise(nome_documento, document_path, image_resized_path):\n",
    "    \n",
    "    \"\"\"# 1. remocao do sufixo .pdf\n",
    "    if doc2convert.split(\".\")[1].islower():\n",
    "        nameImage= doc2convert.removesuffix(\".pdf\")\n",
    "    else:\n",
    "        nameImage= doc2convert.removesuffix(\".PDF\")\"\"\"\n",
    "    \n",
    "    # 2. construo um novo nome para o documento imagem\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(nome_documento)}.jpg')\n",
    "    \n",
    "    # 3. Conversao para imagem\n",
    "    pages = convert_from_path(document_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((2067, 2923))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "    return resized_pages[0], image_resized_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pequisaModel(image_name):\n",
    "    \n",
    "    analise_pesquisa_nf = {}\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = 406\n",
    "    y0 = 0\n",
    "    x1= 1540\n",
    "    y1 = 380\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "\n",
    "    # 5. Interacao para pesquisar prefeitura\n",
    "    for value in values:\n",
    "        nome_prefeitura_match = re.search(r'PREFEITURA (.+)', value)\n",
    "        if nome_prefeitura_match:\n",
    "            nome_prefeitura = \"PREFEITURA \" + nome_prefeitura_match.group(1)\n",
    "            return  nome_prefeitura\n",
    "        else:\n",
    "            raise ValueError(\"Nao acho nome de prefeitura\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> Tratamento de exceçoes </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minha_funcao():\n",
    "    meu_dict = {\"erros\": []}\n",
    "    \n",
    "    try:\n",
    "        valor = \"teste\"\n",
    "        if valor == \"teste\":\n",
    "            try:\n",
    "                numero = \"outro dado\"\n",
    "                resultado = int(numero)\n",
    "            except ValueError as e:\n",
    "                meu_dict[\"erros\"].append(f\"Erro na primeira parte: {str(e)}\")\n",
    "                    \n",
    "        valor = int(\"texto\")  # Isso vai lançar ValueError\n",
    "    except ValueError as e:\n",
    "        meu_dict[\"erros\"].append(f\"Erro na segunda parte: {str(e)}\")\n",
    "    \n",
    "    if meu_dict[\"erros\"]:\n",
    "        print(\"Erros encontrados.\")\n",
    "    \n",
    "    return meu_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resultado = minha_funcao()\n",
    "print(\"Resultado: \", resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move NF processadas ok\n",
    "def move_pdf_processed_ok(document_path, nf_processada_path, batch_name, doc2convert):\n",
    "    # Determine the destination directory\n",
    "    destination_dir = os.path.join(nf_processada_path, batch_name)\n",
    "\n",
    "    # Check if the destination directory exists; if not, create it\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "\n",
    "    # Determine the destination path including the filename\n",
    "    destination_path = os.path.join(destination_dir, os.path.basename(document_path))\n",
    "\n",
    "    # Move the file from the source path to the destination path\n",
    "    try:\n",
    "        shutil.move(document_path, destination_path)\n",
    "        print(f\"Sucesso ao mover: {document_path} para: {destination_path}\")\n",
    "        return True, destination_path, None  # Success, destination path, no error\n",
    "    except Exception as e:\n",
    "        error_message = f\"Erro ao mover: {document_path} para: {destination_path}: {str(e)}\"\n",
    "        print(error_message)\n",
    "        return False, None, error_message  # Failure, no destination path, error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_to_checke.resize(2067, 2923)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_document.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_document = fitz.open(documento_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "\n",
    "# Get the current date for filename\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "log_filename = f\"log_{current_date}.txt\"\n",
    "\n",
    "log_file_path = os.path.join(log_path, log_filename)\n",
    "\n",
    "\n",
    "# Configure the logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%d/%m/%Y %H:%M:%S',  \n",
    "    filename=log_file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dicionarios de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Leitura do arquivo CSV e criação do dicionário modelos\n",
    "def create_model_dictionary(model_dict_path):\n",
    "    model_dictionary = {}\n",
    "    with open(model_dict_path, 'r') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            prefeitura_name = row['prefeitura']\n",
    "            model_name = row['model']\n",
    "\n",
    "            if prefeitura_name not in model_dictionary:\n",
    "                model_dictionary[prefeitura_name] = model_name\n",
    "            \n",
    "            #model_dictionary[prefeitura_name].append(model_name)\n",
    "    \n",
    "    return model_dictionary\n",
    "\n",
    "\n",
    "# 3. Cria o dict de modelos\n",
    "model_dict = create_model_dictionary(model_dict_path) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Le a planilha e cria do DF\n",
    "frames_nf_v3_df = pd.read_excel(nf_model_path)\n",
    "\n",
    "# Cria dicionários para armazenar diferentes tipos de elementos do modelo\n",
    "document_info = frames_nf_v3_df[frames_nf_v3_df['type'] == 'document'].iloc[0]\n",
    "boundaries_info = frames_nf_v3_df[frames_nf_v3_df['type'] == 'boundaries']\n",
    "sections_info = frames_nf_v3_df[frames_nf_v3_df['type'] == 'section']\n",
    "frames_info = frames_nf_v3_df[frames_nf_v3_df['type'] == 'frame']\n",
    "sframe_fields_info = frames_nf_v3_df[frames_nf_v3_df['type'] == 'sframe_field']\n",
    "field_boxes_info = frames_nf_v3_df[frames_nf_v3_df['type'] == 'field_box']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. CNAE Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mage_cnae_x_item_servico_df = pd.read_excel('pipeline_extracao_documentos/datasets/MAGE_CNAE_X_ITEM_SERVICO_V1.xlsx')\n",
    "\n",
    "# Creating a dictionary for CNAE codes and descriptions\n",
    "cnae_dict = dict(zip(mage_cnae_x_item_servico_df['cnae'], mage_cnae_x_item_servico_df['descricao_cnae']))\n",
    "item_servico_dict = dict(zip(mage_cnae_x_item_servico_df['item_servico'], mage_cnae_x_item_servico_df['descricao_item_servico']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_servico_dict[7.02].upper()\n",
    "cnae_dict[4313400].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnae_dict[4313400].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnae_dict[3812200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mage_cnae_x_item_servico_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize a default dictionary with list as the default factory\n",
    "cnae_to_item_servico_dict = defaultdict(list)\n",
    "\n",
    "# Iterate through the DataFrame and populate the dictionary\n",
    "for index, row in mage_cnae_x_item_servico_df.iterrows():\n",
    "    cnae = row['cnae']\n",
    "    cnae_cod = row['cnae_cod']\n",
    "    descricao_cnae = row['descricao_cnae']\n",
    "    item_servico = row['item_servico']\n",
    "    descricao_item_servico = row['descricao_item_servico']\n",
    "    cnae_to_item_servico_dict[cnae_cod].append(item_servico)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnae_to_item_servico_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cnae_to_item_servico_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Funçoes gerais de imagem e consistencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Funcoes de tratamento de imagem e  PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Funcao de conversao e resize do documento\n",
    "def convertResize(doc2convert, document_path, image_resized_path):\n",
    "    \n",
    "    \"\"\"# 1. remocao do sufixo .pdf\n",
    "    if doc2convert.split(\".\")[1].islower():\n",
    "        nameImage= doc2convert.removesuffix(\".pdf\")\n",
    "    else:\n",
    "        nameImage= doc2convert.removesuffix(\".PDF\")\"\"\"\n",
    "    \n",
    "    # 2. construo um novo nome para o documento imagem\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(doc2convert)}.jpg')\n",
    "    \n",
    "    # 3. Conversao para imagem\n",
    "    pages = convert_from_path(document_path, 500, poppler_path=poppler_path)\n",
    "    \n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((2067, 2923))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "        resized_pages[0].save(image_resized_name, 'JPEG')\n",
    "        \n",
    "    return resized_pages[0], image_resized_name\n",
    "\n",
    "# 2. Pesquisa prefeitura no documento\n",
    "def pequisaModel(image_name):\n",
    "\n",
    "    # 1. Definindo as coordenadas do frame\n",
    "    x0 = 406\n",
    "    y0 = 0\n",
    "    x1= 1540\n",
    "    y1 = 380\n",
    "\n",
    "    # 2. Definir frame_image\n",
    "    frame_image = image_name.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # 3. Extraia texto usando OCR com configuração de idioma padrão para este frame\n",
    "    extracted_text_frame = pytesseract.image_to_string(frame_image, lang='por', config=tessdata_dir_config).strip()\n",
    "\n",
    "    # 4. Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "    values = extracted_text_frame.split('\\n')\n",
    "\n",
    "    # 5. Interacao para pesquisar prefeitura\n",
    "    for value in values:\n",
    "        nome_prefeitura_match = re.search(r'PREFEITURA (.+)', value)\n",
    "        if nome_prefeitura_match:\n",
    "            nome_prefeitura = \"PREFEITURA \" + nome_prefeitura_match.group(1) \n",
    "            return  nome_prefeitura\n",
    "   \n",
    "# 3. Ajusta o filename tirando caracteres especiais \n",
    "def conv_filename(title):\n",
    "    \n",
    "    # Divida o título em nome e extensão\n",
    "    name, extension = title.rsplit('.', 1) if '.' in title else (title, \"\")\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    # Adicione a extensão de volta, se houver\n",
    "    if extension:\n",
    "        filename += '.' + extension.lower()\n",
    "\n",
    "    return filename\n",
    "\n",
    "# 4. Ajusta o filename tirando caracteres especiais e a\n",
    "def conv_filename_no_ext(title):\n",
    "    # Divida o título em nome e extensão (mas ignore a extensão)\n",
    "    name = title.rsplit('.', 1)[0] if '.' in title else title\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    return filename     \n",
    "\n",
    "# 5. Verifica se PDF e pesquisavel ou nao e grava metadados dele\n",
    "def is_pdf_searchable(pdf_path):\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        is_searchable = all(page.get_text(\"text\") != \"\" for page in pdf_document)\n",
    "        dados_pdf = pdf_document.metadata\n",
    "        pdf_document.close()\n",
    "        return is_searchable, dados_pdf\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao verificar o PDF: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Funcoes de suporte e extracao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funçao de formatacao de numeros\n",
    "\"\"\"def format_number(number_str):\n",
    "    number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # multiplica por 100 para fields %\n",
    "    return float(number_str)\"\"\"\n",
    "    \n",
    "    \n",
    "def format_number(number_str):\n",
    "    # Check for percentage and handle it\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # You can multiply by 100 here if needed\n",
    "\n",
    "    # Check if the string contains \"R$\" or a comma, indicating the original format\n",
    "    if 'R$' in number_str or ',' in number_str:\n",
    "        # Original format: Remove 'R$', replace dots with nothing, and replace commas with dots\n",
    "        number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    else:\n",
    "        # New format: Extract only the numeric part using regex\n",
    "        number_str = re.findall(r'[\\d\\.]+', number_str)[-1]\n",
    "\n",
    "    return float(number_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Funcoes suporte para extracao\n",
    "\n",
    "def extract_text_from_frame(image, coordinates, config):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    frame_image = image.crop((x0, y0, x1, y1))\n",
    "    extracted_text = pytesseract.image_to_string(frame_image, lang='por', config=config).strip()\n",
    "    return extracted_text\n",
    "\n",
    "def process_line(value, reference, label):\n",
    "    name_match = re.search(fr'{reference} (.+)', value)\n",
    "    if name_match:\n",
    "        extracted_value = reference + \" \" + name_match.group(1)\n",
    "        return {label: extracted_value}\n",
    "    return None\n",
    "\n",
    "\n",
    "def format_number(number_str):\n",
    "    # Remove unwanted characters\n",
    "    number_str = number_str.replace('R$', '').replace('%', '')\n",
    "\n",
    "    # Extract the last numeric part using regex\n",
    "    number_str = re.findall(r'[\\d,\\.]+', number_str)[-1]\n",
    "\n",
    "    # Replace commas with dots (if you use commas as decimal separators)\n",
    "    number_str = number_str.replace(',', '.')\n",
    "\n",
    "    # Split by dots and rejoin, treating all but the last dot as thousands separators\n",
    "    parts = number_str.split('.')\n",
    "    if len(parts) > 1:\n",
    "        number_str = ''.join(parts[:-1]) + '.' + parts[-1]\n",
    "\n",
    "    return float(number_str)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Dicionário para armazenar o texto final processado para cada field\n",
    "    final_extracted_data = {}\n",
    "\n",
    "    # Itere pelos dados de texto extraídos e processe o texto para manter apenas os valores\n",
    "    for reference, extracted_text in extracted_text_data_default_lang.items():\n",
    "        # Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "        value = extracted_text.split('\\n')[-1]\n",
    "        # Remova qualquer espaço em branco à esquerda ou à direita\n",
    "        value = value.strip()\n",
    "        # Formate o valor usando a função format_number\n",
    "        value = format_number(value)\n",
    "        # Armazene o valor processado no dicionário final\n",
    "        final_extracted_data[reference] = value\n",
    "\n",
    "    # Display the final extracted data for the first few fields\n",
    "    list(final_extracted_data.items())[:15]\n",
    "    \n",
    "    \n",
    "def extract_fields_prestador_cnpj(text): # Função para extrair campos e valores dentro de um retângulo\n",
    "    nf_data_prestador_cnpj = {}\n",
    "    \n",
    "    nf_data_prestador_cnpj['secao'] = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    \n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in text:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "                        nf_data_prestador_cnpj['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                        nf_data_prestador_cnpj['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "\n",
    "\n",
    "    # Extrair Telefone\n",
    "    telefone_str = None\n",
    "    \n",
    "    #telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-])', text)\n",
    "    telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', text)\n",
    "    if telefone_match: \n",
    "        telefone_str = telefone_match.group(1)\n",
    "        # Remover quebras de linha\n",
    "        telefone_str = telefone_str.replace('.', '')\n",
    "        telefone_str = telefone_str.replace('\\n', '')\n",
    "                \n",
    "        nf_data_prestador_cnpj['telefone'] = telefone_str\n",
    "    else:\n",
    "        nf_data_prestador_cnpj['telefone'] = None   \n",
    "    \n",
    "    \n",
    "    return nf_data_prestador_cnpj \n",
    "\n",
    "\n",
    "def extract_fields_tomador_cnpj(text):\n",
    "    nf_data_tomador_cnpj = {}\n",
    "    \n",
    "    \n",
    "    nf_data_tomador_cnpj['secao'] = \"3.TOMADOR DE SERVIÇO\"\n",
    "    \n",
    "    \n",
    "    # Extrair CPF/CNPJ com máscara 1\n",
    "    if \"CPF/CNPJ:\" in text:\n",
    "        cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "        if cpf_cnpj_formatado_match:\n",
    "                        nf_data_tomador_cnpj['cpf_cnpj_com_mascara'] = cpf_cnpj_formatado_match.group(1)\n",
    "                        nf_data_tomador_cnpj['cpf_cnpj_sem_mascara'] = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "\n",
    "    \n",
    "    # Extrair Telefone\n",
    "    telefone_match = re.search(r'Telefone:\\s+(.+)', text)\n",
    "    if telefone_match:\n",
    "        telefone_str = telefone_match.group(1)\n",
    "        if telefone_str == 'Inscrição Estadual:':\n",
    "            nf_data_tomador_cnpj['telefone'] = None  # Valor padrão quando não há correspondência\n",
    "        elif telefone_str == '':\n",
    "            nf_data_tomador_cnpj['telefone'] = None  # Valor padrão quando não há correspondência\n",
    "                    \n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['telefone'] = telefone_match.group(1)\n",
    "            \n",
    "    \n",
    "    \n",
    "    # Extrair Inscrição Municipal\n",
    "    inscricao_municipal_match = re.search(r'Inscrição Municipal:\\s+(.+)', text)\n",
    "    if inscricao_municipal_match:\n",
    "        inscricao_municipal_str = inscricao_municipal_match.group(1)\n",
    "        if inscricao_municipal_str == \"Telefone:\": \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = inscricao_municipal_str\n",
    "    \n",
    "    insc_municipal_match = re.search(r'INSC:MUNICIPAL:\\s+(.+)', text)\n",
    "    if insc_municipal_match:\n",
    "        insc_municipal_str = insc_municipal_match.group(1)\n",
    "        if insc_municipal_str == \"Telefone:\":\n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_cnpj['inscricao_municipal'] = insc_municipal_str\n",
    "    else:\n",
    "        nf_data_tomador_cnpj['inscricao_municipal'] = None\n",
    "            \n",
    "    \n",
    "    return nf_data_tomador_cnpj\n",
    "\n",
    "\n",
    "def extract_fields_tomador_inscricao(text):\n",
    "    nf_data_tomador_inscricao = {}\n",
    "    \n",
    "    # Extrair RG    \n",
    "    rg_match = re.search(r'RG:\\s+(.+)', text)   \n",
    "    if rg_match:\n",
    "        rg_str = rg_match.group(1)\n",
    "        if rg_str == 'Inscrição Estadual:':\n",
    "            nf_data_tomador_inscricao['rg'] = None  # Valor padrão quando não há correspondência\n",
    "        else:    \n",
    "            nf_data_tomador_inscricao['rg'] = rg_match.group(1)  \n",
    " \n",
    "        \n",
    "                \n",
    "    # Extrair Inscrição Estadual\n",
    "    inscricao_estadual_match = re.search(r'Inscrição Estadual:\\s+(.+)', text)\n",
    "    if inscricao_estadual_match:\n",
    "        inscricao_estadual_str = inscricao_estadual_match.group(1)\n",
    "        if inscricao_estadual_str == '':\n",
    "            nf_data_tomador_inscricao['inscricao_estadual'] = None\n",
    "        else:    \n",
    "            nf_data_tomador_inscricao['inscricao_estadual'] = inscricao_estadual_match.group(1)   \n",
    "    else: \n",
    "        nf_data_tomador_inscricao['inscricao_estadual'] = None           \n",
    "\n",
    "    return nf_data_tomador_inscricao\n",
    "\n",
    "\n",
    "def extract_fields_tomador_dados(text):\n",
    "    \n",
    "    nf_data_tomador = {}\n",
    "\n",
    "    # Dividir o texto em linhas\n",
    "    linhas = text.split('\\n')\n",
    "\n",
    "    # Inicializar variáveis para armazenar os valores\n",
    "    nome_razao_social = None\n",
    "    endereco = None\n",
    "    email = None\n",
    "\n",
    "    # Iterar pelas linhas para identificar os campos e valores\n",
    "    i = 0\n",
    "    while i < len(linhas):\n",
    "        linha = linhas[i]\n",
    "        if \"Nome/Razão Social:\" in linha:\n",
    "            nome_razao_social = linhas[i + 1].strip()\n",
    "            i += 2\n",
    "        elif \"Endereço:\" in linha:\n",
    "            endereco = linhas[i + 1].strip()\n",
    "            i += 2\n",
    "        elif \"E-mail:\" in linha:\n",
    "            email = linhas[i + 1].strip()\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "\n",
    "    nf_data_tomador['razao_social'] = nome_razao_social\n",
    "    nf_data_tomador['endereco'] = endereco\n",
    "    nf_data_tomador['email'] = email\n",
    "    \n",
    "    return nf_data_tomador\n",
    "\n",
    "###################Funcoes Modelo e Frames\n",
    "\n",
    "# 1 - CABECALHO\n",
    "def processa_cabecalho():\n",
    "\n",
    "    nf_data_cabecalho = {}\n",
    "    model = \"mage_1\"\n",
    "    # Filtrar o DataFrame para incluir apenas linhas onde a coluna \"model\" oriundo de: modelo\n",
    "    filtered_frames_info = frames_info[frames_info['model'] == model]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "        frame_model = row_frame['model']\n",
    "        if frame_model == model:\n",
    "            frame_father = row_frame['label']\n",
    "            # ... Select specific frames ...\n",
    "            if frame_father == \"1_frame_prefeitura_nf\" or frame_father == \"1_frame_dados_nf\":\n",
    "                \n",
    "                # Extrai coordenadas para recorte\n",
    "                x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "                extracted_text_frame = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "                \n",
    "                seq = row_frame['seq']\n",
    "                #print(f'\\nLoop 1: {frame_model}, seq.: {seq}, row_frame[label]: {frame_father}\\n')\n",
    "                nf_data_cabecalho['secao'] = \"1 - CABECALHO\"\n",
    "                \n",
    "                for index_field, row_field in sframe_fields_info.iterrows():\n",
    "                    item_son = row_field['father']\n",
    "                    frame_father = row_frame['label']\n",
    "                    if item_son == frame_father:\n",
    "\n",
    "                        model_value = row_field['model']\n",
    "                        type_value = row_field['type']\n",
    "                        label_value = row_field['label']\n",
    "                        reference_value = row_field['reference']\n",
    "                        seq_value = row_field['seq']\n",
    "                        father_value = row_field['father']\n",
    "                        # ... Select specific fields ...\n",
    "                        if label_value == \"nome_prefeitura\" and model_value == frame_model: \n",
    "                            #print(f'  Loop 2: {model_value}, seq.: {seq_value}, type_value: {type_value}, label_value: {label_value}')\n",
    "                            values = extracted_text_frame.split('\\n')\n",
    "                            for value in values:\n",
    "                                result = process_line(value, reference_value, label_value)\n",
    "                                if result:\n",
    "                                    nf_data_cabecalho.update(result)\n",
    "                                    \n",
    "                        elif label_value == \"secretaria\" and model_value == frame_model: \n",
    "                            values = extracted_text_frame.split('\\n')\n",
    "                            for value in values:\n",
    "                                result = process_line(value, reference_value, label_value)\n",
    "                                if result:\n",
    "                                    nf_data_cabecalho.update(result)\n",
    "                                    \n",
    "                        elif label_value == \"tipo_nota_fiscal\" and model_value == frame_model:\n",
    "                            values = extracted_text_frame.split('\\n')\n",
    "                            for value in values:\n",
    "                                result = process_line(value, reference_value, label_value)\n",
    "                                if result:\n",
    "                                    nf_data_cabecalho.update(result)\n",
    "                        \n",
    "                        #Extraçao de Dados da NF            \n",
    "                        elif father_value == \"1_frame_dados_nf\" and model_value == frame_model:\n",
    "                            values = extracted_text_frame.split('\\n')\n",
    "                            data_list = [item for item in values if item != '']\n",
    "                            \n",
    "                            data_dict = {}\n",
    "                            for i in range(0, len(data_list), 2):\n",
    "                                key = data_list[i]\n",
    "                                value = data_list[i+1]\n",
    "                                data_dict[key] = value\n",
    "\n",
    "                            # You can now access the values using the corresponding labels\n",
    "                            nro_nota_fiscal = data_dict['Número da Nota:']\n",
    "                            nf_data_cabecalho['numero_nota_fiscal'] = data_dict['Número da Nota:']\n",
    "                            nf_data_cabecalho['competencia'] = data_dict['Competência:']\n",
    "                            nf_data_cabecalho['dt_hr_emissao'] = data_dict['ata e Hora da Emissão:']\n",
    "                            nf_data_cabecalho['codigo_verificacao'] = data_dict['Código Verificação:']\n",
    "                            \n",
    "    return nf_data_cabecalho, nro_nota_fiscal \n",
    "\n",
    "# 2 - PRESTADOR DE SERVIÇO\n",
    "def processa_prestador():\n",
    "\n",
    "    nf_data_prestador = {}\n",
    "    \n",
    "    warning = {}\n",
    "\n",
    "    # Filtrar o DataFrame para incluir apenas linhas onde a coluna \"model\" oriundo de: modelo\n",
    "    filtered_frames_info = frames_info[frames_info['model'] == modelo]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "        frame_model = row_frame['model']\n",
    "        if frame_model == model:\n",
    "            frame_father = row_frame['label']\n",
    "            # ... Select specific frames ...\n",
    "            if frame_father == \"2_frame_cnpj_prestador\":\n",
    "                \n",
    "                # Extrai coordenadas para recorte\n",
    "                x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "                extracted_text_frame = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "                nf_data_prestador = extract_fields_prestador_cnpj(extracted_text_frame)\n",
    "                \n",
    "            elif frame_father == \"2_frame_inscricao_prestador\":\n",
    "                # Extrai coordenadas para recorte\n",
    "                x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "                extracted_text_frame = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "                # 1. Prrestador de Servico - INSCRICAO ESTADUAL/MUNICIPAL\n",
    "                nf_data_prestador_incricao = {}\n",
    "\n",
    "                # Dividir o texto em linhas\n",
    "                linhas = extracted_text_frame.split('\\n')\n",
    "\n",
    "                # Inicializar variáveis para armazenar os valores\n",
    "                inscricao_municipal = None\n",
    "                inscricao_estadual = None\n",
    "\n",
    "\n",
    "                # Initialize variables\n",
    "                inscricao_municipal = \"\"\n",
    "                inscricao_estadual = \"\"\n",
    "\n",
    "                # Iterar pelas linhas para identificar os campos e valores\n",
    "                i = 0\n",
    "                while i < len(linhas):\n",
    "                    linha = linhas[i]\n",
    "                    try:\n",
    "                        if \"Inscrição Municipal:\" in linha:\n",
    "                            inscricao_municipal = linhas[i + 1].strip()\n",
    "                            i += 2\n",
    "                        elif \"Inscrição Estadual:\" in linha: \n",
    "                            inscricao_estadual = linhas[i + 1].strip()   \n",
    "                            i += 2\n",
    "                        else:\n",
    "                            i += 1\n",
    "                    except IndexError as e:\n",
    "                        # Log the error or print a warning\n",
    "                        print(f\"Aviso: Não é possível processar a linha {i}. {str(e)}\")\n",
    "                        warning = True\n",
    "                        warning_message = f\"Aviso: Não é possível processar a linha {i}. {str(e)}\"\n",
    "                        \n",
    "                        break # Exit the loop or continue based on your needs\n",
    "\n",
    "                if inscricao_municipal == \"\":\n",
    "                    inscricao_municipal = \"None\"        \n",
    "                if inscricao_estadual == \"\":\n",
    "                    inscricao_estadual = \"None\"\n",
    "\n",
    "                nf_data_prestador['inscricao_municipal'] = inscricao_municipal\n",
    "                nf_data_prestador['inscricao_estadual'] = inscricao_estadual\n",
    "\n",
    "\n",
    "                \n",
    "            elif frame_father == \"2_frame_dados_prestador\":\n",
    "                # Extrai coordenadas para recorte\n",
    "                x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "                extracted_text_frame = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "                tipo = None\n",
    "\n",
    "                if \"\\n\\n\" in extracted_text_frame:\n",
    "                    linhas = extracted_text_frame.split('\\n\\n')\n",
    "                    tipo = 2\n",
    "                    \n",
    "                else:\n",
    "                    linhas = extracted_text_frame.split('\\n')    \n",
    "                    tipo = 1\n",
    "                \n",
    "                # Inicializar variáveis para armazenar os valores\n",
    "                nome_razao_social = None\n",
    "                nome_fantasia = None\n",
    "                endereco = None\n",
    "                email = None\n",
    "\n",
    "                # Iterar pelas linhas para identificar os campos e valores\n",
    "                i = 0\n",
    "                while i < len(linhas):\n",
    "                    linha = linhas[i]\n",
    "                    if \"Nome/Razão Social:\" in linha:\n",
    "                        if tipo == 2:\n",
    "                            texto1 = linhas[i + 1].strip()\n",
    "                            texto2 = texto1.split('\\n')\n",
    "                            nome_razao_social = texto2[i].strip()\n",
    "                        else:\n",
    "                            nome_razao_social = linhas[i + 1].strip()   \n",
    "\n",
    "                        i += 2\n",
    "                    elif \"Nome de Fantasia:\" in linha:\n",
    "                        nome_fantasia = linhas[i + 1].strip()\n",
    "                        i += 2\n",
    "                    elif \"Endereço:\" in linha:\n",
    "    \n",
    "                        if tipo == 2:\n",
    "                            texto3 = linhas[i + 1].strip()\n",
    "                            texto4 = texto3.split('\\n')\n",
    "                            \n",
    "                            endereco = texto4[0].strip()\n",
    "                        else:\n",
    "                            endereco = linhas[i + 1].strip()      \n",
    "                        i += 2\n",
    "                    elif \"E-mail:\" in linha:\n",
    "                        email = linhas[i + 1].strip()\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        i += 1\n",
    "\n",
    "                nf_data_prestador['razao_social'] = nome_razao_social\n",
    "                nf_data_prestador['nome_fantasia'] = nome_fantasia\n",
    "                nf_data_prestador['endereco'] = endereco\n",
    "                nf_data_prestador['email'] = email\n",
    "\n",
    "    return nf_data_prestador\n",
    "\n",
    "# 3 - TOMADOR DE SERVIÇO\n",
    "def processa_tomador():\n",
    "    nf_data_tomador = {}\n",
    "\n",
    "    # Filtrar o DataFrame para incluir apenas linhas onde a coluna \"model\" oriundo de: modelo\n",
    "    filtered_frames_info = frames_info[frames_info['model'] == modelo]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "        frame_model = row_frame['model']\n",
    "        if frame_model == model:\n",
    "            frame_father = row_frame['label']\n",
    "            # ... Select specific frames ...\n",
    "            if frame_father == \"3_frame_cnpj_tomador\":\n",
    "                # Extrai coordenadas para recorte\n",
    "                x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "                extracted_text_frame = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "                nf_data_tomador_cnpj = extract_fields_tomador_cnpj(extracted_text_frame)\n",
    "                \n",
    "            elif frame_father == \"3_frame_inscricao_tomador\":\n",
    "                # Extrai coordenadas para recorte\n",
    "                x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "                extracted_text_frame = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "                nf_data_tomador_inscricao = extract_fields_tomador_inscricao(extracted_text_frame)\n",
    "            \n",
    "            elif frame_father == \"3_frame_dados_tomador\":\n",
    "                # Extrai coordenadas para recorte\n",
    "                x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "                extracted_text_frame = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "                nf_data_tomador = extract_fields_tomador_dados(extracted_text_frame)\n",
    "                \n",
    "    nf_data_tomador.update(nf_data_tomador_cnpj) \n",
    "    nf_data_tomador.update(nf_data_tomador_inscricao)\n",
    "    nf_data_tomador.update(nf_data_tomador)\n",
    "    \n",
    "    return nf_data_tomador     \n",
    "\n",
    "# 4 - DESCRIMINACAO DOS SERVIÇOS\n",
    "def processa_servico():\n",
    "    \n",
    "    nf_data_servico = {}\n",
    "\n",
    "    # Filtrar o DataFrame para incluir apenas linhas onde a coluna \"model\" oriundo de: modelo\n",
    "    filtered_frames_info = frames_info[frames_info['model'] == modelo]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "        frame_model = row_frame['model']\n",
    "        if frame_model == model:\n",
    "            frame_father = row_frame['label']\n",
    "            # ... Select specific frames ...\n",
    "            if frame_father == \"4_frame_descricao_totais\":\n",
    "                nf_data_servico['secao'] = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "                # Extrai coordenadas para recorte\n",
    "                x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "                extracted_text_frame = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "                text = extracted_text_frame.replace('\\n', ' ')\n",
    "                label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "                if text.startswith(label):\n",
    "                    text = text[len(label):].strip()\n",
    "                nf_data_servico['discriminacao_servicos'] = text \n",
    "            \n",
    "    return nf_data_servico\n",
    "\n",
    "# 5 - VALOR TOTAL\n",
    "def processa_total():\n",
    "    \n",
    "    nf_data_valor_total = {}\n",
    "    # Filtrar o DataFrame para incluir apenas linhas onde a coluna \"model\" oriundo de: modelo\n",
    "    filtered_frames_info = frames_info[frames_info['model'] == modelo]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "        frame_model = row_frame['model']\n",
    "        if frame_model == model:\n",
    "            frame_father = row_frame['label']\n",
    "            # ... Select specific frames ...\n",
    "            if frame_father == \"4_frame_descricao_totais\":\n",
    "                nf_data_valor_total['secao'] = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "                # Extrai coordenadas para recorte\n",
    "                x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "                extracted_text_frame = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "                text = extracted_text_frame.replace('\\n', ' ')\n",
    "                label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "                if text.startswith(label):\n",
    "                    text = text[len(label):].strip()\n",
    "                nf_data_valor_total['discriminacao_servicos'] = text\n",
    "                 \n",
    "    return nf_data_valor_total  \n",
    "\n",
    "# 6 - CNAE e Item da Lista de Serviços\n",
    "def processa_cnae_itens():\n",
    "    \n",
    "    nf_data_cnae = {}\n",
    "\n",
    "\n",
    "    # Filtrar o DataFrame para incluir apenas linhas onde a coluna \"model\" oriundo de: modelo\n",
    "    filtered_frames_info = frames_info[frames_info['model'] == modelo]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "        frame_model = row_frame['model']\n",
    "        if frame_model == model:\n",
    "            frame_father = row_frame['label']\n",
    "            # ... Select specific frames ...\n",
    "            if frame_father == \"4_frame_cnae_itens_servico\":\n",
    "                nf_data_cnae['secao'] = \"6. CNAE e Item da Lista de Serviços\"\n",
    "                x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "                extracted_text_frame = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "                \n",
    "                linhas = extracted_text_frame.split('\\n')\n",
    "                i = len(linhas)\n",
    "                texto_cnae = linhas[0].replace(' . .', '')\n",
    "                cnae = re.sub(r'^CNAE - ', '', texto_cnae, count=1)\n",
    "\n",
    "                # Item da Lista de Serviços \n",
    "                texto_novo = linhas[1]\n",
    "                texto_novo = texto_novo.replace('\\n', '')\n",
    "\n",
    "                item_lista_servico = re.sub(r'^Item da Lista de Serviços - ', '', texto_novo, count=1)\n",
    "\n",
    "                nf_data_cnae['cnae'] = cnae\n",
    "                nf_data_cnae['item_lista_servicos'] = item_lista_servico\n",
    "                \n",
    "    return nf_data_cnae  \n",
    "\n",
    "# 7 - VALORES E IMPOSTOS\n",
    "def extract_fields_box(modelo, father_value, section):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    filtered_boxes_info = field_boxes_info[(field_boxes_info['father'] == father_value) & (field_boxes_info['model'] == modelo)]\n",
    "    # Iterate nas informações dos boxes de fields e extraia o texto de cada field\n",
    "    for index_field, row_field in filtered_boxes_info.iterrows():\n",
    "        x0, y0, x1, y1 = row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        #print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference'], row_field['x0'], row_field['y0'], row_field['x1'], row_field['y1'] ))\n",
    "        # Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "        value = extracted_text_box.split('\\n')[-1]\n",
    "        # Remova qualquer espaço em branco à esquerda ou à direita\n",
    "        value = value.strip()\n",
    "        if row_field['t_value'] == 'number':\n",
    "            # Formate o valor usando a função format_number\n",
    "            #print(\"vou verificar valor\")\n",
    "            value = format_number(value)\n",
    "            #print(value)\n",
    "        # Armazene o texto extraído com o rótulo correspondente\n",
    "        label = row_field['label']\n",
    "        data_box_valores[label] = value\n",
    "        \n",
    "    return data_box_valores\n",
    "\n",
    "# secao: 8 - DADOS COMPLEMENTARES & 10. OBSERVACOES\n",
    "def extract_dados_comple_obs(modelo, frame_father, section):\n",
    "    \n",
    "    data_dados_complementares = {}\n",
    "    #frame_label = frame_father\n",
    "    \n",
    "    # 1. Filtrando o frames_info para buscar os dados de corte\n",
    "    filtered_frames_info = frames_info[(frames_info['label'] == frame_father) & (frames_info['model'] == modelo)]\n",
    "\n",
    "    # 2. Filtrando o sframe_fields_info para buscar os dados dos campos que estao nos frames\n",
    "    filtered_sframe_fields_info = sframe_fields_info[(sframe_fields_info['father'] == frame_father) & (sframe_fields_info['model'] == modelo)]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "        \n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        \n",
    "        print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_frame['seq'], row_frame['model'], row_frame['father'], row_frame['label'], row_frame['reference'], row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'] ))\n",
    "        for index_field, row_field in filtered_sframe_fields_info.iterrows():\n",
    "            #print(\"{:<5} {:<10} {:<30} {:<20} {:<20}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference']))\n",
    "            \n",
    "            if frame_father == \"5_frame_dados_complementares\":\n",
    "                nf_data_dados_complementares = {}\n",
    "                nf_data_dados_complementares['section'] = section\n",
    "                \n",
    "                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^DADOS COMPLEMENTARES', '', extracted_text_box, count=1)\n",
    "                if text == '':\n",
    "                    text = None\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text\n",
    "                else:    \n",
    "                    # Extrair texto dentro do retângulo\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "                    \n",
    "                return nf_data_dados_complementares                \n",
    "                \n",
    "            elif frame_father == \"5_frame_observacao\":\n",
    "                nf_data_observacao = {}\n",
    "                nf_data_observacao['section'] = section \n",
    "                                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^Observação:', '', extracted_text_box, count=1)\n",
    "\n",
    "                # Remover quebras de linha\n",
    "                text = text.replace('\\n', ' ')\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                nf_data_observacao['observacao'] = text.strip()\n",
    "                \n",
    "                return nf_data_observacao        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_data_tomador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcoes de manejo de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move NF processadas ok\n",
    "def move_pdf_processed_ok(document_path, nf_processada_path, batch_name, doc2convert):\n",
    "    # Determine the destination directory\n",
    "    destination_dir = os.path.join(nf_processada_path, batch_name)\n",
    "\n",
    "    # Check if the destination directory exists; if not, create it\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "\n",
    "    # Determine the destination path including the filename\n",
    "    destination_path = os.path.join(destination_dir, os.path.basename(document_path))\n",
    "\n",
    "    # Move the file from the source path to the destination path\n",
    "    try:\n",
    "        shutil.move(document_path, destination_path)\n",
    "        print(f\"Sucesso ao mover: {document_path} para: {destination_path}\")\n",
    "        return True, destination_path, None  # Success, destination path, no error\n",
    "    except Exception as e:\n",
    "        error_message = f\"Erro ao mover: {document_path} para: {destination_path}: {str(e)}\"\n",
    "        print(error_message)\n",
    "        return False, None, error_message  # Failure, no destination path, error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_dict[name_prefeitura]:\n",
    "    modelo = model_dict[name_prefeitura]\n",
    "    model = model_dict[name_prefeitura]\n",
    "    message_log = f\"Nome de prefeitura: {name_prefeitura} encontrada para: {doc2convert} modelo a ser utilizado: {modelo}\"\n",
    "    print(message_log)\n",
    "    #logging.info(message_log) \n",
    "else:\n",
    "    modelo = model_padrao\n",
    "    message_log = f'\\nmodelo nao encontrado para prefeitura {name_prefeitura}, sera utilizado o modelo {modelo}'   \n",
    "    print(message_log)\n",
    "    #logging.info(message_log)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Leitura recursiva de diretorios e arquivos a partir de root\n",
    "#logging.info(\"Processo iniciado.\")\n",
    "#warning_messages = {}\n",
    "nf_data_servico = {}\n",
    "pdf_info = {}  # Dicionário para armazenar informações sobre PDFs\n",
    "\n",
    "for roots, directories, documents in os.walk(root_pdf_path):\n",
    "    batch_name = os.path.basename(roots)\n",
    "    source_path = roots\n",
    "    for document in documents:\n",
    "        doc2convert = document\n",
    "        document_path_1 = os.path.join(roots, document)\n",
    "        message_log = f\"Processo iniciado para lote: {directories}\"\n",
    "        #logging.info(message_log)\n",
    "        print(f'{roots}  | Batch: {batch_name} | doc2convert: {doc2convert} | document_path_1: {document_path_1}\\n')\n",
    "        #logging.info(\"Processando arquivo: %s\", document_path_1)\n",
    "        message_log = is_pdf_searchable(document_path_1)\n",
    "        #logging.info(message_log)\n",
    "        \n",
    "\n",
    "        \n",
    "        message_log = f\"Processo de extraçao iniciado para documento: {doc2convert}\"\n",
    "        #logging.info(message_log)\n",
    "        \n",
    "        root = roots\n",
    "        \n",
    "        # 2. Ajusta o nome do arquivo tirando caracteres especiais e a extensao\n",
    "        doc2convert_named = conv_filename_no_ext(doc2convert)\n",
    "        \n",
    "        message_log = f\"Processado nome para documento: {doc2convert} -> {doc2convert_named}\"\n",
    "        #logging.info(message_log)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # 3. Converte e resize da imagem \"on the fly\"\n",
    "        image_2work, name_image_2work = convertResize(doc2convert_named, document_path_1, image_resized_path) #Utilizado como origem: document_path_1\n",
    "\n",
    "        #print(name_image_2work)\n",
    "        message_log = f\"Documento {doc2convert} convertido e ajustado para tamanho arquivo: {name_image_2work}\"\n",
    "        #logging.info(message_log) \n",
    "\n",
    "\n",
    "        # 4. Busco nome da prefeitura e modelo\n",
    "        name_prefeitura = pequisaModel(image_2work)\n",
    "\n",
    "        modelo = model_dict[name_prefeitura]\n",
    "\n",
    "        if model_dict[name_prefeitura]:\n",
    "            modelo = model_dict[name_prefeitura]\n",
    "            model = model_dict[name_prefeitura]\n",
    "            message_log = f\"Nome de prefeitura: {name_prefeitura} encontrada para: {doc2convert} modelo a ser utilizado: {modelo}\"\n",
    "            #logging.info(message_log) \n",
    "        else:\n",
    "            modelo = model_padrao\n",
    "            message_log = f'\\nmodelo nao encontrado para prefeitura {name_prefeitura}, sera utilizado o modelo {modelo}'   \n",
    "            print(message_log)\n",
    "            #logging.info(message_log)   \n",
    "        \n",
    "        nro_nota = None\n",
    "            \n",
    "        # secao: 1 - CABECALHO\n",
    "        data_cabecalho = {}\n",
    "        data_cabecalho, nro_nota = processa_cabecalho() \n",
    "        \n",
    "        # secao: 2 - PRESTADOR DE SERVIÇO\n",
    "        data_prestador = {}\n",
    "        data_prestador = processa_prestador()  \n",
    "        \n",
    "        # secao: 3 - TOMADOR DE SERVIÇO\n",
    "        data_tomador = {}\n",
    "        data_tomador = processa_tomador()  \n",
    "        \n",
    "        # secao: 4 - DESCRIMINACAO DOS SERVIÇOS\n",
    "        data_servico = {}\n",
    "        data_servico = processa_servico()\n",
    "        \n",
    "        # secao: 5 - VALOR TOTAL\n",
    "        data_valor_total = {}\n",
    "        #data_valor_total = processa_total()\n",
    "        father_value = \"4_frame_valor_total\"\n",
    "        section = \"5. VALOR TOTAL\"\n",
    "        \n",
    "        result = extract_fields_box(modelo, father_value, section)\n",
    "        if result:\n",
    "            data_valor_total.update(result)\n",
    "        \n",
    "        # secao: 6 - CNAE e Item da Lista de Serviços\n",
    "        data_CNAE = {}\n",
    "        data_CNAE = processa_cnae_itens()\n",
    "\n",
    "        # secao: 7 - VALORES E IMPOSTOS & 9 - OUTRAS INFORMAÇOES / CRITICAS\n",
    "        data_valores = {}\n",
    "        father_value = \"5_frame_valores_impostos\"\n",
    "        section = \"7. VALORES E IMPOSTOS\"\n",
    "\n",
    "        result = extract_fields_box(modelo, father_value, section)\n",
    "        if result:\n",
    "            data_valores.update(result)\n",
    "            \n",
    "        # secao: 8 - DADOS COMPLEMENTARES\"\n",
    "        data_dados_complementares = {}\n",
    "        f_father = \"5_frame_dados_complementares\"\n",
    "        section = \"8. DADOS COMPLEMENTARES\"\n",
    "\n",
    "        data_dados_complementares = extract_dados_comple_obs(modelo, f_father, section)                                           \n",
    "                                \n",
    "                                \n",
    "        # secao: 9 - OUTRAS INFORMAÇOES / CRITICAS\n",
    "        data_outras_informacoes = {}\n",
    "        father_value = \"5_frame_inf_criticas\"\n",
    "        section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "\n",
    "        result = extract_fields_box(modelo, father_value, section)\n",
    "        if result:\n",
    "            data_outras_informacoes.update(result)                        \n",
    "                            \n",
    "\n",
    "        # secao: 10. OBSERVACOES\n",
    "        data_observacao = {}\n",
    "        f_father = \"5_frame_observacao\"\n",
    "        section = \"10. OBSERVACOES\"\n",
    "\n",
    "        data_observacao = extract_dados_comple_obs(modelo, f_father, section)\n",
    "        \n",
    "        \n",
    "        #nr_nro_nf = nro_nota\n",
    "        \n",
    "        # calling this function    \n",
    "        success, dest_path, error_message = move_pdf_processed_ok(document_path_1, nf_processada_path, batch_name, doc2convert)\n",
    "        if success:\n",
    "            print(f\"Sucesso: {dest_path}\")\n",
    "        else:\n",
    "            print(f\"Um erro ocorreu: {error_message}\") \n",
    "        \n",
    "            \n",
    "        nome_arquivo_json = os.path.basename(root) + \".json\"\n",
    "        nome_arquivo = doc2convert\n",
    "                                    \n",
    "        pdf_info[nro_nota] = {\n",
    "            \"dados_NF_PDF\": {\n",
    "                \"data_cabecalho\": data_cabecalho,\n",
    "                \"data_prestador\": data_prestador,\n",
    "                \"data_tomador\": data_tomador,\n",
    "                \"data_servico\": data_servico,\n",
    "                \"data_valor_total\": data_valor_total,\n",
    "                \"data_CNAE\": data_CNAE,\n",
    "                \"data_valores\": data_valores,\n",
    "                \"data_dados_complementares\": data_dados_complementares,\n",
    "                \"data_outras_informacoes\": data_outras_informacoes,\n",
    "                \"data_observacao\": data_observacao,\n",
    "                },\n",
    "            \"diretorio\": os.path.basename(root),\n",
    "            \"nome_arquivo\": nome_arquivo,    \n",
    "        }            \n",
    "            \n",
    "                        \n",
    "# Salvando as informações em um arquivo JSON\n",
    "json_file_path = os.path.join(json_path, nome_arquivo_json)\n",
    "with open(json_file_path, \"w\", encoding='utf-8') as json_file:\n",
    "    json.dump(pdf_info, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"As informações foram salvas em {json_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_image_2work = \"pipeline_extracao_documentos/6_geral_administacao/images/processadas/nota fiscal 37420230815_11212276.pdf.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping for color names to RGB values\n",
    "color_mapping = {\n",
    "    \"red\": (255, 0, 0),\n",
    "    \"purple\": (128, 0, 128),\n",
    "    \"orange\": (255, 165, 0),\n",
    "    \"green\": (0, 128, 50),\n",
    "    \"blue\": (0, 0, 255),\n",
    "    \"yellow\": (255, 255, 0)\n",
    "}\n",
    "\n",
    "# Reload the image to start fresh\n",
    "image = Image.open(name_image_2work)\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Define a font size for the labels using the default PIL font\n",
    "font_size = 100\n",
    "#font = ImageFont.load_default()\n",
    "\n",
    "font = ImageFont.truetype(\"/usr/share/fonts/truetype/ubuntu/Ubuntu-M.ttf\", 30, encoding=\"unic\")\n",
    "\n",
    "# Update the draw_box function to use the larger font size with the default font\n",
    "def draw_box(row):\n",
    "    x0, y0, x1, y1 = row['x0'], row['y0'], row['x1'], row['y1']\n",
    "    color = color_mapping.get(row['color'], (0, 0, 0)) # Default to black if color not found\n",
    "    draw.rectangle([x0, y0, x1, y1], outline=color, width=3)\n",
    "    label = str(row['label']) if pd.notnull(row['label']) else None # Check for missing label\n",
    "    if label:\n",
    "        draw.text((x0 + 5, y0 + 5), label, fill=color, font=font)\n",
    "\n",
    "# Draw the boundaries\n",
    "#draw_box(boundaries_info)\n",
    "\n",
    "\n",
    "def draw_box_model(modelo,\n",
    "                   boundaries_info=None,\n",
    "                   sections_info=None,\n",
    "                   frames_info=None,\n",
    "                   field_boxes_info=None,\n",
    "                   draw_boundaries=True,\n",
    "                   draw_sections=True,\n",
    "                   draw_frames=True,\n",
    "                   draw_field_boxes=True):\n",
    "    \n",
    "    # Draw boundaries if requested\n",
    "    if draw_boundaries and boundaries_info is not None:\n",
    "        filtered_boundaries_info = boundaries_info[boundaries_info['model'] == modelo]\n",
    "        for index, row in filtered_boundaries_info.iterrows():\n",
    "            draw_box(row)\n",
    "\n",
    "    # Draw sections if requested\n",
    "    if draw_sections and sections_info is not None:\n",
    "        filtered_sections_info = sections_info[sections_info['model'] == modelo]\n",
    "        for index, row in filtered_sections_info.iterrows():\n",
    "            draw_box(row)\n",
    "            \n",
    "    # Draw frames if requested\n",
    "    if draw_frames and frames_info is not None:\n",
    "        filtered_frames_info = frames_info[frames_info['model'] == modelo]\n",
    "        for index, row in filtered_frames_info.iterrows():\n",
    "            draw_box(row)\n",
    "            \n",
    "    # Draw field boxes if requested\n",
    "    if draw_field_boxes and field_boxes_info is not None:\n",
    "        filtered_field_boxes_info = field_boxes_info[field_boxes_info['model'] == modelo]\n",
    "        for index, row in filtered_field_boxes_info.iterrows():\n",
    "            draw_box(row)\n",
    "    \n",
    "    # Show the image with selected drawings\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = \"mage_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To draw everything\n",
    "draw_box_model(modelo, boundaries_info, sections_info, frames_info, field_boxes_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To draw only boundaries and sections:\n",
    "draw_box_model(modelo, boundaries_info, sections_info, draw_frames=False, draw_field_boxes=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To draw only field boxes:\n",
    "draw_box_model(modelo, field_boxes_info=field_boxes_info, draw_boundaries=False, draw_sections=False, draw_frames=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chamada de funcoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Ajusta o nome do arquivo tirando caracteres especiais e a extensao\n",
    "doc2convert_named = conv_filename_no_ext(doc2convert)\n",
    "\n",
    "\n",
    "# 3. Converte e resize da imagem \"on the fly\"\n",
    "image_2work, name_image_2work = convertResize(doc2convert_named, document_path_1, image_resized_path) #Utilizado como origem: document_path_1\n",
    "\n",
    "#print(name_image_2work) \n",
    "\n",
    "\n",
    "# 4. Busco nome da prefeitura e modelo\n",
    "name_prefeitura = pequisaModel(image_2work)\n",
    "\n",
    "modelo = model_dict[name_prefeitura]\n",
    "\n",
    "if model_dict[name_prefeitura]:\n",
    "    modelo = model_dict[name_prefeitura]\n",
    "    print(f'\\nNome Prefeitura: {name_prefeitura},  modelo: {modelo}') \n",
    "else:\n",
    "    modelo = model_padrao\n",
    "    message = f'\\nmodelo nao encontrado para prefeitura {name_prefeitura}, sera utilizado o modelo {modelo}'   \n",
    "    print(message)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines Atuais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final - tratamento dos arquivos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipo_pdf = \"pequisavel\"\n",
    "tipo_pdf = \"nao pequisavel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Definir o tipo de PDF a buscar  (TESTE APENAS)\n",
    "#source_directory = root_external_pdf_path\n",
    "\n",
    "#print(source_directory )\n",
    "destination_directory = root_pdf_path\n",
    "if tipo_pdf == \"pequisavel\":\n",
    "    source_directory = root_external_pdf_pesquisavel_path\n",
    "    print(f'\\nTipo PDF a utilizar: {tipo_pdf} | source_directory: {source_directory} \\n\\nDocumento que sera movido: {document_path_1}\\n')\n",
    "    \n",
    "else:\n",
    "    source_directory = root_external_pdf_path\n",
    "\n",
    "#print(f'\\nTipo PDF a utilizar: {tipo_pdf} | source_directory: {source_directory} \\n\\nDocumento que sera movido: {document_path_1}\\n')   \n",
    "\n",
    "\n",
    "# 5. MOVER PDF para diretorio de processados\n",
    "source_path = document_path_1\n",
    "destination_path = os.path.join(f'{nf_processada_path}/{str(doc2convert)}')\n",
    "\n",
    "print(f'\\nsource_path: {source_path} | destination_path: {destination_path} \\n') \n",
    "\n",
    "\n",
    "shutil.move(source_path, destination_path)\n",
    "\n",
    "# 6. ***DELETAR**** o arquivo de imagem gerado\n",
    "image_path_to_delete = name_image_2work\n",
    "os.remove(image_path_to_delete)\n",
    "\n",
    "\n",
    "# Verifique se o diretório de destino está vazio\n",
    "if os.listdir(destination_directory):\n",
    "    raise Exception(\"O diretório de destino não está vazio!\")\n",
    "\n",
    "for roots, directories, documents in os.walk(source_directory):\n",
    "    # Filtre os documentos para incluir apenas aqueles com extensão .pdf\n",
    "    pdf_files = [doc for doc in documents if doc.lower().endswith('.pdf')]\n",
    "    \n",
    "    if pdf_files: # Verifique se há algum arquivo PDF no diretório\n",
    "        first_pdf_file = pdf_files[0] # Obtenha o primeiro arquivo PDF\n",
    "        source_path = os.path.join(roots, first_pdf_file) # Construa o caminho completo para o primeiro arquivo PDF\n",
    "        print(f\"Found the first PDF file: {source_path}\")\n",
    "        \n",
    "        # Mova o arquivo para o diretório de destino\n",
    "        shutil.move(source_path, destination_directory)\n",
    "        \n",
    "        break # Saia do loop após encontrar o primeiro arquivo PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outros itens do Processo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVISAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "               # Iterar pelas linhas para identificar os campos e valores\n",
    "                i = 0\n",
    "                while i < len(linhas):\n",
    "                    linha = linhas[i]\n",
    "                    if \"Inscrição Municipal:\" in linha:\n",
    "                        inscricao_municipal = linhas[i + 1].strip()\n",
    "                        i += 2\n",
    "                    elif \"Inscrição Estadual:\" in linha: \n",
    "                        inscricao_estadual = linhas[i + 1].strip()   \n",
    "                        i += 2\n",
    "                    else:\n",
    "                        i += 1\n",
    "                        \n",
    "                if inscricao_municipal == \"\":\n",
    "                    inscricao_municipal = \"None\"        \n",
    "                if inscricao_estadual == \"\":\n",
    "                    inscricao_estadual = \"None\"\n",
    "\n",
    "                nf_data_prestador['inscricao_municipal'] = inscricao_municipal\n",
    "                nf_data_prestador['inscricao_estadual'] = inscricao_estadual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos de pipelines utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(root_pdf_path):\n",
    "    print(f'{root}  | {dirs} | {document} | {files}\\n')\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        #if file.lower().endswith('.pdf'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo iniciado com funçoes do model\n",
    "\n",
    "nf_model = \"mage_1\"\n",
    "\n",
    "nf_data_servico = {}\n",
    "\n",
    "pdf_info = {}  # Dicionário para armazenar informações sobre PDFs\n",
    "\n",
    "for root, dirs, files in os.walk(root_directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        if file.lower().endswith('.pdf'):\n",
    "            #print(file_path)\n",
    "                \n",
    "            # 1. Conversao para imagem e resize\n",
    "            converte2image(file_path)\n",
    "                \n",
    "            # 2. Resize\n",
    "            resizeImage(image_src)\n",
    "                \n",
    "            # 3. Executar cortes\n",
    "            #file_path = img_resi_path\n",
    "            #image_to_crop = Image.open(img_resi_path).convert(\"RGB\")\n",
    "            #width, height = image_to_crop.size\n",
    "\n",
    "            cropSections(nf_model, img_resi_path)\n",
    "                \n",
    "            for roots, directories, images in os.walk(root_dir_section_images):\n",
    "                for image in images:\n",
    "                    #print(f'\\n{roots}, {directories}, {images}')\n",
    "                    print(image)\n",
    "                    image_path = os.path.join(roots, image)\n",
    "                    #print(image_path, image)\n",
    "                    frame = image\n",
    "                    if frame == \"0_frame_dados_nf.jpg\":\n",
    "                        nro_nota = 0\n",
    "                        image_to_ocr = Image.open(image_path).convert('RGB')\n",
    "                        text = pytesseract.image_to_string(image_to_ocr, lang='por', config=tessdata_dir_config)\n",
    "                        nf_data_dados, nro_nota = extract_fields_dados(text)\n",
    "\n",
    "                    if frame == \"0_frame_prefeitura_nf.jpg\":\n",
    "                        image_to_ocr = Image.open(image_path).convert('RGB')\n",
    "                        text = pytesseract.image_to_string(image_to_ocr, lang='por', config=tessdata_dir_config)\n",
    "                        nf_data_prefeitura, nome_prefeitura  = extract_fields_prefeitura(text)    \n",
    "                        \n",
    "                    if frame == \"1_frame_prestador_cnpj.jpg\": \n",
    "                        image_to_ocr = Image.open(image_path).convert('RGB')\n",
    "                        text = pytesseract.image_to_string(image_to_ocr, lang='por', config=tessdata_dir_config)\n",
    "                        nf_data_prestador = extract_fields_prestador(text)\n",
    "                        \n",
    "                               \n",
    "                    if frame == \"1_frame_prestador_inscricao.jpgg\": \n",
    "                        image_to_ocr = Image.open(image_path).convert('RGB')\n",
    "                        text = pytesseract.image_to_string(image_to_ocr, lang='por', config=tessdata_dir_config)\n",
    "                        nf_data_prestador = extract_fields_prestador(text)    \n",
    "                            \n",
    "                    if frame == \"1_frame_prestador_servico.jpg\": \n",
    "                        image_to_ocr = Image.open(image_path).convert('RGB')\n",
    "                        text = pytesseract.image_to_string(image_to_ocr, lang='por', config=tessdata_dir_config)\n",
    "                        nf_data_prestador = extract_fields_prestador(text)      \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    if frame == \"2_frame_tomador_cnpj.jpg\": \n",
    "                        image_to_ocr = Image.open(image_path).convert('RGB')\n",
    "                        text = pytesseract.image_to_string(image_to_ocr, lang='por', config=tessdata_dir_config)\n",
    "                        nf_data_tomador_cnpj = extract_fields_tomador_cnpj(text)                                                       \n",
    "                    \n",
    "                    \n",
    "                    if frame == \"2_frame_tomador_inscricao.jpg\": \n",
    "                        image_to_ocr = Image.open(image_path).convert('RGB')\n",
    "                        text = pytesseract.image_to_string(image_to_ocr, lang='por', config=tessdata_dir_config)\n",
    "                        nf_data_tomador_inscricao = extract_fields_tomador_inscricao(text)                            \n",
    "                        \n",
    "                        \n",
    "                    if frame == \"2_frame_tomador_servico.jpg\": \n",
    "                        image_to_ocr = Image.open(image_path).convert('RGB')\n",
    "                        text = pytesseract.image_to_string(image_to_ocr, lang='por', config=tessdata_dir_config)\n",
    "                        nf_data_tomador = extract_fields_tomador(text)  \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # 6. Processa analise de tabela\n",
    "                            #results = processaTabela(image_path)\n",
    "                            \n",
    "                            # 7. Processa OCR\n",
    "                            #dados_da_tabela = processaOcrTable(image_path, results)\n",
    "                            \n",
    "                            #for i in dados_da_tabela:\n",
    "                                #if dados_da_tabela[i]['label'] == \"table column\":\n",
    "                                    #text = dados_da_tabela[i]['texto']\n",
    "                                    #print(f\"\\n{i}: \\n{dados_da_tabela[i]['texto']}\")\n",
    "                                    \n",
    "                    nr_nro_nf = nro_nota\n",
    "                        \n",
    "                    nome_arquivo_json = os.path.basename(root) + \".json\"\n",
    "                    nome_arquivo = file\n",
    "                                                \n",
    "                    pdf_info[nr_nro_nf] = {\n",
    "                        \"dados_NF_PDF\": {\n",
    "                            \"data_nf\": nf_data_dados,\n",
    "                            \"data_prefeitura\": nf_data_prefeitura,\n",
    "                            \"data_prestador\": nf_data_prestador,\n",
    "                            \"data_tomador_cnpj\": nf_data_tomador_cnpj,\n",
    "                            \"data_tomador_inscricao\": nf_data_tomador_inscricao,\n",
    "                            \"data_tomador\": nf_data_tomador,\n",
    "                            },\n",
    "                        \"diretorio\": os.path.basename(root),\n",
    "                        \"nome_arquivo\": nome_arquivo,    \n",
    "                    }            \n",
    "                        \n",
    "                        \n",
    "# Salvando as informações em um arquivo JSON\n",
    "json_file_path = os.path.join(target_directory, nome_arquivo_json)\n",
    "with open(json_file_path, \"w\", encoding='utf-8') as json_file:\n",
    "    json.dump(pdf_info, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"As informações foram salvas em {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chamada das funcoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secao: 1 - CABECALHO\n",
    "data_cabecalho = {}\n",
    "data_cabecalho = processa_cabecalho()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secao: 2 - PRESTADOR DE SERVIÇO\n",
    "data_prestador = {}\n",
    "data_prestador = processa_prestador()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secao: 3 - TOMADOR DE SERVIÇO\n",
    "data_tomador = {}\n",
    "data_tomador = processa_tomador()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secao: 4 - DESCRIMINACAO DOS SERVIÇOS\n",
    "data_servico = {}\n",
    "data_servico = processa_servico()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secao: 5 - VALOR TOTAL\n",
    "data_valor_total = {}\n",
    "data_valor_total = processa_total()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secao: 6 - CNAE e Item da Lista de Serviços\n",
    "data_CNAE = {}\n",
    "data_CNAE = processa_cnae_itens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secao: 7 - VALORES E IMPOSTOS & 9 - OUTRAS INFORMAÇOES / CRITICAS\n",
    "data_valores = {}\n",
    "father_value = \"5_frame_valores_impostos\"\n",
    "section = \"7. VALORES E IMPOSTOS\"\n",
    "\n",
    "result = extract_fields_box(modelo, father_value, section)\n",
    "if result:\n",
    "    data_valores.update(result)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secao: 8 - DADOS COMPLEMENTARES\"\n",
    "data_dados_complementares = {}\n",
    "f_father = \"5_frame_dados_complementares\"\n",
    "section = \"8. DADOS COMPLEMENTARES\"\n",
    "\n",
    "data_dados_complementares = extract_dados_comple_obs(modelo, f_father, section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secao: 9 - OUTRAS INFORMAÇOES / CRITICAS\n",
    "data_outras_informacoes = {}\n",
    "father_value = \"5_frame_inf_criticas\"\n",
    "section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "\n",
    "result = extract_fields_box(modelo, father_value, section)\n",
    "if result:\n",
    "    data_outras_informacoes.update(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secao: 10. OBSERVACOES\n",
    "data_observacao = {}\n",
    "f_father = \"5_frame_observacao\"\n",
    "section = \"10. OBSERVACOES\"\n",
    "\n",
    "data_observacao = extract_dados_comple_obs(modelo, f_father, section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping for color names to RGB values\n",
    "color_mapping = {\n",
    "    \"red\": (255, 0, 0),\n",
    "    \"purple\": (128, 0, 128),\n",
    "    \"orange\": (255, 165, 0),\n",
    "    \"green\": (0, 128, 50),\n",
    "    \"blue\": (0, 0, 255),\n",
    "    \"yellow\": (255, 255, 0)\n",
    "}\n",
    "\n",
    "# Reload the image to start fresh\n",
    "image = Image.open(name_image_2work)\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Define a font size for the labels using the default PIL font\n",
    "font_size = 100\n",
    "#font = ImageFont.load_default()\n",
    "\n",
    "font = ImageFont.truetype(\"/usr/share/fonts/truetype/ubuntu/Ubuntu-M.ttf\", 30, encoding=\"unic\")\n",
    "\n",
    "# Update the draw_box function to use the larger font size with the default font\n",
    "def draw_box(row):\n",
    "    x0, y0, x1, y1 = row['x0'], row['y0'], row['x1'], row['y1']\n",
    "    color = color_mapping.get(row['color'], (0, 0, 0)) # Default to black if color not found\n",
    "    draw.rectangle([x0, y0, x1, y1], outline=color, width=3)\n",
    "    label = str(row['label']) if pd.notnull(row['label']) else None # Check for missing label\n",
    "    if label:\n",
    "        draw.text((x0 + 5, y0 + 5), label, fill=color, font=font)\n",
    "\n",
    "# Draw the boundaries\n",
    "#draw_box(boundaries_info)\n",
    "\n",
    "\n",
    "def draw_box_model(modelo,\n",
    "                   boundaries_info=None,\n",
    "                   sections_info=None,\n",
    "                   frames_info=None,\n",
    "                   field_boxes_info=None,\n",
    "                   draw_boundaries=True,\n",
    "                   draw_sections=True,\n",
    "                   draw_frames=True,\n",
    "                   draw_field_boxes=True):\n",
    "    \n",
    "    # Draw boundaries if requested\n",
    "    if draw_boundaries and boundaries_info is not None:\n",
    "        filtered_boundaries_info = boundaries_info[boundaries_info['model'] == modelo]\n",
    "        for index, row in filtered_boundaries_info.iterrows():\n",
    "            draw_box(row)\n",
    "\n",
    "    # Draw sections if requested\n",
    "    if draw_sections and sections_info is not None:\n",
    "        filtered_sections_info = sections_info[sections_info['model'] == modelo]\n",
    "        for index, row in filtered_sections_info.iterrows():\n",
    "            draw_box(row)\n",
    "            \n",
    "    # Draw frames if requested\n",
    "    if draw_frames and frames_info is not None:\n",
    "        filtered_frames_info = frames_info[frames_info['model'] == modelo]\n",
    "        for index, row in filtered_frames_info.iterrows():\n",
    "            draw_box(row)\n",
    "            \n",
    "    # Draw field boxes if requested\n",
    "    if draw_field_boxes and field_boxes_info is not None:\n",
    "        filtered_field_boxes_info = field_boxes_info[field_boxes_info['model'] == modelo]\n",
    "        for index, row in filtered_field_boxes_info.iterrows():\n",
    "            draw_box(row)\n",
    "    \n",
    "    # Show the image with selected drawings\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To draw only boundaries and sections:\n",
    "draw_box_model(modelo, boundaries_info, sections_info, draw_frames=False, draw_field_boxes=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To draw everything\n",
    "draw_box_model(modelo, boundaries_info, sections_info, frames_info, field_boxes_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To draw only field boxes:\n",
    "draw_box_model(modelo, field_boxes_info=field_boxes_info, draw_boundaries=False, draw_sections=False, draw_frames=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline reserva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo anterior do pipeline\n",
    "\n",
    "nf_data_servico = {}\n",
    "\n",
    "pdf_info = {}  # Dicionário para armazenar informações sobre PDFs\n",
    "\n",
    "for root, dirs, files in os.walk(root_directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        \n",
    "        if file.lower().endswith('.pdf'):\n",
    "            if is_pdf_searchable(file_path):\n",
    "                \n",
    "                status = \"O PDF é pesquisável\"\n",
    "                nro_nota = 0\n",
    "                nr_nro_nf = 0\n",
    "                \n",
    "                #Definindo a pagina\n",
    "                # Carregar o arquivo PDF\n",
    "                pdf_document = fitz.open(file_path)\n",
    "                # Página do PDF\n",
    "                page_number = 0  # Defina o número da página que deseja analisar\n",
    "                page = pdf_document[page_number]\n",
    "\n",
    "                # Definir retângulo de interesse\n",
    "                x0 = 0\n",
    "                y0 = 4\n",
    "                x1 = 600\n",
    "                y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "                if text:\n",
    "                    page_number = 0\n",
    "                else:\n",
    "                    page_number = 1\n",
    "                \n",
    "                \n",
    "                # 1 - cabecalho\n",
    "                #pdf_document = fitz.open(file_path)\n",
    "                #page_number = 0  # Defina o número da página que deseja analisar\n",
    "                page = pdf_document[page_number]\n",
    "                x0 = 0\n",
    "                y0 = 0\n",
    "                x1 = 600\n",
    "                y1 = 110\n",
    "                \n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "                nf_data_cabecalho, nro_nota = extract_fields_cabecalho(text)\n",
    "                \n",
    "                \n",
    "                # 2. PRESTADOR DE SERVIÇO\n",
    "                # Definir retângulo de interesse\n",
    "                x0 = 0\n",
    "                y0 = 100\n",
    "                x1 = 600\n",
    "                y1 = 236  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "                \n",
    "                nf_data_prestador = extract_fields_prestador(text)\n",
    "                \n",
    "                # 3. TOMADOR DE SERVIÇO\n",
    "                # Definir retângulo de interesse\n",
    "                x0 = 0\n",
    "                y0 = 210\n",
    "                x1 = 600\n",
    "                y1 = 340  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "                nf_data_tomador = extract_fields_tomador(text)\n",
    "                \n",
    "                \n",
    "                # 4. DESCRIMINACAO DOS SERVIÇOS\n",
    "                nf_data_servico = {}\n",
    "                nf_data_servico['secao'] = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "                # Definir retângulo de interesse\n",
    "                x0 = 0\n",
    "                y0 = 330\n",
    "                x1 = 600\n",
    "                y1 = 500  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "                # Remover quebras de linha e rótulo\n",
    "                text = text.replace('\\n', ' ')\n",
    "                label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "                if text.startswith(label):\n",
    "                    text = text[len(label):].strip()\n",
    "\n",
    "                # Atribuir texto ao dicionário\n",
    "                nf_data_servico['discriminacao_servicos'] = text\n",
    "                \n",
    "                \n",
    "                # 5. VALOR TOTAL\n",
    "                nf_data_valor_total = {}\n",
    "                nf_data_valor_total['secao'] = \"5. VALOR TOTAL\"\n",
    "\n",
    "                # Definir retângulo de interesse\n",
    "                x0 = 0\n",
    "                y0 = 500\n",
    "                x1 = 600\n",
    "                y1 = 535  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "                # Usar expressão regular para extrair apenas os caracteres numéricos e pontos decimais\n",
    "                valor_total_match = re.search(r'R\\$ ([\\d,.]+)', text)\n",
    "                if valor_total_match:\n",
    "                    valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "                    nf_data_valor_total['valor_total_nota'] = float(valor_total_sem_formatacao)\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                # 6. CNAE e Item da Lista de Serviços\n",
    "                nf_data_CNAE = {}\n",
    "                nf_data_CNAE['Secao'] = \"6. CNAE e Item da Lista de Serviços\"\n",
    "\n",
    "                # Definir retângulo de interesse CNAE\n",
    "                x0 = 0\n",
    "                y0 = 530\n",
    "                x1 = 600\n",
    "                y1 = 540  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "\n",
    "                # Extrair CNAE\n",
    "                nf_data_CNAE_match = re.search(r'CNAE\\s+(.+)', text)\n",
    "                if nf_data_CNAE_match:\n",
    "                    # Remove a primeira ocorrência de \"CNAE:\"\n",
    "                    nf_data_CNAE_str = re.sub(r'^CNAE - ', '', text, count=1)\n",
    "                    # Remover quebras de linha\n",
    "                    nf_data_CNAE_str = nf_data_CNAE_str.replace('\\n', ' ')\n",
    "                    nf_data_CNAE['cnae'] = nf_data_CNAE_str\n",
    "                    \n",
    "                \n",
    "                \n",
    "                # Item da Lista de Serviços    \n",
    "                # Definir retângulo de interesse - Item da Lista de Serviços\n",
    "                x0 = 0\n",
    "                y0 = 545\n",
    "                x1 = 600\n",
    "                y1 = 560  # Ajuste este valor para delimitar a região vertical    \n",
    "                # Extrair texto dentro do retângulo\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))     \n",
    "                    \n",
    "                # Extrair Item da Lista de Serviços\n",
    "                nf_item_lista_servicos_match = re.search(r'Item da Lista de Serviços\\s+(.+)', text)\n",
    "                if nf_item_lista_servicos_match:\n",
    "                    nf_item_lista_servicos_str = re.sub(r'^Item da Lista de Serviços - ', '', text, count=1) \n",
    "                    # Remover quebras de linha\n",
    "                    #nf_item_lista_servicos_str = nf_item_lista_servicos_str.replace('\\n \\n', '')\n",
    "                    nf_item_lista_servicos_str = nf_item_lista_servicos_str.replace('\\n', ' ')\n",
    "                    nf_data_CNAE['item_lista_servicos'] = nf_item_lista_servicos_str\n",
    "                      \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                # 7. VALORES E IMPOSTOS\n",
    "                # Definir retângulo de interesse\n",
    "                x0 = 0\n",
    "                y0 = 550\n",
    "                x1 = 600\n",
    "                y1 = 650  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "                # Extrair campos e valores\n",
    "                nf_data_valores = extract_fields_impostos(text)\n",
    "                \n",
    "                # 8. DADOS COMPLEMENTARES\n",
    "                nf_data_dados_complementares = {}\n",
    "                nf_data_dados_complementares['secao'] = \"8. DADOS COMPLEMENTARES\"\n",
    "\n",
    "                # Definir retângulo de interesse\n",
    "                x0 = 0\n",
    "                y0 = 650\n",
    "                x1 = 600\n",
    "                y1 = 680  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^DADOS COMPLEMENTARES', '', text, count=1)\n",
    "                if text == \" \":\n",
    "                    text = \"NONE\"\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "                else:    \n",
    "                    # Extrair texto dentro do retângulo\n",
    "                    nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "                \n",
    "                \n",
    "                \n",
    "                # 9. OUTRAS INFORMAÇOES / CRITICAS  \n",
    "                # Definir retângulo de interesse\n",
    "                x0 = 0\n",
    "                y0 = 680\n",
    "                x1 = 600\n",
    "                y1 = 725  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "                # Extrair campos e valores\n",
    "                nf_data_outras_informacoes = extract_fields_outras_info(text)\n",
    "                \n",
    "                \n",
    "                # 10. OBSERVACOES\n",
    "                nf_data_observacao = {}\n",
    "                nf_data_observacao['secao'] = \"10. OBSERVACOES\"\n",
    "                # Definir retângulo de interesse\n",
    "                x0 = 0\n",
    "                y0 = 725\n",
    "                x1 = 600\n",
    "                y1 = 760  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "\n",
    "                # Remove a primeira ocorrência de \"Observação:\"\n",
    "                text = re.sub(r'^Observação:', '', text, count=1)\n",
    "\n",
    "                # Remover quebras de linha\n",
    "                text = text.replace('\\n', ' ')\n",
    "\n",
    "                # Extrair texto dentro do retângulo\n",
    "                nf_data_observacao['observacao'] = text.strip()\n",
    "                \n",
    "                nr_nro_nf = nro_nota\n",
    "                \n",
    "                nome_arquivo_json = os.path.basename(root) + \".json\"\n",
    "                \n",
    "                #json_file_path = os.path.join(target_directory, diretorio, \".json\")\n",
    "                \n",
    "                nome_arquivo = file\n",
    "                #pdf_info[\"diretorio\"] = os.path.basename(root)\n",
    "                pdf_info[nr_nro_nf] = {\n",
    "                    \"dados_NF_PDF\": {\n",
    "                        \"data_cabecalho\": nf_data_cabecalho,\n",
    "                        \"data_prestador\": nf_data_prestador,\n",
    "                        \"data_tomador\": nf_data_tomador,\n",
    "                        \"data_servico\": nf_data_servico,\n",
    "                        \"data_valor_total\": nf_data_valor_total,\n",
    "                        \"data_CNAE\": nf_data_CNAE,\n",
    "                        \"data_valores\": nf_data_valores,\n",
    "                        \"data_dados_complementares\": nf_data_dados_complementares,\n",
    "                        \"data_outras_informacoes\": nf_data_outras_informacoes,\n",
    "                        \"data_observacao\": nf_data_observacao,\n",
    "                    },\n",
    "                    \"diretorio\": os.path.basename(root),\n",
    "                    \"nome_arquivo\": nome_arquivo,\n",
    "                }\n",
    "                \n",
    "                \n",
    "                pdf_document.close()\n",
    "\n",
    "# Salvando as informações em um arquivo JSON\n",
    "json_file_path = os.path.join(target_directory, nome_arquivo_json)\n",
    "with open(json_file_path, \"w\", encoding='utf-8') as json_file:\n",
    "    json.dump(pdf_info, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"As informações foram salvas em {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chamada das funçoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de consistencia de cnae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume extracted_cnae contains the extracted CNAE code or description from the document\n",
    "extracted_cnae = 'some_value'\n",
    "\n",
    "# Search the DataFrame for the extracted CNAE information (adjust the column name as needed)\n",
    "matching_row = cnae_df[cnae_df['cnae_column'] == extracted_cnae]\n",
    "\n",
    "# Use the values from the matching row\n",
    "if not matching_row.empty:\n",
    "    whole_value = matching_row.iloc[0] # Adjust to get the specific value you need\n",
    "    # Process the whole_value as needed\n",
    "else:\n",
    "    print('CNAE not found in repository')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mage_cnae_x_item_servico_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_servico_dict[7.02].upper()\n",
    "cnae_dict[4313400].upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template de funçao: frames_info -> sframe_fields_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields_frame(modelo, father_value, section):\n",
    "    \n",
    "\n",
    "    frame_label = father_value\n",
    "    print(section)\n",
    "    \n",
    "    # 1. Filtrando o frames_info para buscar os dados de corte\n",
    "    filtered_frames_info = frames_info[(frames_info['label'] == frame_label) & (frames_info['model'] == modelo)]\n",
    "    \n",
    "    # 2. Filtrando o sframe_fields_info para buscar os dados dos campos que estao nos frames\n",
    "    filtered_sframe_fields_info = sframe_fields_info[(sframe_fields_info['father'] == father_value) & (sframe_fields_info['model'] == modelo)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "        \n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        #extracted_text_box = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        \n",
    "        print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_frame['seq'], row_frame['model'], row_frame['father'], row_frame['label'], row_frame['reference'], row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'] ))\n",
    "        \n",
    "        # Divida o texto por nova linha e mantenha apenas a última parte (assume que o valor está sempre no final)\n",
    "        #value = extracted_text_box.split('\\n')[-1]\n",
    "        # Remova qualquer espaço em branco à esquerda ou à direita\n",
    "        #value = value.strip()\n",
    "        \n",
    "        for index_field, row_field in filtered_sframe_fields_info.iterrows():\n",
    "            \n",
    "            print(\"{:<5} {:<10} {:<30} {:<20} {:<20}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outras possiveis chamadas de funcao mais inteligente\n",
    "\n",
    "- data_prestador\n",
    "\n",
    "- data_CNAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_prestador\n",
    "data_prestador = {}\n",
    "father_value = \"2_frame_cnpj_prestador\"\n",
    "section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "\n",
    "\n",
    "extract_fields_frame(modelo, father_value, section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_CNAE\n",
    "data_CNAE = {}\n",
    "father_value = \"4_frame_cnae_itens_servico\"\n",
    "section = \"6. CNAE e Item da Lista de Serviços\"\n",
    "\n",
    "\n",
    "extract_fields_frame(modelo, father_value, section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Informacoes dos DFSs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaçao sobre os DF sendo utilizados\n",
    "\n",
    "\n",
    "sections_info\n",
    "\n",
    "frames_info.head()\n",
    "\n",
    "sframe_fields_info.head()\n",
    "\n",
    "field_boxes_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sframe_fields_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_boxes_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filtrando o frames_info para buscar os dados de corte\n",
    "filtered_frames_info = frames_info[(frames_info['label'] == father_value) & (frames_info['model'] == modelo)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Filtrando o sframe_fields_info para buscar os dados dos campos que estao nos frames\n",
    "filtered_sframe_fields_info = sframe_fields_info[(sframe_fields_info['father'] == father_value) & (sframe_fields_info['model'] == modelo)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Filtrando o field_boxes_info para buscar os dados de corte dos campos com BOX\n",
    "filtered_boxes_info = field_boxes_info[(field_boxes_info['father'] == father_value) & (field_boxes_info['model'] == modelo)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Operaçoes com arquivos e diretorios & Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> Config </mark>\n",
    "- root_pdf_path  (local inicial dos PDFs)\n",
    "\n",
    "- image_resized_path (local da imagem que foi convetida e resized)\n",
    "\n",
    "- nf_processada_path (local onde ficam os PDFs processados)\n",
    "\n",
    "\n",
    "<mark> Criados no processo </mark>\n",
    "\n",
    "- image_2work (arquivo de imagem resized criado)\n",
    "\n",
    "- name_image_2work (path e nome da imagem resized criada)\n",
    "\n",
    "- document_path (path e nome do PDF utilizado para tratamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_pdf_processed_ok(document_path):\n",
    "    \n",
    "    source_path = document_path\n",
    "    destination_path = os.path.join(f'{nf_processada_path}/{str(doc2convert)}')\n",
    "    shutil.move(source_path, destination_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mover PDF para diretorio de processados\n",
    "source_path = document_path\n",
    "destination_path = os.path.join(f'{nf_processada_path}/{str(doc2convert)}')\n",
    "shutil.move(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***DELETAR**** o arquivo de imagem gerado\n",
    "image_path_to_delete = name_image_2work\n",
    "os.remove(image_path_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voltar (mover) PDF para diretorio para ser processado\n",
    "source_path = destination_path\n",
    "\n",
    "destination_path = document_path\n",
    "\n",
    "shutil.move(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processo para iterar estrutura de arquivos, diretorios\n",
    "for roots, directories, documents in os.walk(image_resized_path):\n",
    "    print(f'\\n {documents} {name_image_2work}\\n\\n')\n",
    "    #if documents == :\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Buscar o primeiro documento do diretorio EXTERNO para processar\n",
    "source_directory = root_external_pdf_path\n",
    "destination_directory = root_pdf_path\n",
    "\n",
    "# Verifique se o diretório de destino está vazio\n",
    "if os.listdir(destination_directory):\n",
    "    raise Exception(\"O diretório de destino não está vazio!\")\n",
    "\n",
    "for roots, directories, documents in os.walk(source_directory):\n",
    "    # Filtre os documentos para incluir apenas aqueles com extensão .pdf\n",
    "    pdf_files = [doc for doc in documents if doc.lower().endswith('.pdf')]\n",
    "    \n",
    "    if pdf_files: # Verifique se há algum arquivo PDF no diretório\n",
    "        first_pdf_file = pdf_files[0] # Obtenha o primeiro arquivo PDF\n",
    "        source_path = os.path.join(roots, first_pdf_file) # Construa o caminho completo para o primeiro arquivo PDF\n",
    "        print(f\"Found the first PDF file: {source_path}\")\n",
    "        \n",
    "        # Mova o arquivo para o diretório de destino\n",
    "        shutil.move(source_path, destination_directory)\n",
    "        \n",
    "        break # Saia do loop após encontrar o primeiro arquivo PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOG - Single File vs. Separate Files\n",
    "\n",
    "1. Single File vs. Separate Files\n",
    "Single File: Suitable if the volume of logs is manageable and you want a consolidated view of all activities.\n",
    "Separate Files: Useful if you want to organize logs by day or by a specific module or process. This can help in troubleshooting specific issues and managing large volumes of logs.\n",
    "2. Concise vs. Detailed Logs\n",
    "You can create different log levels to handle concise and detailed logging:\n",
    "\n",
    "Concise Log: Use the INFO level to log only essential information, such as process start and end times, success messages, etc.\n",
    "Detailed Log: Use the DEBUG level to log detailed information, like variable values, intermediate steps, etc.\n",
    "Example with Python's logging Module\n",
    "Here's an example of how you might set up logging to handle both separate files and different log levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "log_file_path = 'novo_modelo/execucao_pipeline_extracao_pdf.log'\n",
    "logging.basicConfig(\n",
    "    filename=log_file_path,\n",
    "    filemode='a', # Append mode\n",
    "    level=logging.INFO, # Log level (e.g., INFO, WARNING, ERROR)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s', # Log message format\n",
    "    datefmt='%Y-%m-%d %H:%M:%S' # Date format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logger\n",
    "logger = logging.getLogger('my_logger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create handlers\n",
    "info_handler = logging.FileHandler('info.log')\n",
    "info_handler.setLevel(logging.INFO)\n",
    "\n",
    "error_handler = logging.FileHandler('error.log')\n",
    "error_handler.setLevel(logging.ERROR)\n",
    "\n",
    "# Create a formatter and set it for the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "info_handler.setFormatter(formatter)\n",
    "error_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the handlers to the logger\n",
    "logger.addHandler(info_handler)\n",
    "logger.addHandler(error_handler)\n",
    "\n",
    "# Log messages\n",
    "logger.info('This is an info message.')\n",
    "logger.error('This is an error message.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of concise log (INFO level)\n",
    "logging.info(\"Processo iniciado.\")\n",
    "\n",
    "# Example of detailed log (DEBUG level)\n",
    "logging.debug(\"Variable x value is 10.\")\n",
    "\n",
    "# ... rest of your code ...\n",
    "\n",
    "# Handle exceptions with logging\n",
    "try:\n",
    "    # Your code that might raise an exception\n",
    "    pass\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "log_file_path = 'novo_modelo/execucao_pipeline_extracao_pdf.log'\n",
    "logging.basicConfig(\n",
    "    filename=log_file_path,\n",
    "    filemode='a', # Append mode\n",
    "    level=logging.INFO, # Log level (e.g., INFO, WARNING, ERROR)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s', # Log message format\n",
    "    datefmt='%Y-%m-%d %H:%M:%S' # Date format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Pipeline iniciado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the logger\n",
    "logging.info(\"Pipeline started.\")\n",
    "logging.info(\"Processing file: %s\", first_pdf_file_path)\n",
    "logging.warning(\"File has more than one page.\")\n",
    "logging.error(\"An error occurred while processing file: %s\", first_pdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Pipeline iniciado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the logger\n",
    "logging.info(\"Pipeline started.\")\n",
    "logging.info(\"Processing file: %s\", first_pdf_file_path)\n",
    "logging.warning(\"File has more than one page.\")\n",
    "logging.error(\"An error occurred while processing file: %s\", first_pdf_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the log level, you can control the verbosity of the logs. If you want separate log files for each day, the above code will create a new file every day based on the date.\n",
    "\n",
    "Remember, logging strategy should align with your application's needs, including debugging, auditing, and compliance requirements. Make sure to review and test your logging setup to ensure it meets your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "\n",
    "# Get the current date for filename\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "log_filename = f\"log_{current_date}.txt\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=log_filename,\n",
    "    level=logging.DEBUG,  # Log everything (change to INFO for concise logs)\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "# Example of concise log (INFO level)\n",
    "logging.info(\"Processing started.\")\n",
    "\n",
    "# Example of detailed log (DEBUG level)\n",
    "logging.debug(\"Variable x value is 10.\")\n",
    "\n",
    "# ... rest of your code ...\n",
    "\n",
    "# Handle exceptions with logging\n",
    "try:\n",
    "    # Your code that might raise an exception\n",
    "    pass\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Teste e codigos para analisar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcoes de melhoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#Funçao de formatacao de numeros\n",
    "def format_number(number_str):\n",
    "    number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # multiplica por 100 para campos %\n",
    "    return float(number_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dados_cabecalho(modelo, frame_father, section):\n",
    "    \n",
    "    data_dados_complementares = {}\n",
    "    #frame_label = frame_father\n",
    "    \n",
    "    # 1. Filtrando o frames_info para buscar os dados de corte\n",
    "    filtered_frames_info = frames_info[(frames_info['label'] == frame_father) & (frames_info['model'] == modelo)]\n",
    "\n",
    "    # 2. Filtrando o sframe_fields_info para buscar os dados dos campos que estao nos frames\n",
    "    filtered_sframe_fields_info = sframe_fields_info[(sframe_fields_info['father'] == frame_father) & (sframe_fields_info['model'] == modelo)]\n",
    "\n",
    "    for index_frame, row_frame in filtered_frames_info.iterrows():\n",
    "        \n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        extracted_text_frame = extract_text_from_frame(image_2work, (x0, y0, x1, y1), tessdata_dir_config)\n",
    "        values = extracted_text_frame.split('\\n')\n",
    "        print(\"{:<5} {:<10} {:<30} {:<20} {:<20} {:<7} {:<7} {:<7} {:<7}\".format(row_frame['seq'], row_frame['model'], row_frame['father'], row_frame['label'], row_frame['reference'], row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'] ))\n",
    "        for index_field, row_field in filtered_sframe_fields_info.iterrows():\n",
    "            print(\"{:<5} {:<10} {:<30} {:<20} {:<20}\".format(row_field['seq'], row_field['model'], row_field['father'], row_field['label'], row_field['reference']))\n",
    "            nf_data_cabecalho_prefeitura = {}\n",
    "            nf_data_cabecalho_prefeitura['secao'] = section\n",
    "            type_value = row_field['type']\n",
    "            label_value = row_field['label']\n",
    "            reference_value = row_field['reference']\n",
    "            for value in values:\n",
    "                result = process_line(value, reference_value, label_value)\n",
    "                if result:\n",
    "                    nf_data_cabecalho_prefeitura.update(result)\n",
    "            \n",
    "        return nf_data_cabecalho_prefeitura   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_cabecalho\n",
    "f_father = \"1_frame_prefeitura_nf\"\n",
    "section = \"1 - CABECALHO\"\n",
    "\n",
    "#dict consolidador\n",
    "data_cabecalho = {}\n",
    "\n",
    "data_cabecalho = extract_dados_cabecalho(modelo, f_father, section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> Testar esta configuraçao de Tesseract </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(cropped_frame, is_number = False):\n",
    "    if (is_number):\n",
    "        text = pytesseract.image_to_string(cropped_frame,\n",
    "                                           config ='-c tessedit_char_whitelist=0123456789 --psm 10 --oem 2')\n",
    "    else:\n",
    "        text = pytesseract.image_to_string(cropped_frame, config='--psm 10')        \n",
    "        \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Boxes with matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Matplotlib library to draw boxes on an image is entirely feasible and provides more control over the appearance and customization of the drawings. Below, I'll provide an example of how you can draw boxes on an image using Matplotlib.\n",
    "\n",
    "First, make sure you have the necessary libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def draw_boxes(image, boxes_info, color='r'):\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(1)\n",
    "    \n",
    "    # Display the image\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Draw the boxes\n",
    "    for index, row in boxes_info.iterrows():\n",
    "        x0, y0, x1, y1 = row['x0'], row['y0'], row['x1'], row['y1']\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((x0, y0), x1 - x0, y1 - y0, linewidth=1, edgecolor=color, facecolor='none')\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call this function with your image object and DataFrame containing the coordinates of the boxes to be drawn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_boxes(image, boundaries_info, color='r')\n",
    "draw_boxes(image, sections_info, color='g')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can customize the appearance of the boxes by changing the parameters of the patches.Rectangle function, such as line width, edge color, and face color.\n",
    "\n",
    "Using Matplotlib in this way allows you to have more control over the appearance of the plot, and you can easily combine it with other Matplotlib features to create complex visualizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tecnicas de manitpulacao de imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "def convert_pdf_to_string(file_path):\n",
    "\n",
    "\toutput_string = StringIO()\n",
    "\twith open(file_path, 'rb') as in_file:\n",
    "\t    parser = PDFParser(in_file)\n",
    "\t    doc = PDFDocument(parser)\n",
    "\t    rsrcmgr = PDFResourceManager()\n",
    "\t    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "\t    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\t    for page in PDFPage.create_pages(doc):\n",
    "\t        interpreter.process_page(page)\n",
    "\n",
    "\treturn(output_string.getvalue())\n",
    "\n",
    "\n",
    "2407-106 TERE SÃO PEDRO.pdf\n",
    "                \n",
    "def convert_title_to_filename(title):\n",
    "    filename = title.lower()\n",
    "    filename = filename.replace(' ', '_')\n",
    "    return filename\n",
    "\n",
    "\n",
    "def split_to_title_and_pagenum(table_of_contents_entry):\n",
    "    title_and_pagenum = table_of_contents_entry.strip()\n",
    "    \n",
    "    title = None\n",
    "    pagenum = None\n",
    "    \n",
    "    if len(title_and_pagenum) > 0:\n",
    "        if title_and_pagenum[-1].isdigit():\n",
    "            i = -2\n",
    "            while title_and_pagenum[i].isdigit():\n",
    "                i -= 1\n",
    "\n",
    "            title = title_and_pagenum[:i].strip()\n",
    "            pagenum = int(title_and_pagenum[i:].strip())\n",
    "        \n",
    "    return title, pagenum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import fitz\n",
    "import PySimpleGUI as sg\n",
    "\n",
    "print(fitz.__doc__)\n",
    "\n",
    "if not tuple(map(int, fitz.version[0].split(\".\"))) >= (1, 18, 18):\n",
    "    raise SystemExit(\"require PyMuPDF v1.18.18+\")\n",
    "\n",
    "dimlimit = 0  # 100  # each image side must be greater than this\n",
    "relsize = 0  # 0.05  # image : image size ratio must be larger than this (5%)\n",
    "abssize = 0  # 2048  # absolute image size limit 2 KB: ignore if smaller\n",
    "imgdir = \"output\"  # found images are stored in this subfolder\n",
    "\n",
    "if not os.path.exists(imgdir):  # make subfolder if necessary\n",
    "    os.mkdir(imgdir)\n",
    "\n",
    "\n",
    "def recoverpix(doc, item):\n",
    "    xref = item[0]  # xref of PDF image\n",
    "    smask = item[1]  # xref of its /SMask\n",
    "\n",
    "    # special case: /SMask or /Mask exists\n",
    "    if smask > 0:\n",
    "        pix0 = fitz.Pixmap(doc.extract_image(xref)[\"image\"])\n",
    "        if pix0.alpha:  # catch irregular situation\n",
    "            pix0 = fitz.Pixmap(pix0, 0)  # remove alpha channel\n",
    "        mask = fitz.Pixmap(doc.extract_image(smask)[\"image\"])\n",
    "\n",
    "        try:\n",
    "            pix = fitz.Pixmap(pix0, mask)\n",
    "        except:  # fallback to original base image in case of problems\n",
    "            pix = fitz.Pixmap(doc.extract_image(xref)[\"image\"])\n",
    "\n",
    "        if pix0.n > 3:\n",
    "            ext = \"pam\"\n",
    "        else:\n",
    "            ext = \"png\"\n",
    "\n",
    "        return {  # create dictionary expected by caller\n",
    "            \"ext\": ext,\n",
    "            \"colorspace\": pix.colorspace.n,\n",
    "            \"image\": pix.tobytes(ext),\n",
    "        }\n",
    "\n",
    "    # special case: /ColorSpace definition exists\n",
    "    # to be sure, we convert these cases to RGB PNG images\n",
    "    if \"/ColorSpace\" in doc.xref_object(xref, compressed=True):\n",
    "        pix = fitz.Pixmap(doc, xref)\n",
    "        pix = fitz.Pixmap(fitz.csRGB, pix)\n",
    "        return {  # create dictionary expected by caller\n",
    "            \"ext\": \"png\",\n",
    "            \"colorspace\": 3,\n",
    "            \"image\": pix.tobytes(\"png\"),\n",
    "        }\n",
    "    return doc.extract_image(xref)\n",
    "\n",
    "\n",
    "fname = sys.argv[1] if len(sys.argv) == 2 else None\n",
    "if not fname:\n",
    "    fname = sg.PopupGetFile(\"Select file:\", title=\"PyMuPDF PDF Image Extraction\")\n",
    "if not fname:\n",
    "    raise SystemExit()\n",
    "\n",
    "t0 = time.time()\n",
    "doc = fitz.open(fname)\n",
    "\n",
    "page_count = doc.page_count  # number of pages\n",
    "\n",
    "xreflist = []\n",
    "imglist = []\n",
    "for pno in range(page_count):\n",
    "    sg.QuickMeter(\n",
    "        \"Extract Images\",  # show our progress\n",
    "        pno + 1,\n",
    "        page_count,\n",
    "        \"*** Scanning Pages ***\",\n",
    "    )\n",
    "\n",
    "    il = doc.get_page_images(pno)\n",
    "    imglist.extend([x[0] for x in il])\n",
    "    for img in il:\n",
    "        xref = img[0]\n",
    "        if xref in xreflist:\n",
    "            continue\n",
    "        width = img[2]\n",
    "        height = img[3]\n",
    "        if min(width, height) <= dimlimit:\n",
    "            continue\n",
    "        image = recoverpix(doc, img)\n",
    "        n = image[\"colorspace\"]\n",
    "        imgdata = image[\"image\"]\n",
    "\n",
    "        if len(imgdata) <= abssize:\n",
    "            continue\n",
    "        if len(imgdata) / (width * height * n) <= relsize:\n",
    "            continue\n",
    "\n",
    "        imgfile = os.path.join(imgdir, \"img%05i.%s\" % (xref, image[\"ext\"]))\n",
    "        fout = open(imgfile, \"wb\")\n",
    "        fout.write(imgdata)\n",
    "        fout.close()\n",
    "        xreflist.append(xref)\n",
    "\n",
    "t1 = time.time()\n",
    "imglist = list(set(imglist))\n",
    "print(len(set(imglist)), \"images in total\")\n",
    "print(len(xreflist), \"images extracted\")\n",
    "print(\"total time %g sec\" % (t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A basic text-to-PDF converter\n",
    "--------------------------------------------------------------------------------\n",
    "License: GNU GPL V3\n",
    "(c) 2018 Jorj X. McKie\n",
    "\n",
    "Usage\n",
    "-----\n",
    "python convert.py input.txt\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import fitz\n",
    "\n",
    "assert len(sys.argv) == 2, \"usage: python %s text.file\" % (sys.argv[0],)\n",
    "ifn = sys.argv[1]\n",
    "ofn = \"output1.pdf\"\n",
    "\n",
    "width, height = fitz.paper_size(\"a4\")\n",
    "fontsz = 10\n",
    "lineheight = fontsz * 1.2\n",
    "\n",
    "nlines = int((height - 108.0) / lineheight)\n",
    "\n",
    "sourcefile = open(ifn)\n",
    "line_ctr = 0  # page line counter\n",
    "total_ctr = 0  # total line counter\n",
    "out_ctr = 0  # count output lines\n",
    "out_buf = \"\"  # text of one page\n",
    "\n",
    "doc = fitz.open()\n",
    "\n",
    "\n",
    "def page_out(b):\n",
    "    page = doc.new_page(width=width, height=height)\n",
    "    return page.insert_text(\n",
    "        (50, 72),\n",
    "        text=b,\n",
    "        fontsize=fontsz,\n",
    "    )\n",
    "\n",
    "\n",
    "while True:\n",
    "    line = sourcefile.readline()\n",
    "    if line == \"\":\n",
    "        break\n",
    "    out_buf += line\n",
    "    line_ctr += 1\n",
    "    total_ctr += 1\n",
    "    if line_ctr == nlines:\n",
    "        out_ctr += page_out(out_buf)\n",
    "        out_buf = \"\"\n",
    "        line_ctr = 0\n",
    "\n",
    "if len(out_buf) > 0:\n",
    "    out_ctr += page_out(out_buf)\n",
    "\n",
    "print(\"PDF conversion results for file '%s':\" % (ifn,))\n",
    "print(out_ctr, \"lines read,\", total_ctr, \"lines written,\", nlines, \"lines per page.\")\n",
    "print(ofn, \"contains\", len(doc), \"pages.\")\n",
    "\n",
    "# Now add a header and footer to each page\n",
    "hdr_fontsz = 16\n",
    "ftr_fontsz = 8\n",
    "blue = fitz.pdfcolor[\"blue\"]\n",
    "pspace = 500\n",
    "\n",
    "for page in doc:\n",
    "    footer = \"%i (%i)\" % (page.number + 1, len(doc))  # footer text\n",
    "    plen_ftr = fitz.get_text_length(footer, fontname=\"Helvetica\", fontsize=ftr_fontsz)\n",
    "    page.insert_text(\n",
    "        (50, 50), ifn, color=blue, fontsize=hdr_fontsz  # header = input filename\n",
    "    )\n",
    "    page.draw_line(\n",
    "        fitz.Point(50, 60),\n",
    "        fitz.Point(50 + pspace, 60),  # line below hdr\n",
    "        color=blue,\n",
    "        width=0.5,\n",
    "    )\n",
    "    page.draw_line(\n",
    "        fitz.Point(50, height - 33),  # line above footer\n",
    "        fitz.Point(50 + pspace, height - 33),\n",
    "        color=blue,\n",
    "        width=0.5,\n",
    "    )\n",
    "    page.insert_text(\n",
    "        (50 + pspace - plen_ftr, height - 33 + ftr_fontsz * 1.2),  # insert footer\n",
    "        footer,\n",
    "        fontsize=ftr_fontsz,\n",
    "        color=blue,\n",
    "    )\n",
    "    page.clean_contents()\n",
    "\n",
    "doc.set_metadata(\n",
    "    {\n",
    "        \"creationDate\": fitz.get_pdf_now(),\n",
    "        \"modDate\": fitz.get_pdf_now(),\n",
    "        \"creator\": \"convert.py\",\n",
    "        \"producer\": \"PyMuPDF %s\" % fitz.VersionBind,\n",
    "        \"title\": \"Content of file \" + ifn,\n",
    "        \"subject\": \"Demonstrate methods new_page, insert_text and draw_line\",\n",
    "        \"author\": \"Jorj McKie\",\n",
    "    }\n",
    ")\n",
    "doc.subset_fonts()\n",
    "doc.ez_save(ofn, garbage=4, pretty=True)\n",
    "doc.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotinas anteriores - email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Obtém o nome base do arquivo ZIP para usar como subdiretório\n",
    "zip_basename = os.path.splitext(os.path.basename(zip_file_path))[0]\n",
    "\n",
    "# Cria o subdiretório com base no nome do arquivo ZIP\n",
    "root_output_dir = os.path.join(output_dir, zip_basename)\n",
    "if not os.path.exists(root_output_dir):\n",
    "    os.makedirs(root_output_dir)\n",
    "\n",
    "# Dicionário para guardar o nome da pasta e os arquivos associados\n",
    "folder_file_dict = {}\n",
    "\n",
    "# Abre o arquivo ZIP\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    for member in zip_ref.namelist():\n",
    "        # Separa o nome da pasta e o nome do arquivo usando barra invertida como delimitador\n",
    "        parts = member.rsplit('\\\\', 1)\n",
    "        folder_name = parts[0] if len(parts) > 1 else ''\n",
    "        filename = parts[-1]\n",
    "\n",
    "        if filename:  # ignora diretórios\n",
    "            # Adiciona ao dicionário\n",
    "            folder_file_dict.setdefault(folder_name, []).append(filename)\n",
    "\n",
    "            # Cria um subdiretório se ele não existir\n",
    "            sub_dir = os.path.join(root_output_dir, folder_name)\n",
    "            if not os.path.exists(sub_dir):\n",
    "                os.makedirs(sub_dir)\n",
    "\n",
    "            # Salva o arquivo no subdiretório especificado\n",
    "            source = zip_ref.open(member)\n",
    "            target_path = os.path.join(sub_dir, filename)\n",
    "            with open(target_path, \"wb\") as target:\n",
    "                target.write(source.read())\n",
    "\n",
    "# Imprime o dicionário\n",
    "for folder, files in folder_file_dict.items():\n",
    "    print(f\"Pasta: {folder}, Arquivos: {files}\")\n",
    "\n",
    "print(f\"Arquivos extraídos para {root_output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from outlook_msg import Message\n",
    "\n",
    "# Caminho da pasta onde você quer salvar os anexos\n",
    "pasta_destino = msg_attachment_zip\n",
    "\n",
    "# Verifica se a pasta existe; se não, cria ela\n",
    "if not os.path.exists(pasta_destino):\n",
    "    os.makedirs(pasta_destino)\n",
    "\n",
    "with open(msg_file_path) as msg_file:\n",
    "    msg = Message(msg_file)\n",
    "\n",
    "# Contents are the plaintext body of the email\n",
    "contents = msg.body\n",
    "\n",
    "# Attachments can be read and saved like so\n",
    "first_attachment = msg.attachments[0]\n",
    "\n",
    "# Constrói o caminho completo para o arquivo anexo\n",
    "caminho_completo_anexo = os.path.join(pasta_destino, first_attachment.filename)\n",
    "\n",
    "with first_attachment.open() as attachment_fp, open(caminho_completo_anexo, 'wb') as output_fp:\n",
    "    output_fp.write(attachment_fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_zip = 'pipeline_extracao_documentos/emails/emails_documentos_recebidos/attachments/arquivos_compactados'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório onde você quer salvar os arquivos extraídos\n",
    "output_dir = 'pipeline_extracao_documentos/emails/emails_documentos_recebidos/documentos'\n",
    "\n",
    "for root, dirs, files in os.walk(root_zip):\n",
    "    #print(f'{root}  | {dirs} | {document} | {files}\\n')\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        if file.lower().endswith('.zip'):\n",
    "            zip_file = file\n",
    "            zip_file_path = os.path.join(root, file)\n",
    "            \n",
    "            # Dicionário para guardar o nome da pasta e os arquivos associados\n",
    "            folder_file_dict = {}\n",
    "\n",
    "            # Abre o arquivo ZIP\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                for member in zip_ref.namelist():\n",
    "                    # Separa o nome da pasta e o nome do arquivo usando barra invertida como delimitador\n",
    "                    parts = member.rsplit('\\\\', 1)\n",
    "                    folder_name = parts[0] if len(parts) > 1 else ''\n",
    "                    filename = parts[-1]\n",
    "                    #iter_output_dir = os.path.join(output_dir, filename)\n",
    "                    \n",
    "                    iter_output_dir = os.path.join(output_dir, folder_name)\n",
    "                    \n",
    "                    print(\"Nome da pasta no zip: \", iter_output_dir)\n",
    "                    print()\n",
    "                    print(\"Nome do arquivo na pasta: \", filename)\n",
    "                    \n",
    "                    \n",
    "                    if not os.path.exists(iter_output_dir):\n",
    "                        os.makedirs(iter_output_dir)\n",
    "\n",
    "                    if filename:  # ignora diretórios\n",
    "                        # Adiciona ao dicionário\n",
    "                        folder_file_dict.setdefault(folder_name, []).append(filename)\n",
    "\n",
    "                        # Salva o arquivo no diretório especificado\n",
    "                        source = zip_ref.open(member)\n",
    "                        target_path = os.path.join(iter_output_dir, filename)\n",
    "                        with open(target_path, \"wb\") as target:\n",
    "                            target.write(source.read())\n",
    "\n",
    "            # Imprime o dicionário\n",
    "            for folder, files in folder_file_dict.items():\n",
    "                print(f\"Pasta: {folder}, Arquivos: {files}\")\n",
    "\n",
    "            print(f\"Arquivos extraídos para {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracao do ZIP\n",
    "\n",
    "# Caminho do arquivo ZIP\n",
    "zip_file_path = 'pipeline_extracao_documentos/emails/emails_documentos_recebidos/attachments/arquivos_compactados/SAO PEDRO DA ALDEIA_PDF_17282023_1532.zip'\n",
    "\n",
    "# Path para area ZIP\n",
    "zip_dir_path = 'pipeline_extracao_documentos/emails/emails_documentos_recebidos/attachments/arquivos_compactados'\n",
    "\n",
    "# Diretório onde você quer salvar os arquivos extraídos\n",
    "output_dir = 'pipeline_extracao_documentos/emails/emails_documentos_recebidos/documentos'\n",
    "\n",
    "# Verifica se o diretório de destino existe; se não, cria um\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Dicionário para guardar o nome da pasta e os arquivos associados\n",
    "folder_file_dict = {}\n",
    "\n",
    "# Abre o arquivo ZIP\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    for member in zip_ref.namelist():\n",
    "        # Separa o nome da pasta e o nome do arquivo usando barra invertida como delimitador\n",
    "        parts = member.rsplit('\\\\', 1)\n",
    "        folder_name = parts[0] if len(parts) > 1 else ''\n",
    "        filename = parts[-1]\n",
    "\n",
    "        if filename:  # ignora diretórios\n",
    "            # Adiciona ao dicionário\n",
    "            folder_file_dict.setdefault(folder_name, []).append(filename)\n",
    "\n",
    "            # Salva o arquivo no diretório especificado\n",
    "            source = zip_ref.open(member)\n",
    "            target_path = os.path.join(output_dir, filename)\n",
    "            with open(target_path, \"wb\") as target:\n",
    "                target.write(source.read())\n",
    "\n",
    "# Imprime o dicionário\n",
    "for folder, files in folder_file_dict.items():\n",
    "    print(f\"Pasta: {folder}, Arquivos: {files}\")\n",
    "\n",
    "print(f\"Arquivos extraídos para {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for roots, directories, documents in os.walk(output_dir):\n",
    "    dir_path = os.path.basename(roots)\n",
    "    #source_path = roots\n",
    "    for document in documents:\n",
    "        document_zip = document\n",
    "        \n",
    "        print(f'{roots}   {dir_path} {document_zip} \\n')\n",
    "        #logging.info(\"Processando arquivo: %s\", document_path_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from outlook_msg import Message\n",
    "\n",
    "# Caminho da pasta onde você quer salvar os anexos\n",
    "pasta_destino = msg_attachment_zip\n",
    "\n",
    "# Verifica se a pasta existe; se não, cria ela\n",
    "if not os.path.exists(pasta_destino):\n",
    "    os.makedirs(pasta_destino)\n",
    "\n",
    "with open(email_path) as msg_file:\n",
    "    msg = Message(msg_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tables-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
