{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> > 2. </b> Pipeline de extracao de dados de documentos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2_extracao_pipeline_v1.ipynb = 2_root_doc_extract_v1.ipynb</b>    |     Atual notebook com as funçoes para processamento de PDF Pesquisavel e Raster e a criaçao dos Dataframes de forma independente e unica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import platform\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from urllib import response\n",
    "\n",
    "from outlook_msg import Message\n",
    "import extract_msg\n",
    "import zipfile\n",
    "from pyunpack import Archive\n",
    "import py7zr\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import PyPDF2\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "import locale\n",
    "import time, copy\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import cv2\n",
    "import fitz  # Módulo PyMuPDF\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import Image, ImageDraw\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar\n",
    "import matplotlib.pyplot as plt\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import logging\n",
    "\n",
    "# Modulos da solucao\n",
    "# import modules.extrai_pdf_pesquisavel as Extc\n",
    "import modules.cronometro as cron\n",
    "import modules.nova_extracao_pdf_pesquisavel as novaextra \n",
    "import modules.trata_model as tmod\n",
    "import modules.trata_pdf as tpdf\n",
    "import modules.utils as utl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Path para planilha de processamento de batches\n",
    "conf_export_plan_path = 'processamentos/processamento_batches/df_conf_export_batch.xlsx'\n",
    "\n",
    "\n",
    "\n",
    "# 2. XXX  Tratando nome de carga do df_processamento\n",
    "map_analise_path = \"processamentos/mapeamento_analise\"\n",
    "\n",
    "# 3. XXX  prefixo de nome do arquivo de exportaçao\n",
    "df_root_pipe_file = \"df_root_\"\n",
    "\n",
    "\n",
    "\n",
    "#### Config - E-mail\n",
    "# 1. Caminho do arquivo uma mensagem especifica\n",
    "msg_dir_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails'\n",
    "\n",
    "# 2. Path para arquivos atachados compactados\n",
    "msg_attachment_zip = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/13_attachments'\n",
    "\n",
    "\n",
    "#### Config - messages\n",
    "# 3. Caminho do arquivo uma mensagem especifica\n",
    "msg_outros_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/12_messages'\n",
    "\n",
    "# 4. Path para arquivos recebidos manualmente\n",
    "arquivos_recebidos_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/14_documentos_recebidos'\n",
    "\n",
    "\n",
    "####Config Processamento Pipeline\n",
    "\n",
    "# 5. Path para documentos para extracao\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "# 6. Path para gestao de imagens resized\n",
    "image_resized_path = \"pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas\"\n",
    "\n",
    "\n",
    "\n",
    "# 7. path para arquivos json\n",
    "json_path = \"pipeline_extracao_documentos/5_documentos_processados/jsons\"\n",
    "\n",
    "# 7. Path para DFs e CSVs exportados\n",
    "export_path = \"pipeline_extracao_documentos/6_geral_administacao/exports\"\n",
    "\n",
    "# 8. Path para lixeira\n",
    "root_garbage_path = \"pipeline_extracao_documentos/0_lixeira\"\n",
    "\n",
    "\n",
    "#### paths de objetos para criacao/gestao (dicionarios/datasets)\n",
    "cnae_dict_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets/CNAE_X_ITEM_SERVICO_PREFEITURAS.xlsx\"\n",
    "\n",
    "\n",
    "# 12. poppler path\n",
    "poppler_path = \"/home/dani-boy/miniconda3/envs/tables-detr/bin\"\n",
    "\n",
    "# 13. path para config Tesseract\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "#Modelo atual\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "# definindo localizadcao para pt_BR\n",
    "locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='config/log_ocorrencias.log',\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "    datefmt='%d/%m/%Y %H:%M:%S'\n",
    ")\n",
    "\n",
    "logging.info(\"kernel reiniciado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b>2.1</b> Extraction Tools </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. XXX Ajusta textoYYY\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF_Pesquisavel-  NO CABECALHO\n",
    "def texto_extraido_nf(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF RASTER NO CABECALHO\n",
    "def texto_extraido_cabecalho(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    text_splited = [s.replace(\")\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#1. funcao: find_value_after_keyword_out_frame_up\n",
    "def find_value_after_keyword_out_frame_up(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "        if index + 1 < len(text_list) and text_list[index + 1] not in default_keyword_list:\n",
    "            return text_list[index + 1]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            for default_keyword in default_keyword_list:\n",
    "                if default_keyword in text_list:\n",
    "                    # Caso especial para 'Nome/Razão Social:'\n",
    "                    if keyword == 'Nome/Razão Social:':\n",
    "                        return text_list[0]\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#2. find_value_after_keyword_out_frame_down  \n",
    "def find_value_after_keyword_out_frame_down(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o índice seguinte está dentro da lista\n",
    "        if index + 1 < len(text_list):\n",
    "            # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            try:\n",
    "                index = text_list.index(default_keyword_list[-1])\n",
    "                return text_list[index - 1]\n",
    "            except ValueError:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "\n",
    "#3. find_value_after_keyword_fuzz\n",
    "def find_value_after_keyword_fuzz(keyword, text_list, default_keyword_list=None, fuzziness_threshold=80):\n",
    "    closest_match = None\n",
    "    closest_match_score = 0\n",
    "    \n",
    "    for i, text in enumerate(text_list):\n",
    "        score = fuzz.ratio(keyword, text)\n",
    "        \n",
    "        if score > closest_match_score:\n",
    "            closest_match_score = score\n",
    "            closest_match = text\n",
    "        \n",
    "        if closest_match_score > fuzziness_threshold:\n",
    "            break\n",
    "\n",
    "    if closest_match_score > fuzziness_threshold:\n",
    "        index = text_list.index(closest_match)\n",
    "        if index + 1 < len(text_list):\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "\n",
    "def pesquisa_keyword(string_pesquisa, text_splited, keyword_list):\n",
    "    resultado_extraido_fuzz = find_value_after_keyword_fuzz(string_pesquisa, text_splited, keyword_list)\n",
    "\n",
    "    if resultado_extraido_fuzz == None:\n",
    "        resultado_extraido_frame_up = find_value_after_keyword_out_frame_up(string_pesquisa, text_splited, keyword_list)\n",
    "        if resultado_extraido_frame_up == None:\n",
    "            resultado_extraido_frame_down = find_value_after_keyword_out_frame_down(string_pesquisa, text_splited, keyword_list)\n",
    "            resultado_extraido = resultado_extraido_frame_down\n",
    "        else:\n",
    "            resultado_extraido = resultado_extraido_frame_up\n",
    "    else:\n",
    "        resultado_extraido = resultado_extraido_fuzz\n",
    "\n",
    "    # Verifica se o resultado extraído é uma das palavras-chave, indicando um erro\n",
    "    if resultado_extraido in keyword_list:\n",
    "        resultado_extraido = None\n",
    "\n",
    "    return resultado_extraido\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# XXX Pequenos mas poderosos\n",
    "def extract_text_PIL(image, coordinates):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    image_croped = image.crop((x0, y0, x1, y1))\n",
    "    texto_extraido = pytesseract.image_to_string(image_croped, lang='por', config='--psm 6')\n",
    "    return texto_extraido \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# Funcao importante - process_line\n",
    "def process_line(value, reference, label):\n",
    "    name_match = re.search(fr'{reference} (.+)', value)\n",
    "    if name_match:\n",
    "        extracted_value = reference + \" \" + name_match.group(1)\n",
    "        return {label: extracted_value}\n",
    "    return None\n",
    "\n",
    "# Função para extrair número da string\n",
    "def extract_number(text):\n",
    "    match = re.search(r'\\b\\d+(\\.\\d+)?\\b', text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def corrigir_email(texto):\n",
    "    # Padrão de regex para identificar e-mails potenciais\n",
    "    padrao_email = re.compile(r'[a-zA-Z0-9_.+-]+[)!Q@][a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+')\n",
    "    \n",
    "    # Encontrar todos os padrões que se assemelham a um e-mail\n",
    "    possiveis_emails = padrao_email.findall(texto)\n",
    "    \n",
    "    for email in possiveis_emails:\n",
    "        # Se \"@\" não estiver presente, tentamos corrigir substituindo \")\" ou \"!\" por \"@\"\n",
    "        if \"@\" not in email:\n",
    "            email_corrigido = email.replace(\")\", \"@\").replace(\"Q\", \"@\")\n",
    "            texto = texto.replace(email, email_corrigido)\n",
    "    \n",
    "    return texto\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b>2.2</b> FunÇoes de Extracao de dados no Pipeline </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================#\n",
    "#                                                                                           #\n",
    "#                           1. PROCESSAMENTO - PDF PESQUISAVEL                              #\n",
    "#                                                                                           #   \n",
    "#===========================================================================================#\n",
    "# XXXpara buscar melhor as coordendas dos FRAMES\n",
    "def get_coordinates_filter(pdf_pesquisavel_map, model, tipo, label, section):\n",
    "    \n",
    "    row_frame = utl.filtrar_df(frames_nf_v4_df, model=model, type=tipo, label=label, section_json=section)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [((row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p']) if pdf_pesquisavel_map else (row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1']))]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates\n",
    "\n",
    "\n",
    "# 0.A Dados iniciais - PDF PESQUISAVEL\t\n",
    "def pesquisa_prefeitura_pdf_pesquisavel(idx, row, row_info, map_directory, original_file_name, file_path, debug):    \n",
    "    \n",
    "    \n",
    "   # Carregar o arquivo PDF\n",
    "    pdf_document = fitz.open(file_path)\n",
    "\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 4\n",
    "    x1 = 600\n",
    "    y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    \n",
    "    if debug:\n",
    "        print(f'\\ndentro da funçao: pesquisa_prefeitura_pdf_pesquisavel: doc.:{original_file_name} | diretorio: {map_directory}  text: \\n\\n{text}\\n\\n')\n",
    "    \n",
    "    if text:\n",
    "       page_number = 0\n",
    "       #print(page_number)\n",
    "    else:\n",
    "       page_number = 1\n",
    "       #print(page_number)\n",
    "    \n",
    "    pdf_document.close()\n",
    "   \n",
    "    return text\n",
    "\n",
    "# XXX Funcoes de Regex - cabecalho - documento pdf pesquisavel\n",
    "nf_data_servico = {}\n",
    "nf_data_erros = {}\n",
    "nf_lista_erros = []\n",
    "\n",
    "# 0. Pesquisa PDF\n",
    "def is_pdf_searchable(pdf_path):\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        is_searchable = all(page.get_text(\"text\") != \"\" for page in pdf_document)\n",
    "        pdf_document.close()\n",
    "        return is_searchable\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao verificar o PDF: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# 1.A CABECALHO - PDF PESQUISAVEL  \n",
    "def extrai_cabecalho_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "    \n",
    "    nf_data_cabecalho = {}\n",
    "    lista_erros = []\n",
    "    label = \"1_frame_dados_nf\"\n",
    "    \n",
    "    batch_name_row_info = row_info.get('batch')\n",
    "    information_row_info = row_info.get('informations')\n",
    "    action_item_row_info = row_info.get('action_item')\n",
    "    \n",
    "    nf_data_cabecalho['secao'] = section\n",
    "    nf_data_cabecalho['action_item'] = action_item_row_info\n",
    "    nf_data_cabecalho['informations'] = information_row_info\n",
    "    nf_data_cabecalho['processo'] = 'mapeamento regex - PDF pesquisavel'\n",
    "    \n",
    "    if debug:\n",
    "        print(f'\\n\\n2. dentro da funçao extrai_cabecalho_PDF: batch_name: {batch_name_row_info}\\n\\n')\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    tipo = \"frame\"\n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    \n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    if debug:\n",
    "        print(f'\\n3. x0: {x0}, y0: {y0}, x1: {x1}, y1: {y1} f_0: {f_0} f_1: {f_1} | text: \\n{text} \\n\\n')\n",
    "\n",
    "    try:\n",
    "        numero_nota_match = re.search(r'Número da Nota:\\s+(\\d+)', text)\n",
    "        if numero_nota_match:\n",
    "            numero_nf = numero_nota_match.group(1)\n",
    "            nf_data_cabecalho['numero_nota_fiscal'] = numero_nf\n",
    "            #nf_data_cabecalho['informations'] = 'documento com numero de nota fiscal'\n",
    "            if debug:\n",
    "                print(f'\\nnr_nro_nf: {nr_nro_nf} - doc: {original_file_name}\\n')\n",
    "        else:\n",
    "            msg = (f\"Número da Nota não encontrado\")\n",
    "            nf_data_cabecalho['numero_nota_fiscal'] = None\n",
    "            information_row_info = 'Número da Nota não encontrado'\n",
    "            nf_data_cabecalho['informations'] = information_row_info\n",
    "            action_item_row_info = 'BREAK_PROCESS'\n",
    "            nf_data_cabecalho['action_item'] = action_item_row_info\n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} | numero NF nao encontrado {e}\")\n",
    "        nf_data_cabecalho['numero_nota_fiscal'] = None\n",
    "        information_row_info = 'Número da Nota não encontrado'\n",
    "        nf_data_cabecalho['informations'] = information_row_info\n",
    "        action_item_row_info = 'BREAK_PROCESS'\n",
    "        nf_data_cabecalho['action_item'] = action_item_row_info\n",
    "\n",
    "    # Extrair Competência\n",
    "    competencia_match = re.search(r'Competência:\\s+(.+)', text)\n",
    "    if competencia_match:\n",
    "        nf_data_cabecalho['competencia'] = competencia_match.group(1)\n",
    "\n",
    "    # Extrair Data e Hora de Emissão\n",
    "    data_emissao_match = re.search(r'Data e Hora da Emissão:\\s+(.+)', text)\n",
    "    if data_emissao_match:\n",
    "        nf_data_cabecalho['dt_hr_emissao'] = data_emissao_match.group(1)\n",
    "        \n",
    "    # Extrair codigo Verificacao\n",
    "    codigo_verificacao_match = re.search(r'Código Verificação:\\s+(.+)', text)\n",
    "    if codigo_verificacao_match:\n",
    "        codigo_verificacao_nf = codigo_verificacao_match.group(1)\n",
    "        nf_data_cabecalho['codigo_verificacao'] =  codigo_verificacao_nf\n",
    "        tam_codigo_verificacao = len(codigo_verificacao_nf)\n",
    "        nf_data_cabecalho['conf_cod'] = tam_codigo_verificacao\n",
    "        \n",
    "    \n",
    "    \n",
    "    pdf_document.close()\n",
    "    \n",
    "    return nf_data_cabecalho\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2.A PRESTADOR DE SERVIÇO - PDF PESQUISAVEL\n",
    "def extrai_prestador_PDF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "    \n",
    "    nf_data_prestador = {}\n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    label = \"2_frame_cnpj_prestador\"\n",
    "\n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    tipo = \"frame\"\n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    #print(label)\n",
    "    #print(x0,x1,y0,y1)\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    nf_data_prestador = novaextra.extract_fields_prestador(text)\n",
    "\n",
    "    pdf_document.close()\n",
    "\n",
    "    return nf_data_prestador \n",
    "\n",
    "  \n",
    "# 3.A. TOMADOR DE SERVIÇO - PDF PESQUISAVEL\n",
    "def extrai_tomador_PDF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "\n",
    "    nf_data_tomador = {}\n",
    "    section = \"3. TOMADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = True\n",
    "\n",
    "    label = \"3_frame_cnpj_tomador\"\n",
    "\n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    tipo = \"frame\"\n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    #print(label)\n",
    "    #print(x0,x1,y0,y1)\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    nf_data_tomador = novaextra.extract_fields_tomador(text)\n",
    "           \n",
    "\n",
    "    pdf_document.close()\n",
    "\n",
    "    return nf_data_tomador \n",
    "\n",
    "\n",
    "# 4.A DESCRIMINACAO DOS SERVIÇOS - PDF PESQUISAVEL\n",
    "def processar_servicos_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "    \n",
    "    section = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "    frame_type = 'frame'\n",
    "    f_frame_label = \"4_frame_descricao_totais\"\n",
    "    data_box_valores = {'secao': section}\n",
    "    nf_data_servico = {}\n",
    "    message_erro = []\n",
    "    nf_data_servico['original_file_name'] = original_file_name\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == frame_type) & (frames_nf_v4_df['label'] == f_frame_label)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        frame_id = row_frame['id']\n",
    "        label = row_frame['label']\n",
    "        x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "        #print(f'model: {model_map} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: {label:>30} | id: {frame_id:>3} | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6} ')\n",
    "        \n",
    "        pdf_document = fitz.open(file_path)\n",
    "        # Página do PDF\n",
    "        page_number = 0  # Defina o número da página que deseja analisar\n",
    "        page = pdf_document[page_number]\n",
    "        #x0, y0, x1, y1 = (0, 0, 600, 110)\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        \n",
    "        # Remover quebras de linha e rótulo\n",
    "        text = text.replace('\\n', ' ')\n",
    "        label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "        if text.startswith(label):\n",
    "            text = text[len(label):].strip()\n",
    "\n",
    "\n",
    "        try:\n",
    "            discrimanacao_servico = text   \n",
    "        except Exception as e:\n",
    "            msg = (f\"doc: | {e}\")\n",
    "            discrimanacao_servico = \"Descricao nao encontrada\"\n",
    "\n",
    "        # Atribuir texto ao dicionário\n",
    "        nf_data_servico['discriminacao_servicos'] = discrimanacao_servico\n",
    "        \n",
    "        pdf_document.close()\n",
    "        \n",
    "    return nf_data_servico \n",
    "            \n",
    "                \n",
    "# 5.A VALOR TOTAL - PDF PESQUISAVEL\n",
    "def processar_valor_total_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "    \n",
    "    #nf_data_valor_total = {}\n",
    "    \n",
    "    process = ['4_frame_valor_total']\n",
    "    \n",
    "    #nf_data_valor_total['secao'] = section\n",
    "    pdf_pesquisavel_map = True\n",
    "        \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    valor_total_nf = 0.0\n",
    "        \n",
    "    tipo= \"frame\"\n",
    "    message_erro = []\n",
    "    #nf_dados_prestador = {}\n",
    "    # model = row['model']\n",
    "    for father in process:\n",
    "        label = father\n",
    "        if label == \"4_frame_valor_total\":\n",
    "            try: \n",
    "                #print(model_map)\n",
    "                coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "                #print(f'model: {model_map} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6}\\n {text}')\n",
    "                valor_total_match = re.search(r'R\\$ ([\\d,.]+)', text)\n",
    "                if valor_total_match:\n",
    "                    valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "                    if debug:\n",
    "                        print(f'valor_total_sem_formatacao: {valor_total_sem_formatacao}')\n",
    "                    valor_total_nf = float(valor_total_sem_formatacao)\n",
    "                    #nf_data_valor_total['valor_total_nota'] = nf_data_valor_total\n",
    "                    if debug:\n",
    "                        print(f'valor_total_nota: {valor_total_nota}')\n",
    "            except Exception as e:\n",
    "                msg = (f\"doc: | {e}\")\n",
    "                valor_total_nf = 0.0\n",
    "                # nf_data_valor_total['valor_total_nota'] = 0.0\n",
    "                logging.error(f\" {batch_name} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")\n",
    "                \n",
    "    pdf_document.close()            \n",
    "                \n",
    "    return valor_total_nf  \n",
    "\n",
    "\n",
    "# 6.A CNAE e Item da Lista de Serviços\n",
    "def extrai_consiste_cnae_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_pm, model_map, f_0_cnae, f_1_cnae, f_0_it, f_1_it, original_file_name, file_path, debug):\n",
    "\n",
    "    nf_data_CNAE = {}\n",
    "    message_erro = []\n",
    "    cnae_value = None\n",
    "    item_servico_value = None\n",
    "    \n",
    "    nf_data_CNAE['Secao'] = section\n",
    "    nf_data_CNAE['texto_cnae'] = None\n",
    "    nf_data_CNAE['texto_item_servico'] = None\n",
    "    nf_data_CNAE['original_file_name'] = original_file_name\n",
    "    \n",
    "    process = [\"cnae\", 'item_lista_servicos']\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    tipo = \"sframe_field\"\n",
    "    \n",
    "    #print(f'\\n item: {original_file_name}\\n')\n",
    "    # model = row['model']\n",
    "    for father in process:\n",
    "        cnae_value = None\n",
    "        item_servico_value = None\n",
    "        label = father\n",
    "        if label == \"cnae\": \n",
    "            tipo = \"sframe_field\"\n",
    "            coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            #print(f'1.A TRATAMENTO CNAE : (coordenadas originais):  x0:{x0} y0:{y0} x1:{x1} y1:{y1}')\n",
    "            #print(f'1. label: {label} | coordenadas originais: x0:{x0} y0:{y0} x1:{x1} y1:{y1} item:')\n",
    "            y0 = y0 * f_0_cnae\n",
    "            y1 = y1 * f_1_cnae\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            nf_data_CNAE['texto_cnae'] = text\n",
    "            #print(f'1.A TRATAMENTO CNAE - texto extraido em  coordenadas: x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0_cnae: {f_0_cnae}, f_1_cnae: {f_1_cnae} TEXT:\\n{text}\\n')\n",
    "            \n",
    "            # if debug:\n",
    "            print(f'1.A TRATAMENTO CNAE - texto extraido em  coordenadas: x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0_cnae: {f_0_cnae}, f_1_cnae: {f_1_cnae} TEXT:\\n{text}\\n')\n",
    "            cnae_processado = processa_cnae_dict(text, de_para_pm, debug)\n",
    "            # if debug:\n",
    "            print(f'\\n1.B CNAE PROCESSADO: {cnae_processado}\\n')\n",
    "            \n",
    "        elif label == \"item_lista_servicos\":\n",
    "            tipo = \"sframe_field\"\n",
    "            #print(model_map)\n",
    "            coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            #print(f'2.A TRATAMENTO ITENS DE SERVICO:(coordenadas originais):  x0:{x0} y0:{y0} x1:{x1} y1:{y1}')\n",
    "            y0 = y0 * f_0_it\n",
    "            y1 = y1 * f_1_it\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            nf_data_CNAE['texto_item_servico'] = text\n",
    "            # if debug:\n",
    "            print(f'2.A TRATAMENTO ITENS DE SERVICO -  texto extraido em  coordenadas originais: x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0_it:{f_0_it}, f_1_it: {f_1_it} TEXT:\\n{text}\\n')\n",
    "            #print(text)\n",
    "            item_servico_processado = processa_itens_servico_dict(text, de_para_pm, debug)\n",
    "            # if debug:\n",
    "            print(f' 2.B ITEM DE SERVICO PROCESSADO: {item_servico_processado} \\n\\n')\n",
    "\n",
    "    pdf_document.close()\n",
    "    \n",
    "    return cnae_processado, item_servico_processado, nf_data_CNAE\n",
    "\n",
    "\n",
    "# 6.A XXX Funcao generica de extracao - CNAE_ITEM - PDF_PESQUISAVEL\n",
    "def extracao_documento_CNAE_ITEM_PDF_P(idx, row, row_info, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, def_replace, original_file_name, file_path, debug):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "\n",
    "\n",
    "    # busco coordenadas para o contexto\n",
    "    if mapping_method == \"frame_&_sframe_field\":\n",
    "        tipo_4_coordinates = \"frame\"\n",
    "        tipo_4_filter = \"sframe_field\"\n",
    "        \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]     \n",
    "\n",
    "    coordinates = get_coordinates_filter_by_context(pdf_pesquisavel_map, model_map, context_mapping, tipo_4_coordinates)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    print(x0, y0, x1, y1)  \n",
    "\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    text_splited = text.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "\n",
    "    # 8. Efetuo o filtro para a iteracao\n",
    "    filtered_frame_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['context_mapping'] == context_mapping) & (frames_nf_v4_df['type'] == tipo_4_filter)]\n",
    "\n",
    "    # 9. iter sobre o filtro\n",
    "    for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "        section = row_frame['section_json']\n",
    "        label = row_frame['label']\n",
    "        reference = row_frame['reference']\n",
    "        marcador_inicio = row_frame['marcador_inicio']\n",
    "        marcador_fim = row_frame['marcador_fim']\n",
    "        texto_extraido = extrair_texto_entre_marcadores(text_splited, marcador_inicio, marcador_fim)\n",
    "        if texto_extraido:\n",
    "            texto_extraido = texto_extraido.replace(marcador_inicio, \"\").strip()\n",
    "            data_box_valores[label] = texto_extraido\n",
    "            # print(texto_extraido)\n",
    "            # print()\n",
    "        else:\n",
    "            texto_completo = \" \".join(text_splited)\n",
    "            texto_pesquisa = encontrar_texto_fuzzy_marcador_inicial(texto_completo, marcador_inicio, marcador_fim) \n",
    "            if texto_pesquisa:\n",
    "                texto_pesquisa = texto_pesquisa.replace(marcador_inicio, \"\").strip()\n",
    "                data_box_valores[label] = texto_pesquisa\n",
    "                # print(texto_pesquisa)\n",
    "                # print()\n",
    "            else:\n",
    "                texto_pesquisa2 = encontrar_texto_fuzzy_2_marcadores(texto_completo, marcador_inicio, marcador_fim)\n",
    "                if texto_pesquisa2:\n",
    "                    texto_pesquisa2 = texto_pesquisa2.replace(marcador_inicio, \"\").strip()\n",
    "                    data_box_valores[label] = texto_pesquisa2\n",
    "    \n",
    "    pdf_document.close()  \n",
    "          \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "# 6.A.1 CNAE Outros\n",
    "def processa_cnae_outros(text):\n",
    "    \n",
    "    nf_data_CNAE_match = re.search(r'CNAE\\s+(.+)', text)\n",
    "    if nf_data_CNAE_match:\n",
    "        try:\n",
    "            # Remove a primeira ocorrência de \"CNAE:\"\n",
    "            nf_data_CNAE_str = re.sub(r'^CNAE - ', '', text, count=1)\n",
    "            # Remover quebras de linha\n",
    "            nf_data_CNAE_str = nf_data_CNAE_str.replace('\\n', ' ')\n",
    "            return nf_data_CNAE_str \n",
    "        except Exception as e:\n",
    "            print(f\"Erro busca cnae: {e}\") \n",
    "        \n",
    "    return None \n",
    "\n",
    "# 6.A.1 CNAE e Item da Lista de Serviços\n",
    "def processa_item_sevico_outros(text):\n",
    "\n",
    "    nf_item_lista_servicos_match = re.search(r'Item da Lista de Serviços\\s+(.+)', text)\n",
    "    if nf_item_lista_servicos_match:\n",
    "        try:\n",
    "            # Remove a primeira ocorrência de \"Item de Servico:\"\n",
    "            nf_item_lista_servicos_str = re.sub(r'^Item da Lista de Serviços - ', '', text, count=1) \n",
    "            # Remover quebras de linha\n",
    "            #nf_item_lista_servicos_str = nf_item_lista_servicos_str.replace('\\n \\n', '')\n",
    "            nf_item_lista_servicos_str = nf_item_lista_servicos_str.replace('\\n', ' ')\n",
    "            return nf_item_lista_servicos_str\n",
    "        except Exception as e:\n",
    "            print(f\"Erro busca cnae: {e}\")\n",
    "            \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# 7.A VALORES E IMPOSTOS - PDF Pesquisavel\n",
    "def extrai_valores_impostos_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "\n",
    "    nf_data_valores = {}\n",
    "    \n",
    "    \n",
    "    f_frame_label = \"5_frame_valores_impostos\"\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    tipo = \"frame\"\n",
    "    message_erro = []\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number] \n",
    "    \n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=f_frame_label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    y0_tpt = y0\n",
    "    y1_tpt = y1\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    #print(f'Importos: labe: {label:>30} | template:  x0:{x0:>6} | y0:{y0_tpt:>6} | x1:{x1:>6} | y1:{y1_tpt:>6} | ajuste: y0: {y0} | y1: {y1} \\n {text}')\n",
    "    \n",
    "    nf_data_valores = novaextra.extract_fields_impostos(text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    #print(text)\n",
    "    \n",
    "    pdf_document.close()\n",
    "    \n",
    "    return  nf_data_valores\n",
    "    \n",
    "\n",
    "# 8.A DADOS COMPLEMENTARES - PDF Pesquisavel\n",
    "def extrai_dados_complementares_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "    \n",
    "    \n",
    "    f_frame_label = \"5_frame_dados_complementares\"\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    tipo = \"frame\"\n",
    "    message_erro = []\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number] \n",
    "    \n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=f_frame_label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    y0_tpt = y0\n",
    "    y1_tpt = y1\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    #print(f'Considerar: label: {f_frame_label:>30} | templt.x0:{x0:>6} | y0:{y0_tpt:>6} | x1:{x1:>6} | y1:{y1_tpt:>6} | ajuste: y0: {y0} | y1: {y1} \\n {text}')\n",
    "    \n",
    "    nf_data_dados_complementares = {}\n",
    "    nf_data_dados_complementares['secao'] = \"8. DADOS COMPLEMENTARES\"\n",
    "\n",
    "    text = re.sub(r'^DADOS COMPLEMENTARES', '', text, count=1)\n",
    "    if text == \"\":\n",
    "        text = \"None\"\n",
    "        nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "    else:    \n",
    "        # Extrair texto dentro do retângulo\n",
    "        nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "    \n",
    "    pdf_document.close()\n",
    "                        \n",
    "    return nf_data_dados_complementares  \n",
    "\n",
    "\n",
    "# 9.A OUTRAS INFORMAÇOES / CRITICAS - PDF Pesquisavel \n",
    "def extrai_outras_informacoes_PDF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, map_original_file_name, file_path):\n",
    "    \n",
    "    nf_data_outras_informacoes = {}\n",
    "    section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "    f_frame_label = \"5_frame_inf_criticas\"\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    tipo = \"frame\"\n",
    "    message_erro = []\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number] \n",
    "    \n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=f_frame_label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    y0_tpt = y0\n",
    "    y1_tpt = y1\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    #print(f'Considerar: label: {f_frame_label:>30} | templt.x0:{x0:>6} | y0:{y0_tpt:>6} | x1:{x1:>6} | y1:{y1_tpt:>6} | ajuste: y0: {y0} | y1: {y1} \\n {text}')\n",
    "    nf_data_outras_informacoes = novaextra.extract_fields_outras_info(text)\n",
    "    \n",
    "    pdf_document.close()\n",
    "    \n",
    "    return nf_data_outras_informacoes       \n",
    "\n",
    "    \n",
    " \n",
    "# 10.A OBSERVACOES  - PDF Pesquisavel\n",
    "def extrai_outras_informacoes_PDF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path):\n",
    "    \n",
    "    nf_data_observacao   = {}\n",
    "    section = \"10. OBSERVACOES\"\n",
    "    f_frame_label = \"5_frame_observacao\"\n",
    "    pdf_pesquisavel_map = True\n",
    "\n",
    "    tipo = \"frame\"\n",
    "    message_erro = []\n",
    "\n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number] \n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=f_frame_label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    y0_tpt = y0\n",
    "    y1_tpt = y1\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    \n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    #print(f'Considerar: label: {f_frame_label:>30} | templt.x0:{x0:>6} | y0:{y0_tpt:>6} | x1:{x1:>6} | y1:{y1_tpt:>6} | ajuste: y0: {y0} | y1: {y1} \\n {text}')\n",
    "    # Remove a primeira ocorrência de \"Observação:\"\n",
    "    text = re.sub(r'^Observação:', '', text, count=1)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    nf_data_observacao['observacao'] = text.strip()\n",
    "\n",
    "\n",
    "    pdf_document.close()\n",
    "    \n",
    "    return nf_data_observacao  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#===========================================================================================#\n",
    "#                                                                                           #\n",
    "#                           2. PROCESSAMENTO - RASTER PDF                                   #\n",
    "#                                                                                           #   \n",
    "#===========================================================================================#\n",
    "\n",
    "# funçao importante para buscar coordenadas do frame em funçao do contexto\n",
    "def get_coordinates_filter_by_context(pdf_pesquisavel_map, model_map, context_mapping, tipo):\n",
    "    \n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model_map, context_mapping=context_mapping, type=tipo)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [((row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p']) if pdf_pesquisavel_map else (row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1']))]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates\n",
    "\n",
    "# REAVALIAR TODAS - USAR get_coordinates_filter()\n",
    "def get_coordinates_filter_R_PDF(model_map, tipo, label, section):\n",
    "    \n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model_map, type=tipo, label=label, section_json=section)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [(row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1'])]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates\n",
    "\n",
    "\n",
    "def get_coordinates_filter_pdf_pesquisavel(model_map, tipo, label, section):\n",
    "    \n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model_map, type=tipo, label=label, section_json=section)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [(row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p'])]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates\n",
    "\n",
    "\n",
    "# 0. INFOMACOES INICIAIS - RASTER PDF\n",
    "def processar_dados_iniciais(idx, row, row_info, section, map_directory, original_file_name, file_path, debug):\n",
    "    \n",
    "    # lista_texto_extraido = []\n",
    "\n",
    "    nf_dados_doc = {}\n",
    "    nf_dados_doc['secao'] = section\n",
    "    pdf_pesquisavel = None\n",
    "    extracted_txt = pesquisa_prefeitura_pdf_pesquisavel(idx, row, row_info, map_directory, original_file_name, file_path, debug)\n",
    "    if debug:\n",
    "        print(f'\\n1. funcao: processar_dados_iniciais: doc.:{original_file_name} | diretorio: {map_directory} apos funcao: pesquisa_prefeitura_pdf_pesquisavel: extracted_txt:\\n{extracted_txt}\\n\\n')\n",
    "    \n",
    "    if extracted_txt:\n",
    "        pdf_pesquisavel = True\n",
    "    else:\n",
    "        pdf_pesquisavel = False \n",
    "       \n",
    "        x0 = 220\n",
    "        y0 = 0\n",
    "        x1= 3858\n",
    "        y1 = 1572\n",
    "        \n",
    "        # usando novo processo que gera o arquivo \"on the fly\" imagem_gray (converte PDF para imagem de tamanho grande (4134, 5846) - torna-a cinza e a salva)\n",
    "        imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "        extracted_txt = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "        if debug:\n",
    "            print(f'\\n2. funcao: processar_dados_iniciaisdoc.:{original_file_name} | diretorio: {map_directory}  apos : extract_text_PIL: extracted_txt:\\n{extracted_txt}\\n\\n')\n",
    "    \n",
    "    nf_dados_doc['file_name'] = original_file_name    \n",
    "    nf_dados_doc['pdf_pesquisavel'] = pdf_pesquisavel \n",
    "    value = {}   \n",
    "    texto_tratado = texto_extraido(extracted_txt)\n",
    "    value = define_dados_iniciais(idx, row, row_info, texto_tratado, debug)\n",
    "    if debug:\n",
    "        print(f'\\n3. funcao: processar_dados_iniciais doc.:{original_file_name} | diretorio: {map_directory} | apos funcao: define_dados_iniciais() value \\n{value}\\n\\n')\n",
    "    if value:\n",
    "        nf_dados_doc.update(value)\n",
    "   \n",
    "\n",
    "\n",
    "    return nf_dados_doc\n",
    "\n",
    "\n",
    "# 1.B CABECALHO XXX Funcoes de extracao -cabecalho Raster\n",
    "def processar_cabecalho_R_PDF(idx, row, row_info, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "    \n",
    "    data_box_valores = {}\n",
    "    data_box_conferencia = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    \n",
    "    batch_name_row_info = row_info.get('batch')\n",
    "    #status_documento_row_info = row_info.get('status_documento')\n",
    "    information_row_info = row_info.get('informations')\n",
    "    action_item_row_info = row_info.get('action_item')\n",
    "    \n",
    "    # Busco a imagem np do documento\n",
    "    image_np_row_info = row_info.get('image_np')\n",
    "    \n",
    "    data_box_valores['action_item'] = action_item_row_info\n",
    "    data_box_valores['informations'] = information_row_info\n",
    "    data_box_valores['processo'] = context_mapping\n",
    "    data_box_valores['conf_cod'] = 0\n",
    "\n",
    "\n",
    "                     \n",
    "    \n",
    "    # busco coordenadas para o contexto\n",
    "    if mapping_method == \"frame_&_sframe_field\":\n",
    "        tipo_4_coordinates = \"frame\"\n",
    "        tipo_4_filter = \"sframe_field\"\n",
    "    \n",
    "   \n",
    "    # 2. usando a funcao de extracao de coordenadas por contexto    \n",
    "    coordinates = get_coordinates_filter_by_context(pdf_pesquisavel_map, model_map, context_mapping, tipo_4_coordinates)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    x0 = int(x0)\n",
    "    y0 = int(y0)\n",
    "    x1 = int(x1)\n",
    "    y1 = int(y1) \n",
    "    # 3. Cropo a imagem - novo modelo\n",
    "    cropped_image_np = image_np_row_info[y0:y1, x0:x1] # ajustar nos demais\n",
    "    data_box_conferencia[f'box_{context_mapping}'] = cropped_image_np\n",
    "    data_box_conferencia[f'coordinates_{context_mapping}'] = coordinates\n",
    "    # 4. Converto para PIL\n",
    "    cropped_image_pil = Image.fromarray(cropped_image_np)\n",
    "    # 6. Executo OCR\n",
    "    texto_extraido = pytesseract.image_to_string(cropped_image_pil, lang='por')\n",
    "    # 7. Trato o texto extraido = text_splited\n",
    "    text_splited = texto_extraido_cabecalho(texto_extraido)\n",
    "    if debug:\n",
    "        print()\n",
    "        plt.imshow(cropped_image_np)\n",
    "        plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "        plt.show()\n",
    "        print(f'\\ncoordinates {coordinates} - \\ntexto_extraido:\\n{text_splited}\\n')\n",
    "        \n",
    "    # 8. Efetuo o filtro para a iteracao\n",
    "    filtered_frame_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['context_mapping'] == context_mapping) & (frames_nf_v4_df['type'] == tipo_4_filter)]\n",
    "    \n",
    "    # 9. iter sobre o filtro\n",
    "    for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "        try:\n",
    "            section = row_frame['section_json']\n",
    "            label = row_frame['label']\n",
    "            reference = row_frame['reference']\n",
    "            string_pesquisa = row_frame['marcador_inicio']  \n",
    "            keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_box_valores[label] = texto\n",
    "            if debug:\n",
    "               print(f'\\nidx: {index_frame:> 3} | label: {label} |  string_pesquisa:{string_pesquisa} | dentro do try do raster PDF cabecalho - texto: \\n{texto}\\n\\n')\n",
    "        except Exception as e:\n",
    "            msg = (f\"{e}\")\n",
    "            data_box_conferencia[label] = msg\n",
    "    \n",
    "\n",
    "    # Verificações após o loop\n",
    "    for key, value in data_box_valores.items():\n",
    "        if key == 'numero_nota_fiscal' and value is None:\n",
    "            action_item_row_info = 'BREAK_PROCESS'\n",
    "            information_row_info = 'Número da Nota não encontrado'\n",
    "            #logging.error(f\" {batch_name} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")  # Ou registre o erro de outra forma que preferir\n",
    "        \n",
    "        elif key == 'codigo_verificacao' and value != None:\n",
    "            codigo_verificacao_nf = value\n",
    "            tam_codigo_verificacao = len(codigo_verificacao_nf)\n",
    "            data_box_valores['conf_cod'] = tam_codigo_verificacao\n",
    "            \n",
    "        \n",
    "        elif key != 'numero_nota_fiscal' and value is None:\n",
    "            logging.error(f\" {batch_name_row_info} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")  # Ou registre o erro de outra forma que preferir\n",
    "\n",
    "            \n",
    "      # if value is None:\n",
    "        #     logging.error(f\" {batch_name} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")  # Ou registre o erro de outra forma que preferir\n",
    "\n",
    "    data_box_valores['action_item'] = action_item_row_info\n",
    "    data_box_valores['informations'] = information_row_info\n",
    "\n",
    "    \n",
    "    return data_box_valores\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# 2.B PRESTADOR DE SERVIÇO - RASTER_PDF\n",
    "def extrai_prestador_R_PDF(idx, row, row_info, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "    \n",
    "    nf_data_prestador = {}\n",
    "    data_box_conferencia = {}\n",
    "    dic_erros = {}\n",
    "    message_erro = []\n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = False\n",
    "    nf_data_prestador['secao'] = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    # 1 busco o documento - image_np\n",
    "    image_np = row_info['image_np']\n",
    "    batch_name = row_info['batch']\n",
    "\n",
    "    process = ['2_frame_cnpj_prestador', '2_frame_inscricao_prestador', '2_frame_dados_prestador']\n",
    "    tipo = \"frame\"\n",
    "\n",
    "    # usando novo processo que gera o arquivo \"on the fly\" imagem_gray (converte PDF para imagem de tamanho grande (4134, 5846) - torna-a cinza e a salva)\n",
    "    imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "\n",
    "    for label in process:\n",
    "        seq = process.index(label) + 1\n",
    "        if label == \"2_frame_cnpj_prestador\":\n",
    "            coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            x0 = int(x0)\n",
    "            y0 = int(y0)\n",
    "            x1 = int(x1)\n",
    "            y1 = int(y1)\n",
    "            y0 = y0 * f_0\n",
    "            y1 = y1 * f_1 \n",
    "            # 3. Cropo a imagem - novo modelo\n",
    "            cropped_image_np = image_np[y0:y1, x0:x1]\n",
    "            data_box_conferencia[f'box_{label}'] = cropped_image_np\n",
    "            data_box_conferencia[f'coordinates_{label}'] = coordinates\n",
    "            # 4. Converto para PIL\n",
    "            cropped_image_pil = Image.fromarray(cropped_image_np)\n",
    "            # 6. Executo OCR\n",
    "            texto_extraido = pytesseract.image_to_string(cropped_image_pil, lang='por')\n",
    "            text_splited = texto_extraido.split('\\n')\n",
    "            if debug:\n",
    "                plt.imshow(cropped_image_np)\n",
    "                plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "                plt.show()\n",
    "                print(f'\\ncoordinates {coordinates} - \\ntexto_extraido:\\n{text_splited}\\n')\n",
    "            #print(f'1. : (coordenadas originais):  x0:{x0} y0:{y0} x1:{x1} y1:{y1}')\n",
    "\n",
    "            #print(f'2.A   coordenadas ajustadas:   x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0:{f_0}, f_1: {f_1}\\n')\n",
    "            # texto_extraido = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "            \n",
    "            text_splited = [x for x in text_splited if x.strip()]\n",
    "            keyword_list = ['CPF/CNPJ:', 'Telefone:']\n",
    "            string_pesquisa = \"CPF/CNPJ:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', texto)\n",
    "            if cpf_cnpj_formatado_match:\n",
    "                prestador_cpf_cnpj_com_mascara = cpf_cnpj_formatado_match.group(1)\n",
    "                nf_data_prestador['p_cpf_cnpj_com_mascara'] = prestador_cpf_cnpj_com_mascara\n",
    "                prestador_cpf_cnpj_sem_mascara = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "                nf_data_prestador['p_cpf_cnpj_sem_mascara'] = prestador_cpf_cnpj_sem_mascara\n",
    "            else:\n",
    "                cpf_cnpj_com_mascara = None\n",
    "                cpf_cnpj_sem_mascara = None\n",
    "            try:    \n",
    "                string_pesquisa = \"Telefone:\"  \n",
    "                texto_tel = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', texto_tel) \n",
    "                if telefone_match: \n",
    "                    prestador_telefone_str = telefone_match.group(1)\n",
    "                else:\n",
    "                    telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', texto)\n",
    "                    if telefone_match: \n",
    "                        prestador_telefone_str = telefone_match.group(1)\n",
    "                        nf_data_prestador['p_telefone'] = prestador_telefone_str \n",
    "                    else:\n",
    "                        nf_data_prestador['p_telefone'] = texto_tel\n",
    "            except Exception as e:\n",
    "                nf_data_prestador['p_telefone'] = None\n",
    "                new_row = {\n",
    "                    \"row_index\": idx,  # Substitua 'index' pela variável que contém o índice da linha atual\n",
    "                    \"erro_inscricao\": str(e),\n",
    "                    \"file\": original_file_name,\n",
    "                    \"process_label\": label\n",
    "                }              \n",
    "        \n",
    "        elif label == \"2_frame_inscricao_prestador\":\n",
    "            coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            x0 = int(x0)\n",
    "            y0 = int(y0)\n",
    "            x1 = int(x1)\n",
    "            y1 = int(y1)\n",
    "            y0 = y0 * f_0\n",
    "            y1 = y1 * f_1 \n",
    "            # 3. Cropo a imagem - novo modelo\n",
    "            cropped_image_np = image_np[y0:y1, x0:x1]\n",
    "            data_box_conferencia[f'box_{label}'] = cropped_image_np\n",
    "            data_box_conferencia[f'coordinates_{label}'] = coordinates\n",
    "            # 4. Converto para PIL\n",
    "            cropped_image_pil = Image.fromarray(cropped_image_np)\n",
    "            # 6. Executo OCR\n",
    "            texto_extraido = pytesseract.image_to_string(cropped_image_pil, lang='por')\n",
    "            text_splited = texto_extraido.split('\\n')\n",
    "            if debug:\n",
    "                plt.imshow(cropped_image_np)\n",
    "                plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "                plt.show()\n",
    "                print(f'\\ncoordinates {coordinates} - \\ntexto_extraido:\\n{text_splited}\\n')\n",
    "            # y0 = y0 * f_0\n",
    "            # y1 = y1 * f_1\n",
    "            # texto_extraido = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "            # text_splited = texto_extraido.split('\\n')\n",
    "            text_splited = [x for x in text_splited if x.strip()]\n",
    "            keyword_list = ['Inscrição Municipal:', 'Inscrição Estadual:']\n",
    "            string_pesquisa = \"Inscrição Municipal:\"\n",
    "            prestadpor_inscricao_municipal = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            nf_data_prestador['p_inscricao_municipal'] = prestadpor_inscricao_municipal\n",
    "            \n",
    "            string_pesquisa = \"Inscrição Estadual:\"\n",
    "            prestador_inscricao_estadual = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            nf_data_prestador['p_inscricao_estadual'] = prestador_inscricao_estadual                    \n",
    "\n",
    "        elif label == \"2_frame_dados_prestador\":\n",
    "            coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            x0 = int(x0)\n",
    "            y0 = int(y0)\n",
    "            x1 = int(x1)\n",
    "            y1 = int(y1)\n",
    "            y0 = y0 * f_0\n",
    "            y1 = y1 * f_1 \n",
    "            # 3. Cropo a imagem - novo modelo\n",
    "            cropped_image_np = image_np[y0:y1, x0:x1]\n",
    "            data_box_conferencia[f'box_{label}'] = cropped_image_np\n",
    "            data_box_conferencia[f'coordinates_{label}'] = coordinates\n",
    "            # 4. Converto para PIL\n",
    "            cropped_image_pil = Image.fromarray(cropped_image_np)\n",
    "            # 6. Executo OCR\n",
    "            texto_extraido = pytesseract.image_to_string(cropped_image_pil, lang='por')\n",
    "            text_splited = texto_extraido.split('\\n')\n",
    "            if debug:\n",
    "                plt.imshow(cropped_image_np)\n",
    "                plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "                plt.show()\n",
    "                print(f'\\ncoordinates {coordinates} - \\ntexto_extraido:\\n{text_splited}\\n')\n",
    "            #print(f'1. : (coordenadas originais):  x0:{x0} y0:{y0} x1:{x1} y1:{y1}')\n",
    "            # y0 = y0 * f_0\n",
    "            # y1 = y1 * f_1\n",
    "            # #print(f'2.A   coordenadas ajustadas:   x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0:{f_0}, f_1: {f_1}\\n')\n",
    "            # texto_extraido = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "            # text_splited = texto_extraido.split('\\n')\n",
    "            text_splited = [x for x in text_splited if x.strip()]\n",
    "            \n",
    "            keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "            string_pesquisa = \"Nome/Razão Social:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            prestador_razao_social = texto\n",
    "            nf_data_prestador['p_razao_social'] = prestador_razao_social\n",
    "\n",
    "            string_pesquisa = \"Nome de Fantasia:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            prestador_nome_fantasia = texto\n",
    "            nf_data_prestador['p_nome_fantasia'] = prestador_nome_fantasia\n",
    "            \n",
    "            string_pesquisa = \"Endereço:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            prestador_endereco = texto\n",
    "            nf_data_prestador['p_endereco'] = prestador_endereco\n",
    "            \n",
    "            string_pesquisa = \"E-mail:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            prestador_email = texto\n",
    "            if prestador_email and prestador_email != None:\n",
    "                prestador_email = corrigir_email(prestador_email)\n",
    " \n",
    "            nf_data_prestador['p_email'] = prestador_email\n",
    "            \n",
    "    return nf_data_prestador  \n",
    "\n",
    "# 3.A. TOMADOR DE SERVIÇO - RASTER PDF\n",
    "def extrai_tomador_R_PDF(idx, row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "    \n",
    "    nf_data_tomador = {}\n",
    "    section = \"3. TOMADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = False\n",
    "    cpf_cnpj_tomador = {}\n",
    "    \n",
    "    dic_erros = {}\n",
    "    message_erro = []\n",
    "\n",
    "    nf_data_tomador['secao'] = section\n",
    "\n",
    "    process = ['3_frame_cnpj_tomador', '3_frame_inscricao_tomador', '3_frame_dados_tomador']\n",
    "    tipo = \"frame\"\n",
    "\n",
    "    # usando novo processo que gera o arquivo \"on the fly\" imagem_gray (converte PDF para imagem de tamanho grande (4134, 5846) - torna-a cinza e a salva)\n",
    "    imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "    \n",
    "    i = 1\n",
    "    for label in process:\n",
    "        seq = process.index(label) + 1\n",
    "        if label == \"3_frame_cnpj_tomador\":\n",
    "            \n",
    "            try:\n",
    "                coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                #print(f'1. : (coordenadas originais):  x0:{x0} y0:{y0} x1:{x1} y1:{y1}')\n",
    "                y0 = y0 * f_0\n",
    "                y1 = y1 * f_1\n",
    "                #print(f'2.A   coordenadas ajustadas:   x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0:{f_0}, f_1: {f_1}\\n')\n",
    "                texto_extraido = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "                texto_tomador_cnpj = texto_extraido\n",
    "                file_name = row['original_file_name']\n",
    "                #print(f'\\ni = {i} | seq.: {seq} file: {file_name} \\ntexto_tomador_cnpj:\\n{texto_tomador_cnpj}\\n')\n",
    "                \n",
    "                text_splited = texto_extraido.split('\\n')\n",
    "                text_splited = [x for x in text_splited if x.strip()]\n",
    "                keyword_list = ['CPF/CNPJ:', 'Telefone:'] # 'Nome/Razão Social:'\n",
    "                string_pesquisa = \"CPF/CNPJ:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                #print(f'texto antes do try: {texto}')\n",
    "                cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})|(\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2})|(\\d{11})', texto)\n",
    "                if cpf_cnpj_formatado_match:\n",
    "                    texto_cpf_cnpj = cpf_cnpj_formatado_match.group()\n",
    "                    # print(f'\\ntexto_cpf_cnpj: {texto_cpf_cnpj}\\n')  # Aqui usamos group() sem argumentos para pegar toda a string que correspondeu\n",
    "                    cpf_cnpj_tomador['t_cpf_cnpj_com_mascara'] = texto_cpf_cnpj  # Aqui atribuímos a string correspondente diretamente\n",
    "                    tomador_cpf_cnpj_sem_mascara = re.sub(r'\\D', '', texto_cpf_cnpj)  # Aqui removemos todos os caracteres não-dígitos da string correspondente\n",
    "                    cpf_cnpj_tomador['t_cpf_cnpj_sem_mascara'] = tomador_cpf_cnpj_sem_mascara\n",
    "                    \n",
    "\n",
    "                string_pesquisa = \"Telefone:\"  \n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                telefone_match = re.search(r'(\\(?\\+?[0-9]*\\)?[-. \\s]?[0-9]+[-. \\s]?[0-9]+)', texto)\n",
    "                if telefone_match:\n",
    "                    telefone = telefone_match.group(0)\n",
    "                else:\n",
    "                    telefone = None\n",
    "\n",
    "                if telefone and len(re.findall(r'\\d', telefone)) >= 8:\n",
    "                    # A string contém pelo menos 8 dígitos, então assumimos que é um número de telefone válido\n",
    "                    nf_data_tomador['t_telefone'] = telefone\n",
    "                else:\n",
    "                    nf_data_tomador['t_telefone'] = None\n",
    "\n",
    "            except Exception as e:\n",
    "                nf_data_tomador['t_telefone'] = None\n",
    "                new_row = {\n",
    "                    \"row_index\": idx,  # Substitua 'index' pela variável que contém o índice da linha atual\n",
    "                    \"erro_inscricao\": str(e),\n",
    "                    \"file\": original_file_name,\n",
    "                    \"process_label\": label\n",
    "                }              \n",
    "        \n",
    "        elif label == \"3_frame_inscricao_tomador\":\n",
    "            # valor_cpf_cnpj_apos = nf_data_tomador['t_cpf_cnpj_com_mascara']\n",
    "            # print(f'valor no elif da inscricao:{valor_cpf_cnpj_apos}\\n')\n",
    "            coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            y0 = y0 * f_0\n",
    "            y1 = y1 * f_1\n",
    "            texto_extraido = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "            texto_tomador_inscricao = texto_extraido\n",
    "            #print(f'\\ni = {i} | seq.: {seq} file: {original_file_name} \\ntexto_tomador_cnpj:\\n{texto_tomador_inscricao}\\n')\n",
    "            \n",
    "            text_splited = texto_extraido.split('\\n')\n",
    "            text_splited = [x for x in text_splited if x.strip()]\n",
    "            keyword_list = ['Inscrição Municipal:', 'RG:', 'Inscrição Estadual:']\n",
    "            string_pesquisa = \"Inscrição Municipal:\"\n",
    "            tomador_inscricao_municipal = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            nf_data_tomador['t_inscricao_municipal'] = tomador_inscricao_municipal\n",
    "            \n",
    "            string_pesquisa = \"RG:\"\n",
    "            tomador_rg = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            nf_data_tomador['t_rg'] = tomador_rg\n",
    "            \n",
    "            string_pesquisa = \"Inscrição Estadual:\"\n",
    "            tomador_inscricao_estadual = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            nf_data_tomador['t_inscricao_estadual'] = tomador_inscricao_estadual                    \n",
    "\n",
    "        elif label == \"3_frame_dados_tomador\":\n",
    "            coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            #print(f'1. : (coordenadas originais):  x0:{x0} y0:{y0} x1:{x1} y1:{y1}')\n",
    "            y0 = y0 * f_0\n",
    "            y1 = y1 * f_1\n",
    "            #print(f'2.A   coordenadas ajustadas:   x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0:{f_0}, f_1: {f_1}\\n')\n",
    "            texto_extraido = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "            texto_tomador_dados = texto_extraido\n",
    "            #print(f'\\ni = {i} | seq.: {seq} file: {original_file_name} \\ntexto_tomador_cnpj:\\n{texto_tomador_dados}\\n')\n",
    "            text_splited = texto_extraido.split('\\n')\n",
    "            text_splited = [x for x in text_splited if x.strip()]\n",
    "            \n",
    "            keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "            string_pesquisa = \"Nome/Razão Social:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            texto_limpo = texto.replace('“', '').replace('”', '')\n",
    "            tomador_razao_social = texto_limpo\n",
    "            nf_data_tomador['t_razao_social'] = tomador_razao_social\n",
    "\n",
    "            string_pesquisa = \"Endereço:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            tomador_endereco = texto\n",
    "            nf_data_tomador['t_endereco'] = tomador_endereco\n",
    "            \n",
    "            string_pesquisa = \"E-mail:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            tomador_email = corrigir_email(texto)\n",
    "            \n",
    "            nf_data_tomador['t_email'] = tomador_email\n",
    "            \n",
    "            # valor_cpf_cnpj_apos = nf_data_tomador['t_cpf_cnpj_com_mascara']\n",
    "            # print(f'valor no no fim do bloco:{valor_cpf_cnpj_apos}\\n')\n",
    "            \n",
    "        i += 1\n",
    "        #print(nf_data_tomador)\n",
    "    nf_data_tomador['t_cpf_cnpj_com_mascara'] = cpf_cnpj_tomador['t_cpf_cnpj_com_mascara']\n",
    "    nf_data_tomador['t_cpf_cnpj_sem_mascara'] = cpf_cnpj_tomador['t_cpf_cnpj_sem_mascara'] \n",
    "            \n",
    "    return nf_data_tomador\n",
    "\n",
    "\n",
    "\n",
    "# 7.B VALORES E IMPOSTOS - Raster_PDF\n",
    "def extracao_impostos_R_PDF(section, tipo, father_value, de_para_pm, model_map, original_file_name, file_path):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    \n",
    "    #print(f'father_value: {father_value}, section: {section}, tipo: {tipo}, model_map: {model_map}, de_para_pm: {de_para_pm}, original_file_name: {original_file_name}\\n')\n",
    "\n",
    "    # trato a imagem logo no começo\n",
    "    imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "\n",
    "    # Estabeleco o filtro\n",
    "    filtered_boxes_info = frames_nf_v4_df[((frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == tipo) & (frames_nf_v4_df['section_json'] == section) & (frames_nf_v4_df['father'] == father_value))]\n",
    "\n",
    "    for idx_frame, row_frame in filtered_boxes_info.iterrows():\n",
    "        extracted_text_box = None\n",
    "        reference = row_frame['reference']\n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        index = idx_frame\n",
    "        image_gray_croped = imagem_gray.crop((x0, y0, x1, y1))\n",
    "        extracted_text_box = (pytesseract.image_to_string(image_gray_croped, lang='por'))\n",
    "        linhas = extracted_text_box.split('\\n')\n",
    "        label = row_frame['label']\n",
    "        #print(f'para label:{label:>15} e extracted_text_box: {extracted_text_box}  | x0: {x0:>6} | y0: {y0:>6} | x1: {x1:>6} | y1: {y1:>6} |\\n\\n')\n",
    "        for texto in linhas:\n",
    "            valores = re.findall(r'R\\$ *([\\d\\.]+,\\d{1,2})|([\\d\\.]+,\\d{1,2})%', texto)\n",
    "            if 'R$' in texto or ',' in texto:\n",
    "                # Original format: Remove 'R$', replace dots with nothing, and replace commas with dots\n",
    "                number_str = texto.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "                value = float(number_str)\n",
    "                #print(f'label: {label:>20} | valor: {value}')\n",
    "                label = row_frame['label']\n",
    "                data_box_valores[label] = value \n",
    "            elif '%' in texto:\n",
    "                percent_str = texto.replace('%', '')\n",
    "                value = float(percent_str)  \n",
    "                #print(f'label: {label:>20} | valor: {value}')\n",
    "                data_box_valores[label] = value \n",
    "                \n",
    "        label = row_frame['label']\n",
    "        data_box_valores[label] = value        \n",
    "           \n",
    "        \n",
    "    return data_box_valores\n",
    "\n",
    "# 8.B DADOS COMPLEMENTARES - Raster_PDF\n",
    "def extracao_complementares_R_PDF(row, section, tipo, father_value, de_para_pm, model_map, original_file_name, file_path):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    \n",
    "    #print(f'\\n0. entrei na func. -  model_map: {model_map} | section: {section} | tipo: {tipo} | father_value: {father_value} n\\n')\n",
    "    imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "    filtered_boxes_info = frames_nf_v4_df[((frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == tipo) & (frames_nf_v4_df['father'] == father_value))]\n",
    "    for idx_frame, row_frame in filtered_boxes_info.iterrows():\n",
    "        extracted_text_box = None\n",
    "        father_value = row_frame['father']\n",
    "        label = row_frame['label']\n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        index = idx_frame\n",
    "        #print(f'1. idx: {idx_frame} label:{label:>15} \\n\\n')\n",
    "        image_gray_croped = imagem_gray.crop((x0, y0, x1, y1))\n",
    "        extracted_text_box = (pytesseract.image_to_string(image_gray_croped, lang='por'))\n",
    "        texto = extracted_text_box\n",
    "        text = re.sub(r'^DADOS COMPLEMENTARES', '', extracted_text_box, count=1)\n",
    "        if text == '':\n",
    "            value = None\n",
    "                # data_box_valores[label] = value\n",
    "\n",
    "        \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "# 9.B OUTRAS INFORMAÇOES / CRITICAS - Raster_PDF \n",
    "def extracao_inforacoes_criticas_R_PDF(section, tipo, father_value, de_para_pm, model_map, original_file_name, file_path):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "\n",
    "    #print(f'father_value: {father_value}, section: {section}, tipo: {tipo}, model_map: {model_map}, de_para_pm: {de_para_pm}, original_file_name: {original_file_name}\\n')\n",
    "    # trato a imagem logo no começo\n",
    "    imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "\n",
    "    # Estabeleco o filtro\n",
    "    filtered_boxes_info = frames_nf_v4_df[((frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == tipo) & (frames_nf_v4_df['section_json'] == section) & (frames_nf_v4_df['father'] == father_value))]\n",
    "\n",
    "    for idx_frame, row_frame in filtered_boxes_info.iterrows():\n",
    "        extracted_text_box = None\n",
    "        reference = row_frame['reference']\n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        index = idx_frame\n",
    "        \n",
    "        label = row_frame['label']\n",
    "        image_gray_croped = imagem_gray.crop((x0, y0, x1, y1))\n",
    "        extracted_text_box = (pytesseract.image_to_string(image_gray_croped, lang='por'))\n",
    "        #print(extracted_text_box)\n",
    "        texto = extracted_text_box\n",
    "        if label == \"exigibilidade_iss\":\n",
    "            texto = extracted_text_box\n",
    "            linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "            marcador = \"EXIGIBILIDADE ISS\"\n",
    "            value  = encontrar_valor_por_marcador(marcador, linhas)\n",
    "            data_box_valores[label] = value\n",
    "            \n",
    "        if label == \"regime_tributacao\":\n",
    "            texto = extracted_text_box\n",
    "            linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "            marcador = \"REGIME TRIBUTAÇÃO\"\n",
    "            value = encontrar_valor_por_marcador(marcador, linhas)\n",
    "            data_box_valores[label] = value\n",
    "            \n",
    "        if label == \"simples_nacional\":\n",
    "            texto = extracted_text_box\n",
    "            linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "            marcador = \"SIMPLES NACIONAL\"\n",
    "            value = encontrar_valor_por_marcador(marcador, linhas) \n",
    "            data_box_valores[label] = value \n",
    "        \n",
    "        if label == \"issqn_retido\":\n",
    "            texto = extracted_text_box \n",
    "            linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "            marcador = \"ISSQN RETIDO\"\n",
    "            valor = encontrar_valor_por_marcador(marcador, linhas)\n",
    "            data_box_valores[label] = value \n",
    "            \n",
    "            \n",
    "        if label == \"local_pretacao_servico\":\n",
    "            texto = extracted_text_box\n",
    "            texto = texto.replace('\\n\\n', \" \").strip()\n",
    "            linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "            marcador = \"LOCAL. PRESTAÇÃO SERVIÇO\"\n",
    "            value = encontrar_valor_por_marcador(marcador, linhas)\n",
    "            data_box_valores[label] = value \n",
    "            \n",
    "        if label == \"local_incidencia\":\n",
    "            texto = extracted_text_box\n",
    "            linhas = [linha for linha in texto.split('\\n') if linha.strip()] \n",
    "            marcador = \"LOCAL INCIDÊNCIA\"\n",
    "            value = encontrar_valor_por_marcador(marcador, linhas)  \n",
    "            data_box_valores[label] = value              \n",
    "\n",
    "                \n",
    "        #print(f'label:{label:>25} | value: {value}\\n')         \n",
    "           \n",
    "        \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "def extrair_exigibilidade_iss(texto):\n",
    "    # Lista de possíveis valores para este campo\n",
    "    possiveis_valores = [\"Exigível\", \"Não Exigível\"]\n",
    "    \n",
    "    # Encontrando o valor mais semelhante no texto\n",
    "    valor, score = process.extractOne(texto, possiveis_valores)\n",
    "    \n",
    "    # Você pode ajustar o limite de score conforme necessário\n",
    "    if score > 85:\n",
    "        return valor\n",
    "    return None\n",
    "\n",
    "def extrair_regime_tributacao(texto):\n",
    "    # Lista de possíveis valores para este campo\n",
    "    possiveis_valores = [\n",
    "        \"Sociedade Limitada\", \n",
    "        \"Microempresário Individual (MEI)\", \n",
    "        \"Sociedade anônima\", \n",
    "        \"Microempresa municipal\", \n",
    "        \"Microempresário e Empresa de Pequeno Porte (ME EPP)\"\n",
    "    ]\n",
    "    \n",
    "    # Encontrando o valor mais semelhante no texto\n",
    "    valor, score = process.extractOne(texto, possiveis_valores)\n",
    "    \n",
    "    # Você pode ajustar o limite de score conforme necessário\n",
    "    if score > 85:\n",
    "        return valor\n",
    "    return None\n",
    "\n",
    "def extrair_simples_nacional(texto):\n",
    "    # Verificando se o texto contém 'Sim' ou 'Não'\n",
    "    if 'Sim' in texto:\n",
    "        # Tentando extrair o valor percentual\n",
    "        match = re.search(r'Sim \\((.*?)%\\)', texto)\n",
    "        if match:\n",
    "            return f'Sim ({match.group(1)}%)'\n",
    "        return 'Sim'\n",
    "    elif 'Não' in texto:\n",
    "        return 'Não'\n",
    "    return None\n",
    "\n",
    "def extrair_issqn_retido(texto):\n",
    "    # Verificando se o texto contém 'Sim' ou 'Não'\n",
    "    if 'Sim' in texto:\n",
    "        return 'Sim'\n",
    "    elif 'Não' in texto:\n",
    "        return 'Não'\n",
    "    return None\n",
    "\n",
    "def extrair_local_prestacao(texto):\n",
    "    # Lista de possíveis valores para este campo\n",
    "    possiveis_valores = [\n",
    "        'Magé - RJ', 'São Pedro da Aldeia - RJ', 'Armação dos Búzios - RJ',\n",
    "        'Cabo Frio - RJ', 'Araruama - RJ', 'Rio de Janeiro - RJ',\n",
    "        'Mesquita - RJ', 'SAO PEDRO DA ALDEIA/RJ', 'MESQUITA/RJ', 'Macaé - RJ'\n",
    "    ]\n",
    "    \n",
    "    # Encontrando o valor mais semelhante no texto\n",
    "    valor, score = process.extractOne(texto, possiveis_valores)\n",
    "    \n",
    "    # Você pode ajustar o limite de score conforme necessário\n",
    "    if score > 85:\n",
    "        return valor\n",
    "    return None\n",
    "\n",
    "def encontrar_valor_por_marcador(marcador, lista_strings):\n",
    "    # Mapeando marcadores para funções de pós-processamento\n",
    "    funcoes_pos_processamento = {\n",
    "        \"EXIGIBILIDADE ISS\": extrair_exigibilidade_iss,\n",
    "        \"REGIME TRIBUTAÇÃO\": extrair_regime_tributacao,\n",
    "        \"SIMPLES NACIONAL\": extrair_simples_nacional,\n",
    "        \"ISSQN RETIDO\": extrair_issqn_retido,\n",
    "        \"LOCAL. PRESTAÇÃO SERVIÇO\": extrair_local_prestacao,\n",
    "        \"LOCAL INCIDÊNCIA\": extrair_local_prestacao  # Usando a mesma função que 'LOCAL. PRESTAÇÃO SERVIÇO'\n",
    "    }\n",
    "    \n",
    "    # Encontre o índice do marcador na lista de strings\n",
    "    for i, linha in enumerate(lista_strings):\n",
    "        if marcador.lower() in linha.lower():\n",
    "            # Suponha que o valor está na próxima linha\n",
    "            valor_bruto = lista_strings[i + 1]\n",
    "            \n",
    "            # Obtemos a função de pós-processamento correspondente ao marcador\n",
    "            funcao_pos_processamento = funcoes_pos_processamento.get(marcador)\n",
    "            \n",
    "            # Se encontrarmos uma função de pós-processamento correspondente, aplicamo-la ao valor bruto\n",
    "            if funcao_pos_processamento:\n",
    "                return funcao_pos_processamento(valor_bruto)\n",
    "            \n",
    "            return valor_bruto\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def extracao_complementar_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_pm, model_map, def_replace, original_file_name, debug):\n",
    "    \n",
    "    #print(idx, row, guarda_texto_doc, section, label, de_para_pm, model_map, def_replace, original_file_name)\n",
    "    map_document_unique_id = idx\n",
    "    nf_data_dados_complementares = {}\n",
    "    label = label\n",
    "    section = section\n",
    "    model_map = model_map\n",
    "    tipo = tipo\n",
    "    #print(f'\\nidx: {idx} | label: {label} | section: {section} | model_map: {model_map} | tipo: {tipo} | def_replace: {def_replace} | original_file_name: {original_file_name}\\n\\n')\n",
    "    \n",
    "    if guarda_texto_doc['document_unique_id'] == map_document_unique_id:\n",
    "        texto_documento_uso = guarda_texto_doc['texto_documento']\n",
    "        #print(f'\\n\\ntexto_documento_uso: {texto_documento_uso}')\n",
    "    if texto_documento_uso:\n",
    "        # Busco marcadores\n",
    "        texto_completo = \" \".join(texto_documento_uso)\n",
    "        \n",
    "        marcador_inicio, marcador_fim, modelo = busca_marcadores(model_map, section, tipo, label)\n",
    "        #print(marcador_inicio, marcador_fim, modelo)\n",
    "        \n",
    "        if modelo == model_map:\n",
    "            texto_extraido = encontrar_texto(texto_completo, marcador_inicio, marcador_fim)\n",
    "            \n",
    "            if def_replace:\n",
    "                texto_extraido = texto_extraido.replace(marcador_inicio, \"\").strip()\n",
    "            \n",
    "                texto_extraido_strip = texto_extraido.strip()\n",
    "                #print(f'texto_extraido_strip: {texto_extraido_strip}')\n",
    "                nf_data_dados_complementares['dados_complementares'] = texto_extraido_strip\n",
    "                \n",
    "                return nf_data_dados_complementares\n",
    "            else:\n",
    "                #print(\"texto_extraido\", texto_extraido)\n",
    "                nf_data_dados_complementares['dados_complementares'] = texto_extraido\n",
    "                \n",
    "                return nf_data_dados_complementares\n",
    "         \n",
    "            \n",
    "        print(\"nao achou modelo\")\n",
    "        return None \n",
    "            \n",
    "    else:\n",
    "        print(\"nao achou texto\")\n",
    "        return None \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def extracao_observacoees_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_pm, model_map, def_replace, original_file_name, debug):\n",
    "    \n",
    "    #print(idx, row, guarda_texto_doc, section, label, de_para_pm, model_map, def_replace, original_file_name)\n",
    "    map_document_unique_id = idx\n",
    "    nf_data_observacao = {}\n",
    "    label = label\n",
    "    section = section\n",
    "    model_map = model_map\n",
    "    tipo = tipo\n",
    "    #print(f'\\nidx: {idx} | label: {label} | section: {section} | model_map: {model_map} | tipo: {tipo} | def_replace: {def_replace} | original_file_name: {original_file_name}\\n\\n')\n",
    "    \n",
    "    if guarda_texto_doc['document_unique_id'] == map_document_unique_id:\n",
    "        texto_documento_uso = guarda_texto_doc['texto_documento']\n",
    "        #print(f'\\n\\ntexto_documento_uso: {texto_documento_uso}')\n",
    "    if texto_documento_uso:\n",
    "        # Busco marcadores\n",
    "        texto_completo = \" \".join(texto_documento_uso)\n",
    "        \n",
    "        marcador_inicio, marcador_fim, modelo = busca_marcadores(model_map, section, tipo, label)\n",
    "        #print(marcador_inicio, marcador_fim, modelo)\n",
    "        \n",
    "        texto_extraido = encontrar_texto(texto_completo, marcador_inicio, marcador_fim)\n",
    "        if texto_extraido:\n",
    "            nf_data_observacao['observacao'] = texto_extraido\n",
    "            \n",
    "            return nf_data_observacao\n",
    "        \n",
    "        else:\n",
    "            marcador_fim = None\n",
    "            texto_extraido = encontrar_texto(texto_completo, marcador_inicio, marcador_fim)\n",
    "            if texto_extraido:\n",
    "                nf_data_observacao['observacao'] = texto_extraido\n",
    "                \n",
    "                return nf_data_observacao\n",
    "            else:\n",
    "                return None\n",
    "                 \n",
    "            \n",
    "        print(\"nao achou modelo\")\n",
    "        return None \n",
    "            \n",
    "    else:\n",
    "        print(\"nao achou texto\")\n",
    "        return None  \n",
    "\n",
    "\n",
    "\n",
    "# 10.B OBSERVACOES  - Raster_PDF \n",
    "def extracao_observacao_R_PDF(row, section, tipo, father_value, de_para_pm, model_map, original_file_name, file_path):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    #print(f'\\n0. entrei na func. -  model_map: {model_map} | section: {section} | tipo: {tipo} | father_value: {father_value} n\\n')\n",
    "    imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "    filtered_boxes_info = frames_nf_v4_df[((frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == tipo) & (frames_nf_v4_df['father'] == father_value))]\n",
    "    for idx_frame, row_frame in filtered_boxes_info.iterrows():\n",
    "        extracted_text_box = None\n",
    "        reference = row_frame['reference']\n",
    "        father_value = row_frame['father']\n",
    "        label = row_frame['label']\n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        index = idx_frame\n",
    "        image_gray_croped = imagem_gray.crop((x0, y0, x1, y1))\n",
    "        extracted_text_box = (pytesseract.image_to_string(image_gray_croped, lang='por'))\n",
    "        texto = extracted_text_box\n",
    "        text = re.sub(r'^Observação:', '', extracted_text_box, count=1)\n",
    "        value = text.replace('\\n', ' ')\n",
    "        data_box_valores[label] = value\n",
    "        #print(f'idx: {idx_frame} label:{label:>15}    | x0: {x0:>6} | y0: {y0:>6} | x1: {x1:>6} | y1: {y1:>6} |\\n{text}\\n')\n",
    "   \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "# XXX IMPORTANTE - ESTA E A FUNCAO PARA SER UTILIZADA: POIS CONVERTE PARA CINZA E RESIZE: (4134, 5846)\n",
    "def convert_resize_gray(original_file_name, file_path, image_resized_path):\n",
    "\n",
    "    name_image = utl.conv_filename_no_ext(original_file_name)\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(name_image)}.jpg')\n",
    "    pages = convert_from_path(file_path, 500, poppler_path=poppler_path)\n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((4134, 5846))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "    imagem_gray = resized_pages[0].convert('L')\n",
    "    imagem_gray.save(image_resized_name, 'JPEG')\n",
    "\n",
    "    return  imagem_gray, image_resized_name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1 XXX Extracao de dados do documento todo\n",
    "def cria_guarda_doc_ref_R_PDF(idx, row, de_para_pm, model_map, original_file_name, file_path, image_resized_path, debug):\n",
    "    \n",
    "    guarda_texto_doc = {}\n",
    "    \n",
    "    imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "\n",
    "    texto = (pytesseract.image_to_string(imagem_gray, lang='por'))\n",
    "    linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "    guarda_texto_doc['document_unique_id'] = idx\n",
    "    guarda_texto_doc['original_file_name'] = original_file_name\n",
    "    guarda_texto_doc['texto_documento'] = linhas\n",
    "    \n",
    "    return guarda_texto_doc, linhas\n",
    "\n",
    "\n",
    "# 2. XXX FunÇao para pesquisar entre marcadores do texto\n",
    "def extrair_texto_entre_marcadores(texto, marcador_inicio, marcador_fim):\n",
    "    try:\n",
    "        # Encontra os índices dos marcadores de início e fim\n",
    "        indice_inicio = next(i for i, s in enumerate(texto) if marcador_inicio in s)\n",
    "        indice_fim = next(i for i, s in enumerate(texto) if marcador_fim in s)\n",
    "\n",
    "        # Se o marcador de início e fim estão na mesma linha\n",
    "        if indice_inicio == indice_fim:\n",
    "            inicio = texto[indice_inicio].find(marcador_inicio) + len(marcador_inicio)\n",
    "            fim = texto[indice_fim].find(marcador_fim)\n",
    "            return texto[indice_inicio][inicio:fim].strip()\n",
    "        else:\n",
    "            # Extrai e retorna o texto entre os marcadores\n",
    "            return \" \".join(texto[indice_inicio:indice_fim])\n",
    "    except StopIteration:\n",
    "        # Retorna None se algum dos marcadores não for encontrado\n",
    "        return None\n",
    "    \n",
    "    \n",
    "# 5 XXX Busca os marcadores no template\n",
    "def busca_marcadores(model_map, section, tipo, label):\n",
    "    \n",
    "    #print('fantes da query em busca_marcadores: {mpdel_map} | {section} | {tipo} | {label}}')\n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model_map, type=tipo, label=label, section_json=section)\n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        modelo = [(row_frame.iloc[0]['model'])]\n",
    "        marc_ini = [(row_frame.iloc[0]['marcador_inicio'])]\n",
    "        marc_fim = [(row_frame.iloc[0]['marcador_fim'])]\n",
    "        prestador = [(row_frame.iloc[0]['prestador'])]\n",
    "        coodinates = [(row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1'])]\n",
    "        modelo = [(row_frame.iloc[0]['model'])]\n",
    "        #print(f'\\n\\n - Dentro do busca marcadores:  modelo: {modelo}   | label: {label} | marcador_inicio: {marcador_inicio} | marcador_fim: {marcador_fim}')\n",
    "        \n",
    "    return marc_ini[0], marc_fim[0], modelo[0]  \n",
    "\n",
    "\n",
    "# 6 XXX Funcao generica de extracao\n",
    "def extracao_documento_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_pm, model_map, def_replace, original_file_name, debug):\n",
    "    \n",
    "    #print(idx, row, guarda_texto_doc, section, label, de_para_pm, model_map, def_replace, original_file_name)\n",
    "    map_document_unique_id = idx\n",
    "    \n",
    "    label = label\n",
    "    section = section\n",
    "    model_map = model_map\n",
    "    tipo = tipo\n",
    "    #print(f'\\nidx: {idx} | label: {label} | section: {section} | model_map: {model_map} | tipo: {tipo} | def_replace: {def_replace} | original_file_name: {original_file_name}\\n\\n')\n",
    "    \n",
    "    if guarda_texto_doc['document_unique_id'] == map_document_unique_id:\n",
    "        texto_documento_uso = guarda_texto_doc['texto_documento']\n",
    "        #print(f'\\n\\ntexto_documento_uso: {texto_documento_uso}')\n",
    "    if texto_documento_uso:\n",
    "        # Busco marcadores\n",
    "        marcador_inicio, marcador_fim, modelo = busca_marcadores(model_map, section, tipo, label)\n",
    "        #print(marcador_inicio, marcador_fim, modelo)\n",
    "        \n",
    "        if modelo == model_map:\n",
    "            texto_extraido = extrair_texto_entre_marcadores(texto_documento_uso, marcador_inicio, marcador_fim)\n",
    "            \n",
    "            if def_replace:\n",
    "                texto_extraido = texto_extraido.replace(marcador_inicio, \"\").strip()\n",
    "            \n",
    "                texto_extraido_strip = texto_extraido.strip()\n",
    "                #print(f'texto_extraido_strip: {texto_extraido_strip}')\n",
    "                return texto_extraido_strip\n",
    "            else:\n",
    "                #print(\"texto_extraido\", texto_extraido)\n",
    "                return texto_extraido\n",
    "         \n",
    "            \n",
    "        print(\"nao achou modelo\")\n",
    "        return None \n",
    "            \n",
    "    else:\n",
    "        print(\"nao achou texto\")\n",
    "        return None \n",
    "    \n",
    "\n",
    "# 6.B XXX Funcao generica de extracao - CNAE_ITEM - RASTER PDF\n",
    "def extracao_documento_CNAE_ITEM_R_PDF(idx, row, row_info, guarda_texto_doc, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, def_replace, original_file_name, file_path, debug):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    # busco coordenadas para o contexto\n",
    "    if mapping_method == \"frame_&_sframe_field\":\n",
    "        tipo_4_coordinates = \"frame\"\n",
    "        tipo_4_filter = \"sframe_field\"\n",
    "    \n",
    "    map_document_unique_id = idx\n",
    "    \n",
    "    if guarda_texto_doc['document_unique_id'] == map_document_unique_id:\n",
    "        texto_documento_uso = guarda_texto_doc['texto_documento']\n",
    "\n",
    "    # 8. Efetuo o filtro para a iteracao\n",
    "    filtered_frame_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['context_mapping'] == context_mapping) & (frames_nf_v4_df['type'] == tipo_4_filter)]\n",
    "\n",
    "    # 9. iter sobre o filtro\n",
    "    for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "        section = row_frame['section_json']\n",
    "        label = row_frame['label']\n",
    "        reference = row_frame['reference']\n",
    "        marcador_inicio = row_frame['marcador_inicio']\n",
    "        marcador_fim = row_frame['marcador_fim']\n",
    "        texto_extraido = extrair_texto_entre_marcadores(texto_documento_uso, marcador_inicio, marcador_fim)\n",
    "        if texto_extraido:\n",
    "            texto_extraido = texto_extraido.replace(marcador_inicio, \"\").strip()\n",
    "            data_box_valores[label] = texto_extraido\n",
    "            # print(texto_extraido)\n",
    "            # print()\n",
    "        else:\n",
    "            texto_completo = \" \".join(text_splited)\n",
    "            texto_pesquisa = encontrar_texto_fuzzy_marcador_inicial(texto_completo, marcador_inicio, marcador_fim) \n",
    "            if texto_pesquisa:\n",
    "                texto_pesquisa = texto_pesquisa.replace(marcador_inicio, \"\").strip()\n",
    "                data_box_valores[label] = texto_pesquisa\n",
    "                # print(texto_pesquisa)\n",
    "                # print()\n",
    "            else:\n",
    "                texto_pesquisa2 = encontrar_texto_fuzzy_2_marcadores(texto_completo, marcador_inicio, marcador_fim)\n",
    "                if texto_pesquisa2:\n",
    "                    texto_pesquisa2 = texto_pesquisa2.replace(marcador_inicio, \"\").strip()\n",
    "                    data_box_valores[label] = texto_pesquisa2\n",
    "                    # print(texto_pesquisa2)\n",
    "\n",
    "    \n",
    "    \n",
    "    return data_box_valores\n",
    "\n",
    "    \n",
    " \n",
    "# VERIFICAR QUAL SERA USADA\n",
    "def encontrar_valores1(texto, marcador1, marcador2):\n",
    "    # Encontrando o texto entre os dois marcadores\n",
    "    resultado = re.search(f'{re.escape(marcador1)}(.*?){re.escape(marcador2)}', texto, re.DOTALL)\n",
    "    if resultado:\n",
    "        segmento = resultado.group(1)\n",
    "        \n",
    "        # Encontrando todos os valores no formato R$ 9,99\n",
    "        valores = re.findall(r'R\\$ \\d+,\\d{2}', segmento)\n",
    "        \n",
    "        # Encontrando as posições dos valores na string original\n",
    "        posicoes = [m.start() for m in re.finditer(r'R\\$ \\d+,\\d{2}', segmento)]\n",
    "        \n",
    "        # Criando uma lista de tuplas (valor, posição)\n",
    "        valores_com_posicoes = list(zip(valores, posicoes))\n",
    "        \n",
    "        return valores_com_posicoes\n",
    "    else:\n",
    "        return None\n",
    " \n",
    " \n",
    "    \n",
    "# VERIFICAR QUAL SERA USADA    \n",
    "def encontrar_valores2(texto, marcador1, marcador2):\n",
    "    # Certificando-se de que todos os parâmetros são strings\n",
    "    if isinstance(texto, list):\n",
    "        texto = '\\n'.join(texto)\n",
    "    if not isinstance(marcador1, str):\n",
    "        marcador1 = str(marcador1)\n",
    "    if not isinstance(marcador2, str):\n",
    "        marcador2 = str(marcador2)\n",
    "\n",
    "    # Encontrando o texto entre os dois marcadores\n",
    "    resultado = re.search(f'{re.escape(marcador1)}(.*?){re.escape(marcador2)}', texto, re.DOTALL)\n",
    "    if resultado:\n",
    "        segmento = resultado.group(1)\n",
    "        \n",
    "        # Encontrando todos os valores no formato R$ 9,99\n",
    "        valores = re.findall(r'R\\$ \\d+,\\d{2}', segmento)\n",
    "        \n",
    "        # Encontrando as posições dos valores na string original\n",
    "        posicoes = [m.start() for m in re.finditer(r'R\\$ \\d+,\\d{2}', segmento)]\n",
    "        \n",
    "        # Criando uma lista de tuplas (valor, posição)\n",
    "        valores_com_posicoes = list(zip(valores, posicoes))\n",
    "        \n",
    "        return valores_com_posicoes\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "# VERIFICAR QUAL SERA USADA    \n",
    "def encontrar_valores3(texto, marcador1, marcador2):\n",
    "    # Certificando-se de que todos os parâmetros são strings\n",
    "    if isinstance(texto, list):\n",
    "        texto = '\\n'.join(texto)\n",
    "    if not isinstance(marcador1, str):\n",
    "        marcador1 = str(marcador1)\n",
    "    if not isinstance(marcador2, str):\n",
    "        marcador2 = str(marcador2)\n",
    "\n",
    "    # Encontrando o texto entre os dois marcadores\n",
    "    resultado = re.search(f'{re.escape(marcador1)}(.*?){re.escape(marcador2)}', texto, re.DOTALL)\n",
    "    if resultado:\n",
    "        segmento = resultado.group(1)\n",
    "        \n",
    "        # Encontrando todos os valores no formato R$ 9,99 e as alíquotas como 3%\n",
    "        valores = re.findall(r'R\\$ \\d+,\\d{2}|(?:\\d+,\\d{1,2}|\\d+)%', segmento)\n",
    "        \n",
    "        # Encontrando as posições dos valores na string original\n",
    "        posicoes = [m.start() for m in re.finditer(r'R\\$ \\d+,\\d{2}|(?:\\d+,\\d{1,2}|\\d+)%', segmento)]\n",
    "        \n",
    "        # Criando uma lista de tuplas (valor, posição)\n",
    "        valores_com_posicoes = list(zip(valores, posicoes))\n",
    "        \n",
    "        return valores_com_posicoes\n",
    "    else:\n",
    "        return None\n",
    " \n",
    "\n",
    "def encontrar_texto_fuzzy_2_marcadores(texto, marcador_inicial, marcador_final, limite_score=80):\n",
    "    # Usar FuzzyWuzzy para encontrar o melhor match para o marcador inicial\n",
    "    palavras = texto.split()\n",
    "    melhor_match_inicial, score_inicial = process.extractOne(marcador_inicial, palavras)\n",
    "    \n",
    "    if score_inicial < limite_score:\n",
    "        return None\n",
    "    \n",
    "    # Encontrar a posição inicial do melhor match\n",
    "    inicio = texto.find(melhor_match_inicial)\n",
    "    #print('inicio: ',inicio)\n",
    "    \n",
    "    # Cortar o texto para começar após o marcador inicial\n",
    "    texto_cortado = texto[inicio + len(melhor_match_inicial):]\n",
    "    #print('texto_cortado: ', texto_cortado)\n",
    "    \n",
    "    # Usar FuzzyWuzzy para encontrar o melhor match para o marcador final\n",
    "    melhor_match_final, score_final = process.extractOne(marcador_final, palavras)\n",
    "    #print('melhor_match_final: ', melhor_match_final)\n",
    "    \n",
    "    if score_final < limite_score:\n",
    "        return None\n",
    "    \n",
    "    # Encontrar a posição do melhor match\n",
    "    fim = texto_cortado.find(melhor_match_final)\n",
    "    #print('fim: ', fim)\n",
    "    if fim == -1:\n",
    "        return None\n",
    "    \n",
    "    return texto_cortado[:fim].strip()\n",
    "\n",
    "def encontrar_texto_fuzzy_marcador_inicial(texto, marcador_inicial, marcador_final, limite_score=80):\n",
    "    # Encontrar a posição inicial do marcador\n",
    "    inicio = texto.find(marcador_inicial)\n",
    "    if inicio == -1:\n",
    "        return None\n",
    "    \n",
    "    # Cortar o texto para começar após o marcador inicial\n",
    "    texto_cortado = texto[inicio + len(marcador_inicial):]\n",
    "    \n",
    "    # Usar FuzzyWuzzy para encontrar o melhor match para o marcador final\n",
    "    palavras = texto_cortado.split()\n",
    "    melhor_match, score = process.extractOne(marcador_final, palavras)\n",
    "    \n",
    "    if score >= limite_score:\n",
    "        # Encontrar a posição do melhor match\n",
    "        fim = texto_cortado.find(melhor_match)\n",
    "        return texto_cortado[:fim].strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    " \n",
    " \n",
    "def encontrar_texto(texto, marcador_inicial, marcador_final=None):\n",
    "    if marcador_final:\n",
    "        # Procura pelo texto entre os dois marcadores\n",
    "        resultado = re.search(f'{re.escape(marcador_inicial)}(.*?){re.escape(marcador_final)}', texto, re.DOTALL)\n",
    "        if resultado:\n",
    "            return resultado.group(1).strip()\n",
    "    else:\n",
    "        # Procura pelo texto do marcador inicial até o final\n",
    "        resultado = re.search(f'{re.escape(marcador_inicial)}(.*)', texto, re.DOTALL)\n",
    "        if resultado:\n",
    "            return resultado.group(1).strip()\n",
    "\n",
    "    # Retorna None se não encontrar nada\n",
    "    return None\n",
    "       \n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "\n",
    "# VERIFICAR DUPLICIDADE\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "def format_number(number_str):\n",
    "    # Check for percentage and handle it\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # You can multiply by 100 here if needed\n",
    "\n",
    "    # Check if the string contains \"R$\" or a comma, indicating the original format\n",
    "    if 'R$' in number_str or ',' in number_str:\n",
    "        # Original format: Remove 'R$', replace dots with nothing, and replace commas with dots\n",
    "        number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    else:\n",
    "        # New format: Extract only the numeric part using regex\n",
    "        number_str = re.findall(r'[\\d\\.]+', number_str)[-1]\n",
    "\n",
    "    return float(number_str)\n",
    "\n",
    "# Funçao de formatacao de numeros\n",
    "def format_number2(number_str):\n",
    "    number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # multiplica por 100 para fields %\n",
    "    return float(number_str)\n",
    "\n",
    "\n",
    "\n",
    "# Função para extrair número da string\n",
    "def extract_number(text):\n",
    "    match = re.search(r'\\b\\d+(\\.\\d+)?\\b', text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return None            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b>2.3</b> Templates e Dics </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames_nf_v4_df: 2.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def define_dados_iniciais(idx, row, row_info, texto_tratado, debug):\n",
    "    \n",
    "    dados_iniciais_nf = {}\n",
    "    #status_documento_row_info = row_info.get('status_documento')\n",
    "    action_item_row_info = row_info.get('action_item')\n",
    "    information_row_info = row_info.get('informations')\n",
    "    \n",
    "    dados_iniciais_nf['action_item'] = action_item_row_info\n",
    "    dados_iniciais_nf['informations'] = information_row_info\n",
    "   \n",
    "\n",
    "\n",
    "    prefeitura_encontrada = None\n",
    "    de_para_encontrado = None\n",
    "\n",
    "    # 7. ZZZ Dicionário para mapear Prefeitura com sua sigla\n",
    "    de_para_prefeitura = {\n",
    "        \"PREFEITURA DA CIDADE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA DA CIDADE DE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA MUNICIPAL DE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\": \"PM_SPA\",\n",
    "        \"MUNICIPAL DE SAO PEDRO DA ALDEIA\": \"PM_SPA\",\n",
    "        \"PREFEITURA MUNICIPAL DE SAO PEDRO DA\\nALDEIA\": \"PM_SPA\",\n",
    "        \"PREFEITURA MUNICIPAL DE SAO PEDRO DA\": \"PM_SPA\",\n",
    "        \"PREFEITURA MUNICIPAL DE MESQUITA\": \"PM_MESQUITA\",\n",
    "        \"PREFEITURA MUNICIPAL DE DE MESQUITA\": \"PM_MESQUITA\",\n",
    "        # ... adicione \n",
    "    }\n",
    "    \n",
    "\n",
    "    templates = {\n",
    "        (\"PM_MAGE\", None): \"MAGE\",\n",
    "        (\"PM_SPA\", None): \"SPA\",\n",
    "        (\"PM_MESQUITA\", None): \"MESQUITA\",\n",
    "        (\"Pague agora com o seu Pix\", None): \"NAO_PROCESSAR\",\n",
    "        # ... adicione outras combinações aqui\n",
    "    }\n",
    "\n",
    "    cnpj_encontrado = None\n",
    "    # Verifique cada linha do texto\n",
    "    for linha in texto_tratado:\n",
    "        for pref in de_para_prefeitura.keys():\n",
    "            if pref in linha:\n",
    "                #print(linha)\n",
    "                prefeitura_encontrada = pref\n",
    "                dados_iniciais_nf['prefeitura'] = prefeitura_encontrada\n",
    "                if debug:\n",
    "                    print(f'\\n4.funcao: define_dados_iniciais(texto_tratado) - dentro do loop for de pesquisa prefeitura - prefeitura_encontrada: \\n{prefeitura_encontrada}\\n\\n')\n",
    "    # Saímos do loop, agora vamos verificar qual template usar\n",
    "    if prefeitura_encontrada:\n",
    "        de_para_pm = de_para_prefeitura.get(prefeitura_encontrada)\n",
    "        dados_iniciais_nf['de_para_pm'] = de_para_pm\n",
    "        if debug:\n",
    "            print(f'\\n5.funcao: define_dados_iniciais(texto_tratado) - if prefeitura_encontrada - de_para_pm \\n{de_para_pm}\\n\\n')\n",
    "        if not de_para_pm:\n",
    "            de_para_pm = de_para_prefeitura.get(prefeitura_encontrada, \"NAO_PROCESSAR\")\n",
    "            dados_iniciais_nf['de_para_pm'] = de_para_pm\n",
    "            #print(de_para_pm)\n",
    "    else:\n",
    "        de_para_pm = \"NAO_PROCESSAR\"\n",
    "        action_item_row_info = 'BREAK_PROCESS'\n",
    "        information_row_info = 'Nao identificado dados iniciais para o documento'\n",
    "        \n",
    "     \n",
    "        \n",
    "    # Verifique cada linha do texto\n",
    "    for linha in texto_tratado:\n",
    "        for de_para, cnpj in templates.keys():\n",
    "            if cnpj and cnpj in linha:\n",
    "                cnpj_encontrado = cnpj\n",
    "                dados_iniciais_nf['cnpj_encontrado'] = cnpj_encontrado\n",
    "                \n",
    "                \n",
    "    # Saímos do loop, agora vamos verificar qual template usar\n",
    "    if de_para_pm:\n",
    "        template_usar = templates.get((de_para_pm, cnpj_encontrado))\n",
    "        logging.info(f'usara template {template_usar} para: {cnpj_encontrado}')\n",
    "        # print(template_usar)\n",
    "        dados_iniciais_nf['model'] = template_usar\n",
    "        if not template_usar:\n",
    "            template_usar = templates.get((de_para_pm, None), \"TEMPLATE_NAO_ENCONTRADO\")\n",
    "            dados_iniciais_nf['model'] = 'NAO_ENC.' \n",
    "            action_item_row_info = 'BREAK_PROCESS'\n",
    "            information_row_info = 'model nao encontrado'\n",
    "    else:\n",
    "        template_usar = \"TEMPLATE_NAO_ENCONTRADO\"\n",
    "        dados_iniciais_nf['model'] = 'NAO_ENC.'\n",
    "        action_item_row_info = 'BREAK_PROCESS'\n",
    "        information_row_info = 'model nao encontrado'\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    #Confirmando se template existe em frames    \n",
    "    try:        \n",
    "        f_type = 'frame'\n",
    "        #template_usar = 'SAO_PEDRO_SUPERMIX'\n",
    "        result = filtrar_df(frames_nf_v4_df, type=f_type, de_para_pm=de_para_pm, model=template_usar)\n",
    "        model = result['model'].values[0]\n",
    "        if model:\n",
    "            template_oficial = model\n",
    "            if model == template_usar:\n",
    "                dados_iniciais_nf['model'] = template_oficial\n",
    "            else:    \n",
    "                template_usar = \"necessario cadastrar\"\n",
    "                dados_iniciais_nf['model'] = \"CADASTRAR\"\n",
    "                \n",
    "            dados_iniciais_nf['model'] = template_usar\n",
    "        else:\n",
    "            template_usar = \"necessario cadastrar\"\n",
    "            dados_iniciais_nf['model'] = \"CADASTRAR\"\n",
    "\n",
    "                \n",
    "    except Exception as e:\n",
    "       error_msg = (f\"Erro busca do template: {e}\") \n",
    "    \n",
    "    dados_iniciais_nf['action_item'] = action_item_row_info \n",
    "    dados_iniciais_nf['informations'] = information_row_info         \n",
    "        \n",
    "    return dados_iniciais_nf  \n",
    "\n",
    "\n",
    "\n",
    "nf_model_path = \"config/modelos/frames_nf_v11.xlsx\"\n",
    "\n",
    "#Le a planilha e cria do DF\n",
    "frames_nf_v4_df = pd.read_excel(nf_model_path)\n",
    "\n",
    "\n",
    "# Cria dicionários para armazenar diferentes tipos de elementos do modelo\n",
    "document_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'document'].iloc[0]\n",
    "boundaries_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'boundaries']\n",
    "sections_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'section']\n",
    "frames_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'frame']\n",
    "sframe_fields_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'sframe_field']\n",
    "field_boxes_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'field_box']\n",
    "\n",
    "ver = tmod.get_template_version(frames_nf_v4_df, 'MAGE')\n",
    "\n",
    "frames_nf_v4_df.head(5)\n",
    "\n",
    "print(f'frames_nf_v4_df: {ver}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b>2.4</b> ExecuÇao do Pipeline de Extracao </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>date_time</th>\n",
       "      <th>batch</th>\n",
       "      <th>fase_processo</th>\n",
       "      <th>nome_atividade</th>\n",
       "      <th>status_documento</th>\n",
       "      <th>acao_executada</th>\n",
       "      <th>original_file_name</th>\n",
       "      <th>directory</th>\n",
       "      <th>one_page</th>\n",
       "      <th>pages</th>\n",
       "      <th>palavra_chave</th>\n",
       "      <th>document_tag</th>\n",
       "      <th>action_item</th>\n",
       "      <th>level</th>\n",
       "      <th>parent_document_unique_id</th>\n",
       "      <th>file_hash</th>\n",
       "      <th>file_path</th>\n",
       "      <th>informations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6f1a0643-7918-493d-8d93-39ae66a23b5d</th>\n",
       "      <td>1</td>\n",
       "      <td>24/09/2023 09:23:12</td>\n",
       "      <td>Batch_23</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>MESQUITA_PDF_31282023_2258.zip</td>\n",
       "      <td>root_dir</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>zip</td>\n",
       "      <td>doc_zip</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>2</td>\n",
       "      <td>8c142beb-753b-4ad8-adff-4ca10f0bf7e7</td>\n",
       "      <td>8d7038d712373364fa4c7680a887a0ceed01c8692d6958...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3350d180-8ed9-4334-8189-796f4499d851</th>\n",
       "      <td>2</td>\n",
       "      <td>24/09/2023 09:23:12</td>\n",
       "      <td>Batch_23</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>1.pdf</td>\n",
       "      <td>teste</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>default</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>8c142beb-753b-4ad8-adff-4ca10f0bf7e7</td>\n",
       "      <td>66a7db9ee1500d5f9fa5da26563cfd7b68f1f5ba3daba2...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93d95698-277c-40b2-b501-cb84476109bb</th>\n",
       "      <td>3</td>\n",
       "      <td>24/09/2023 09:23:12</td>\n",
       "      <td>Batch_23</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Livro de Registro do ISSQN.pdf</td>\n",
       "      <td>115964</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>livro</td>\n",
       "      <td>prov_livro_registro</td>\n",
       "      <td>NO_PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>8c142beb-753b-4ad8-adff-4ca10f0bf7e7</td>\n",
       "      <td>b960962503987f6e05f5646d71a789facfe4e80ccb8890...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84bc5464-fcdb-4245-8b10-b2167a80405b</th>\n",
       "      <td>4</td>\n",
       "      <td>24/09/2023 09:23:12</td>\n",
       "      <td>Batch_23</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>2023 -5.pdf</td>\n",
       "      <td>159871</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>default</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>8c142beb-753b-4ad8-adff-4ca10f0bf7e7</td>\n",
       "      <td>23a28a363c2d2c8b700ac4775164f7c0f0e2d6cef6166d...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752b1a8-9e22-477a-a317-5eb636327c9d</th>\n",
       "      <td>5</td>\n",
       "      <td>24/09/2023 09:23:12</td>\n",
       "      <td>Batch_23</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>root_analise</td>\n",
       "      <td>Analise</td>\n",
       "      <td>2023 -7.pdf</td>\n",
       "      <td>159871</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>default</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>3</td>\n",
       "      <td>8c142beb-753b-4ad8-adff-4ca10f0bf7e7</td>\n",
       "      <td>54045f4c09341d9f8d69438e7afe71eff46bb4e731392b...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      seq            date_time     batch  \\\n",
       "document_unique_id                                                         \n",
       "6f1a0643-7918-493d-8d93-39ae66a23b5d    1  24/09/2023 09:23:12  Batch_23   \n",
       "3350d180-8ed9-4334-8189-796f4499d851    2  24/09/2023 09:23:12  Batch_23   \n",
       "93d95698-277c-40b2-b501-cb84476109bb    3  24/09/2023 09:23:12  Batch_23   \n",
       "84bc5464-fcdb-4245-8b10-b2167a80405b    4  24/09/2023 09:23:12  Batch_23   \n",
       "2752b1a8-9e22-477a-a317-5eb636327c9d    5  24/09/2023 09:23:12  Batch_23   \n",
       "\n",
       "                                     fase_processo nome_atividade  \\\n",
       "document_unique_id                                                  \n",
       "6f1a0643-7918-493d-8d93-39ae66a23b5d       analise   scan_analise   \n",
       "3350d180-8ed9-4334-8189-796f4499d851       analise   scan_analise   \n",
       "93d95698-277c-40b2-b501-cb84476109bb       analise   scan_analise   \n",
       "84bc5464-fcdb-4245-8b10-b2167a80405b       analise   scan_analise   \n",
       "2752b1a8-9e22-477a-a317-5eb636327c9d       analise   scan_analise   \n",
       "\n",
       "                                        status_documento acao_executada  \\\n",
       "document_unique_id                                                        \n",
       "6f1a0643-7918-493d-8d93-39ae66a23b5d        root_analise        Analise   \n",
       "3350d180-8ed9-4334-8189-796f4499d851  PREPROCESS_EXTRACT        Analise   \n",
       "93d95698-277c-40b2-b501-cb84476109bb        root_analise        Analise   \n",
       "84bc5464-fcdb-4245-8b10-b2167a80405b        root_analise        Analise   \n",
       "2752b1a8-9e22-477a-a317-5eb636327c9d        root_analise        Analise   \n",
       "\n",
       "                                                  original_file_name  \\\n",
       "document_unique_id                                                     \n",
       "6f1a0643-7918-493d-8d93-39ae66a23b5d  MESQUITA_PDF_31282023_2258.zip   \n",
       "3350d180-8ed9-4334-8189-796f4499d851                           1.pdf   \n",
       "93d95698-277c-40b2-b501-cb84476109bb  Livro de Registro do ISSQN.pdf   \n",
       "84bc5464-fcdb-4245-8b10-b2167a80405b                     2023 -5.pdf   \n",
       "2752b1a8-9e22-477a-a317-5eb636327c9d                     2023 -7.pdf   \n",
       "\n",
       "                                     directory  one_page  pages palavra_chave  \\\n",
       "document_unique_id                                                              \n",
       "6f1a0643-7918-493d-8d93-39ae66a23b5d  root_dir     False      0           zip   \n",
       "3350d180-8ed9-4334-8189-796f4499d851     teste      True      1       default   \n",
       "93d95698-277c-40b2-b501-cb84476109bb    115964     False      4         livro   \n",
       "84bc5464-fcdb-4245-8b10-b2167a80405b    159871      True      1       default   \n",
       "2752b1a8-9e22-477a-a317-5eb636327c9d    159871      True      1       default   \n",
       "\n",
       "                                             document_tag action_item  level  \\\n",
       "document_unique_id                                                             \n",
       "6f1a0643-7918-493d-8d93-39ae66a23b5d              doc_zip  NO_PROCESS      2   \n",
       "3350d180-8ed9-4334-8189-796f4499d851     prov_nota_fiscal     PROCESS      3   \n",
       "93d95698-277c-40b2-b501-cb84476109bb  prov_livro_registro  NO_PROCESS      3   \n",
       "84bc5464-fcdb-4245-8b10-b2167a80405b     prov_nota_fiscal     PROCESS      3   \n",
       "2752b1a8-9e22-477a-a317-5eb636327c9d     prov_nota_fiscal     PROCESS      3   \n",
       "\n",
       "                                                 parent_document_unique_id  \\\n",
       "document_unique_id                                                           \n",
       "6f1a0643-7918-493d-8d93-39ae66a23b5d  8c142beb-753b-4ad8-adff-4ca10f0bf7e7   \n",
       "3350d180-8ed9-4334-8189-796f4499d851  8c142beb-753b-4ad8-adff-4ca10f0bf7e7   \n",
       "93d95698-277c-40b2-b501-cb84476109bb  8c142beb-753b-4ad8-adff-4ca10f0bf7e7   \n",
       "84bc5464-fcdb-4245-8b10-b2167a80405b  8c142beb-753b-4ad8-adff-4ca10f0bf7e7   \n",
       "2752b1a8-9e22-477a-a317-5eb636327c9d  8c142beb-753b-4ad8-adff-4ca10f0bf7e7   \n",
       "\n",
       "                                                                              file_hash  \\\n",
       "document_unique_id                                                                        \n",
       "6f1a0643-7918-493d-8d93-39ae66a23b5d  8d7038d712373364fa4c7680a887a0ceed01c8692d6958...   \n",
       "3350d180-8ed9-4334-8189-796f4499d851  66a7db9ee1500d5f9fa5da26563cfd7b68f1f5ba3daba2...   \n",
       "93d95698-277c-40b2-b501-cb84476109bb  b960962503987f6e05f5646d71a789facfe4e80ccb8890...   \n",
       "84bc5464-fcdb-4245-8b10-b2167a80405b  23a28a363c2d2c8b700ac4775164f7c0f0e2d6cef6166d...   \n",
       "2752b1a8-9e22-477a-a317-5eb636327c9d  54045f4c09341d9f8d69438e7afe71eff46bb4e731392b...   \n",
       "\n",
       "                                                                              file_path  \\\n",
       "document_unique_id                                                                        \n",
       "6f1a0643-7918-493d-8d93-39ae66a23b5d  pipeline_extracao_documentos/2_documentos_para...   \n",
       "3350d180-8ed9-4334-8189-796f4499d851  pipeline_extracao_documentos/2_documentos_para...   \n",
       "93d95698-277c-40b2-b501-cb84476109bb  pipeline_extracao_documentos/2_documentos_para...   \n",
       "84bc5464-fcdb-4245-8b10-b2167a80405b  pipeline_extracao_documentos/2_documentos_para...   \n",
       "2752b1a8-9e22-477a-a317-5eb636327c9d  pipeline_extracao_documentos/2_documentos_para...   \n",
       "\n",
       "                                     informations  \n",
       "document_unique_id                                 \n",
       "6f1a0643-7918-493d-8d93-39ae66a23b5d               \n",
       "3350d180-8ed9-4334-8189-796f4499d851               \n",
       "93d95698-277c-40b2-b501-cb84476109bb               \n",
       "84bc5464-fcdb-4245-8b10-b2167a80405b               \n",
       "2752b1a8-9e22-477a-a317-5eb636327c9d               "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. XXX Buscar proximo Batch caso nao esteja rodando email\n",
    "batch_name = utl.busca_proximo_batch(conf_export_plan_path)\n",
    "\n",
    "# 2. XXX Definiçao do path para salvar o arquivo\n",
    "file_path_root_pipe = os.path.join(map_analise_path, df_root_pipe_file + batch_name + \".xlsx\")\n",
    "\n",
    "\n",
    "#3. XXX Ler a planilha e cria df_documento_recebido\n",
    "df_root_pipe = pd.read_excel(file_path_root_pipe)\n",
    "\n",
    "\n",
    "#4. XXX  Ajustar o indice\n",
    "df_root_pipe.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "\n",
    "df_root_pipe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta e o principio da melhor funcao do mundo\n",
    "def extracao_pipeline(qualquer_df, fase, atividade, status, debug=False, prestador=True, tomador=True, servicos=True, total=True, cnae=True, valores_impostos=True, complementares=True, outras_informacoes=True, observacoes=True):\n",
    "    \n",
    "    doc_info = {}\n",
    "    resumo = {}\n",
    "    row_teste_info = []\n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    func_fase = fase\n",
    "    func_atividade = atividade\n",
    "    func_status = status\n",
    "    lista_dicts = []\n",
    "    conf_processo = {}\n",
    "    lista_conferencia = []\n",
    "   \n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        dados_iniciais = {}\n",
    "        row_info = row.to_dict()\n",
    "        message_erro = []\n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        map_document_unique_id = idx\n",
    "        map_seq = row['seq']\n",
    "        map_batch_name = row['batch']\n",
    "        map_fase_processo = row['fase_processo']\n",
    "        map_nome_atividade = row['nome_atividade']\n",
    "        map_status_documento = row['status_documento']\n",
    "        map_original_file_name = row['original_file_name']\n",
    "        map_directory = row['directory']\n",
    "        map_one_page = row['one_page']\n",
    "        map_palavra_chave = row['palavra_chave']\n",
    "        map_document_tag = row['document_tag']\n",
    "        map_action_item = row['action_item']\n",
    "        map_level = row['level']\n",
    "        file_path = row['file_path']\n",
    "        row_info['document_unique_id'] = map_document_unique_id\n",
    "    \n",
    "        # XXX Nivel 1 - Definindo que documentos serao tratados    \n",
    "        if map_status_documento == 'PREPROCESS_EXTRACT':\n",
    "            \n",
    "            action_item_row_info = 'CONTINUE_PROCESS'\n",
    "            row_info['action_item'] = action_item_row_info\n",
    "            information_row_info = 'iniciado processamento'\n",
    "            row_info['informations'] = information_row_info\n",
    "            \n",
    "            \n",
    "            # 0. DADOS GERAIS DOCUMENTO\n",
    "            section = \"0. DADOS INICIAIS\"\n",
    "            try:\n",
    "                valores = {}\n",
    "                # 0.1. Busco prefeitura, de/para e modelo - se nao achar seta status documento para NO_PROCESS\n",
    "                valores = processar_dados_iniciais(idx, row, row_info, section, map_directory, map_original_file_name, file_path, debug)\n",
    "            except Exception as e:\n",
    "                msg = (f'Erro ao processar_dados_iniciais: {e}')\n",
    "            finally:\n",
    "                row_info.update(valores)\n",
    "            \n",
    "            #map_status_documento_row_info = row_info.get('status_documento')\n",
    "            action_item_row_info = row_info.get('action_item')\n",
    "            \n",
    "            # XXX Nivel 2 - Definindo que os documentos legiveis serao tratados\n",
    "            if action_item_row_info == 'CONTINUE_PROCESS':\n",
    "                \n",
    "                prefeitura_map = row_info.get('prefeitura')\n",
    "                pdf_pesquisavel_map = row_info.get('pdf_pesquisavel')\n",
    "                de_para_map = row_info.get('de_para_pm')\n",
    "                model_map = row_info.get('model')\n",
    "                \n",
    "                \n",
    "                if not pdf_pesquisavel_map:\n",
    "                    # NOVO PROCESSO DE TRATAMENTO DE IMAGEM - Convertendo a imagem para numpy array\n",
    "                    if debug:\n",
    "                        print(\"irei gerar a imagem_np\")\n",
    "                    imagem_gray, image_resized_name = convert_resize_gray(map_original_file_name, file_path, image_resized_path)\n",
    "                    imagem_gray_rgb = imagem_gray.convert(\"RGB\")\n",
    "                    imagem_gray_np = np.array(imagem_gray_rgb)\n",
    "                    row_info['image_np'] = imagem_gray_np\n",
    "                \n",
    "                # 1. CABECALHO\n",
    "                # try:\n",
    "                section = \"1. CABECALHO\"\n",
    "                valores = {}\n",
    "                #valores_P = {}\n",
    "                f_0 = 1\n",
    "                f_1 = 1\n",
    "                mapping_method = \"frame_&_sframe_field\" # significa que as coordenadas estao em frames e os valores dos campos nos sframe_fields\n",
    "                context_mapping = \"data_cabecalho\"\n",
    "                def_replace = True \n",
    "                \n",
    "                if pdf_pesquisavel_map:\n",
    "                    valores = extrai_cabecalho_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                    row_info.update(valores) \n",
    "                else:\n",
    "                    valores = processar_cabecalho_R_PDF(idx, row, row_info, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, map_original_file_name, file_path, debug)   \n",
    "                    row_info.update(valores)\n",
    "         \n",
    "                #status_documento_row_info = row_info.get('status_documento')\n",
    "                action_item_row_info = row_info.get('action_item')\n",
    "                information_row_info = row_info.get('informations')   \n",
    "                \n",
    "                \n",
    "                # XXX Nivel 3 - Definindo que os documentos legiveis serao tratados realmente\n",
    "                if action_item_row_info == 'BREAK_PROCESS':\n",
    "                    #msg = (f'Processo inicial: {map_batch_name} | {map_original_file_name} | diretorio: {map_directory} - information_row_info: {information_row_info}')\n",
    "                    if debug:\n",
    "                        print(f'\\nINFELIZMENTE - seq: {map_seq} doc: {map_original_file_name} dir: {map_directory} - NAO SERA PROCESSADO  | inf: {information_row_info} \\n\\n')\n",
    "               \n",
    "                    #row_info['informations'] = msg\n",
    "                    # logging.error(msg)\n",
    "                    lista_dicts.append(row_info)\n",
    "                    continue \n",
    "                \n",
    "                    \n",
    "                elif action_item_row_info == 'CONTINUE_PROCESS':\n",
    "                    if debug:\n",
    "                        print(f'\\nEBA, BORA CONTINUAR - seq: {map_seq} - proxima section: | PDF Pesquisavel: {pdf_pesquisavel_map} doc: {map_original_file_name} dir: {map_directory} | action_item: {action_item_row_info} | inf: {information_row_info} \\n\\n')\n",
    "                        print()\n",
    "                        print(valores)\n",
    "                    \n",
    "                    information_row_info = 'Cabecalho processado'\n",
    "                    row_info['informations'] = information_row_info\n",
    "                    \n",
    "                    \n",
    "                    guarda_texto_doc = {}\n",
    "                    guarda_texto_doc, linhas = cria_guarda_doc_ref_R_PDF(idx, row, de_para_map, model_map, map_original_file_name, file_path, image_resized_path, debug)\n",
    "            \n",
    "                    # 2. PRESTADOR DE SERVIÇO\n",
    "                    if prestador == True:\n",
    "                        section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        valores = {}\n",
    "                        erros_prestador = {}\n",
    "                        data_tomador = {}\n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valores = extrai_prestador_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                        else:\n",
    "                            valores = extrai_prestador_R_PDF(idx, row, row_info, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                        \n",
    "                        if not isinstance(valores, dict):\n",
    "                            msg_erro = (f\"\\nErro na linha {idx}: 'valores' não é um dicionário. Tipo: {type(valores)}, Valor: {valores}\")\n",
    "                        else:\n",
    "                            row_info.update(valores)\n",
    "                            \n",
    "                        # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                        # if debug:\n",
    "                        #     print(msg)\n",
    "                        # logging.info(msg)\n",
    "                    \n",
    "                    # 3. TOMADOR DE SERVIÇO\n",
    "                    if tomador == True:\n",
    "                        section = \"3. TOMADOR DE SERVIÇO\"\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        \n",
    "                        valores = {}\n",
    "                        erros = []\n",
    "                        data_tomador = {}\n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        \n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valores = extrai_tomador_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                        else:   \n",
    "                            valores = extrai_tomador_R_PDF(idx, row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                            \n",
    "                        if not isinstance(valores, dict):\n",
    "                            print(f\"\\nErro na linha {idx}: 'valores' não é um dicionário. Tipo: {type(valores)}, Valor: {valores}\")\n",
    "                        else:\n",
    "                            row_info.update(valores)\n",
    "                        \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)\n",
    "                    \n",
    "                    # 4. DESCRIMINACAO DOS SERVIÇOS\n",
    "                    if servicos == True:\n",
    "                        if debug:\n",
    "                            print(f'processando servicos para: {map_original_file_name}')\n",
    "                        section = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "                        valores = {}\n",
    "                        nf_data_servico = {} \n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        \n",
    "                        if pdf_pesquisavel_map:\n",
    "                            nf_data_servico = processar_servicos_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, map_original_file_name, file_path, debug)\n",
    "                        else:\n",
    "                            label = \"discriminacao_servicos\"\n",
    "                            tipo = \"field_box\"\n",
    "                            def_replace = True\n",
    "                            \n",
    "                            # ItSs  working\n",
    "                            texto_extraido = extracao_documento_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_map, model_map, def_replace, map_original_file_name, debug)\n",
    "                            row_info[label] = texto_extraido\n",
    "                            \n",
    "                        msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                        if debug:\n",
    "                            print(msg)\n",
    "                        logging.info(msg)     \n",
    "\n",
    "\n",
    "                        try:\n",
    "                            texto_extraido = nf_data_servico['discriminacao_servicos'] \n",
    "                            row_info['discriminacao_servicos'] = texto_extraido \n",
    "                        except Exception as e:\n",
    "                            msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "                            discrimanacao_servico = \"Descricao nao encontrada\"\n",
    "                            row_info['discriminacao_servicos'] = texto_extraido\n",
    "\n",
    "                    \n",
    "                    # 5. VALOR TOTAL\n",
    "                    if total == True:\n",
    "                        section = \"5. VALOR TOTAL\"\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        #valores = {}\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valor_total_documento = processar_valor_total_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, model_map, map_original_file_name, file_path, debug)\n",
    "                            if valor_total_documento:\n",
    "                                if debug:\n",
    "                                    print(f'\\nvalor_total_documento: {valor_total_documento} | doc: {map_original_file_name}\\n')\n",
    "                                row_info['valor_total_nota'] = valor_total_documento\n",
    "                        else:\n",
    "                            label = \"valor_total_nota\"\n",
    "                            tipo = \"field_box\"\n",
    "                            def_replace = True\n",
    "                            texto_extraido = extracao_documento_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_map, model_map, def_replace, map_original_file_name, debug)\n",
    "                            if texto_extraido: \n",
    "                                valor_total_match = re.search(r'R\\$ ([\\d,.]+)', texto_extraido)\n",
    "                                if valor_total_match:\n",
    "                                    valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "                                    try:\n",
    "                                        # valores['secao'] = section\n",
    "                                        valor_total_documento = float(valor_total_sem_formatacao)\n",
    "                                    except Exception as e:\n",
    "                                        # valores['secao'] = section\n",
    "                                        valor_total_documento = 0.0\n",
    "                                        msg = (f'Processo inicial: {batch_name} | {map_original_file_name:>25} | diretorio: {map_directory} | {e}')\n",
    "                                        #logging.error(f\" {batch_name} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")\n",
    "                        \n",
    "                                    if valor_total_documento:\n",
    "                                        if debug:\n",
    "                                            print(f'\\nvalor_total_documento: {valor_total_documento} | doc: {map_original_file_name}\\n')\n",
    "                                        row_info['valor_total_nota'] = valor_total_documento\n",
    "         \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)\n",
    "                    \n",
    "                    # 6. CNAE e Item da Lista de Serviços \n",
    "                    if cnae == True:\n",
    "                        section = \"6. CNAE e Item da Lista de Serviços\"\n",
    "                        data_box_valores = {}\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        f_0_cnae = 0.95\n",
    "                        f_1_cnae = 1.15\n",
    "                        f_0_it = 0.95     #0.95\n",
    "                        f_1_it = 1.15    # 1\n",
    "                        \n",
    "                        mapping_method = \"frame_&_sframe_field\" # significa que as coordenadas estao em frames e os valores dos campos nos sframe_fields\n",
    "                        context_mapping = \"data_cnae\"\n",
    "                        def_replace = True\n",
    "                        \n",
    "                        if pdf_pesquisavel_map:\n",
    "                            data_box_valores = extracao_documento_CNAE_ITEM_PDF_P(idx, row, row_info, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, def_replace, map_original_file_name, file_path, debug)\n",
    "                        else:\n",
    "                            data_box_valores = extracao_documento_CNAE_ITEM_R_PDF(idx, row, row_info, guarda_texto_doc, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, def_replace, map_original_file_name, file_path, debug)\n",
    "                            \n",
    "                        if data_box_valores:\n",
    "                            row_info.update(data_box_valores)    \n",
    "\n",
    "                    \n",
    "                    # 7. VALORES E IMPOSTOS\n",
    "                    if valores_impostos == True:\n",
    "                        section = \"7. VALORES E IMPOSTOS\"\n",
    "                        # if debug:\n",
    "                        print(f'processando {section} para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                        valores = {}\n",
    "                        nf_data_valores = {}\n",
    "                        lista_impostos = []\n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valores = extrai_valores_impostos_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                            row_info.update(valores)\n",
    "                        \n",
    "                        else:\n",
    "                            tipo = \"field_box\"\n",
    "                            father_value = \"5_frame_valores_impostos\"\n",
    "                            valores = extracao_impostos_R_PDF(section, tipo, father_value, de_para_map, model_map, map_original_file_name, file_path)\n",
    "                            #print(valores)\n",
    "                            row_info.update(valores)\n",
    "                        \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg) \n",
    "                    \n",
    "                    # 8. DADOS COMPLEMENTARES\n",
    "                    if complementares == True:\n",
    "                        section = '8. DADOS COMPLEMENTARES'\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        nf_data_dados_complementares = {}\n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            nf_data_valores = extrai_dados_complementares_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                        else:\n",
    "                            label = \"dados_complementares\"\n",
    "                            tipo = \"field_box\"\n",
    "                            def_replace = False\n",
    "                            # ItSs  working\n",
    "                            texto_extraido = extracao_complementar_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_map, model_map, def_replace, map_original_file_name, debug)\n",
    "                            row_info[label] = texto_extraido  \n",
    "\n",
    "                    \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)     \n",
    "                    \n",
    "                    # 9. OUTRAS INFORMAÇOES / CRITICAS\n",
    "                    if outras_informacoes == True:\n",
    "                        section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        tipo = \"field_box\"\n",
    "                        father_value = \"5_frame_inf_criticas\"\n",
    "                        valores = {} \n",
    "                        nf_data_outras_informacoes = {}\n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valores = extrai_outras_informacoes_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "                            if valores:\n",
    "                                row_info.update(valores)\n",
    "                        else:\n",
    "                            section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "                            tipo = \"field_box\"\n",
    "                            father_value = \"5_frame_inf_criticas\"\n",
    "                            valores = extracao_inforacoes_criticas_R_PDF(section, tipo, father_value, de_para_map, model_map, map_original_file_name, file_path)\n",
    "                            if valores:\n",
    "                                row_info.update(valores)\n",
    "                            \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)          \n",
    "                            \n",
    "                    \n",
    "                    # 10. OBSERVACOES\n",
    "                    if observacoes == True:  \n",
    "                        section = \"10. OBSERVACOES\"\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')  \n",
    "                        data_observacao = {}\n",
    "                        valores = {}\n",
    "                        f_0 = 0.9\n",
    "                        f_1 = 1.1\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valores = extrai_outras_informacoes_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "                            if valores:\n",
    "                                row_info.update(valores)\n",
    "                        else:\n",
    "                            section = '10. OBSERVACOES'\n",
    "                            tipo = \"field_box\"\n",
    "                            father_value = \"6_section_inf_complementares_criticas\" \n",
    "                            \n",
    "                            label = \"observacao\"\n",
    "                            tipo = \"field_box\"\n",
    "                            def_replace = True\n",
    "                            valores = extracao_observacoees_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_map, model_map, def_replace, map_original_file_name, debug)\n",
    "                            if valores:\n",
    "                                row_info.update(valores)\n",
    "                    \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)           \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                lista_dicts.append(row_info)\n",
    "                \n",
    "                \n",
    "            elif action_item_row_info == 'BREAK_PROCESS':\n",
    "                \n",
    "                msg = (f'Documento sem qualidade para pesquisa inicial: {map_batch_name} | {map_original_file_name} | diretorio: {map_directory}')\n",
    "                row_info['informations'] = msg  \n",
    "                \n",
    "            \n",
    "                lista_dicts.append(row_info)\n",
    "                continue\n",
    "                         \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        elif map_status_documento == 'NO_PROCESS':\n",
    "            msg = (f'Documento nao sera tratado neste escopo: {map_batch_name} | {map_original_file_name} | diretorio: {map_directory}')\n",
    "            row_info['action_item'] = \"NO_PROCESS\"    \n",
    "            row_info['informations'] = msg \n",
    "            lista_dicts.append(row_info)\n",
    "            continue\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        #lista_dicts.append(row_info)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    logging.info(f'processamento finalizado para: {batch_name}') \n",
    "    \n",
    "    print(f'processamento de {i} documentos')\n",
    "    \n",
    "    novo_df = pd.DataFrame(lista_dicts)\n",
    "    \n",
    "    return novo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processamento de 13 documentos\n"
     ]
    }
   ],
   "source": [
    "# analisar_pdf_pesquisavel\n",
    "fase = 'analise'\n",
    "atividade = 'PREPROCESS' \n",
    "status = 'PREPROCESS_EXTRACT'\n",
    "raw_document_list = []\n",
    "dados_prest = {}\n",
    "\n",
    "lista_dicts = []\n",
    "logging.info(f'Execuçao do pipeline para {batch_name} | df_root_pipe: {file_path_root_pipe} fase: {fase} atividade: {atividade} status: {status}  template: {ver}')\n",
    "\n",
    "\n",
    "# XXX 1.Processar todas as secoes do documento\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=True, tomador=True, servicos=True, total=True, cnae=True, valores_impostos=True, complementares=True, outras_informacoes=True, observacoes=True)\n",
    "\n",
    "# 1. Processar somente dados iniciais e cabeçalho\n",
    "df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=False, cnae=False, valores_impostos=False, complementares=False, outras_informacoes=False, observacoes=False)\n",
    "\n",
    "# 5. Processar valor Total\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=True, cnae=False, valores_impostos=False, complementares=False, outras_informacoes=False, observacoes=False)\n",
    "\n",
    "# 6. Processar CNAE\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=False, cnae=True, valores_impostos=False, complementares=False, outras_informacoes=False, observacoes=False)\n",
    "\n",
    "# 7. Processar Impostos\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=False, cnae=False, valores_impostos=, complementares=False, outras_informacoes=False, observacoes=False)\n",
    "\n",
    "# 8. complementar e observaçoes\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=False, cnae=False, valores_impostos=False, complementares=True, outras_informacoes=True, observacoes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>date_time</th>\n",
       "      <th>batch</th>\n",
       "      <th>fase_processo</th>\n",
       "      <th>nome_atividade</th>\n",
       "      <th>status_documento</th>\n",
       "      <th>acao_executada</th>\n",
       "      <th>original_file_name</th>\n",
       "      <th>directory</th>\n",
       "      <th>one_page</th>\n",
       "      <th>...</th>\n",
       "      <th>pdf_pesquisavel</th>\n",
       "      <th>prefeitura</th>\n",
       "      <th>de_para_pm</th>\n",
       "      <th>model</th>\n",
       "      <th>processo</th>\n",
       "      <th>numero_nota_fiscal</th>\n",
       "      <th>competencia</th>\n",
       "      <th>dt_hr_emissao</th>\n",
       "      <th>codigo_verificacao</th>\n",
       "      <th>conf_cod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>23/09/2023 21:53:23</td>\n",
       "      <td>Batch_23</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>2023 -5.pdf</td>\n",
       "      <td>159871</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MESQUITA</td>\n",
       "      <td>PM_MESQUITA</td>\n",
       "      <td>MESQUITA</td>\n",
       "      <td>mapeamento regex - PDF pesquisavel</td>\n",
       "      <td>20235</td>\n",
       "      <td>Julho/2023</td>\n",
       "      <td>27/07/2023 15:13:00</td>\n",
       "      <td>92ED36652</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq            date_time     batch fase_processo nome_atividade  \\\n",
       "0    3  23/09/2023 21:53:23  Batch_23       analise   scan_analise   \n",
       "\n",
       "     status_documento acao_executada original_file_name directory  one_page  \\\n",
       "0  PREPROCESS_EXTRACT        Analise        2023 -5.pdf    159871      True   \n",
       "\n",
       "   ...  pdf_pesquisavel                        prefeitura   de_para_pm  \\\n",
       "0  ...             True  PREFEITURA MUNICIPAL DE MESQUITA  PM_MESQUITA   \n",
       "\n",
       "      model                            processo numero_nota_fiscal  \\\n",
       "0  MESQUITA  mapeamento regex - PDF pesquisavel              20235   \n",
       "\n",
       "  competencia        dt_hr_emissao codigo_verificacao conf_cod  \n",
       "0  Julho/2023  27/07/2023 15:13:00          92ED36652        9  \n",
       "\n",
       "[1 rows x 32 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>batch</th>\n",
       "      <th>original_file_name</th>\n",
       "      <th>directory</th>\n",
       "      <th>status_documento</th>\n",
       "      <th>model</th>\n",
       "      <th>secao</th>\n",
       "      <th>prefeitura</th>\n",
       "      <th>de_para_pm</th>\n",
       "      <th>model</th>\n",
       "      <th>action_item</th>\n",
       "      <th>pdf_pesquisavel</th>\n",
       "      <th>processo</th>\n",
       "      <th>numero_nota_fiscal</th>\n",
       "      <th>competencia</th>\n",
       "      <th>dt_hr_emissao</th>\n",
       "      <th>codigo_verificacao</th>\n",
       "      <th>conf_cod</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89bb6812-a273-454d-829a-5219b83a322a</th>\n",
       "      <td>3</td>\n",
       "      <td>Batch_23</td>\n",
       "      <td>2023 -5.pdf</td>\n",
       "      <td>159871</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>MESQUITA</td>\n",
       "      <td>1. CABECALHO</td>\n",
       "      <td>PREFEITURA MUNICIPAL DE MESQUITA</td>\n",
       "      <td>PM_MESQUITA</td>\n",
       "      <td>MESQUITA</td>\n",
       "      <td>CONTINUE_PROCESS</td>\n",
       "      <td>True</td>\n",
       "      <td>mapeamento regex - PDF pesquisavel</td>\n",
       "      <td>20235</td>\n",
       "      <td>Julho/2023</td>\n",
       "      <td>27/07/2023 15:13:00</td>\n",
       "      <td>92ED36652</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      seq     batch original_file_name  \\\n",
       "document_unique_id                                                       \n",
       "89bb6812-a273-454d-829a-5219b83a322a    3  Batch_23        2023 -5.pdf   \n",
       "\n",
       "                                     directory    status_documento     model  \\\n",
       "document_unique_id                                                             \n",
       "89bb6812-a273-454d-829a-5219b83a322a    159871  PREPROCESS_EXTRACT  MESQUITA   \n",
       "\n",
       "                                             secao  \\\n",
       "document_unique_id                                   \n",
       "89bb6812-a273-454d-829a-5219b83a322a  1. CABECALHO   \n",
       "\n",
       "                                                            prefeitura  \\\n",
       "document_unique_id                                                       \n",
       "89bb6812-a273-454d-829a-5219b83a322a  PREFEITURA MUNICIPAL DE MESQUITA   \n",
       "\n",
       "                                       de_para_pm     model       action_item  \\\n",
       "document_unique_id                                                              \n",
       "89bb6812-a273-454d-829a-5219b83a322a  PM_MESQUITA  MESQUITA  CONTINUE_PROCESS   \n",
       "\n",
       "                                      pdf_pesquisavel  \\\n",
       "document_unique_id                                      \n",
       "89bb6812-a273-454d-829a-5219b83a322a             True   \n",
       "\n",
       "                                                                processo  \\\n",
       "document_unique_id                                                         \n",
       "89bb6812-a273-454d-829a-5219b83a322a  mapeamento regex - PDF pesquisavel   \n",
       "\n",
       "                                     numero_nota_fiscal competencia  \\\n",
       "document_unique_id                                                    \n",
       "89bb6812-a273-454d-829a-5219b83a322a              20235  Julho/2023   \n",
       "\n",
       "                                            dt_hr_emissao codigo_verificacao  \\\n",
       "document_unique_id                                                             \n",
       "89bb6812-a273-454d-829a-5219b83a322a  27/07/2023 15:13:00          92ED36652   \n",
       "\n",
       "                                      conf_cod  \n",
       "document_unique_id                              \n",
       "89bb6812-a273-454d-829a-5219b83a322a         9  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando o subset para analise\n",
    "df_conf = df[['seq', 'batch', 'original_file_name', 'directory','status_documento', 'model', 'secao', 'prefeitura', 'de_para_pm', 'model', 'action_item', 'pdf_pesquisavel', 'processo', 'numero_nota_fiscal', 'competencia', 'dt_hr_emissao', 'codigo_verificacao','conf_cod' ]]\n",
    "df_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando DF para analises\n",
    "df.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "ordem_status = ['PREPROCESS_EXTRACT', 'NO_PROCESS', 'root_analise']\n",
    "ordem_action_item = ['CONTINUE_PROCESS', 'BREAK_PROCESS', 'NO_PROCESS']\n",
    "\n",
    "\n",
    "df['status_documento'] = pd.Categorical(df['status_documento'], categories=ordem_status, ordered=True)\n",
    "df['action_item'] = pd.Categorical(df['action_item'], categories=ordem_action_item, ordered=True)\n",
    "\n",
    "df.sort_values(by=['status_documento', 'action_item', 'seq'], ascending=[True, True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. XXX Criando o df_conf_export para analise excel\n",
    "df_conf_export = df[['seq', 'level', 'parent_document_unique_id', 'file_hash', 'batch', 'directory', 'original_file_name', 'pages', 'one_page', 'file_path', 'palavra_chave', 'document_tag', 'action_item', 'fase_processo', 'nome_atividade', 'status_documento', 'prefeitura', 'de_para_pm', 'model', 'secao', 'pdf_pesquisavel', 'numero_nota_fiscal', 'competencia', 'dt_hr_emissao', 'codigo_verificacao', 'conf_cod', 'informations',  'p_cpf_cnpj_com_mascara', 'p_cpf_cnpj_sem_mascara','p_telefone', 'p_inscricao_estadual', 'p_inscricao_municipal', 'p_razao_social', 'p_nome_fantasia','p_endereco','p_email', 'cnae', 'item_lista_servicos','t_cpf_cnpj_com_mascara', 't_cpf_cnpj_sem_mascara','t_telefone', 't_inscricao_municipal', 't_razao_social','t_endereco','t_email', 'discriminacao_servicos', 'valor_total_nota', 'cnae', 'item_lista_servicos', 'valor_servicos', 'valor_deducao', 'desc_incond', 'base_calculo', 'aliquota', 'valor_iss', 'valor_iss_retido', 'desc_cond', 'valor_pis', 'valor_cofins', 'valor_ir', 'valor_inss', 'valor_csll', 'outras_retencoes', 'valor_liquido', 'exigibilidade_iss', 'regime_tributacao', 'simples_nacional', 'issqn_retido', 'local_pretacao_servico', 'local_incidencia', 'dados_complementares', 'observacao' ]]\n",
    "df_conf_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. XXX Exportando o df_conf_export para analise excel\n",
    "df_conf_export.to_excel('processamentos/df_conf_export_batch_22.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>2.3.1</b> Conferencia do Processamento </mark> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte2 - dados prestador - OK\n",
    "df_conf2 = df[['seq', 'p_cpf_cnpj_com_mascara', 'p_cpf_cnpj_sem_mascara','p_telefone', 'p_inscricao_estadual', 'p_inscricao_municipal', 'p_razao_social', 'p_nome_fantasia','p_endereco','p_email']] #\n",
    "df_conf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste a largura máxima das colunas para um valor específico (por exemplo, 100 caracteres)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "\n",
    "# Criando o subset para analise\n",
    "df_conf = df[['seq', 'original_file_name', 'file_path']]\n",
    "df_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['action_item'] = pd.Categorical(df['action_item'], categories=ordem_action_item, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste a largura da coluna 'sua_coluna' para 50 caracteres\n",
    "df_conf['file_path'].astype(str).str.ljust(100), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte3 - Dados Tomador - OK\n",
    "df_conf3 = df[['t_cpf_cnpj_com_mascara', 't_cpf_cnpj_sem_mascara','t_telefone', 't_inscricao_municipal', 't_razao_social','t_endereco','t_email', 'discriminacao_servicos', 'pdf_pesquisavel']] #\n",
    "df_conf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 4- Outros itens\n",
    "df_conf4 = df[['seq', 'original_file_name', 'pdf_pesquisavel', 'discriminacao_servicos', 'valor_total_nota', 'cnae', 'item_lista_servicos']] #\n",
    "df_conf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'prefeitura', 'pdf_pesquisavel', 'image_np'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('teste2.xlsx')\n",
    "# IMPORTANTE, saber o tipo da coluna\n",
    "print(df['status_documento'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf0 = df[['seq', 'original_file_name','directory', 'file_hash', 'file_path', 'model', 'pdf_pesquisavel','informations', 'image_np']]\n",
    "df_conf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte1 - dados do cabeçalho - OK\n",
    "df_conf1 = df[['numero_nota_fiscal', 'competencia', 'dt_hr_emissao', 'codigo_verificacao', 'prefeitura', 'pdf_pesquisavel']]\n",
    "df_conf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 4- Outros itens\n",
    "df_conf4 = df[['original_file_name', 'discriminacao_servicos', 'valor_total_nota', 'cnae', 'item_lista_servicos']] #\n",
    "df_conf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 5- Imposto\n",
    "df_conf5 = df[['original_file_name', 'directory', 'pdf_pesquisavel', 'valor_servicos', 'valor_deducao', 'desc_incond', 'base_calculo', 'aliquota', 'valor_iss', 'valor_iss_retido', 'desc_cond', 'valor_pis', 'valor_cofins', 'valor_ir', 'valor_inss', 'valor_csll', 'outras_retencoes', 'valor_liquido', 'de_para_pm', 'batch', 'model','directory','file_path']] #\n",
    "df_conf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 6- Informacoes criticas\n",
    "df_conf2 = df[['original_file_name', 'pdf_pesquisavel', 'exigibilidade_iss', 'regime_tributacao', 'simples_nacional', 'issqn_retido', 'local_pretacao_servico', 'local_incidencia', 'dados_complementares', 'observacao']] #\n",
    "df_conf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF conferencia: \n",
    "df_conferencia = df[['numero_nota_fiscal', 'codigo_verificacao', 'competencia', 'dt_hr_emissao', 'prefeitura', 'p_cpf_cnpj_com_mascara', 'p_cpf_cnpj_sem_mascara','p_telefone', 'p_inscricao_estadual', 'p_inscricao_municipal', 'p_razao_social', 'p_nome_fantasia','p_endereco','p_email', 't_cpf_cnpj_com_mascara', 't_cpf_cnpj_sem_mascara','t_telefone', 't_inscricao_municipal', 't_razao_social','t_endereco','t_email', 'discriminacao_servicos', 'valor_total_nota', 'cnae', 'item_lista_servicos','valor_servicos', 'valor_deducao', 'desc_incond', 'base_calculo', 'aliquota', 'valor_iss', 'valor_iss_retido', 'desc_cond', 'valor_pis', 'valor_cofins', 'valor_ir', 'valor_inss', 'valor_csll', 'outras_retencoes', 'valor_liquido', 'exigibilidade_iss', 'regime_tributacao', 'simples_nacional', 'issqn_retido', 'local_pretacao_servico', 'local_incidencia', 'dados_complementares', 'observacao','de_para_pm', 'batch', 'model','directory','file_path']]\n",
    "df_conferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Volto novamente o indice do DF\n",
    "df_conferencia.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. XXX Salvo em Excel (pode ser feito durante fases)\n",
    "df_conferencia.to_excel(\"processamento_mage_1.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. XXX SE deseja importar o DF df_analise_pipe\n",
    "\n",
    "df_conf_path = \"processamento_mage_1.xlsx\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria df_documento_recebido\n",
    "df_conferencia = pd.read_excel(df_conf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processamento = pd.concat([df_conf, df_conf2, df_conf3, df_conf4,], ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Criando uma nova coluna no DF\n",
    "df_conferencia.insert(loc=50, column='original_file_name', value=df_conferencia['file_path'].apply(lambda x: os.path.basename(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. XXX Salvo em Excel (pode ser feito durante fases)\n",
    "df_root_pipe.to_excel(\"df_analise_pipe_b21.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste a largura máxima das colunas para um valor específico (por exemplo, 100 caracteres)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste a largura da coluna 'sua_coluna' para 50 caracteres\n",
    "df_conf['cabecalho'] = df_conf['cabecalho'].astype(str).str.ljust(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extraçao texto - PDF Pesquisavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_23/MESQUITA_PDF_31282023_2258/159871/2023 -5.pdf\"\n",
    "original_file_name = os.path.basename(file_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"27/07/2023, 15:14\\nNota Fiscal de Serviços Eletrônica (NFSe)\\nhttps://nfe.mesquita.rj.gov.br\\n1/1\\nPREFEITURA MUNICIPAL DE MESQUITA\\nSECRETARIA MUNICIPAL DA FAZENDA\\nNOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e\\nNúmero da Nota:\\n20235\\nCompetência:\\nJulho/2023\\nData e Hora da Emissão:\\n27/07/2023 15:13:00\\nCódigo Verificação:\\n92ED36652\\nPRESTADOR DE SERVIÇOS\\nCPF/CNPJ:\\n 50.921.369/0001-05\\nInscrição Municipal:\\n 952538\\nTelefone:\\n 2297268232..\\nInscrição Estadual:\\n \\nNome/Razão Social:\\nMEDSORIA CLINICA DE AVALIACAO MEDICA E PSICOLOGICA DO TRAFEGO DE MESQUITA LTDA\\nNome de Fantasia:\\nEndereço:\\nRUA PROCOPIO ,631 LOJA A ,SANTO ELIAS - Mesquita-RJ\\nE-mail:\\nLARA_VSORIA@HOTMAIL.COM\\nTOMADOR DE SERVIÇOS\\nCPF/CNPJ:\\n 06.047.087/0033-16    |    \\nINSC:MUNICIPAL:\\nRG:\\n \\nTelefone:\\nInscrição Estadual:\\n \\nNome/Razão Social:\\nREDE D'OR SAO LUIZ S.A.\\nEndereço:\\n OLINDA ELLIS N° 93 BAIRRO: CAMPO GRANDE CIDADE: RIO DE JANEIRO - RJ CEP: 23045160\\nE-mail:\\nNão Informado\\nDISCRIMINAÇÃO DOS SERVIÇOS\\nRef a Plantões de Fevereiro, 2h no Setor de Radiologia - Médica: Lara Veiga Soria Catuladeira.\\nVALOR TOTAL DA NOTA: R$ 1.469,32\\nCNAE - 8630502 - ATIVIDADE MÉDICA AMBULATORIAL COM RECURSOS PARA REALIZAÇÃO DE EXAMES COMPLEMENTARES\\nItem da Lista de Serviços - 4.03 - HOSPITAIS, CLÍNICAS, LABORATÓRIOS, SANATÓRIOS, MANICÔMIOS, CASAS DE SAÚDE, PRONTOS-SOCORROS,\\nAMBULATÓRIOS E CONGÊNERES.\\n \\nVALOR SERVIÇOS:\\nR$ 1.469,32\\nVALOR\\nDEDUÇÃO:\\nR$ 0,00\\nDESC. INCOND:\\nR$ 0,00\\nBASE DE\\nCÁLCULO:\\nR$ 1.469,32\\nALÍQUOTA:\\n2,01%\\nVALOR ISS:\\nR$ 29,53\\nVALOR ISS\\nRETIDO:\\nR$ 0,00\\nDESC. COND:\\nR$ 0,00\\n____________________________________________________________________\\nVALOR PIS:\\nR$ 0,00\\nVALOR COFINS:\\nR$ 0,00\\nVALOR IR:\\nR$ 0,00\\nVALOR INSS:\\nR$ 0,00\\nVALOR CSLL:\\nR$ 0,00\\nOUTRAS RETENÇÕES:\\nR$ 0,00\\nVALOR LÍQUIDO:\\nR$ 1.469,32\\nDADOS COMPLEMENTARES\\nOUTRAS INFORMAÇÕES / CRITICAS\\nEXIGIBILIDADE ISS\\nExigivel\\nREGIME TRIBUTAÇÃO\\nSociedade Limitada\\nSIMPLES NACIONAL\\nSim ( 2,01% )\\nISSQN RETIDO\\nNão\\nLOCAL. PRESTAÇÃO\\nSERVIÇO\\nMesquita - RJ\\nLOCAL INCIDÊNCIA\\nMesquita - RJ\\nObservação: LEI DA TRANSPARÊNCIA FISCAL NR. 12.741, DE 8 DE DEZEMBRO DE 2012.\\n- PRESTADOR OPTANTE DO SIMPLES NACIONAL (ALÍQUOTA: 2,01 %)\\nESTA NFS-E FOI EMITIDA EM SUBSTITUIÇÃO À NFS-E 20232 Valor Aproximado dos Tributos Federais R$ 197,62 (Alíq 13,45), Tributos Estaduais R$ 0,00 (Alíq\\n0,00 IBPT) e Municipal de R$ 32,77 (Alíq IBPT 2,23 IBPT)\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_document = fitz.open(file_path)\n",
    "\n",
    "page_number = 0  # Defina o número da página que deseja analisar\n",
    "page = pdf_document[page_number]\n",
    "\n",
    "# Definir retângulo de interesse\n",
    "x0 = 0\n",
    "y0 = 0\n",
    "x1 = 600\n",
    "y1 = 900  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "# Extrair texto dentro do retângulo\n",
    "texto = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "pdf_document.close()\n",
    "texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['27/07/2023, 15:14',\n",
       " 'Nota Fiscal de Serviços Eletrônica (NFSe)',\n",
       " 'https://nfe.mesquita.rj.gov.br',\n",
       " '1/1',\n",
       " 'PREFEITURA MUNICIPAL DE MESQUITA',\n",
       " 'SECRETARIA MUNICIPAL DA FAZENDA',\n",
       " 'NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e',\n",
       " 'Número da Nota:',\n",
       " '20235',\n",
       " 'Competência:',\n",
       " 'Julho/2023',\n",
       " 'Data e Hora da Emissão:',\n",
       " '27/07/2023 15:13:00',\n",
       " 'Código Verificação:',\n",
       " '92ED36652',\n",
       " 'PRESTADOR DE SERVIÇOS',\n",
       " 'CPF/CNPJ:',\n",
       " ' 50.921.369/0001-05',\n",
       " 'Inscrição Municipal:',\n",
       " ' 952538',\n",
       " 'Telefone:',\n",
       " ' 2297268232..',\n",
       " 'Inscrição Estadual:',\n",
       " 'Nome/Razão Social:',\n",
       " 'MEDSORIA CLINICA DE AVALIACAO MEDICA E PSICOLOGICA DO TRAFEGO DE MESQUITA LTDA',\n",
       " 'Nome de Fantasia:',\n",
       " 'Endereço:',\n",
       " 'RUA PROCOPIO ,631 LOJA A ,SANTO ELIAS - Mesquita-RJ',\n",
       " 'E-mail:',\n",
       " 'LARA_VSORIA@HOTMAIL.COM',\n",
       " 'TOMADOR DE SERVIÇOS',\n",
       " 'CPF/CNPJ:',\n",
       " ' 06.047.087/0033-16    |    ',\n",
       " 'INSC:MUNICIPAL:',\n",
       " 'RG:',\n",
       " 'Telefone:',\n",
       " 'Inscrição Estadual:',\n",
       " 'Nome/Razão Social:',\n",
       " \"REDE D'OR SAO LUIZ S.A.\",\n",
       " 'Endereço:',\n",
       " ' OLINDA ELLIS N° 93 BAIRRO: CAMPO GRANDE CIDADE: RIO DE JANEIRO - RJ CEP: 23045160',\n",
       " 'E-mail:',\n",
       " 'Não Informado',\n",
       " 'DISCRIMINAÇÃO DOS SERVIÇOS',\n",
       " 'Ref a Plantões de Fevereiro, 2h no Setor de Radiologia - Médica: Lara Veiga Soria Catuladeira.',\n",
       " 'VALOR TOTAL DA NOTA: R$ 1.469,32',\n",
       " 'CNAE - 8630502 - ATIVIDADE MÉDICA AMBULATORIAL COM RECURSOS PARA REALIZAÇÃO DE EXAMES COMPLEMENTARES',\n",
       " 'Item da Lista de Serviços - 4.03 - HOSPITAIS, CLÍNICAS, LABORATÓRIOS, SANATÓRIOS, MANICÔMIOS, CASAS DE SAÚDE, PRONTOS-SOCORROS,',\n",
       " 'AMBULATÓRIOS E CONGÊNERES.',\n",
       " 'VALOR SERVIÇOS:',\n",
       " 'R$ 1.469,32',\n",
       " 'VALOR',\n",
       " 'DEDUÇÃO:',\n",
       " 'R$ 0,00',\n",
       " 'DESC. INCOND:',\n",
       " 'R$ 0,00',\n",
       " 'BASE DE',\n",
       " 'CÁLCULO:',\n",
       " 'R$ 1.469,32',\n",
       " 'ALÍQUOTA:',\n",
       " '2,01%',\n",
       " 'VALOR ISS:',\n",
       " 'R$ 29,53',\n",
       " 'VALOR ISS',\n",
       " 'RETIDO:',\n",
       " 'R$ 0,00',\n",
       " 'DESC. COND:',\n",
       " 'R$ 0,00',\n",
       " '____________________________________________________________________',\n",
       " 'VALOR PIS:',\n",
       " 'R$ 0,00',\n",
       " 'VALOR COFINS:',\n",
       " 'R$ 0,00',\n",
       " 'VALOR IR:',\n",
       " 'R$ 0,00',\n",
       " 'VALOR INSS:',\n",
       " 'R$ 0,00',\n",
       " 'VALOR CSLL:',\n",
       " 'R$ 0,00',\n",
       " 'OUTRAS RETENÇÕES:',\n",
       " 'R$ 0,00',\n",
       " 'VALOR LÍQUIDO:',\n",
       " 'R$ 1.469,32',\n",
       " 'DADOS COMPLEMENTARES',\n",
       " 'OUTRAS INFORMAÇÕES / CRITICAS',\n",
       " 'EXIGIBILIDADE ISS',\n",
       " 'Exigivel',\n",
       " 'REGIME TRIBUTAÇÃO',\n",
       " 'Sociedade Limitada',\n",
       " 'SIMPLES NACIONAL',\n",
       " 'Sim ( 2,01% )',\n",
       " 'ISSQN RETIDO',\n",
       " 'Não',\n",
       " 'LOCAL. PRESTAÇÃO',\n",
       " 'SERVIÇO',\n",
       " 'Mesquita - RJ',\n",
       " 'LOCAL INCIDÊNCIA',\n",
       " 'Mesquita - RJ',\n",
       " 'Observação: LEI DA TRANSPARÊNCIA FISCAL NR. 12.741, DE 8 DE DEZEMBRO DE 2012.',\n",
       " '- PRESTADOR OPTANTE DO SIMPLES NACIONAL (ALÍQUOTA: 2,01 %)',\n",
       " 'ESTA NFS-E FOI EMITIDA EM SUBSTITUIÇÃO À NFS-E 20232 Valor Aproximado dos Tributos Federais R$ 197,62 (Alíq 13,45), Tributos Estaduais R$ 0,00 (Alíq',\n",
       " '0,00 IBPT) e Municipal de R$ 32,77 (Alíq IBPT 2,23 IBPT)']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraindo texto - Raster_PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_23/MESQUITA_PDF_31282023_2258/159871/2023 -5.pdf\"\n",
    "original_file_name = os.path.basename(file_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = (pytesseract.image_to_string(imagem_gray, lang='por'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"27/07/2023, 15:14 Nota Fiscal de Serviços Eletrônica (NFSe)\\n\\nNúmero da Nota:\\n\\nPREFEITURA MUNICIPAL DE MESQUITA Competéris\\n\\nSECRETARIA MUNICIPAL DA FAZENDA Julho/2023\\n\\nNOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e ata e Hora da Emissão:\\n27/07/2023 15:13:00\\n\\nCódigo Verificação:\\n92ED36652\\n\\nCPF/CNPJ: Inscrição Municipal:\\n50.921.369/0001-05 952538\\n\\nTelefone: Inscrição Estadual:\\n2297268232...\\nNome/Razão Social:\\nMEDSORIA CLINICA DE AVALIACAO MEDICA E PSICOLOGICA DO TRAFEGO DE MESQUITA LTDA\\nNome de Fantasia:\\nEndereço:\\nRUA PROCOPIO ,631 LOJA A ,SANTO ELIAS - Mesquita-RJ\\nE-mail:\\nLARA VSORIAQHOTMAIL.COM\\n\\nTOMADOR DE SERVIÇOS\\n\\nCPF/CNPJ: .\\n06.047.087/0033-16 | RG:\\n\\nINSC:MUNICIPAL:\\n\\nTelefone: Inscrição Estadual:\\n\\nNome/Razão Social:\\nREDE D'OR SAO LUIZ S.A.\\nEndereço:\\nOLINDA ELLIS Nº 93 BAIRRO: CAMPO GRANDE CIDADE: RIO DE JANEIRO - RJ CEP: 23045160\\nE-mail:\\nNão Informado\\n\\nDISCRIMINAÇÃO DOS SERVIÇOS\\n\\nRef a Plantões de Fevereiro, 2h no Setor de Radiologia - Médica: Lara Veiga Soria Catuladeira.\\n\\nVALOR TOTAL DA NOTA: R$ 1.469,32\\n\\nCNAE - 8630502 - ATIVIDADE MÉDICA AMBULATORIAL COM RECURSOS PARA REALIZAÇÃO DE EXAMES COMPLEMENTARES\\nItem da Lista de Serviços - 4.03 - HOSPITAIS, CLÍNICAS, LABORATÓRIOS, SANATÓRIOS, MANICÔOMIOS, CASAS DE SAUDE, PRONTOS-SOCORROS,\\n\\nAMBULATÓRIOS E CONGÊNERES.\\n\\nVALOR SERVIÇOS: VALOR . DESC. INCOND: BASE DE ALÍQUOTA: VALORISS: — VALORISS | DESC. COND:\\nR$ 1.469,32 DEDUÇÃO: — R$0,00 CÁLCULO: 2,01% R$ 29,53 RETIDO: R$ 0,00\\n\\n[nm] R$ 0,00 R$ 1.469,32 R$ 0,00\\n\\n|]\\nVALOR PIS: VALOR COFINS: VALOR IR: VALORINSS: VALORCSLL: OUTRAS RETENÇÕES: VALOR LÍQUIDO:\\nR$ 0,00 R$ 0,00 R$ 0,00 R$ 0,00 R$ 0,00 R$ 0,00 R$ 1.469,32\\n\\nEXIGIBILIDADE ISS REGIME TRIBUTAÇÃO SIMPLES NACIONAL ISSQN RETIDO LOCAL. PRESTAÇÃO LOCAL INCIDÊNCIA\\nExigivel Sociedade Limitada Sim (2,01% ) Não SERVIÇO Mesquita - RJ\\nMesquita - RJ\\n\\nObservação: LEI DA TRANSPARÊNCIA FISCAL NR. 12.741, DE 8 DE DEZEMBRO DE 2012.\\n\\n- PRESTADOR OPTANTE DO SIMPLES NACIONAL (ALÍQUOTA: 2,01 %)\\n\\nESTA NFS-E FOI EMITIDA EM SUBSTITUIÇÃO À NFS-E 20232 Valor Aproximado dos Tributos Federais R$ 197,62 (Alíq 13,45), Tributos Estaduais R$ 0,00 (Alíq\\n0,00 IBPT) e Municipal de R$ 32,77 (Alíq IBPT 2,23 IBPT)\\n\\nhttps://nfe.mesquita.rj.gov.br\\n\\n1/1\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['27/07/2023, 15:14 Nota Fiscal de Serviços Eletrônica (NFSe)',\n",
       " 'Número da Nota:',\n",
       " 'PREFEITURA MUNICIPAL DE MESQUITA Competéris',\n",
       " 'SECRETARIA MUNICIPAL DA FAZENDA Julho/2023',\n",
       " 'NOTA FISCAL DE SERVIÇOS ELETRÔNICA - NFS-e ata e Hora da Emissão:',\n",
       " '27/07/2023 15:13:00',\n",
       " 'Código Verificação:',\n",
       " '92ED36652',\n",
       " 'CPF/CNPJ: Inscrição Municipal:',\n",
       " '50.921.369/0001-05 952538',\n",
       " 'Telefone: Inscrição Estadual:',\n",
       " '2297268232...',\n",
       " 'Nome/Razão Social:',\n",
       " 'MEDSORIA CLINICA DE AVALIACAO MEDICA E PSICOLOGICA DO TRAFEGO DE MESQUITA LTDA',\n",
       " 'Nome de Fantasia:',\n",
       " 'Endereço:',\n",
       " 'RUA PROCOPIO ,631 LOJA A ,SANTO ELIAS - Mesquita-RJ',\n",
       " 'E-mail:',\n",
       " 'LARA VSORIAQHOTMAIL.COM',\n",
       " 'TOMADOR DE SERVIÇOS',\n",
       " 'CPF/CNPJ: .',\n",
       " '06.047.087/0033-16 | RG:',\n",
       " 'INSC:MUNICIPAL:',\n",
       " 'Telefone: Inscrição Estadual:',\n",
       " 'Nome/Razão Social:',\n",
       " \"REDE D'OR SAO LUIZ S.A.\",\n",
       " 'Endereço:',\n",
       " 'OLINDA ELLIS Nº 93 BAIRRO: CAMPO GRANDE CIDADE: RIO DE JANEIRO - RJ CEP: 23045160',\n",
       " 'E-mail:',\n",
       " 'Não Informado',\n",
       " 'DISCRIMINAÇÃO DOS SERVIÇOS',\n",
       " 'Ref a Plantões de Fevereiro, 2h no Setor de Radiologia - Médica: Lara Veiga Soria Catuladeira.',\n",
       " 'VALOR TOTAL DA NOTA: R$ 1.469,32',\n",
       " 'CNAE - 8630502 - ATIVIDADE MÉDICA AMBULATORIAL COM RECURSOS PARA REALIZAÇÃO DE EXAMES COMPLEMENTARES',\n",
       " 'Item da Lista de Serviços - 4.03 - HOSPITAIS, CLÍNICAS, LABORATÓRIOS, SANATÓRIOS, MANICÔOMIOS, CASAS DE SAUDE, PRONTOS-SOCORROS,',\n",
       " 'AMBULATÓRIOS E CONGÊNERES.',\n",
       " 'VALOR SERVIÇOS: VALOR . DESC. INCOND: BASE DE ALÍQUOTA: VALORISS: — VALORISS | DESC. COND:',\n",
       " 'R$ 1.469,32 DEDUÇÃO: — R$0,00 CÁLCULO: 2,01% R$ 29,53 RETIDO: R$ 0,00',\n",
       " '[nm] R$ 0,00 R$ 1.469,32 R$ 0,00',\n",
       " '|]',\n",
       " 'VALOR PIS: VALOR COFINS: VALOR IR: VALORINSS: VALORCSLL: OUTRAS RETENÇÕES: VALOR LÍQUIDO:',\n",
       " 'R$ 0,00 R$ 0,00 R$ 0,00 R$ 0,00 R$ 0,00 R$ 0,00 R$ 1.469,32',\n",
       " 'EXIGIBILIDADE ISS REGIME TRIBUTAÇÃO SIMPLES NACIONAL ISSQN RETIDO LOCAL. PRESTAÇÃO LOCAL INCIDÊNCIA',\n",
       " 'Exigivel Sociedade Limitada Sim (2,01% ) Não SERVIÇO Mesquita - RJ',\n",
       " 'Mesquita - RJ',\n",
       " 'Observação: LEI DA TRANSPARÊNCIA FISCAL NR. 12.741, DE 8 DE DEZEMBRO DE 2012.',\n",
       " '- PRESTADOR OPTANTE DO SIMPLES NACIONAL (ALÍQUOTA: 2,01 %)',\n",
       " 'ESTA NFS-E FOI EMITIDA EM SUBSTITUIÇÃO À NFS-E 20232 Valor Aproximado dos Tributos Federais R$ 197,62 (Alíq 13,45), Tributos Estaduais R$ 0,00 (Alíq',\n",
       " '0,00 IBPT) e Municipal de R$ 32,77 (Alíq IBPT 2,23 IBPT)',\n",
       " 'https://nfe.mesquita.rj.gov.br',\n",
       " '1/1']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "linhas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tables-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
