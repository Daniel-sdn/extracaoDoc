{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <mark> <b> > 2. </b> Pipeline de extracao de dados de documentos </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2_extracao_pipeline_v1.ipynb = 2_root_doc_extract_v1.ipynb</b>    |     Atual notebook com as funçoes para processamento de PDF Pesquisavel e Raster e a criaçao dos Dataframes de forma independente e unica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import platform\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from urllib import response\n",
    "\n",
    "from outlook_msg import Message\n",
    "import extract_msg\n",
    "import zipfile\n",
    "from pyunpack import Archive\n",
    "import py7zr\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import PyPDF2\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "import locale\n",
    "import time, copy\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import cv2\n",
    "import fitz  # Módulo PyMuPDF\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import Image, ImageDraw\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar\n",
    "import matplotlib.pyplot as plt\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import logging\n",
    "\n",
    "# Modulos da solucao\n",
    "# import modules.extrai_pdf_pesquisavel as Extc\n",
    "import modules.cronometro as cron\n",
    "import modules.nova_extracao_pdf_pesquisavel as novaextra \n",
    "\n",
    "\n",
    "#### Config - E-mail\n",
    "# 1. Caminho do arquivo uma mensagem especifica\n",
    "msg_dir_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails'\n",
    "\n",
    "# 2. Path para arquivos atachados compactados\n",
    "msg_attachment_zip = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/13_attachments'\n",
    "\n",
    "\n",
    "#### Config - messages\n",
    "# 3. Caminho do arquivo uma mensagem especifica\n",
    "msg_outros_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/12_messages'\n",
    "\n",
    "# 4. Path para arquivos recebidos manualmente\n",
    "arquivos_recebidos_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/14_documentos_recebidos'\n",
    "\n",
    "\n",
    "####Config Processamento Pipeline\n",
    "\n",
    "# 5. Path para documentos para extracao\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "# 6. Path para gestao de imagens resized\n",
    "image_resized_path = \"pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas\"\n",
    "\n",
    "\n",
    "\n",
    "# 7. path para arquivos json\n",
    "json_path = \"pipeline_extracao_documentos/5_documentos_processados/jsons\"\n",
    "\n",
    "# 7. Path para DFs e CSVs exportados\n",
    "export_path = \"pipeline_extracao_documentos/6_geral_administacao/exports\"\n",
    "\n",
    "# 8. Path para lixeira\n",
    "root_garbage_path = \"pipeline_extracao_documentos/0_lixeira\"\n",
    "\n",
    "\n",
    "#### paths de objetos para criacao/gestao (dicionarios/datasets)\n",
    "cnae_dict_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets/CNAE_X_ITEM_SERVICO_PREFEITURAS.xlsx\"\n",
    "\n",
    "\n",
    "# 12. poppler path\n",
    "poppler_path = \"/home/dani-boy/miniconda3/envs/tables-detr/bin\"\n",
    "\n",
    "# 13. path para config Tesseract\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "#Modelo atual\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "# definindo localizadcao para pt_BR\n",
    "locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='processamentos/log_ocorrencias.log',\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "    datefmt='%d/%m/%Y %H:%M:%S'\n",
    ")\n",
    "\n",
    "logging.info(\"kernel reiniciado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b>2.1</b> FunÇoes de Gerais </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch</th>\n",
       "      <th>Data</th>\n",
       "      <th>File</th>\n",
       "      <th>Type</th>\n",
       "      <th>Level</th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Parent_Unique_ID</th>\n",
       "      <th>Hash</th>\n",
       "      <th>File_Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Batch, Data, File, Type, Level, Unique_ID, Parent_Unique_ID, Hash, File_Path]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Geraçao do hash do arquivo\n",
    "def generate_file_hash(file_path):\n",
    "    # Abre o arquivo em modo de leitura de bytes\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # Lê o conteúdo do arquivo\n",
    "        file_data = f.read()\n",
    "        # Utiliza o algoritmo SHA-256 para gerar o hash\n",
    "        file_hash = hashlib.sha256(file_data).hexdigest()\n",
    "    return file_hash\n",
    "\n",
    "# 2. Geraçao do Unique_id do arquivo\n",
    "def generate_unique_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# 3. XXX Busca proximo Batch\n",
    "def busca_proximo_batch():\n",
    "    # Abre o arquivo Excel e lê a coluna 'batch'\n",
    "    df = pd.read_excel(\"pipeline_extracao_documentos/6_geral_administacao/exports/df_documento_recebido.xlsx\", usecols=[\"batch\"])\n",
    "    # Pega o último valor da coluna 'batch'\n",
    "    last_value = df.iloc[-1, 0]\n",
    "    \n",
    "    # Extraí o número do último batch e adiciona 1 para o próximo\n",
    "    last_number = int(last_value.split(\"_\")[1])\n",
    "    next_number = last_number + 1\n",
    "    \n",
    "    # Forma o nome do próximo batch\n",
    "    next_batch = f\"Batch_{next_number}\"\n",
    "    \n",
    "    return next_batch    \n",
    "\n",
    "# 4. Função para verificar e criar a pasta se não existir\n",
    "def check_and_create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        \n",
    "# 7. funçao que MOVE documentos e gera add_log_transaction_entry para df_log_transctions\n",
    "def move_doc_processed_file(batch_name, src_path, tgt_path):\n",
    "    \n",
    "    function = \"move_doc_processed_file\"\n",
    "    source_path = src_path\n",
    "    file = os.path.basename(source_path)\n",
    "    sub_dir = os.path.join(tgt_path, batch_name)\n",
    "    destination_path = os.path.join(sub_dir, file)\n",
    "    document_action = \"move_processed_file\"\n",
    "    transaction_detail = (f'document {file} moved by: {function}')\n",
    "    df_move = pd.DataFrame()\n",
    "    try:\n",
    "        document_unique_id = get_document_id_by_file(batch_name, file)\n",
    "        check_and_create_folder(destination_path)\n",
    "        shutil.move(source_path, destination_path)\n",
    "        sucess = True\n",
    "        move_log = add_log_transaction_entry(document_unique_id, batch_name, file, document_action, src_path, tgt_path, transaction_detail, sucess)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao mover documento: {e}\")\n",
    "        sucess = False\n",
    "    \n",
    "    return move_log    \n",
    "\n",
    "# 8. Função para adicionar um novo registro em df_source\n",
    "def add_source_entry(batch_name, file_path, file, type, level, parent_unique_id):\n",
    "    #unique_id = generate_unique_id(type)\n",
    "    unique_id = generate_unique_id()\n",
    "    time_now = cron.timenow_pt_BR()   \n",
    "    file_hash = generate_file_hash(file_path) \n",
    "    if level == 1:\n",
    "        parent_unique_id = unique_id\n",
    "    data = {\n",
    "        'Batch': batch_name,\n",
    "        'Data': time_now,\n",
    "        'File': file,\n",
    "        'Type': type,\n",
    "        'Level': level,\n",
    "        'Unique_ID': unique_id,\n",
    "        'Parent_Unique_ID': parent_unique_id,\n",
    "        'Hash': file_hash,\n",
    "        'File_Path': file_path\n",
    "    }\n",
    "    return data\n",
    "\n",
    "# 9. Add nova linha para atualizar df_log_transctions\n",
    "def add_log_transaction_entry(document_unique_id,batch_name, file, document_action, src_path, tgt_path, transaction_detail, sucess=True):\n",
    "\n",
    "    data_log = {\n",
    "        'Dt_Time': cron.timenow_pt_BR(),\n",
    "        'Batch': batch_name,\n",
    "        'File' : file,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Action': document_action,\n",
    "        'Sorce': src_path,\n",
    "        'Target': tgt_path,\n",
    "        'Transction_Detail': transaction_detail,\n",
    "        'Sucess': sucess,    \n",
    "    }\n",
    "    \n",
    "        \n",
    "    return data_log\n",
    "\n",
    "\n",
    "# 10. Consulta multiparametros\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# 12. Busca filhos - simples\n",
    "def get_children(batch, file_path):\n",
    "    \n",
    "    file = os.path.basename(file_path)\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    # Buscando todos dos filhos de um documento\n",
    "    return filtrar_df(df_id_relations, Batch=batch, Parent_Unique_ID=document_unique_id)\n",
    "\n",
    "\n",
    "# 13. Busca pai -simples\n",
    "def get_father(batch, file_path):\n",
    "    \n",
    "    file = os.path.basename(file_path)\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    # Buscando todos dos filhos de um documento\n",
    "    return filtrar_df(df_id_relations, Batch=batch, Unique_ID=parent_unique_id)\n",
    "\n",
    "\n",
    "# 14. Pesquiso pai pelo Unique_ID e trago um dict\n",
    "def get_father_data_by_children_file(batch, file):\n",
    "    \n",
    "    src_result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    src_parent_unique_id = src_result['Parent_Unique_ID'].values[0]\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=src_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }\n",
    "\n",
    "\n",
    "# 15. Pesquiso pai pelo Unique_ID (document_parent_unique_id) e cria DICT\n",
    "def get_father_by_unique_id(batch, document_parent_unique_id):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=document_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }\n",
    "    \n",
    "# 16. Pesquiso pai pelo file do filho e cria DICT\n",
    "def get_father_data_by_children_file(batch, file):\n",
    "    \n",
    "    src_result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    src_parent_unique_id = src_result['Parent_Unique_ID'].values[0]\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, Unique_ID=src_parent_unique_id)\n",
    "    document_batch = result['Batch'].values[0]\n",
    "    document_data = result['Data'].values[0]\n",
    "    document_file = result['File'].values[0]\n",
    "    document_type = result['Type'].values[0]\n",
    "    document_level = result['Level'].values[0]\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    document_parent_unique_id = result['Parent_Unique_ID'].values[0]\n",
    "    document_hash = result['Hash'].values[0]\n",
    "    document_file_path = result['File_Path'].values[0]\n",
    "    \n",
    "    return {\n",
    "        'Batch': document_batch, \n",
    "        'Data': document_data,\n",
    "        'File' : document_file,\n",
    "        'Type': document_type,\n",
    "        'Level': document_level,\n",
    "        'Unique_ID': document_unique_id,\n",
    "        'Parent_Unique_ID': document_parent_unique_id,\n",
    "        'Hash': document_hash,\n",
    "        'File_Path': document_file_path,\n",
    "    }        \n",
    "        \n",
    "\n",
    "# 17. Busca o 'Unique_ID' para definir o Parent_Unique_ID sem considerar 'Level'\n",
    "def get_parent_unique_id(df_id_relations, batch_name, file, type):\n",
    "    try:\n",
    "        parent_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type)]['Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        parent_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return parent_unique_id\n",
    "\n",
    "\n",
    "# 18. funcao para trazer somente o 'Unique_ID'\n",
    "def get_document_unique_id(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)]['Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        document_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# 19. funcao para trazer somente o 'Parent_Unique_ID'\n",
    "def get_document_parent_unique_id(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_parent_unique_id = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)]['Parent_Unique_ID'].values[0]\n",
    "    except IndexError:\n",
    "        document_parent_unique_id = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_parent_unique_id\n",
    "\n",
    "\n",
    "# 20. funçao para trazer toda a row de df_id_relations para o documento\n",
    "def get_document_id_relations(df_id_relations, batch_name, file, type, level):\n",
    "    try:\n",
    "        document_id_relations = df_id_relations[(df_id_relations['Batch'] == batch_name) & (df_id_relations['File'] == file) & (df_id_relations['Type'] == type) & (df_id_relations['Level'] == level)].values[0]\n",
    "    except IndexError:\n",
    "        document_id_relations = None\n",
    "        print(f\"Unique_ID para Batch {batch_name} e type: {type} nao encontrado em df_id_relations.\")\n",
    "    return document_id_relations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# EXEMPLOS de Pesquisa DFss\n",
    "    # get_document_unique_id(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "    # # Busca somente o 'Parent_Unique_ID'\n",
    "    # get_document_parent_unique_id(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "\n",
    "    # #Busca todos os dados da row do documento encontrado\n",
    "    # document_id_relations = get_document_id_relations(df_id_relations, batch_name, file, type, level)\n",
    "\n",
    "    # document_batch = document_id_relations[0]\n",
    "    # document_date = document_id_relations[1]\n",
    "    # document_name = document_id_relations[2]\n",
    "    # document_type = document_id_relations[3]\n",
    "    # document_level = document_id_relations[4]\n",
    "    # document_unique_id = document_id_relations[5]\n",
    "    # document_parent_unique_id = document_id_relations[6]\n",
    "    # document_hash = document_id_relations[7]\n",
    "    # document_path = document_id_relations[8]\n",
    "\n",
    "    # # Insercao de um registro pela func add_source_entry\n",
    "    # file_path = \"pipeline_extracao_documentos/1_emails_documentos_recebidos/12_attachments/SPA 15082023.rar\"\n",
    "\n",
    "    # file = os.path.basename(file_path)\n",
    "\n",
    "    # type = \"compressed_file_attachment\"\n",
    "\n",
    "    # level = 1\n",
    "\n",
    "    # parent_unique_id = ''\n",
    "\n",
    "    # # Adicionando um novo registro (substitua 'batch_name' e 'email' conforme necessário)\n",
    "    # new_entry = add_source_entry(batch_name, file_path, file, type, level, parent_unique_id)\n",
    "\n",
    "    # df_id_relations = df_id_relations.append(new_entry, ignore_index=True)\n",
    "\n",
    "    # df_id_relations\n",
    "\n",
    "\n",
    "# Busca proximo Batch caso nao esteja rodando email\n",
    "batch_name = busca_proximo_batch()\n",
    "\n",
    "# 1. Criaçao do DataFrame para armazenar as relações de Unique_ID e Parent_Unique_ID\n",
    "df_id_relations = pd.DataFrame(columns=['Batch', 'Data' ,'File', 'Type', 'Level', 'Unique_ID', 'Parent_Unique_ID', 'Hash', 'File_Path'])\n",
    "\n",
    "# 2. Criaçao do DataFrame para df_start_pipe:\n",
    "#df_start_pipe = pd.DataFrame(columns=['Batch', 'Data' ,'File', 'Type', 'Level', 'Unique_ID', 'dt_hora', 'de', 'assunto', 'email', 'Hash'])\n",
    "\n",
    "def get_template_version(model):\n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model)\n",
    "    if not row_frame.empty:\n",
    "            # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "            version = [((row_frame.iloc[0]['version']))]\n",
    "            \n",
    "    return version[0]  \n",
    "\n",
    "\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# XXXpara buscar melhor as coordendas dos FRAMES\n",
    "def get_coordinates_filter(pdf_pesquisavel_map, model, tipo, label, section):\n",
    "    \n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model, type=tipo, label=label, section_json=section)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [((row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p']) if pdf_pesquisavel_map else (row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1']))]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. XXX Analisa nro de paginas\n",
    "def analisa_nro_pages(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    pages = pdf_document.pages() # generator object\n",
    "\n",
    "    page_nro = []\n",
    "    for page in pages:\n",
    "        page_nro.append(page)\n",
    "        \n",
    "    nro_paginas = len(page_nro)    \n",
    "    if nro_paginas > 1:\n",
    "        doc_1_page = False\n",
    "        return doc_1_page, nro_paginas    \n",
    "    else:\n",
    "        doc_1_page = True\n",
    "        return doc_1_page, nro_paginas  \n",
    "    pdf_document.close()\n",
    "    \n",
    " \n",
    "\n",
    "# XXX FUNCAO DE SPLIT\n",
    "def split_documentos(qualquer_df, fase, atividade, status):\n",
    "    \n",
    "    documentos_splitados = []\n",
    "    doc_info = {}\n",
    "    rows_list = []\n",
    "    documentos = []\n",
    "    #output_dir = os.path.join(documentos_scan_path, batch_name)\n",
    "    num_linhas_df = qualquer_df.shape[0]\n",
    "\n",
    "    i = num_linhas_df + 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        message_erro = []\n",
    "        nun_pages = row['pages']\n",
    "        batch_name = row['batch']\n",
    "        original_file_name = row['original_file_name']\n",
    "        folder_name = row['directory']\n",
    "        file_path = row['file_path']\n",
    "        level = row['level']\n",
    "        document_type = row['document_type']\n",
    "        doc_action = row['doc_action']\n",
    "        document_unique_id = idx\n",
    "        new_level = level + 1\n",
    "        \n",
    "        if (doc_action == 'splitar') and (status == 'root_analise'):\n",
    "            if nun_pages > 1:\n",
    "                try:\n",
    "                    pdf = fitz.open(file_path)\n",
    "                    # Número total de páginas no PDF\n",
    "                    total_pages = len(pdf)\n",
    "                except Exception as e:\n",
    "                    print(f\"Nao congui abrir o PDF: {e}\")    \n",
    "\n",
    "                # Nome base para os arquivos de saída\n",
    "                base_name = file_path.split('.')[0]  # Remove a extensão do arquivo\n",
    "                file_to_delete = file_path\n",
    "                # Loop para criar um novo PDF para cada página\n",
    "                for page_num in range(total_pages):\n",
    "                    # Cria um novo objeto PDF\n",
    "                    new_pdf = fitz.open()\n",
    "                    # Adiciona a página atual ao novo PDF\n",
    "                    new_pdf.insert_pdf(pdf, from_page=page_num, to_page=page_num)\n",
    "                    # Nome do novo arquivo PDF\n",
    "                    new_pdf_name = f\"{base_name}_page_{page_num + 1}.pdf\"\n",
    "                    # Salva o novo PDF\n",
    "                    new_pdf.save(new_pdf_name)\n",
    "                    # Fecha o novo PDF\n",
    "                    new_pdf.close()\n",
    "                    rotulo = \"prov_nota_fiscal\"\n",
    "                    acao_sugerida = sugestoes_acao.get(rotulo, \"no_defined_action\")\n",
    "                    acao_executada = \"novo_doc_criado\"\n",
    "                    informations = (f'documento criado a partir do split do documento: {original_file_name}')  \n",
    "                    name_pdf_splited = os.path.basename(new_pdf_name)\n",
    "\n",
    "                    new_row = {\n",
    "                        \"seq\": i,\n",
    "                        \"date_time\": cron.timenow_pt_BR(),\n",
    "                        \"batch\": batch_name,\n",
    "                        \"fase_processo\": fase,\n",
    "                        \"nome_atividade\": atividade,\n",
    "                        \"status_documento\": status,\n",
    "                        \"acao_executada\": acao_executada,\n",
    "                        \"original_file_name\": new_pdf_name,\n",
    "                        \"directory\": folder_name,\n",
    "                        \"one_page\": True,\n",
    "                        \"pages\": 1,\n",
    "                        \"document_type\": rotulo,\n",
    "                        \"doc_action\": acao_sugerida,\n",
    "                        \"level\": level,\n",
    "                        \"document_unique_id\": generate_unique_id(),\n",
    "                        \"parent_document_unique_id\": document_unique_id,\n",
    "                        \"file_hash\": generate_file_hash(file_path),\n",
    "                        \"file_path\": file_path,\n",
    "                        \"informations\": informations,\n",
    "                    }\n",
    "                    rows_list.append(new_row)\n",
    "                    i += 1\n",
    "                qualquer_df.loc[idx, 'status_documento'] = \"NAO_PROCESSAR\" \n",
    "                qualquer_df.loc[idx, 'informations'] = \"Paginas splitada em multiplos documentos\" \n",
    "                qualquer_df.loc[idx, 'date_time'] = cron.timenow_pt_BR() \n",
    "    \n",
    "    total_split = i - 1        \n",
    "    df_split = pd.DataFrame(rows_list)\n",
    "    \n",
    "    \n",
    "    return df_split, rows_list\n",
    "\n",
    "\n",
    "# XXX Usando na criacao da imagem \n",
    "def conv_filename_no_ext(title):\n",
    "    \n",
    "    # Divida o título em nome e extensão (mas ignore a extensão)\n",
    "    name = title.rsplit('.', 1)[0] if '.' in title else title\n",
    "\n",
    "    # Remova acentos e caracteres especiais do nome\n",
    "    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Substitua espaços e hífens por sublinhados\n",
    "    filename = name.replace(' ', '_').replace('-', '_')\n",
    "\n",
    "    # Remova quaisquer outros caracteres não alfanuméricos, exceto sublinhados\n",
    "    filename = re.sub(r'[^\\w_]', '', filename)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    filename = filename.lower()\n",
    "\n",
    "    return filename \n",
    "\n",
    "\n",
    "\n",
    "def apagar_zone(documentos_extracao_path):\n",
    "    # Para apagar arquivos PDF:Zone\n",
    "    for root, dirs, files in os.walk(documentos_extracao_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            #print(file)\n",
    "            if \":Zone\" in file:\n",
    "                file_to_delete = file_path\n",
    "                os.remove(file_to_delete)\n",
    "                #print(file, \"termina, pode eliminar\")\n",
    "                \n",
    "                \n",
    "def confirma_pdf_pequisavel(file_path):\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "    # Definir retângulo de interesse\n",
    "    try:\n",
    "        x0 = 0\n",
    "        y0 = 4\n",
    "        x1 = 600\n",
    "        y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "        # Extrair texto dentro do retângulo\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        if text:\n",
    "            page_number = 0\n",
    "            pdf_pequisavel = True\n",
    "        #print(page_number)\n",
    "        else:\n",
    "            page_number = 1\n",
    "            pdf_pequisavel = False\n",
    "        #print(page_number)\n",
    "    except Exception as e:\n",
    "        msg_error = (f\"Erro ao abrir pagina do PDF: {e}\")\n",
    "        pdf_pequisavel = False\n",
    "        pdf_document.close()   \n",
    "         \n",
    "        return pdf_pequisavel\n",
    "                   \n",
    "\n",
    "\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "#generated_parent_document_unique_id = generate_unique_id()  \n",
    "\n",
    "# Processo de deleçao e atualizacao de documentos\n",
    "#e_deleta_peloamor(df_docs_splitados)\n",
    "\n",
    "#me_atualiza_logo_vai_2(novo_df)\n",
    "\n",
    "# apagar_zone(documentos_extracao_path)\n",
    "\n",
    "\n",
    "# 5. XXX Ajusta textoYYY\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF_Pesquisavel-  NO CABECALHO\n",
    "def texto_extraido_nf(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "# 6. XXX Ajusta texto para PDF RASTER NO CABECALHO\n",
    "def texto_extraido_cabecalho(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    text_splited = [s.replace(\")\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#1. funcao: find_value_after_keyword_out_frame_up\n",
    "def find_value_after_keyword_out_frame_up(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "        if index + 1 < len(text_list) and text_list[index + 1] not in default_keyword_list:\n",
    "            return text_list[index + 1]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            for default_keyword in default_keyword_list:\n",
    "                if default_keyword in text_list:\n",
    "                    # Caso especial para 'Nome/Razão Social:'\n",
    "                    if keyword == 'Nome/Razão Social:':\n",
    "                        return text_list[0]\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#2. find_value_after_keyword_out_frame_down  \n",
    "def find_value_after_keyword_out_frame_down(keyword, text_list, default_keyword_list=None):\n",
    "    try:\n",
    "        index = text_list.index(keyword)\n",
    "        # Verifica se o índice seguinte está dentro da lista\n",
    "        if index + 1 < len(text_list):\n",
    "            # Verifica se o valor seguinte não é outra keyword da lista default_keyword_list\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        if default_keyword_list:\n",
    "            try:\n",
    "                index = text_list.index(default_keyword_list[-1])\n",
    "                return text_list[index - 1]\n",
    "            except ValueError:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "\n",
    "#3. find_value_after_keyword_fuzz\n",
    "def find_value_after_keyword_fuzz(keyword, text_list, default_keyword_list=None, fuzziness_threshold=80):\n",
    "    closest_match = None\n",
    "    closest_match_score = 0\n",
    "    \n",
    "    for i, text in enumerate(text_list):\n",
    "        score = fuzz.ratio(keyword, text)\n",
    "        \n",
    "        if score > closest_match_score:\n",
    "            closest_match_score = score\n",
    "            closest_match = text\n",
    "        \n",
    "        if closest_match_score > fuzziness_threshold:\n",
    "            break\n",
    "\n",
    "    if closest_match_score > fuzziness_threshold:\n",
    "        index = text_list.index(closest_match)\n",
    "        if index + 1 < len(text_list):\n",
    "            if text_list[index + 1] not in default_keyword_list:\n",
    "                return text_list[index + 1]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pesquisa_keyword(string_pesquisa, text_splited, keyword_list):\n",
    "    resultado_extraido_fuzz = find_value_after_keyword_fuzz(string_pesquisa, text_splited, keyword_list)\n",
    "\n",
    "    if resultado_extraido_fuzz == None:\n",
    "        resultado_extraido_frame_up = find_value_after_keyword_out_frame_up(string_pesquisa, text_splited, keyword_list)\n",
    "        if resultado_extraido_frame_up == None:\n",
    "            resultado_extraido_frame_down = find_value_after_keyword_out_frame_down(string_pesquisa, text_splited, keyword_list)\n",
    "            resultado_extraido = resultado_extraido_frame_down\n",
    "        else:\n",
    "            resultado_extraido = resultado_extraido_frame_up\n",
    "    else:\n",
    "        resultado_extraido = resultado_extraido_fuzz\n",
    "\n",
    "    # Verifica se o resultado extraído é uma das palavras-chave, indicando um erro\n",
    "    if resultado_extraido in keyword_list:\n",
    "        resultado_extraido = None\n",
    "\n",
    "    return resultado_extraido\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# XXX Pequenos mas poderosos\n",
    "def extract_text_PIL(image, coordinates):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    image_croped = image.crop((x0, y0, x1, y1))\n",
    "    texto_extraido = pytesseract.image_to_string(image_croped, lang='por', config='--psm 6')\n",
    "    return texto_extraido \n",
    "\n",
    "\n",
    "# 10. Consulta multiparametros\n",
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "\n",
    "\n",
    "# Funcao importante - process_line\n",
    "def process_line(value, reference, label):\n",
    "    name_match = re.search(fr'{reference} (.+)', value)\n",
    "    if name_match:\n",
    "        extracted_value = reference + \" \" + name_match.group(1)\n",
    "        return {label: extracted_value}\n",
    "    return None\n",
    "\n",
    "# Função para extrair número da string\n",
    "def extract_number(text):\n",
    "    match = re.search(r'\\b\\d+(\\.\\d+)?\\b', text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def corrigir_email(texto):\n",
    "    # Padrão de regex para identificar e-mails potenciais\n",
    "    padrao_email = re.compile(r'[a-zA-Z0-9_.+-]+[)!Q@][a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+')\n",
    "    \n",
    "    # Encontrar todos os padrões que se assemelham a um e-mail\n",
    "    possiveis_emails = padrao_email.findall(texto)\n",
    "    \n",
    "    for email in possiveis_emails:\n",
    "        # Se \"@\" não estiver presente, tentamos corrigir substituindo \")\" ou \"!\" por \"@\"\n",
    "        if \"@\" not in email:\n",
    "            email_corrigido = email.replace(\")\", \"@\").replace(\"Q\", \"@\")\n",
    "            texto = texto.replace(email, email_corrigido)\n",
    "    \n",
    "    return texto\n",
    "\n",
    "\n",
    "\n",
    "# 1 XXX Crio o DF  cnae_x_item_servico_df\n",
    "cnae_x_item_servico_df = pd.read_excel(cnae_dict_path)\n",
    "\n",
    "# Mapeando prefeitura e CNAE para a descrição do CNAE\n",
    "cnae_dict = dict(zip(zip(cnae_x_item_servico_df['PREFE'], cnae_x_item_servico_df['CNA_NUMERO']), cnae_x_item_servico_df['CNA_NOME']))\n",
    "\n",
    "# Mapeando prefeitura e item de serviço para a descrição do item de serviço e o CNAE associado\n",
    "item_servico_dict = dict(zip(zip(cnae_x_item_servico_df['PREFE'], cnae_x_item_servico_df['ATV_CODIGO']), zip(cnae_x_item_servico_df['ATV_DESCRICAO'], cnae_x_item_servico_df['CNA_NUMERO'])))\n",
    "\n",
    "\n",
    "# 2. XXX  Tratando o CNAE com dict criado\n",
    "def processa_cnae_dict(Texto_extraido, de_para_pm, debug):\n",
    "\n",
    "    text_splited = Texto_extraido.split('\\n')\n",
    "    # Processando CNAE\n",
    "    cnae_lines = [line for line in text_splited if 'CNAE' in line]\n",
    "\n",
    "    if cnae_lines:\n",
    "        cnae_line = cnae_lines[0]\n",
    "        #print(f'cnae_line: {cnae_line}')\n",
    "        \n",
    "        cnae_number = int(extract_number(cnae_line))\n",
    "        \n",
    "        cnae_value = cnae_dict.get((de_para_pm, cnae_number),(\"Valor não encontrado\"))\n",
    "        if cnae_value != \"Valor não encontrado\":\n",
    "            cnae_value = cnae_value.upper()\n",
    "            cnae_value = str(cnae_number) + \" - \" + cnae_value\n",
    "            return cnae_value\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        cnae_value = processa_cnae_outros(Texto_extraido)\n",
    "        cnae_number = int(extract_number(cnae_value))\n",
    "\n",
    "        cnae_value = cnae_dict.get((de_para_pm, cnae_number),(\"Valor não encontrado\"))\n",
    "        if cnae_value != \"Valor não encontrado\":\n",
    "            cnae_value = cnae_value.upper()\n",
    "            cnae_value = str(cnae_number) + \" - \" + cnae_value\n",
    "            return cnae_value\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "     \n",
    "\n",
    "# 3. XXX  Tratando Item de Servico com dict criado\n",
    "def processa_itens_servico_dict(Texto_extraido, de_para_pm, debug):\n",
    "    \n",
    "    text_splited = Texto_extraido.split('\\n')\n",
    "    # Encontrando a linha que contém o texto desejado\n",
    "    item_servico_lines = [line for line in text_splited if 'Item da Lista de Serviços' in line]\n",
    "    #print(f'item_servico_lines (fora do if): {item_servico_lines}')\n",
    "    # Verificando se encontramos uma linha válida\n",
    "    if item_servico_lines:\n",
    "        #print(f'item_servico_lines: {item_servico_lines}')\n",
    "        item_servico_line = item_servico_lines[0]\n",
    "        item_servico_cod = float(extract_number(item_servico_line))\n",
    "        item_servico, cnae_associado = item_servico_dict.get((de_para_pm, item_servico_cod), (\"Valor não encontrado\", None))\n",
    "        item_servico = item_servico.upper()\n",
    "        item_servico_value = str(item_servico_cod) + \" - \" + item_servico\n",
    "        return item_servico_value\n",
    "    \n",
    "    else:\n",
    "        #print(\"Linha com 'Item da Lista de Serviços' não encontrada\")\n",
    "        item_servico_line = processa_item_sevico_outros(Texto_extraido)\n",
    "        if item_servico_line:\n",
    "            item_servico_cod = float(extract_number(item_servico_line))\n",
    "            item_servico, cnae_associado = item_servico_dict.get((de_para_pm, item_servico_cod), (\"Valor não encontrado\", None))\n",
    "            item_servico = item_servico.upper()\n",
    "            item_servico_value = str(item_servico_cod) + \" - \" + item_servico\n",
    "            return item_servico_value\n",
    "        \n",
    "        else:\n",
    "            return None\n",
    "        #return None\n",
    "\n",
    "\n",
    "def define_rotulo_acao(nome_arquivo, debug):\n",
    "    \n",
    "    for palavra_chave, rotulo in mapeamento_palavras_chave.items():\n",
    "        if palavra_chave.lower() in nome_arquivo.lower():\n",
    "            break\n",
    "    else:\n",
    "        rotulo = 'prov_nota_fiscal' #\"sem_rotulo\"\n",
    "        palavra_chave = 'default'\n",
    "        acao_sugerida = sugestoes_acao.get(rotulo, 'None')\n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "        # palavra_chave = 'None' #\"sem_palavra_chave\"\n",
    "        # acao_sugerida = 'None' #\"sem_acao_sugerida\"\n",
    "        \n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "        #print(f'nome_arquivo: {nome_arquivo} | rotulo: {rotulo}')\n",
    "    if rotulo != 'None': #\"sem_rotulo\"\n",
    "        acao_sugerida = sugestoes_acao.get(rotulo, 'None') # \"Ação não definida\"\n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "\n",
    "\n",
    "\n",
    "df_id_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b>2.2</b> FunÇoes de Extracao de dados no Pipeline </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================#\n",
    "#                                                                                           #\n",
    "#                           1. PROCESSAMENTO - PDF PESQUISAVEL                              #\n",
    "#                                                                                           #   \n",
    "#===========================================================================================#\n",
    "# XXXpara buscar melhor as coordendas dos FRAMES\n",
    "def get_coordinates_filter(pdf_pesquisavel_map, model, tipo, label, section):\n",
    "    \n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model, type=tipo, label=label, section_json=section)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [((row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p']) if pdf_pesquisavel_map else (row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1']))]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates\n",
    "\n",
    "\n",
    "# 0.A Dados iniciais - PDF PESQUISAVEL\t\n",
    "def pesquisa_prefeitura_pdf_pesquisavel(idx, row, row_info, map_directory, original_file_name, file_path, debug):    \n",
    "    \n",
    "    \n",
    "   # Carregar o arquivo PDF\n",
    "    pdf_document = fitz.open(file_path)\n",
    "\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 4\n",
    "    x1 = 600\n",
    "    y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    \n",
    "    if debug:\n",
    "        print(f'\\ndentro da funçao: pesquisa_prefeitura_pdf_pesquisavel: doc.:{original_file_name} | diretorio: {map_directory}  text: \\n\\n{text}\\n\\n')\n",
    "    \n",
    "    if text:\n",
    "       page_number = 0\n",
    "       #print(page_number)\n",
    "    else:\n",
    "       page_number = 1\n",
    "       #print(page_number)\n",
    "    \n",
    "    pdf_document.close()\n",
    "   \n",
    "    return text\n",
    "\n",
    "# XXX Funcoes de Regex - cabecalho - documento pdf pesquisavel\n",
    "nf_data_servico = {}\n",
    "nf_data_erros = {}\n",
    "nf_lista_erros = []\n",
    "\n",
    "# 0. Pesquisa PDF\n",
    "def is_pdf_searchable(pdf_path):\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        is_searchable = all(page.get_text(\"text\") != \"\" for page in pdf_document)\n",
    "        pdf_document.close()\n",
    "        return is_searchable\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao verificar o PDF: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# 1.A CABECALHO - PDF PESQUISAVEL  \n",
    "def extrai_cabecalho_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "    \n",
    "    nf_data_cabecalho = {}\n",
    "    lista_erros = []\n",
    "    label = \"1_frame_dados_nf\"\n",
    "    \n",
    "    batch_name_row_info = row_info.get('batch')\n",
    "    information_row_info = row_info.get('informations')\n",
    "    action_item_row_info = row_info.get('action_item')\n",
    "    \n",
    "    nf_data_cabecalho['secao'] = section\n",
    "    nf_data_cabecalho['action_item'] = action_item_row_info\n",
    "    nf_data_cabecalho['informations'] = information_row_info\n",
    "    nf_data_cabecalho['processo'] = 'mapeamento regex - PDF pesquisavel'\n",
    "    \n",
    "    if debug:\n",
    "        print(f'\\n\\n2. dentro da funçao extrai_cabecalho_PDF: batch_name: {batch_name_row_info}\\n\\n')\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    tipo = \"frame\"\n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    \n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    if debug:\n",
    "        print(f'\\n3. x0: {x0}, y0: {y0}, x1: {x1}, y1: {y1} f_0: {f_0} f_1: {f_1} | text: \\n{text} \\n\\n')\n",
    "\n",
    "    try:\n",
    "        numero_nota_match = re.search(r'Número da Nota:\\s+(\\d+)', text)\n",
    "        if numero_nota_match:\n",
    "            numero_nf = numero_nota_match.group(1)\n",
    "            nf_data_cabecalho['numero_nota_fiscal'] = numero_nf\n",
    "            #nf_data_cabecalho['informations'] = 'documento com numero de nota fiscal'\n",
    "            if debug:\n",
    "                print(f'\\nnr_nro_nf: {nr_nro_nf} - doc: {original_file_name}\\n')\n",
    "        else:\n",
    "            msg = (f\"Número da Nota não encontrado\")\n",
    "            nf_data_cabecalho['numero_nota_fiscal'] = None\n",
    "            information_row_info = 'Número da Nota não encontrado'\n",
    "            nf_data_cabecalho['informations'] = information_row_info\n",
    "            action_item_row_info = 'BREAK_PROCESS'\n",
    "            nf_data_cabecalho['action_item'] = action_item_row_info\n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} | numero NF nao encontrado {e}\")\n",
    "        nf_data_cabecalho['numero_nota_fiscal'] = None\n",
    "        information_row_info = 'Número da Nota não encontrado'\n",
    "        nf_data_cabecalho['informations'] = information_row_info\n",
    "        action_item_row_info = 'BREAK_PROCESS'\n",
    "        nf_data_cabecalho['action_item'] = action_item_row_info\n",
    "\n",
    "    # Extrair Competência\n",
    "    competencia_match = re.search(r'Competência:\\s+(.+)', text)\n",
    "    if competencia_match:\n",
    "        nf_data_cabecalho['competencia'] = competencia_match.group(1)\n",
    "\n",
    "    # Extrair Data e Hora de Emissão\n",
    "    data_emissao_match = re.search(r'Data e Hora da Emissão:\\s+(.+)', text)\n",
    "    if data_emissao_match:\n",
    "        nf_data_cabecalho['dt_hr_emissao'] = data_emissao_match.group(1)\n",
    "        \n",
    "    # Extrair codigo Verificacao\n",
    "    codigo_verificacao_match = re.search(r'Código Verificação:\\s+(.+)', text)\n",
    "    if codigo_verificacao_match:\n",
    "        codigo_verificacao_nf = codigo_verificacao_match.group(1)\n",
    "        nf_data_cabecalho['codigo_verificacao'] =  codigo_verificacao_nf\n",
    "        tam_codigo_verificacao = len(codigo_verificacao_nf)\n",
    "        nf_data_cabecalho['conf_cod'] = tam_codigo_verificacao\n",
    "        \n",
    "    \n",
    "    \n",
    "    pdf_document.close()\n",
    "    \n",
    "    return nf_data_cabecalho\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2.A PRESTADOR DE SERVIÇO - PDF PESQUISAVEL\n",
    "def extrai_prestador_PDF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "    \n",
    "    nf_data_prestador = {}\n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    label = \"2_frame_cnpj_prestador\"\n",
    "\n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    tipo = \"frame\"\n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    #print(label)\n",
    "    #print(x0,x1,y0,y1)\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    nf_data_prestador = novaextra.extract_fields_prestador(text)\n",
    "\n",
    "    pdf_document.close()\n",
    "\n",
    "    return nf_data_prestador \n",
    "\n",
    "  \n",
    "# 3.A. TOMADOR DE SERVIÇO - PDF PESQUISAVEL\n",
    "def extrai_tomador_PDF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "\n",
    "    nf_data_tomador = {}\n",
    "    section = \"3. TOMADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = True\n",
    "\n",
    "    label = \"3_frame_cnpj_tomador\"\n",
    "\n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    tipo = \"frame\"\n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    #print(label)\n",
    "    #print(x0,x1,y0,y1)\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    nf_data_tomador = novaextra.extract_fields_tomador(text)\n",
    "           \n",
    "\n",
    "    pdf_document.close()\n",
    "\n",
    "    return nf_data_tomador \n",
    "\n",
    "\n",
    "# 4.A DESCRIMINACAO DOS SERVIÇOS - PDF PESQUISAVEL\n",
    "def processar_servicos_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "    \n",
    "    section = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "    frame_type = 'frame'\n",
    "    f_frame_label = \"4_frame_descricao_totais\"\n",
    "    data_box_valores = {'secao': section}\n",
    "    nf_data_servico = {}\n",
    "    message_erro = []\n",
    "    nf_data_servico['original_file_name'] = original_file_name\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    filtered_frames_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == frame_type) & (frames_nf_v4_df['label'] == f_frame_label)]\n",
    "    \n",
    "    for index_frame, row_frame in filtered_frames_nf_v4_df.iterrows():\n",
    "        frame_id = row_frame['id']\n",
    "        label = row_frame['label']\n",
    "        x0, y0, x1, y1 = (row_frame['x0_p'], row_frame['y0_p'], row_frame['x1_p'], row_frame['y1_p']) if pdf_pesquisavel_map else (row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1'])\n",
    "        #print(f'model: {model_map} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: {label:>30} | id: {frame_id:>3} | x0: {x0:>6} y0: {y0:>6} x1: {x1:>6} y1: {y1:>6} ')\n",
    "        \n",
    "        pdf_document = fitz.open(file_path)\n",
    "        # Página do PDF\n",
    "        page_number = 0  # Defina o número da página que deseja analisar\n",
    "        page = pdf_document[page_number]\n",
    "        #x0, y0, x1, y1 = (0, 0, 600, 110)\n",
    "        text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "        \n",
    "        # Remover quebras de linha e rótulo\n",
    "        text = text.replace('\\n', ' ')\n",
    "        label = \"DISCRIMINAÇÃO DOS SERVIÇOS\"\n",
    "        if text.startswith(label):\n",
    "            text = text[len(label):].strip()\n",
    "\n",
    "\n",
    "        try:\n",
    "            discrimanacao_servico = text   \n",
    "        except Exception as e:\n",
    "            msg = (f\"doc: | {e}\")\n",
    "            discrimanacao_servico = \"Descricao nao encontrada\"\n",
    "\n",
    "        # Atribuir texto ao dicionário\n",
    "        nf_data_servico['discriminacao_servicos'] = discrimanacao_servico\n",
    "        \n",
    "        pdf_document.close()\n",
    "        \n",
    "    return nf_data_servico \n",
    "            \n",
    "                \n",
    "# 5.A VALOR TOTAL - PDF PESQUISAVEL\n",
    "def processar_valor_total_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "    \n",
    "    #nf_data_valor_total = {}\n",
    "    \n",
    "    process = ['4_frame_valor_total']\n",
    "    \n",
    "    #nf_data_valor_total['secao'] = section\n",
    "    pdf_pesquisavel_map = True\n",
    "        \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    valor_total_nf = 0.0\n",
    "        \n",
    "    tipo= \"frame\"\n",
    "    message_erro = []\n",
    "    #nf_dados_prestador = {}\n",
    "    # model = row['model']\n",
    "    for father in process:\n",
    "        label = father\n",
    "        if label == \"4_frame_valor_total\":\n",
    "            try: \n",
    "                #print(model_map)\n",
    "                coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "                #print(f'model: {model_map} {\"pdf_pesquisavel\" if pdf_pesquisavel_map else \"raster_pdf\"}: labe: {label:>30} || x0:{x0:>6} | y0:{y0:>6} | x1:{x1:>6} | y1:{y1:>6}\\n {text}')\n",
    "                valor_total_match = re.search(r'R\\$ ([\\d,.]+)', text)\n",
    "                if valor_total_match:\n",
    "                    valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "                    if debug:\n",
    "                        print(f'valor_total_sem_formatacao: {valor_total_sem_formatacao}')\n",
    "                    valor_total_nf = float(valor_total_sem_formatacao)\n",
    "                    #nf_data_valor_total['valor_total_nota'] = nf_data_valor_total\n",
    "                    if debug:\n",
    "                        print(f'valor_total_nota: {valor_total_nota}')\n",
    "            except Exception as e:\n",
    "                msg = (f\"doc: | {e}\")\n",
    "                valor_total_nf = 0.0\n",
    "                # nf_data_valor_total['valor_total_nota'] = 0.0\n",
    "                logging.error(f\" {batch_name} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")\n",
    "                \n",
    "    pdf_document.close()            \n",
    "                \n",
    "    return valor_total_nf  \n",
    "\n",
    "\n",
    "# 6.A CNAE e Item da Lista de Serviços\n",
    "def extrai_consiste_cnae_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_pm, model_map, f_0_cnae, f_1_cnae, f_0_it, f_1_it, original_file_name, file_path, debug):\n",
    "\n",
    "    nf_data_CNAE = {}\n",
    "    message_erro = []\n",
    "    cnae_value = None\n",
    "    item_servico_value = None\n",
    "    \n",
    "    nf_data_CNAE['Secao'] = section\n",
    "    nf_data_CNAE['texto_cnae'] = None\n",
    "    nf_data_CNAE['texto_item_servico'] = None\n",
    "    nf_data_CNAE['original_file_name'] = original_file_name\n",
    "    \n",
    "    process = [\"cnae\", 'item_lista_servicos']\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    tipo = \"sframe_field\"\n",
    "    \n",
    "    #print(f'\\n item: {original_file_name}\\n')\n",
    "    # model = row['model']\n",
    "    for father in process:\n",
    "        cnae_value = None\n",
    "        item_servico_value = None\n",
    "        label = father\n",
    "        if label == \"cnae\": \n",
    "            tipo = \"sframe_field\"\n",
    "            coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            #print(f'1.A TRATAMENTO CNAE : (coordenadas originais):  x0:{x0} y0:{y0} x1:{x1} y1:{y1}')\n",
    "            #print(f'1. label: {label} | coordenadas originais: x0:{x0} y0:{y0} x1:{x1} y1:{y1} item:')\n",
    "            y0 = y0 * f_0_cnae\n",
    "            y1 = y1 * f_1_cnae\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            nf_data_CNAE['texto_cnae'] = text\n",
    "            #print(f'1.A TRATAMENTO CNAE - texto extraido em  coordenadas: x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0_cnae: {f_0_cnae}, f_1_cnae: {f_1_cnae} TEXT:\\n{text}\\n')\n",
    "            \n",
    "            # if debug:\n",
    "            print(f'1.A TRATAMENTO CNAE - texto extraido em  coordenadas: x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0_cnae: {f_0_cnae}, f_1_cnae: {f_1_cnae} TEXT:\\n{text}\\n')\n",
    "            cnae_processado = processa_cnae_dict(text, de_para_pm, debug)\n",
    "            # if debug:\n",
    "            print(f'\\n1.B CNAE PROCESSADO: {cnae_processado}\\n')\n",
    "            \n",
    "        elif label == \"item_lista_servicos\":\n",
    "            tipo = \"sframe_field\"\n",
    "            #print(model_map)\n",
    "            coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            #print(f'2.A TRATAMENTO ITENS DE SERVICO:(coordenadas originais):  x0:{x0} y0:{y0} x1:{x1} y1:{y1}')\n",
    "            y0 = y0 * f_0_it\n",
    "            y1 = y1 * f_1_it\n",
    "            text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "            nf_data_CNAE['texto_item_servico'] = text\n",
    "            # if debug:\n",
    "            print(f'2.A TRATAMENTO ITENS DE SERVICO -  texto extraido em  coordenadas originais: x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0_it:{f_0_it}, f_1_it: {f_1_it} TEXT:\\n{text}\\n')\n",
    "            #print(text)\n",
    "            item_servico_processado = processa_itens_servico_dict(text, de_para_pm, debug)\n",
    "            # if debug:\n",
    "            print(f' 2.B ITEM DE SERVICO PROCESSADO: {item_servico_processado} \\n\\n')\n",
    "\n",
    "    pdf_document.close()\n",
    "    \n",
    "    return cnae_processado, item_servico_processado, nf_data_CNAE\n",
    "\n",
    "\n",
    "# 6.A XXX Funcao generica de extracao - CNAE_ITEM - PDF_PESQUISAVEL\n",
    "def extracao_documento_CNAE_ITEM_PDF_P(idx, row, row_info, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, def_replace, original_file_name, file_path, debug):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "\n",
    "\n",
    "    # busco coordenadas para o contexto\n",
    "    if mapping_method == \"frame_&_sframe_field\":\n",
    "        tipo_4_coordinates = \"frame\"\n",
    "        tipo_4_filter = \"sframe_field\"\n",
    "        \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]     \n",
    "\n",
    "    coordinates = get_coordinates_filter_by_context(pdf_pesquisavel_map, model_map, context_mapping, tipo_4_coordinates)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    print(x0, y0, x1, y1)  \n",
    "\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    text_splited = text.split('\\n')\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "\n",
    "    # 8. Efetuo o filtro para a iteracao\n",
    "    filtered_frame_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['context_mapping'] == context_mapping) & (frames_nf_v4_df['type'] == tipo_4_filter)]\n",
    "\n",
    "    # 9. iter sobre o filtro\n",
    "    for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "        section = row_frame['section_json']\n",
    "        label = row_frame['label']\n",
    "        reference = row_frame['reference']\n",
    "        marcador_inicio = row_frame['marcador_inicio']\n",
    "        marcador_fim = row_frame['marcador_fim']\n",
    "        texto_extraido = extrair_texto_entre_marcadores(text_splited, marcador_inicio, marcador_fim)\n",
    "        if texto_extraido:\n",
    "            texto_extraido = texto_extraido.replace(marcador_inicio, \"\").strip()\n",
    "            data_box_valores[label] = texto_extraido\n",
    "            # print(texto_extraido)\n",
    "            # print()\n",
    "        else:\n",
    "            texto_completo = \" \".join(text_splited)\n",
    "            texto_pesquisa = encontrar_texto_fuzzy_marcador_inicial(texto_completo, marcador_inicio, marcador_fim) \n",
    "            if texto_pesquisa:\n",
    "                texto_pesquisa = texto_pesquisa.replace(marcador_inicio, \"\").strip()\n",
    "                data_box_valores[label] = texto_pesquisa\n",
    "                # print(texto_pesquisa)\n",
    "                # print()\n",
    "            else:\n",
    "                texto_pesquisa2 = encontrar_texto_fuzzy_2_marcadores(texto_completo, marcador_inicio, marcador_fim)\n",
    "                if texto_pesquisa2:\n",
    "                    texto_pesquisa2 = texto_pesquisa2.replace(marcador_inicio, \"\").strip()\n",
    "                    data_box_valores[label] = texto_pesquisa2\n",
    "    \n",
    "    pdf_document.close()  \n",
    "          \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "# 6.A.1 CNAE Outros\n",
    "def processa_cnae_outros(text):\n",
    "    \n",
    "    nf_data_CNAE_match = re.search(r'CNAE\\s+(.+)', text)\n",
    "    if nf_data_CNAE_match:\n",
    "        try:\n",
    "            # Remove a primeira ocorrência de \"CNAE:\"\n",
    "            nf_data_CNAE_str = re.sub(r'^CNAE - ', '', text, count=1)\n",
    "            # Remover quebras de linha\n",
    "            nf_data_CNAE_str = nf_data_CNAE_str.replace('\\n', ' ')\n",
    "            return nf_data_CNAE_str \n",
    "        except Exception as e:\n",
    "            print(f\"Erro busca cnae: {e}\") \n",
    "        \n",
    "    return None \n",
    "\n",
    "# 6.A.1 CNAE e Item da Lista de Serviços\n",
    "def processa_item_sevico_outros(text):\n",
    "\n",
    "    nf_item_lista_servicos_match = re.search(r'Item da Lista de Serviços\\s+(.+)', text)\n",
    "    if nf_item_lista_servicos_match:\n",
    "        try:\n",
    "            # Remove a primeira ocorrência de \"Item de Servico:\"\n",
    "            nf_item_lista_servicos_str = re.sub(r'^Item da Lista de Serviços - ', '', text, count=1) \n",
    "            # Remover quebras de linha\n",
    "            #nf_item_lista_servicos_str = nf_item_lista_servicos_str.replace('\\n \\n', '')\n",
    "            nf_item_lista_servicos_str = nf_item_lista_servicos_str.replace('\\n', ' ')\n",
    "            return nf_item_lista_servicos_str\n",
    "        except Exception as e:\n",
    "            print(f\"Erro busca cnae: {e}\")\n",
    "            \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# 7.A VALORES E IMPOSTOS - PDF Pesquisavel\n",
    "def extrai_valores_impostos_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "\n",
    "    nf_data_valores = {}\n",
    "    \n",
    "    \n",
    "    f_frame_label = \"5_frame_valores_impostos\"\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    tipo = \"frame\"\n",
    "    message_erro = []\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number] \n",
    "    \n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=f_frame_label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    y0_tpt = y0\n",
    "    y1_tpt = y1\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    #print(f'Importos: labe: {label:>30} | template:  x0:{x0:>6} | y0:{y0_tpt:>6} | x1:{x1:>6} | y1:{y1_tpt:>6} | ajuste: y0: {y0} | y1: {y1} \\n {text}')\n",
    "    \n",
    "    nf_data_valores = novaextra.extract_fields_impostos(text)\n",
    "    \n",
    "  \n",
    "    \n",
    "    #print(text)\n",
    "    \n",
    "    pdf_document.close()\n",
    "    \n",
    "    return  nf_data_valores\n",
    "    \n",
    "\n",
    "# 8.A DADOS COMPLEMENTARES - PDF Pesquisavel\n",
    "def extrai_dados_complementares_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "    \n",
    "    \n",
    "    f_frame_label = \"5_frame_dados_complementares\"\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    tipo = \"frame\"\n",
    "    message_erro = []\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number] \n",
    "    \n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=f_frame_label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    y0_tpt = y0\n",
    "    y1_tpt = y1\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    #print(f'Considerar: label: {f_frame_label:>30} | templt.x0:{x0:>6} | y0:{y0_tpt:>6} | x1:{x1:>6} | y1:{y1_tpt:>6} | ajuste: y0: {y0} | y1: {y1} \\n {text}')\n",
    "    \n",
    "    nf_data_dados_complementares = {}\n",
    "    nf_data_dados_complementares['secao'] = \"8. DADOS COMPLEMENTARES\"\n",
    "\n",
    "    text = re.sub(r'^DADOS COMPLEMENTARES', '', text, count=1)\n",
    "    if text == \"\":\n",
    "        text = \"None\"\n",
    "        nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "    else:    \n",
    "        # Extrair texto dentro do retângulo\n",
    "        nf_data_dados_complementares['dados_complementares'] = text.strip()\n",
    "    \n",
    "    pdf_document.close()\n",
    "                        \n",
    "    return nf_data_dados_complementares  \n",
    "\n",
    "\n",
    "# 9.A OUTRAS INFORMAÇOES / CRITICAS - PDF Pesquisavel \n",
    "def extrai_outras_informacoes_PDF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, map_original_file_name, file_path):\n",
    "    \n",
    "    nf_data_outras_informacoes = {}\n",
    "    section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "    f_frame_label = \"5_frame_inf_criticas\"\n",
    "    pdf_pesquisavel_map = True\n",
    "    \n",
    "    tipo = \"frame\"\n",
    "    message_erro = []\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number] \n",
    "    \n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=f_frame_label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    y0_tpt = y0\n",
    "    y1_tpt = y1\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    #print(f'Considerar: label: {f_frame_label:>30} | templt.x0:{x0:>6} | y0:{y0_tpt:>6} | x1:{x1:>6} | y1:{y1_tpt:>6} | ajuste: y0: {y0} | y1: {y1} \\n {text}')\n",
    "    nf_data_outras_informacoes = novaextra.extract_fields_outras_info(text)\n",
    "    \n",
    "    pdf_document.close()\n",
    "    \n",
    "    return nf_data_outras_informacoes       \n",
    "\n",
    "    \n",
    " \n",
    "# 10.A OBSERVACOES  - PDF Pesquisavel\n",
    "def extrai_outras_informacoes_PDF_P(row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path):\n",
    "    \n",
    "    nf_data_observacao   = {}\n",
    "    section = \"10. OBSERVACOES\"\n",
    "    f_frame_label = \"5_frame_observacao\"\n",
    "    pdf_pesquisavel_map = True\n",
    "\n",
    "    tipo = \"frame\"\n",
    "    message_erro = []\n",
    "\n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number] \n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=f_frame_label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    y0_tpt = y0\n",
    "    y1_tpt = y1\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    \n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    #print(f'Considerar: label: {f_frame_label:>30} | templt.x0:{x0:>6} | y0:{y0_tpt:>6} | x1:{x1:>6} | y1:{y1_tpt:>6} | ajuste: y0: {y0} | y1: {y1} \\n {text}')\n",
    "    # Remove a primeira ocorrência de \"Observação:\"\n",
    "    text = re.sub(r'^Observação:', '', text, count=1)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    nf_data_observacao['observacao'] = text.strip()\n",
    "\n",
    "\n",
    "    pdf_document.close()\n",
    "    \n",
    "    return nf_data_observacao  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#===========================================================================================#\n",
    "#                                                                                           #\n",
    "#                           2. PROCESSAMENTO - RASTER PDF                                   #\n",
    "#                                                                                           #   \n",
    "#===========================================================================================#\n",
    "\n",
    "# funçao importante para buscar coordenadas do frame em funçao do contexto\n",
    "def get_coordinates_filter_by_context(pdf_pesquisavel_map, model_map, context_mapping, tipo):\n",
    "    \n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model_map, context_mapping=context_mapping, type=tipo)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [((row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p']) if pdf_pesquisavel_map else (row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1']))]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates\n",
    "\n",
    "# REAVALIAR TODAS - USAR get_coordinates_filter()\n",
    "def get_coordinates_filter_R_PDF(model_map, tipo, label, section):\n",
    "    \n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model_map, type=tipo, label=label, section_json=section)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [(row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1'])]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates\n",
    "\n",
    "\n",
    "def get_coordinates_filter_pdf_pesquisavel(model_map, tipo, label, section):\n",
    "    \n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model_map, type=tipo, label=label, section_json=section)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [(row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p'])]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates\n",
    "\n",
    "\n",
    "# 0. INFOMACOES INICIAIS - RASTER PDF\n",
    "def processar_dados_iniciais(idx, row, row_info, section, map_directory, original_file_name, file_path, debug):\n",
    "    \n",
    "    # lista_texto_extraido = []\n",
    "\n",
    "    nf_dados_doc = {}\n",
    "    nf_dados_doc['secao'] = section\n",
    "    pdf_pesquisavel = None\n",
    "    extracted_txt = pesquisa_prefeitura_pdf_pesquisavel(idx, row, row_info, map_directory, original_file_name, file_path, debug)\n",
    "    if debug:\n",
    "        print(f'\\n1. funcao: processar_dados_iniciais: doc.:{original_file_name} | diretorio: {map_directory} apos funcao: pesquisa_prefeitura_pdf_pesquisavel: extracted_txt:\\n{extracted_txt}\\n\\n')\n",
    "    \n",
    "    if extracted_txt:\n",
    "        pdf_pesquisavel = True\n",
    "    else:\n",
    "        pdf_pesquisavel = False \n",
    "       \n",
    "        x0 = 220\n",
    "        y0 = 0\n",
    "        x1= 3858\n",
    "        y1 = 1572\n",
    "        \n",
    "        # usando novo processo que gera o arquivo \"on the fly\" imagem_gray (converte PDF para imagem de tamanho grande (4134, 5846) - torna-a cinza e a salva)\n",
    "        imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "        extracted_txt = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "        if debug:\n",
    "            print(f'\\n2. funcao: processar_dados_iniciaisdoc.:{original_file_name} | diretorio: {map_directory}  apos : extract_text_PIL: extracted_txt:\\n{extracted_txt}\\n\\n')\n",
    "    \n",
    "    nf_dados_doc['file_name'] = original_file_name    \n",
    "    nf_dados_doc['pdf_pesquisavel'] = pdf_pesquisavel \n",
    "    value = {}   \n",
    "    texto_tratado = texto_extraido(extracted_txt)\n",
    "    value = define_dados_iniciais(idx, row, row_info, texto_tratado, debug)\n",
    "    if debug:\n",
    "        print(f'\\n3. funcao: processar_dados_iniciais doc.:{original_file_name} | diretorio: {map_directory} | apos funcao: define_dados_iniciais() value \\n{value}\\n\\n')\n",
    "    if value:\n",
    "        nf_dados_doc.update(value)\n",
    "   \n",
    "\n",
    "\n",
    "    return nf_dados_doc\n",
    "\n",
    "\n",
    "# 1.B CABECALHO XXX Funcoes de extracao -cabecalho Raster\n",
    "def processar_cabecalho_R_PDF(idx, row, row_info, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "    \n",
    "    data_box_valores = {}\n",
    "    data_box_conferencia = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    \n",
    "    batch_name_row_info = row_info.get('batch')\n",
    "    #status_documento_row_info = row_info.get('status_documento')\n",
    "    information_row_info = row_info.get('informations')\n",
    "    action_item_row_info = row_info.get('action_item')\n",
    "    \n",
    "    # Busco a imagem np do documento\n",
    "    image_np_row_info = row_info.get('image_np')\n",
    "    \n",
    "    data_box_valores['action_item'] = action_item_row_info\n",
    "    data_box_valores['informations'] = information_row_info\n",
    "    data_box_valores['processo'] = context_mapping\n",
    "    data_box_valores['conf_cod'] = 0\n",
    "\n",
    "\n",
    "                     \n",
    "    \n",
    "    # busco coordenadas para o contexto\n",
    "    if mapping_method == \"frame_&_sframe_field\":\n",
    "        tipo_4_coordinates = \"frame\"\n",
    "        tipo_4_filter = \"sframe_field\"\n",
    "    \n",
    "   \n",
    "    # 2. usando a funcao de extracao de coordenadas por contexto    \n",
    "    coordinates = get_coordinates_filter_by_context(pdf_pesquisavel_map, model_map, context_mapping, tipo_4_coordinates)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    x0 = int(x0)\n",
    "    y0 = int(y0)\n",
    "    x1 = int(x1)\n",
    "    y1 = int(y1) \n",
    "    # 3. Cropo a imagem - novo modelo\n",
    "    cropped_image_np = image_np_row_info[y0:y1, x0:x1] # ajustar nos demais\n",
    "    data_box_conferencia[f'box_{context_mapping}'] = cropped_image_np\n",
    "    data_box_conferencia[f'coordinates_{context_mapping}'] = coordinates\n",
    "    # 4. Converto para PIL\n",
    "    cropped_image_pil = Image.fromarray(cropped_image_np)\n",
    "    # 6. Executo OCR\n",
    "    texto_extraido = pytesseract.image_to_string(cropped_image_pil, lang='por')\n",
    "    # 7. Trato o texto extraido = text_splited\n",
    "    text_splited = texto_extraido_cabecalho(texto_extraido)\n",
    "    if debug:\n",
    "        print()\n",
    "        plt.imshow(cropped_image_np)\n",
    "        plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "        plt.show()\n",
    "        print(f'\\ncoordinates {coordinates} - \\ntexto_extraido:\\n{text_splited}\\n')\n",
    "        \n",
    "    # 8. Efetuo o filtro para a iteracao\n",
    "    filtered_frame_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['context_mapping'] == context_mapping) & (frames_nf_v4_df['type'] == tipo_4_filter)]\n",
    "    \n",
    "    # 9. iter sobre o filtro\n",
    "    for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "        try:\n",
    "            section = row_frame['section_json']\n",
    "            label = row_frame['label']\n",
    "            reference = row_frame['reference']\n",
    "            string_pesquisa = row_frame['marcador_inicio']  \n",
    "            keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_box_valores[label] = texto\n",
    "            if debug:\n",
    "               print(f'\\nidx: {index_frame:> 3} | label: {label} |  string_pesquisa:{string_pesquisa} | dentro do try do raster PDF cabecalho - texto: \\n{texto}\\n\\n')\n",
    "        except Exception as e:\n",
    "            msg = (f\"{e}\")\n",
    "            data_box_conferencia[label] = msg\n",
    "    \n",
    "\n",
    "    # Verificações após o loop\n",
    "    for key, value in data_box_valores.items():\n",
    "        if key == 'numero_nota_fiscal' and value is None:\n",
    "            action_item_row_info = 'BREAK_PROCESS'\n",
    "            information_row_info = 'Número da Nota não encontrado'\n",
    "            #logging.error(f\" {batch_name} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")  # Ou registre o erro de outra forma que preferir\n",
    "        \n",
    "        elif key == 'codigo_verificacao' and value != None:\n",
    "            codigo_verificacao_nf = value\n",
    "            tam_codigo_verificacao = len(codigo_verificacao_nf)\n",
    "            data_box_valores['conf_cod'] = tam_codigo_verificacao\n",
    "            \n",
    "        \n",
    "        elif key != 'numero_nota_fiscal' and value is None:\n",
    "            logging.error(f\" {batch_name_row_info} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")  # Ou registre o erro de outra forma que preferir\n",
    "\n",
    "            \n",
    "      # if value is None:\n",
    "        #     logging.error(f\" {batch_name} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")  # Ou registre o erro de outra forma que preferir\n",
    "\n",
    "    data_box_valores['action_item'] = action_item_row_info\n",
    "    data_box_valores['informations'] = information_row_info\n",
    "\n",
    "    \n",
    "    return data_box_valores\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# 2.B PRESTADOR DE SERVIÇO - RASTER_PDF\n",
    "def extrai_prestador_R_PDF(idx, row, row_info, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "    \n",
    "    nf_data_prestador = {}\n",
    "    data_box_conferencia = {}\n",
    "    dic_erros = {}\n",
    "    message_erro = []\n",
    "    section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = False\n",
    "    nf_data_prestador['secao'] = \"2. PRESTADOR DE SERVIÇO\"\n",
    "    # 1 busco o documento - image_np\n",
    "    image_np = row_info['image_np']\n",
    "    batch_name = row_info['batch']\n",
    "\n",
    "    process = ['2_frame_cnpj_prestador', '2_frame_inscricao_prestador', '2_frame_dados_prestador']\n",
    "    tipo = \"frame\"\n",
    "\n",
    "    # usando novo processo que gera o arquivo \"on the fly\" imagem_gray (converte PDF para imagem de tamanho grande (4134, 5846) - torna-a cinza e a salva)\n",
    "    imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "\n",
    "    for label in process:\n",
    "        seq = process.index(label) + 1\n",
    "        if label == \"2_frame_cnpj_prestador\":\n",
    "            coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            x0 = int(x0)\n",
    "            y0 = int(y0)\n",
    "            x1 = int(x1)\n",
    "            y1 = int(y1)\n",
    "            y0 = y0 * f_0\n",
    "            y1 = y1 * f_1 \n",
    "            # 3. Cropo a imagem - novo modelo\n",
    "            cropped_image_np = image_np[y0:y1, x0:x1]\n",
    "            data_box_conferencia[f'box_{label}'] = cropped_image_np\n",
    "            data_box_conferencia[f'coordinates_{label}'] = coordinates\n",
    "            # 4. Converto para PIL\n",
    "            cropped_image_pil = Image.fromarray(cropped_image_np)\n",
    "            # 6. Executo OCR\n",
    "            texto_extraido = pytesseract.image_to_string(cropped_image_pil, lang='por')\n",
    "            text_splited = texto_extraido.split('\\n')\n",
    "            if debug:\n",
    "                plt.imshow(cropped_image_np)\n",
    "                plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "                plt.show()\n",
    "                print(f'\\ncoordinates {coordinates} - \\ntexto_extraido:\\n{text_splited}\\n')\n",
    "            #print(f'1. : (coordenadas originais):  x0:{x0} y0:{y0} x1:{x1} y1:{y1}')\n",
    "\n",
    "            #print(f'2.A   coordenadas ajustadas:   x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0:{f_0}, f_1: {f_1}\\n')\n",
    "            # texto_extraido = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "            \n",
    "            text_splited = [x for x in text_splited if x.strip()]\n",
    "            keyword_list = ['CPF/CNPJ:', 'Telefone:']\n",
    "            string_pesquisa = \"CPF/CNPJ:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', texto)\n",
    "            if cpf_cnpj_formatado_match:\n",
    "                prestador_cpf_cnpj_com_mascara = cpf_cnpj_formatado_match.group(1)\n",
    "                nf_data_prestador['p_cpf_cnpj_com_mascara'] = prestador_cpf_cnpj_com_mascara\n",
    "                prestador_cpf_cnpj_sem_mascara = re.sub(r'\\D', '', cpf_cnpj_formatado_match.group(1))\n",
    "                nf_data_prestador['p_cpf_cnpj_sem_mascara'] = prestador_cpf_cnpj_sem_mascara\n",
    "            else:\n",
    "                cpf_cnpj_com_mascara = None\n",
    "                cpf_cnpj_sem_mascara = None\n",
    "            try:    \n",
    "                string_pesquisa = \"Telefone:\"  \n",
    "                texto_tel = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', texto_tel) \n",
    "                if telefone_match: \n",
    "                    prestador_telefone_str = telefone_match.group(1)\n",
    "                else:\n",
    "                    telefone_match = re.search(r'Telefone:\\s+([0-9.\\s-]+)', texto)\n",
    "                    if telefone_match: \n",
    "                        prestador_telefone_str = telefone_match.group(1)\n",
    "                        nf_data_prestador['p_telefone'] = prestador_telefone_str \n",
    "                    else:\n",
    "                        nf_data_prestador['p_telefone'] = texto_tel\n",
    "            except Exception as e:\n",
    "                nf_data_prestador['p_telefone'] = None\n",
    "                new_row = {\n",
    "                    \"row_index\": idx,  # Substitua 'index' pela variável que contém o índice da linha atual\n",
    "                    \"erro_inscricao\": str(e),\n",
    "                    \"file\": original_file_name,\n",
    "                    \"process_label\": label\n",
    "                }              \n",
    "        \n",
    "        elif label == \"2_frame_inscricao_prestador\":\n",
    "            coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            x0 = int(x0)\n",
    "            y0 = int(y0)\n",
    "            x1 = int(x1)\n",
    "            y1 = int(y1)\n",
    "            y0 = y0 * f_0\n",
    "            y1 = y1 * f_1 \n",
    "            # 3. Cropo a imagem - novo modelo\n",
    "            cropped_image_np = image_np[y0:y1, x0:x1]\n",
    "            data_box_conferencia[f'box_{label}'] = cropped_image_np\n",
    "            data_box_conferencia[f'coordinates_{label}'] = coordinates\n",
    "            # 4. Converto para PIL\n",
    "            cropped_image_pil = Image.fromarray(cropped_image_np)\n",
    "            # 6. Executo OCR\n",
    "            texto_extraido = pytesseract.image_to_string(cropped_image_pil, lang='por')\n",
    "            text_splited = texto_extraido.split('\\n')\n",
    "            if debug:\n",
    "                plt.imshow(cropped_image_np)\n",
    "                plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "                plt.show()\n",
    "                print(f'\\ncoordinates {coordinates} - \\ntexto_extraido:\\n{text_splited}\\n')\n",
    "            # y0 = y0 * f_0\n",
    "            # y1 = y1 * f_1\n",
    "            # texto_extraido = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "            # text_splited = texto_extraido.split('\\n')\n",
    "            text_splited = [x for x in text_splited if x.strip()]\n",
    "            keyword_list = ['Inscrição Municipal:', 'Inscrição Estadual:']\n",
    "            string_pesquisa = \"Inscrição Municipal:\"\n",
    "            prestadpor_inscricao_municipal = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            nf_data_prestador['p_inscricao_municipal'] = prestadpor_inscricao_municipal\n",
    "            \n",
    "            string_pesquisa = \"Inscrição Estadual:\"\n",
    "            prestador_inscricao_estadual = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            nf_data_prestador['p_inscricao_estadual'] = prestador_inscricao_estadual                    \n",
    "\n",
    "        elif label == \"2_frame_dados_prestador\":\n",
    "            coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            x0 = int(x0)\n",
    "            y0 = int(y0)\n",
    "            x1 = int(x1)\n",
    "            y1 = int(y1)\n",
    "            y0 = y0 * f_0\n",
    "            y1 = y1 * f_1 \n",
    "            # 3. Cropo a imagem - novo modelo\n",
    "            cropped_image_np = image_np[y0:y1, x0:x1]\n",
    "            data_box_conferencia[f'box_{label}'] = cropped_image_np\n",
    "            data_box_conferencia[f'coordinates_{label}'] = coordinates\n",
    "            # 4. Converto para PIL\n",
    "            cropped_image_pil = Image.fromarray(cropped_image_np)\n",
    "            # 6. Executo OCR\n",
    "            texto_extraido = pytesseract.image_to_string(cropped_image_pil, lang='por')\n",
    "            text_splited = texto_extraido.split('\\n')\n",
    "            if debug:\n",
    "                plt.imshow(cropped_image_np)\n",
    "                plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "                plt.show()\n",
    "                print(f'\\ncoordinates {coordinates} - \\ntexto_extraido:\\n{text_splited}\\n')\n",
    "            #print(f'1. : (coordenadas originais):  x0:{x0} y0:{y0} x1:{x1} y1:{y1}')\n",
    "            # y0 = y0 * f_0\n",
    "            # y1 = y1 * f_1\n",
    "            # #print(f'2.A   coordenadas ajustadas:   x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0:{f_0}, f_1: {f_1}\\n')\n",
    "            # texto_extraido = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "            # text_splited = texto_extraido.split('\\n')\n",
    "            text_splited = [x for x in text_splited if x.strip()]\n",
    "            \n",
    "            keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "            string_pesquisa = \"Nome/Razão Social:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            prestador_razao_social = texto\n",
    "            nf_data_prestador['p_razao_social'] = prestador_razao_social\n",
    "\n",
    "            string_pesquisa = \"Nome de Fantasia:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            prestador_nome_fantasia = texto\n",
    "            nf_data_prestador['p_nome_fantasia'] = prestador_nome_fantasia\n",
    "            \n",
    "            string_pesquisa = \"Endereço:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            prestador_endereco = texto\n",
    "            nf_data_prestador['p_endereco'] = prestador_endereco\n",
    "            \n",
    "            string_pesquisa = \"E-mail:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            prestador_email = texto\n",
    "            if prestador_email and prestador_email != None:\n",
    "                prestador_email = corrigir_email(prestador_email)\n",
    " \n",
    "            nf_data_prestador['p_email'] = prestador_email\n",
    "            \n",
    "    return nf_data_prestador  \n",
    "\n",
    "# 3.A. TOMADOR DE SERVIÇO - RASTER PDF\n",
    "def extrai_tomador_R_PDF(idx, row, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "    \n",
    "    nf_data_tomador = {}\n",
    "    section = \"3. TOMADOR DE SERVIÇO\"\n",
    "    pdf_pesquisavel_map = False\n",
    "    cpf_cnpj_tomador = {}\n",
    "    \n",
    "    dic_erros = {}\n",
    "    message_erro = []\n",
    "\n",
    "    nf_data_tomador['secao'] = section\n",
    "\n",
    "    process = ['3_frame_cnpj_tomador', '3_frame_inscricao_tomador', '3_frame_dados_tomador']\n",
    "    tipo = \"frame\"\n",
    "\n",
    "    # usando novo processo que gera o arquivo \"on the fly\" imagem_gray (converte PDF para imagem de tamanho grande (4134, 5846) - torna-a cinza e a salva)\n",
    "    imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "    \n",
    "    i = 1\n",
    "    for label in process:\n",
    "        seq = process.index(label) + 1\n",
    "        if label == \"3_frame_cnpj_tomador\":\n",
    "            \n",
    "            try:\n",
    "                coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "                x0, y0, x1, y1 = coordinates[0]\n",
    "                #print(f'1. : (coordenadas originais):  x0:{x0} y0:{y0} x1:{x1} y1:{y1}')\n",
    "                y0 = y0 * f_0\n",
    "                y1 = y1 * f_1\n",
    "                #print(f'2.A   coordenadas ajustadas:   x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0:{f_0}, f_1: {f_1}\\n')\n",
    "                texto_extraido = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "                texto_tomador_cnpj = texto_extraido\n",
    "                file_name = row['original_file_name']\n",
    "                #print(f'\\ni = {i} | seq.: {seq} file: {file_name} \\ntexto_tomador_cnpj:\\n{texto_tomador_cnpj}\\n')\n",
    "                \n",
    "                text_splited = texto_extraido.split('\\n')\n",
    "                text_splited = [x for x in text_splited if x.strip()]\n",
    "                keyword_list = ['CPF/CNPJ:', 'Telefone:'] # 'Nome/Razão Social:'\n",
    "                string_pesquisa = \"CPF/CNPJ:\"\n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                #print(f'texto antes do try: {texto}')\n",
    "                cpf_cnpj_formatado_match = re.search(r'(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})|(\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2})|(\\d{11})', texto)\n",
    "                if cpf_cnpj_formatado_match:\n",
    "                    texto_cpf_cnpj = cpf_cnpj_formatado_match.group()\n",
    "                    # print(f'\\ntexto_cpf_cnpj: {texto_cpf_cnpj}\\n')  # Aqui usamos group() sem argumentos para pegar toda a string que correspondeu\n",
    "                    cpf_cnpj_tomador['t_cpf_cnpj_com_mascara'] = texto_cpf_cnpj  # Aqui atribuímos a string correspondente diretamente\n",
    "                    tomador_cpf_cnpj_sem_mascara = re.sub(r'\\D', '', texto_cpf_cnpj)  # Aqui removemos todos os caracteres não-dígitos da string correspondente\n",
    "                    cpf_cnpj_tomador['t_cpf_cnpj_sem_mascara'] = tomador_cpf_cnpj_sem_mascara\n",
    "                    \n",
    "\n",
    "                string_pesquisa = \"Telefone:\"  \n",
    "                texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "                telefone_match = re.search(r'(\\(?\\+?[0-9]*\\)?[-. \\s]?[0-9]+[-. \\s]?[0-9]+)', texto)\n",
    "                if telefone_match:\n",
    "                    telefone = telefone_match.group(0)\n",
    "                else:\n",
    "                    telefone = None\n",
    "\n",
    "                if telefone and len(re.findall(r'\\d', telefone)) >= 8:\n",
    "                    # A string contém pelo menos 8 dígitos, então assumimos que é um número de telefone válido\n",
    "                    nf_data_tomador['t_telefone'] = telefone\n",
    "                else:\n",
    "                    nf_data_tomador['t_telefone'] = None\n",
    "\n",
    "            except Exception as e:\n",
    "                nf_data_tomador['t_telefone'] = None\n",
    "                new_row = {\n",
    "                    \"row_index\": idx,  # Substitua 'index' pela variável que contém o índice da linha atual\n",
    "                    \"erro_inscricao\": str(e),\n",
    "                    \"file\": original_file_name,\n",
    "                    \"process_label\": label\n",
    "                }              \n",
    "        \n",
    "        elif label == \"3_frame_inscricao_tomador\":\n",
    "            # valor_cpf_cnpj_apos = nf_data_tomador['t_cpf_cnpj_com_mascara']\n",
    "            # print(f'valor no elif da inscricao:{valor_cpf_cnpj_apos}\\n')\n",
    "            coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            y0 = y0 * f_0\n",
    "            y1 = y1 * f_1\n",
    "            texto_extraido = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "            texto_tomador_inscricao = texto_extraido\n",
    "            #print(f'\\ni = {i} | seq.: {seq} file: {original_file_name} \\ntexto_tomador_cnpj:\\n{texto_tomador_inscricao}\\n')\n",
    "            \n",
    "            text_splited = texto_extraido.split('\\n')\n",
    "            text_splited = [x for x in text_splited if x.strip()]\n",
    "            keyword_list = ['Inscrição Municipal:', 'RG:', 'Inscrição Estadual:']\n",
    "            string_pesquisa = \"Inscrição Municipal:\"\n",
    "            tomador_inscricao_municipal = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            nf_data_tomador['t_inscricao_municipal'] = tomador_inscricao_municipal\n",
    "            \n",
    "            string_pesquisa = \"RG:\"\n",
    "            tomador_rg = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            nf_data_tomador['t_rg'] = tomador_rg\n",
    "            \n",
    "            string_pesquisa = \"Inscrição Estadual:\"\n",
    "            tomador_inscricao_estadual = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            nf_data_tomador['t_inscricao_estadual'] = tomador_inscricao_estadual                    \n",
    "\n",
    "        elif label == \"3_frame_dados_tomador\":\n",
    "            coordinates = get_coordinates_filter_R_PDF(model_map, tipo, label, section)\n",
    "            x0, y0, x1, y1 = coordinates[0]\n",
    "            #print(f'1. : (coordenadas originais):  x0:{x0} y0:{y0} x1:{x1} y1:{y1}')\n",
    "            y0 = y0 * f_0\n",
    "            y1 = y1 * f_1\n",
    "            #print(f'2.A   coordenadas ajustadas:   x0:{x0} y0:{y0} x1:{x1} y1:{y1}: f_0:{f_0}, f_1: {f_1}\\n')\n",
    "            texto_extraido = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "            texto_tomador_dados = texto_extraido\n",
    "            #print(f'\\ni = {i} | seq.: {seq} file: {original_file_name} \\ntexto_tomador_cnpj:\\n{texto_tomador_dados}\\n')\n",
    "            text_splited = texto_extraido.split('\\n')\n",
    "            text_splited = [x for x in text_splited if x.strip()]\n",
    "            \n",
    "            keyword_list = ['Nome/Razão Social:', 'Nome de Fantasia:', 'Endereço:', 'E-mail:']\n",
    "            string_pesquisa = \"Nome/Razão Social:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            texto_limpo = texto.replace('“', '').replace('”', '')\n",
    "            tomador_razao_social = texto_limpo\n",
    "            nf_data_tomador['t_razao_social'] = tomador_razao_social\n",
    "\n",
    "            string_pesquisa = \"Endereço:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            tomador_endereco = texto\n",
    "            nf_data_tomador['t_endereco'] = tomador_endereco\n",
    "            \n",
    "            string_pesquisa = \"E-mail:\"\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            tomador_email = corrigir_email(texto)\n",
    "            \n",
    "            nf_data_tomador['t_email'] = tomador_email\n",
    "            \n",
    "            # valor_cpf_cnpj_apos = nf_data_tomador['t_cpf_cnpj_com_mascara']\n",
    "            # print(f'valor no no fim do bloco:{valor_cpf_cnpj_apos}\\n')\n",
    "            \n",
    "        i += 1\n",
    "        #print(nf_data_tomador)\n",
    "    nf_data_tomador['t_cpf_cnpj_com_mascara'] = cpf_cnpj_tomador['t_cpf_cnpj_com_mascara']\n",
    "    nf_data_tomador['t_cpf_cnpj_sem_mascara'] = cpf_cnpj_tomador['t_cpf_cnpj_sem_mascara'] \n",
    "            \n",
    "    return nf_data_tomador\n",
    "\n",
    "\n",
    "\n",
    "# 7.B VALORES E IMPOSTOS - Raster_PDF\n",
    "def extracao_impostos_R_PDF(section, tipo, father_value, de_para_pm, model_map, original_file_name, file_path):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    \n",
    "    #print(f'father_value: {father_value}, section: {section}, tipo: {tipo}, model_map: {model_map}, de_para_pm: {de_para_pm}, original_file_name: {original_file_name}\\n')\n",
    "\n",
    "    # trato a imagem logo no começo\n",
    "    imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "\n",
    "    # Estabeleco o filtro\n",
    "    filtered_boxes_info = frames_nf_v4_df[((frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == tipo) & (frames_nf_v4_df['section_json'] == section) & (frames_nf_v4_df['father'] == father_value))]\n",
    "\n",
    "    for idx_frame, row_frame in filtered_boxes_info.iterrows():\n",
    "        extracted_text_box = None\n",
    "        reference = row_frame['reference']\n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        index = idx_frame\n",
    "        image_gray_croped = imagem_gray.crop((x0, y0, x1, y1))\n",
    "        extracted_text_box = (pytesseract.image_to_string(image_gray_croped, lang='por'))\n",
    "        linhas = extracted_text_box.split('\\n')\n",
    "        label = row_frame['label']\n",
    "        #print(f'para label:{label:>15} e extracted_text_box: {extracted_text_box}  | x0: {x0:>6} | y0: {y0:>6} | x1: {x1:>6} | y1: {y1:>6} |\\n\\n')\n",
    "        for texto in linhas:\n",
    "            valores = re.findall(r'R\\$ *([\\d\\.]+,\\d{1,2})|([\\d\\.]+,\\d{1,2})%', texto)\n",
    "            if 'R$' in texto or ',' in texto:\n",
    "                # Original format: Remove 'R$', replace dots with nothing, and replace commas with dots\n",
    "                number_str = texto.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "                value = float(number_str)\n",
    "                #print(f'label: {label:>20} | valor: {value}')\n",
    "                label = row_frame['label']\n",
    "                data_box_valores[label] = value \n",
    "            elif '%' in texto:\n",
    "                percent_str = texto.replace('%', '')\n",
    "                value = float(percent_str)  \n",
    "                #print(f'label: {label:>20} | valor: {value}')\n",
    "                data_box_valores[label] = value \n",
    "                \n",
    "        label = row_frame['label']\n",
    "        data_box_valores[label] = value        \n",
    "           \n",
    "        \n",
    "    return data_box_valores\n",
    "\n",
    "# 8.B DADOS COMPLEMENTARES - Raster_PDF\n",
    "def extracao_complementares_R_PDF(row, section, tipo, father_value, de_para_pm, model_map, original_file_name, file_path):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    \n",
    "    #print(f'\\n0. entrei na func. -  model_map: {model_map} | section: {section} | tipo: {tipo} | father_value: {father_value} n\\n')\n",
    "    imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "    filtered_boxes_info = frames_nf_v4_df[((frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == tipo) & (frames_nf_v4_df['father'] == father_value))]\n",
    "    for idx_frame, row_frame in filtered_boxes_info.iterrows():\n",
    "        extracted_text_box = None\n",
    "        father_value = row_frame['father']\n",
    "        label = row_frame['label']\n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        index = idx_frame\n",
    "        #print(f'1. idx: {idx_frame} label:{label:>15} \\n\\n')\n",
    "        image_gray_croped = imagem_gray.crop((x0, y0, x1, y1))\n",
    "        extracted_text_box = (pytesseract.image_to_string(image_gray_croped, lang='por'))\n",
    "        texto = extracted_text_box\n",
    "        text = re.sub(r'^DADOS COMPLEMENTARES', '', extracted_text_box, count=1)\n",
    "        if text == '':\n",
    "            value = None\n",
    "                # data_box_valores[label] = value\n",
    "\n",
    "        \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "# 9.B OUTRAS INFORMAÇOES / CRITICAS - Raster_PDF \n",
    "def extracao_inforacoes_criticas_R_PDF(section, tipo, father_value, de_para_pm, model_map, original_file_name, file_path):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "\n",
    "    #print(f'father_value: {father_value}, section: {section}, tipo: {tipo}, model_map: {model_map}, de_para_pm: {de_para_pm}, original_file_name: {original_file_name}\\n')\n",
    "    # trato a imagem logo no começo\n",
    "    imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "\n",
    "    # Estabeleco o filtro\n",
    "    filtered_boxes_info = frames_nf_v4_df[((frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == tipo) & (frames_nf_v4_df['section_json'] == section) & (frames_nf_v4_df['father'] == father_value))]\n",
    "\n",
    "    for idx_frame, row_frame in filtered_boxes_info.iterrows():\n",
    "        extracted_text_box = None\n",
    "        reference = row_frame['reference']\n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        index = idx_frame\n",
    "        \n",
    "        label = row_frame['label']\n",
    "        image_gray_croped = imagem_gray.crop((x0, y0, x1, y1))\n",
    "        extracted_text_box = (pytesseract.image_to_string(image_gray_croped, lang='por'))\n",
    "        #print(extracted_text_box)\n",
    "        texto = extracted_text_box\n",
    "        if label == \"exigibilidade_iss\":\n",
    "            texto = extracted_text_box\n",
    "            linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "            marcador = \"EXIGIBILIDADE ISS\"\n",
    "            value  = encontrar_valor_por_marcador(marcador, linhas)\n",
    "            data_box_valores[label] = value\n",
    "            \n",
    "        if label == \"regime_tributacao\":\n",
    "            texto = extracted_text_box\n",
    "            linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "            marcador = \"REGIME TRIBUTAÇÃO\"\n",
    "            value = encontrar_valor_por_marcador(marcador, linhas)\n",
    "            data_box_valores[label] = value\n",
    "            \n",
    "        if label == \"simples_nacional\":\n",
    "            texto = extracted_text_box\n",
    "            linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "            marcador = \"SIMPLES NACIONAL\"\n",
    "            value = encontrar_valor_por_marcador(marcador, linhas) \n",
    "            data_box_valores[label] = value \n",
    "        \n",
    "        if label == \"issqn_retido\":\n",
    "            texto = extracted_text_box \n",
    "            linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "            marcador = \"ISSQN RETIDO\"\n",
    "            valor = encontrar_valor_por_marcador(marcador, linhas)\n",
    "            data_box_valores[label] = value \n",
    "            \n",
    "            \n",
    "        if label == \"local_pretacao_servico\":\n",
    "            texto = extracted_text_box\n",
    "            texto = texto.replace('\\n\\n', \" \").strip()\n",
    "            linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "            marcador = \"LOCAL. PRESTAÇÃO SERVIÇO\"\n",
    "            value = encontrar_valor_por_marcador(marcador, linhas)\n",
    "            data_box_valores[label] = value \n",
    "            \n",
    "        if label == \"local_incidencia\":\n",
    "            texto = extracted_text_box\n",
    "            linhas = [linha for linha in texto.split('\\n') if linha.strip()] \n",
    "            marcador = \"LOCAL INCIDÊNCIA\"\n",
    "            value = encontrar_valor_por_marcador(marcador, linhas)  \n",
    "            data_box_valores[label] = value              \n",
    "\n",
    "                \n",
    "        #print(f'label:{label:>25} | value: {value}\\n')         \n",
    "           \n",
    "        \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "def extrair_exigibilidade_iss(texto):\n",
    "    # Lista de possíveis valores para este campo\n",
    "    possiveis_valores = [\"Exigível\", \"Não Exigível\"]\n",
    "    \n",
    "    # Encontrando o valor mais semelhante no texto\n",
    "    valor, score = process.extractOne(texto, possiveis_valores)\n",
    "    \n",
    "    # Você pode ajustar o limite de score conforme necessário\n",
    "    if score > 85:\n",
    "        return valor\n",
    "    return None\n",
    "\n",
    "def extrair_regime_tributacao(texto):\n",
    "    # Lista de possíveis valores para este campo\n",
    "    possiveis_valores = [\n",
    "        \"Sociedade Limitada\", \n",
    "        \"Microempresário Individual (MEI)\", \n",
    "        \"Sociedade anônima\", \n",
    "        \"Microempresa municipal\", \n",
    "        \"Microempresário e Empresa de Pequeno Porte (ME EPP)\"\n",
    "    ]\n",
    "    \n",
    "    # Encontrando o valor mais semelhante no texto\n",
    "    valor, score = process.extractOne(texto, possiveis_valores)\n",
    "    \n",
    "    # Você pode ajustar o limite de score conforme necessário\n",
    "    if score > 85:\n",
    "        return valor\n",
    "    return None\n",
    "\n",
    "def extrair_simples_nacional(texto):\n",
    "    # Verificando se o texto contém 'Sim' ou 'Não'\n",
    "    if 'Sim' in texto:\n",
    "        # Tentando extrair o valor percentual\n",
    "        match = re.search(r'Sim \\((.*?)%\\)', texto)\n",
    "        if match:\n",
    "            return f'Sim ({match.group(1)}%)'\n",
    "        return 'Sim'\n",
    "    elif 'Não' in texto:\n",
    "        return 'Não'\n",
    "    return None\n",
    "\n",
    "def extrair_issqn_retido(texto):\n",
    "    # Verificando se o texto contém 'Sim' ou 'Não'\n",
    "    if 'Sim' in texto:\n",
    "        return 'Sim'\n",
    "    elif 'Não' in texto:\n",
    "        return 'Não'\n",
    "    return None\n",
    "\n",
    "def extrair_local_prestacao(texto):\n",
    "    # Lista de possíveis valores para este campo\n",
    "    possiveis_valores = [\n",
    "        'Magé - RJ', 'São Pedro da Aldeia - RJ', 'Armação dos Búzios - RJ',\n",
    "        'Cabo Frio - RJ', 'Araruama - RJ', 'Rio de Janeiro - RJ',\n",
    "        'Mesquita - RJ', 'SAO PEDRO DA ALDEIA/RJ', 'MESQUITA/RJ', 'Macaé - RJ'\n",
    "    ]\n",
    "    \n",
    "    # Encontrando o valor mais semelhante no texto\n",
    "    valor, score = process.extractOne(texto, possiveis_valores)\n",
    "    \n",
    "    # Você pode ajustar o limite de score conforme necessário\n",
    "    if score > 85:\n",
    "        return valor\n",
    "    return None\n",
    "\n",
    "def encontrar_valor_por_marcador(marcador, lista_strings):\n",
    "    # Mapeando marcadores para funções de pós-processamento\n",
    "    funcoes_pos_processamento = {\n",
    "        \"EXIGIBILIDADE ISS\": extrair_exigibilidade_iss,\n",
    "        \"REGIME TRIBUTAÇÃO\": extrair_regime_tributacao,\n",
    "        \"SIMPLES NACIONAL\": extrair_simples_nacional,\n",
    "        \"ISSQN RETIDO\": extrair_issqn_retido,\n",
    "        \"LOCAL. PRESTAÇÃO SERVIÇO\": extrair_local_prestacao,\n",
    "        \"LOCAL INCIDÊNCIA\": extrair_local_prestacao  # Usando a mesma função que 'LOCAL. PRESTAÇÃO SERVIÇO'\n",
    "    }\n",
    "    \n",
    "    # Encontre o índice do marcador na lista de strings\n",
    "    for i, linha in enumerate(lista_strings):\n",
    "        if marcador.lower() in linha.lower():\n",
    "            # Suponha que o valor está na próxima linha\n",
    "            valor_bruto = lista_strings[i + 1]\n",
    "            \n",
    "            # Obtemos a função de pós-processamento correspondente ao marcador\n",
    "            funcao_pos_processamento = funcoes_pos_processamento.get(marcador)\n",
    "            \n",
    "            # Se encontrarmos uma função de pós-processamento correspondente, aplicamo-la ao valor bruto\n",
    "            if funcao_pos_processamento:\n",
    "                return funcao_pos_processamento(valor_bruto)\n",
    "            \n",
    "            return valor_bruto\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def extracao_complementar_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_pm, model_map, def_replace, original_file_name, debug):\n",
    "    \n",
    "    #print(idx, row, guarda_texto_doc, section, label, de_para_pm, model_map, def_replace, original_file_name)\n",
    "    map_document_unique_id = idx\n",
    "    nf_data_dados_complementares = {}\n",
    "    label = label\n",
    "    section = section\n",
    "    model_map = model_map\n",
    "    tipo = tipo\n",
    "    #print(f'\\nidx: {idx} | label: {label} | section: {section} | model_map: {model_map} | tipo: {tipo} | def_replace: {def_replace} | original_file_name: {original_file_name}\\n\\n')\n",
    "    \n",
    "    if guarda_texto_doc['document_unique_id'] == map_document_unique_id:\n",
    "        texto_documento_uso = guarda_texto_doc['texto_documento']\n",
    "        #print(f'\\n\\ntexto_documento_uso: {texto_documento_uso}')\n",
    "    if texto_documento_uso:\n",
    "        # Busco marcadores\n",
    "        texto_completo = \" \".join(texto_documento_uso)\n",
    "        \n",
    "        marcador_inicio, marcador_fim, modelo = busca_marcadores(model_map, section, tipo, label)\n",
    "        #print(marcador_inicio, marcador_fim, modelo)\n",
    "        \n",
    "        if modelo == model_map:\n",
    "            texto_extraido = encontrar_texto(texto_completo, marcador_inicio, marcador_fim)\n",
    "            \n",
    "            if def_replace:\n",
    "                texto_extraido = texto_extraido.replace(marcador_inicio, \"\").strip()\n",
    "            \n",
    "                texto_extraido_strip = texto_extraido.strip()\n",
    "                #print(f'texto_extraido_strip: {texto_extraido_strip}')\n",
    "                nf_data_dados_complementares['dados_complementares'] = texto_extraido_strip\n",
    "                \n",
    "                return nf_data_dados_complementares\n",
    "            else:\n",
    "                #print(\"texto_extraido\", texto_extraido)\n",
    "                nf_data_dados_complementares['dados_complementares'] = texto_extraido\n",
    "                \n",
    "                return nf_data_dados_complementares\n",
    "         \n",
    "            \n",
    "        print(\"nao achou modelo\")\n",
    "        return None \n",
    "            \n",
    "    else:\n",
    "        print(\"nao achou texto\")\n",
    "        return None \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def extracao_observacoees_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_pm, model_map, def_replace, original_file_name, debug):\n",
    "    \n",
    "    #print(idx, row, guarda_texto_doc, section, label, de_para_pm, model_map, def_replace, original_file_name)\n",
    "    map_document_unique_id = idx\n",
    "    nf_data_observacao = {}\n",
    "    label = label\n",
    "    section = section\n",
    "    model_map = model_map\n",
    "    tipo = tipo\n",
    "    #print(f'\\nidx: {idx} | label: {label} | section: {section} | model_map: {model_map} | tipo: {tipo} | def_replace: {def_replace} | original_file_name: {original_file_name}\\n\\n')\n",
    "    \n",
    "    if guarda_texto_doc['document_unique_id'] == map_document_unique_id:\n",
    "        texto_documento_uso = guarda_texto_doc['texto_documento']\n",
    "        #print(f'\\n\\ntexto_documento_uso: {texto_documento_uso}')\n",
    "    if texto_documento_uso:\n",
    "        # Busco marcadores\n",
    "        texto_completo = \" \".join(texto_documento_uso)\n",
    "        \n",
    "        marcador_inicio, marcador_fim, modelo = busca_marcadores(model_map, section, tipo, label)\n",
    "        #print(marcador_inicio, marcador_fim, modelo)\n",
    "        \n",
    "        texto_extraido = encontrar_texto(texto_completo, marcador_inicio, marcador_fim)\n",
    "        if texto_extraido:\n",
    "            nf_data_observacao['observacao'] = texto_extraido\n",
    "            \n",
    "            return nf_data_observacao\n",
    "        \n",
    "        else:\n",
    "            marcador_fim = None\n",
    "            texto_extraido = encontrar_texto(texto_completo, marcador_inicio, marcador_fim)\n",
    "            if texto_extraido:\n",
    "                nf_data_observacao['observacao'] = texto_extraido\n",
    "                \n",
    "                return nf_data_observacao\n",
    "            else:\n",
    "                return None\n",
    "                 \n",
    "            \n",
    "        print(\"nao achou modelo\")\n",
    "        return None \n",
    "            \n",
    "    else:\n",
    "        print(\"nao achou texto\")\n",
    "        return None  \n",
    "\n",
    "\n",
    "\n",
    "# 10.B OBSERVACOES  - Raster_PDF \n",
    "def extracao_observacao_R_PDF(row, section, tipo, father_value, de_para_pm, model_map, original_file_name, file_path):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    #print(f'\\n0. entrei na func. -  model_map: {model_map} | section: {section} | tipo: {tipo} | father_value: {father_value} n\\n')\n",
    "    imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "    filtered_boxes_info = frames_nf_v4_df[((frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['type'] == tipo) & (frames_nf_v4_df['father'] == father_value))]\n",
    "    for idx_frame, row_frame in filtered_boxes_info.iterrows():\n",
    "        extracted_text_box = None\n",
    "        reference = row_frame['reference']\n",
    "        father_value = row_frame['father']\n",
    "        label = row_frame['label']\n",
    "        x0, y0, x1, y1 = row_frame['x0'], row_frame['y0'], row_frame['x1'], row_frame['y1']\n",
    "        index = idx_frame\n",
    "        image_gray_croped = imagem_gray.crop((x0, y0, x1, y1))\n",
    "        extracted_text_box = (pytesseract.image_to_string(image_gray_croped, lang='por'))\n",
    "        texto = extracted_text_box\n",
    "        text = re.sub(r'^Observação:', '', extracted_text_box, count=1)\n",
    "        value = text.replace('\\n', ' ')\n",
    "        data_box_valores[label] = value\n",
    "        #print(f'idx: {idx_frame} label:{label:>15}    | x0: {x0:>6} | y0: {y0:>6} | x1: {x1:>6} | y1: {y1:>6} |\\n{text}\\n')\n",
    "   \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "# XXX IMPORTANTE - ESTA E A FUNCAO PARA SER UTILIZADA: POIS CONVERTE PARA CINZA E RESIZE: (4134, 5846)\n",
    "def convert_resize_gray(original_file_name, file_path, image_resized_path):\n",
    "\n",
    "    name_image = conv_filename_no_ext(original_file_name)\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(name_image)}.jpg')\n",
    "    pages = convert_from_path(file_path, 500, poppler_path=poppler_path)\n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((4134, 5846))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "    imagem_gray = resized_pages[0].convert('L')\n",
    "    imagem_gray.save(image_resized_name, 'JPEG')\n",
    "\n",
    "    return  imagem_gray, image_resized_name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1 XXX Extracao de dados do documento todo\n",
    "def cria_guarda_doc_ref_R_PDF(idx, row, de_para_pm, model_map, original_file_name, file_path, image_resized_path, debug):\n",
    "    \n",
    "    guarda_texto_doc = {}\n",
    "    \n",
    "    imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "\n",
    "    texto = (pytesseract.image_to_string(imagem_gray, lang='por'))\n",
    "    linhas = [linha for linha in texto.split('\\n') if linha.strip()]\n",
    "    guarda_texto_doc['document_unique_id'] = idx\n",
    "    guarda_texto_doc['original_file_name'] = original_file_name\n",
    "    guarda_texto_doc['texto_documento'] = linhas\n",
    "    \n",
    "    return guarda_texto_doc, linhas\n",
    "\n",
    "\n",
    "# 2. XXX FunÇao para pesquisar entre marcadores do texto\n",
    "def extrair_texto_entre_marcadores(texto, marcador_inicio, marcador_fim):\n",
    "    try:\n",
    "        # Encontra os índices dos marcadores de início e fim\n",
    "        indice_inicio = next(i for i, s in enumerate(texto) if marcador_inicio in s)\n",
    "        indice_fim = next(i for i, s in enumerate(texto) if marcador_fim in s)\n",
    "\n",
    "        # Se o marcador de início e fim estão na mesma linha\n",
    "        if indice_inicio == indice_fim:\n",
    "            inicio = texto[indice_inicio].find(marcador_inicio) + len(marcador_inicio)\n",
    "            fim = texto[indice_fim].find(marcador_fim)\n",
    "            return texto[indice_inicio][inicio:fim].strip()\n",
    "        else:\n",
    "            # Extrai e retorna o texto entre os marcadores\n",
    "            return \" \".join(texto[indice_inicio:indice_fim])\n",
    "    except StopIteration:\n",
    "        # Retorna None se algum dos marcadores não for encontrado\n",
    "        return None\n",
    "    \n",
    "    \n",
    "# 5 XXX Busca os marcadores no template\n",
    "def busca_marcadores(model_map, section, tipo, label):\n",
    "    \n",
    "    #print('fantes da query em busca_marcadores: {mpdel_map} | {section} | {tipo} | {label}}')\n",
    "    row_frame = filtrar_df(frames_nf_v4_df, model=model_map, type=tipo, label=label, section_json=section)\n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        modelo = [(row_frame.iloc[0]['model'])]\n",
    "        marc_ini = [(row_frame.iloc[0]['marcador_inicio'])]\n",
    "        marc_fim = [(row_frame.iloc[0]['marcador_fim'])]\n",
    "        prestador = [(row_frame.iloc[0]['prestador'])]\n",
    "        coodinates = [(row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1'])]\n",
    "        modelo = [(row_frame.iloc[0]['model'])]\n",
    "        #print(f'\\n\\n - Dentro do busca marcadores:  modelo: {modelo}   | label: {label} | marcador_inicio: {marcador_inicio} | marcador_fim: {marcador_fim}')\n",
    "        \n",
    "    return marc_ini[0], marc_fim[0], modelo[0]  \n",
    "\n",
    "\n",
    "# 6 XXX Funcao generica de extracao\n",
    "def extracao_documento_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_pm, model_map, def_replace, original_file_name, debug):\n",
    "    \n",
    "    #print(idx, row, guarda_texto_doc, section, label, de_para_pm, model_map, def_replace, original_file_name)\n",
    "    map_document_unique_id = idx\n",
    "    \n",
    "    label = label\n",
    "    section = section\n",
    "    model_map = model_map\n",
    "    tipo = tipo\n",
    "    #print(f'\\nidx: {idx} | label: {label} | section: {section} | model_map: {model_map} | tipo: {tipo} | def_replace: {def_replace} | original_file_name: {original_file_name}\\n\\n')\n",
    "    \n",
    "    if guarda_texto_doc['document_unique_id'] == map_document_unique_id:\n",
    "        texto_documento_uso = guarda_texto_doc['texto_documento']\n",
    "        #print(f'\\n\\ntexto_documento_uso: {texto_documento_uso}')\n",
    "    if texto_documento_uso:\n",
    "        # Busco marcadores\n",
    "        marcador_inicio, marcador_fim, modelo = busca_marcadores(model_map, section, tipo, label)\n",
    "        #print(marcador_inicio, marcador_fim, modelo)\n",
    "        \n",
    "        if modelo == model_map:\n",
    "            texto_extraido = extrair_texto_entre_marcadores(texto_documento_uso, marcador_inicio, marcador_fim)\n",
    "            \n",
    "            if def_replace:\n",
    "                texto_extraido = texto_extraido.replace(marcador_inicio, \"\").strip()\n",
    "            \n",
    "                texto_extraido_strip = texto_extraido.strip()\n",
    "                #print(f'texto_extraido_strip: {texto_extraido_strip}')\n",
    "                return texto_extraido_strip\n",
    "            else:\n",
    "                #print(\"texto_extraido\", texto_extraido)\n",
    "                return texto_extraido\n",
    "         \n",
    "            \n",
    "        print(\"nao achou modelo\")\n",
    "        return None \n",
    "            \n",
    "    else:\n",
    "        print(\"nao achou texto\")\n",
    "        return None \n",
    "    \n",
    "\n",
    "# 6.B XXX Funcao generica de extracao - CNAE_ITEM - RASTER PDF\n",
    "def extracao_documento_CNAE_ITEM_R_PDF(idx, row, row_info, guarda_texto_doc, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, def_replace, original_file_name, file_path, debug):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    # busco coordenadas para o contexto\n",
    "    if mapping_method == \"frame_&_sframe_field\":\n",
    "        tipo_4_coordinates = \"frame\"\n",
    "        tipo_4_filter = \"sframe_field\"\n",
    "    \n",
    "    map_document_unique_id = idx\n",
    "    \n",
    "    if guarda_texto_doc['document_unique_id'] == map_document_unique_id:\n",
    "        texto_documento_uso = guarda_texto_doc['texto_documento']\n",
    "\n",
    "    # 8. Efetuo o filtro para a iteracao\n",
    "    filtered_frame_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['context_mapping'] == context_mapping) & (frames_nf_v4_df['type'] == tipo_4_filter)]\n",
    "\n",
    "    # 9. iter sobre o filtro\n",
    "    for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "        section = row_frame['section_json']\n",
    "        label = row_frame['label']\n",
    "        reference = row_frame['reference']\n",
    "        marcador_inicio = row_frame['marcador_inicio']\n",
    "        marcador_fim = row_frame['marcador_fim']\n",
    "        texto_extraido = extrair_texto_entre_marcadores(texto_documento_uso, marcador_inicio, marcador_fim)\n",
    "        if texto_extraido:\n",
    "            texto_extraido = texto_extraido.replace(marcador_inicio, \"\").strip()\n",
    "            data_box_valores[label] = texto_extraido\n",
    "            # print(texto_extraido)\n",
    "            # print()\n",
    "        else:\n",
    "            texto_completo = \" \".join(text_splited)\n",
    "            texto_pesquisa = encontrar_texto_fuzzy_marcador_inicial(texto_completo, marcador_inicio, marcador_fim) \n",
    "            if texto_pesquisa:\n",
    "                texto_pesquisa = texto_pesquisa.replace(marcador_inicio, \"\").strip()\n",
    "                data_box_valores[label] = texto_pesquisa\n",
    "                # print(texto_pesquisa)\n",
    "                # print()\n",
    "            else:\n",
    "                texto_pesquisa2 = encontrar_texto_fuzzy_2_marcadores(texto_completo, marcador_inicio, marcador_fim)\n",
    "                if texto_pesquisa2:\n",
    "                    texto_pesquisa2 = texto_pesquisa2.replace(marcador_inicio, \"\").strip()\n",
    "                    data_box_valores[label] = texto_pesquisa2\n",
    "                    # print(texto_pesquisa2)\n",
    "\n",
    "    \n",
    "    \n",
    "    return data_box_valores\n",
    "\n",
    "    \n",
    " \n",
    "# VERIFICAR QUAL SERA USADA\n",
    "def encontrar_valores1(texto, marcador1, marcador2):\n",
    "    # Encontrando o texto entre os dois marcadores\n",
    "    resultado = re.search(f'{re.escape(marcador1)}(.*?){re.escape(marcador2)}', texto, re.DOTALL)\n",
    "    if resultado:\n",
    "        segmento = resultado.group(1)\n",
    "        \n",
    "        # Encontrando todos os valores no formato R$ 9,99\n",
    "        valores = re.findall(r'R\\$ \\d+,\\d{2}', segmento)\n",
    "        \n",
    "        # Encontrando as posições dos valores na string original\n",
    "        posicoes = [m.start() for m in re.finditer(r'R\\$ \\d+,\\d{2}', segmento)]\n",
    "        \n",
    "        # Criando uma lista de tuplas (valor, posição)\n",
    "        valores_com_posicoes = list(zip(valores, posicoes))\n",
    "        \n",
    "        return valores_com_posicoes\n",
    "    else:\n",
    "        return None\n",
    " \n",
    " \n",
    "    \n",
    "# VERIFICAR QUAL SERA USADA    \n",
    "def encontrar_valores2(texto, marcador1, marcador2):\n",
    "    # Certificando-se de que todos os parâmetros são strings\n",
    "    if isinstance(texto, list):\n",
    "        texto = '\\n'.join(texto)\n",
    "    if not isinstance(marcador1, str):\n",
    "        marcador1 = str(marcador1)\n",
    "    if not isinstance(marcador2, str):\n",
    "        marcador2 = str(marcador2)\n",
    "\n",
    "    # Encontrando o texto entre os dois marcadores\n",
    "    resultado = re.search(f'{re.escape(marcador1)}(.*?){re.escape(marcador2)}', texto, re.DOTALL)\n",
    "    if resultado:\n",
    "        segmento = resultado.group(1)\n",
    "        \n",
    "        # Encontrando todos os valores no formato R$ 9,99\n",
    "        valores = re.findall(r'R\\$ \\d+,\\d{2}', segmento)\n",
    "        \n",
    "        # Encontrando as posições dos valores na string original\n",
    "        posicoes = [m.start() for m in re.finditer(r'R\\$ \\d+,\\d{2}', segmento)]\n",
    "        \n",
    "        # Criando uma lista de tuplas (valor, posição)\n",
    "        valores_com_posicoes = list(zip(valores, posicoes))\n",
    "        \n",
    "        return valores_com_posicoes\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "# VERIFICAR QUAL SERA USADA    \n",
    "def encontrar_valores3(texto, marcador1, marcador2):\n",
    "    # Certificando-se de que todos os parâmetros são strings\n",
    "    if isinstance(texto, list):\n",
    "        texto = '\\n'.join(texto)\n",
    "    if not isinstance(marcador1, str):\n",
    "        marcador1 = str(marcador1)\n",
    "    if not isinstance(marcador2, str):\n",
    "        marcador2 = str(marcador2)\n",
    "\n",
    "    # Encontrando o texto entre os dois marcadores\n",
    "    resultado = re.search(f'{re.escape(marcador1)}(.*?){re.escape(marcador2)}', texto, re.DOTALL)\n",
    "    if resultado:\n",
    "        segmento = resultado.group(1)\n",
    "        \n",
    "        # Encontrando todos os valores no formato R$ 9,99 e as alíquotas como 3%\n",
    "        valores = re.findall(r'R\\$ \\d+,\\d{2}|(?:\\d+,\\d{1,2}|\\d+)%', segmento)\n",
    "        \n",
    "        # Encontrando as posições dos valores na string original\n",
    "        posicoes = [m.start() for m in re.finditer(r'R\\$ \\d+,\\d{2}|(?:\\d+,\\d{1,2}|\\d+)%', segmento)]\n",
    "        \n",
    "        # Criando uma lista de tuplas (valor, posição)\n",
    "        valores_com_posicoes = list(zip(valores, posicoes))\n",
    "        \n",
    "        return valores_com_posicoes\n",
    "    else:\n",
    "        return None\n",
    " \n",
    "\n",
    "def encontrar_texto_fuzzy_2_marcadores(texto, marcador_inicial, marcador_final, limite_score=80):\n",
    "    # Usar FuzzyWuzzy para encontrar o melhor match para o marcador inicial\n",
    "    palavras = texto.split()\n",
    "    melhor_match_inicial, score_inicial = process.extractOne(marcador_inicial, palavras)\n",
    "    \n",
    "    if score_inicial < limite_score:\n",
    "        return None\n",
    "    \n",
    "    # Encontrar a posição inicial do melhor match\n",
    "    inicio = texto.find(melhor_match_inicial)\n",
    "    #print('inicio: ',inicio)\n",
    "    \n",
    "    # Cortar o texto para começar após o marcador inicial\n",
    "    texto_cortado = texto[inicio + len(melhor_match_inicial):]\n",
    "    #print('texto_cortado: ', texto_cortado)\n",
    "    \n",
    "    # Usar FuzzyWuzzy para encontrar o melhor match para o marcador final\n",
    "    melhor_match_final, score_final = process.extractOne(marcador_final, palavras)\n",
    "    #print('melhor_match_final: ', melhor_match_final)\n",
    "    \n",
    "    if score_final < limite_score:\n",
    "        return None\n",
    "    \n",
    "    # Encontrar a posição do melhor match\n",
    "    fim = texto_cortado.find(melhor_match_final)\n",
    "    #print('fim: ', fim)\n",
    "    if fim == -1:\n",
    "        return None\n",
    "    \n",
    "    return texto_cortado[:fim].strip()\n",
    "\n",
    "def encontrar_texto_fuzzy_marcador_inicial(texto, marcador_inicial, marcador_final, limite_score=80):\n",
    "    # Encontrar a posição inicial do marcador\n",
    "    inicio = texto.find(marcador_inicial)\n",
    "    if inicio == -1:\n",
    "        return None\n",
    "    \n",
    "    # Cortar o texto para começar após o marcador inicial\n",
    "    texto_cortado = texto[inicio + len(marcador_inicial):]\n",
    "    \n",
    "    # Usar FuzzyWuzzy para encontrar o melhor match para o marcador final\n",
    "    palavras = texto_cortado.split()\n",
    "    melhor_match, score = process.extractOne(marcador_final, palavras)\n",
    "    \n",
    "    if score >= limite_score:\n",
    "        # Encontrar a posição do melhor match\n",
    "        fim = texto_cortado.find(melhor_match)\n",
    "        return texto_cortado[:fim].strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    " \n",
    " \n",
    "def encontrar_texto(texto, marcador_inicial, marcador_final=None):\n",
    "    if marcador_final:\n",
    "        # Procura pelo texto entre os dois marcadores\n",
    "        resultado = re.search(f'{re.escape(marcador_inicial)}(.*?){re.escape(marcador_final)}', texto, re.DOTALL)\n",
    "        if resultado:\n",
    "            return resultado.group(1).strip()\n",
    "    else:\n",
    "        # Procura pelo texto do marcador inicial até o final\n",
    "        resultado = re.search(f'{re.escape(marcador_inicial)}(.*)', texto, re.DOTALL)\n",
    "        if resultado:\n",
    "            return resultado.group(1).strip()\n",
    "\n",
    "    # Retorna None se não encontrar nada\n",
    "    return None\n",
    "       \n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id\n",
    "\n",
    "# VERIFICAR DUPLICIDADE\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "def format_number(number_str):\n",
    "    # Check for percentage and handle it\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # You can multiply by 100 here if needed\n",
    "\n",
    "    # Check if the string contains \"R$\" or a comma, indicating the original format\n",
    "    if 'R$' in number_str or ',' in number_str:\n",
    "        # Original format: Remove 'R$', replace dots with nothing, and replace commas with dots\n",
    "        number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    else:\n",
    "        # New format: Extract only the numeric part using regex\n",
    "        number_str = re.findall(r'[\\d\\.]+', number_str)[-1]\n",
    "\n",
    "    return float(number_str)\n",
    "\n",
    "# Funçao de formatacao de numeros\n",
    "def format_number2(number_str):\n",
    "    number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # multiplica por 100 para fields %\n",
    "    return float(number_str)\n",
    "\n",
    "\n",
    "\n",
    "# Função para extrair número da string\n",
    "def extract_number(text):\n",
    "    match = re.search(r'\\b\\d+(\\.\\d+)?\\b', text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return None            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b>2.3</b> Templates e Dics </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames_nf_v4_df: 2.3\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>de_para_pm</th>\n",
       "      <th>cnpj</th>\n",
       "      <th>model</th>\n",
       "      <th>version</th>\n",
       "      <th>seq</th>\n",
       "      <th>prestador</th>\n",
       "      <th>mapping_method</th>\n",
       "      <th>context_mapping</th>\n",
       "      <th>type</th>\n",
       "      <th>...</th>\n",
       "      <th>Largura</th>\n",
       "      <th>Altura</th>\n",
       "      <th>%</th>\n",
       "      <th>x0_p</th>\n",
       "      <th>y0_p</th>\n",
       "      <th>x1_p</th>\n",
       "      <th>y1_p</th>\n",
       "      <th>Largura_p</th>\n",
       "      <th>Altura_p</th>\n",
       "      <th>%_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAGE</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document</td>\n",
       "      <td>...</td>\n",
       "      <td>2067.0</td>\n",
       "      <td>2923.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>760.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>760.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>todos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>boundaries</td>\n",
       "      <td>...</td>\n",
       "      <td>1781.0</td>\n",
       "      <td>2567.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>todos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>section</td>\n",
       "      <td>...</td>\n",
       "      <td>919.0</td>\n",
       "      <td>621.0</td>\n",
       "      <td>24.191663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>todos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>frame</td>\n",
       "      <td>...</td>\n",
       "      <td>1134.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>PM_MAGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>todos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sframe_field</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id de_para_pm  cnpj model  version  seq prestador mapping_method  \\\n",
       "0   1    PM_MAGE   NaN  MAGE      2.3    1       NaN            NaN   \n",
       "1   2    PM_MAGE   NaN  MAGE      NaN    2     todos            NaN   \n",
       "2   3    PM_MAGE   NaN  MAGE      NaN    3     todos            NaN   \n",
       "3   4    PM_MAGE   NaN  MAGE      NaN    4     todos            NaN   \n",
       "4   5    PM_MAGE   NaN  MAGE      NaN    5     todos            NaN   \n",
       "\n",
       "  context_mapping          type  ... Largura  Altura           % x0_p y0_p  \\\n",
       "0             NaN      document  ...  2067.0  2923.0         NaN  0.0  0.0   \n",
       "1             NaN    boundaries  ...  1781.0  2567.0  100.000000  NaN  NaN   \n",
       "2             NaN       section  ...   919.0   621.0   24.191663  NaN  NaN   \n",
       "3             NaN         frame  ...  1134.0   380.0         NaN  0.0  0.0   \n",
       "4             NaN  sframe_field  ...     0.0     0.0         NaN  NaN  NaN   \n",
       "\n",
       "    x1_p   y1_p Largura_p Altura_p    %_p  \n",
       "0  600.0  760.0     600.0    760.0    NaN  \n",
       "1    NaN    NaN       0.0      0.0  100.0  \n",
       "2    NaN    NaN       0.0      0.0    0.0  \n",
       "3  600.0  220.0     600.0    205.0    NaN  \n",
       "4    NaN    NaN       0.0      0.0    NaN  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf_model_path = \"pipeline_extracao_documentos/6_geral_administacao/modelos/frames_nf_v11.xlsx\"\n",
    "\n",
    "# 11. path para datasets CNAE e Itens de Serviço\n",
    "nf_datasets_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria do DF\n",
    "frames_nf_v4_df = pd.read_excel(nf_model_path)\n",
    "\n",
    "# Cria dicionários para armazenar diferentes tipos de elementos do modelo\n",
    "document_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'document'].iloc[0]\n",
    "boundaries_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'boundaries']\n",
    "sections_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'section']\n",
    "frames_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'frame']\n",
    "sframe_fields_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'sframe_field']\n",
    "field_boxes_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'field_box']\n",
    "\n",
    "ver = get_template_version('MAGE')\n",
    "print(f'frames_nf_v4_df: {ver}')\n",
    "print()\n",
    "\n",
    "\n",
    "def define_dados_iniciais(idx, row, row_info, texto_tratado, debug):\n",
    "    \n",
    "    dados_iniciais_nf = {}\n",
    "    #status_documento_row_info = row_info.get('status_documento')\n",
    "    action_item_row_info = row_info.get('action_item')\n",
    "    information_row_info = row_info.get('informations')\n",
    "    \n",
    "    dados_iniciais_nf['action_item'] = action_item_row_info\n",
    "    dados_iniciais_nf['informations'] = information_row_info\n",
    "   \n",
    "\n",
    "\n",
    "    prefeitura_encontrada = None\n",
    "    de_para_encontrado = None\n",
    "\n",
    "    # 7. ZZZ Dicionário para mapear Prefeitura com sua sigla\n",
    "    de_para_prefeitura = {\n",
    "        \"PREFEITURA DA CIDADE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA DA CIDADE DE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA MUNICIPAL DE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\": \"PM_SPA\",\n",
    "        \"MUNICIPAL DE SAO PEDRO DA ALDEIA\": \"PM_SPA\",\n",
    "        \"PREFEITURA MUNICIPAL DE SAO PEDRO DA\\nALDEIA\": \"PM_SPA\",\n",
    "        \"PREFEITURA MUNICIPAL DE SAO PEDRO DA\": \"PM_SPA\",\n",
    "        \"PREFEITURA MUNICIPAL DE MESQUITA\": \"PM_MESQUITA\",\n",
    "        \"PREFEITURA MUNICIPAL DE DE MESQUITA\": \"PM_MESQUITA\",\n",
    "        # ... adicione \n",
    "    }\n",
    "    \n",
    "\n",
    "    templates = {\n",
    "        (\"PM_MAGE\", None): \"MAGE\",\n",
    "        (\"PM_SPA\", None): \"SPA\",\n",
    "        (\"Pague agora com o seu Pix\", None): \"NAO_PROCESSAR\",\n",
    "        # ... adicione outras combinações aqui\n",
    "    }\n",
    "\n",
    "    cnpj_encontrado = None\n",
    "    # Verifique cada linha do texto\n",
    "    for linha in texto_tratado:\n",
    "        for pref in de_para_prefeitura.keys():\n",
    "            if pref in linha:\n",
    "                #print(linha)\n",
    "                prefeitura_encontrada = pref\n",
    "                dados_iniciais_nf['prefeitura'] = prefeitura_encontrada\n",
    "                if debug:\n",
    "                    print(f'\\n4.funcao: define_dados_iniciais(texto_tratado) - dentro do loop for de pesquisa prefeitura - prefeitura_encontrada: \\n{prefeitura_encontrada}\\n\\n')\n",
    "    # Saímos do loop, agora vamos verificar qual template usar\n",
    "    if prefeitura_encontrada:\n",
    "        de_para_pm = de_para_prefeitura.get(prefeitura_encontrada)\n",
    "        dados_iniciais_nf['de_para_pm'] = de_para_pm\n",
    "        if debug:\n",
    "            print(f'\\n5.funcao: define_dados_iniciais(texto_tratado) - if prefeitura_encontrada - de_para_pm \\n{de_para_pm}\\n\\n')\n",
    "        if not de_para_pm:\n",
    "            de_para_pm = de_para_prefeitura.get(prefeitura_encontrada, \"NAO_PROCESSAR\")\n",
    "            dados_iniciais_nf['de_para_pm'] = de_para_pm\n",
    "            #print(de_para_pm)\n",
    "    else:\n",
    "        de_para_pm = \"NAO_PROCESSAR\"\n",
    "        action_item_row_info = 'BREAK_PROCESS'\n",
    "        information_row_info = 'Nao identificado dados iniciais para o documento'\n",
    "        \n",
    "     \n",
    "        \n",
    "    # Verifique cada linha do texto\n",
    "    for linha in texto_tratado:\n",
    "        for de_para, cnpj in templates.keys():\n",
    "            if cnpj and cnpj in linha:\n",
    "                cnpj_encontrado = cnpj\n",
    "                dados_iniciais_nf['cnpj_encontrado'] = cnpj_encontrado\n",
    "                \n",
    "                \n",
    "    # Saímos do loop, agora vamos verificar qual template usar\n",
    "    if de_para_pm:\n",
    "        template_usar = templates.get((de_para_pm, cnpj_encontrado))\n",
    "        logging.info(f'usara template {template_usar} para: {cnpj_encontrado}')\n",
    "        # print(template_usar)\n",
    "        dados_iniciais_nf['model'] = template_usar\n",
    "        if not template_usar:\n",
    "            template_usar = templates.get((de_para_pm, None), \"TEMPLATE_NAO_ENCONTRADO\")\n",
    "            dados_iniciais_nf['model'] = 'NAO_ENC.' \n",
    "            action_item_row_info = 'BREAK_PROCESS'\n",
    "            information_row_info = 'model nao encontrado'\n",
    "    else:\n",
    "        template_usar = \"TEMPLATE_NAO_ENCONTRADO\"\n",
    "        dados_iniciais_nf['model'] = 'NAO_ENC.'\n",
    "        action_item_row_info = 'BREAK_PROCESS'\n",
    "        information_row_info = 'model nao encontrado'\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    #Confirmando se template existe em frames    \n",
    "    try:        \n",
    "        f_type = 'frame'\n",
    "        #template_usar = 'SAO_PEDRO_SUPERMIX'\n",
    "        result = filtrar_df(frames_nf_v4_df, type=f_type, de_para_pm=de_para_pm, model=template_usar)\n",
    "        model = result['model'].values[0]\n",
    "        if model:\n",
    "            template_oficial = model\n",
    "            if model == template_usar:\n",
    "                dados_iniciais_nf['model'] = template_oficial\n",
    "            else:    \n",
    "                template_usar = \"necessario cadastrar\"\n",
    "                dados_iniciais_nf['model'] = \"CADASTRAR\"\n",
    "                \n",
    "            dados_iniciais_nf['model'] = template_usar\n",
    "        else:\n",
    "            template_usar = \"necessario cadastrar\"\n",
    "            dados_iniciais_nf['model'] = \"CADASTRAR\"\n",
    "\n",
    "                \n",
    "    except Exception as e:\n",
    "       error_msg = (f\"Erro busca do template: {e}\") \n",
    "    \n",
    "    dados_iniciais_nf['action_item'] = action_item_row_info \n",
    "    dados_iniciais_nf['informations'] = information_row_info         \n",
    "        \n",
    "    return dados_iniciais_nf  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dicionário para mapear palavras-chave a rótulos\n",
    "mapeamento_palavras_chave = {\n",
    "    \"relatorio\": \"prov_relatorio\",\n",
    "    \"listagem\": \"prov_listagem\",\n",
    "    \"NF\": \"prov_nota_fiscal\",\n",
    "    \"nf\": \"prov_nota_fiscal\",\n",
    "    \"relatorio\": \"prov_listagem\",\n",
    "    \"sintetico\": \"prov_listagem\",\n",
    "    \"livro\": \"prov_livro_registro\",\n",
    "    \"sintético\": \"prov_listagem\",\n",
    "    \"nota\": \"prov_nota_fiscal\",\n",
    "    \"zip\": \"doc_zip\",\n",
    "    \"rar\": \"doc_rar\",\n",
    "    \"valores\": \"prov_dinheiro\",\n",
    "}\n",
    "\n",
    "# Dicionário mapeando rótulos a ações sugeridas\n",
    "sugestoes_acao = {\n",
    "    \"prov_relatorio\": \"NO_PROCESS\",\n",
    "    \"prov_listagem\": \"NO_PROCESS\",\n",
    "    \"prov_nota_fiscal\": \"NO_PROCESS\",\n",
    "    \"sem_rotulo\": \"MANUAL_REV\",\n",
    "    \"prov_livro_registro\": \"NO_PROCESS\",\n",
    "    \"doc_nao_pdf\": \"verificar\",\n",
    "    \"nao_pdf\": \"NO_PROCESS\",\n",
    "    \"doc_zip\": \"NO_PROCESS\",\n",
    "    \"pdf_mul_paginas\": \"SPLIT\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# 2.Testando\n",
    "nome_arquivo = 'batatinha_quando_nasce.pdf' # 'pre-processamento'\n",
    "#palavra_chave, rotulo, acao_sugerida = define_rotulo_acao(nome_arquivo, debug)\n",
    "#print(f'nome_arquivo: {nome_arquivo:>55} | palavra_chave: {palavra_chave:>20} | rotulo: {rotulo:20} | acao_sugerida: {acao_sugerida:30}')    \n",
    "\n",
    "frames_nf_v4_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> ImportaÇao do df_root_pipe </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>date_time</th>\n",
       "      <th>batch</th>\n",
       "      <th>fase_processo</th>\n",
       "      <th>nome_atividade</th>\n",
       "      <th>status_documento</th>\n",
       "      <th>acao_executada</th>\n",
       "      <th>original_file_name</th>\n",
       "      <th>directory</th>\n",
       "      <th>one_page</th>\n",
       "      <th>pages</th>\n",
       "      <th>palavra_chave</th>\n",
       "      <th>document_tag</th>\n",
       "      <th>action_item</th>\n",
       "      <th>level</th>\n",
       "      <th>parent_document_unique_id</th>\n",
       "      <th>file_hash</th>\n",
       "      <th>file_path</th>\n",
       "      <th>informations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>923d536b-2f47-4994-8c77-538eab1c3c5f</th>\n",
       "      <td>1.0</td>\n",
       "      <td>20/09/2023 13:10:24</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NF CRJ PRIMEIRA QUINZENA DE JULHO DE 2023.pdf</td>\n",
       "      <td>11756286.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>b3acfffea4847108d0064ecbd62a73359961f88741a037...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28303a7b-07ee-49f3-b7d6-7bc0f60baaa9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>20/09/2023 13:10:24</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Nota Fiscal Eletrônica Quallit 24072023.pdf</td>\n",
       "      <td>11766341.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nota</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>ae37292a66dff093838f3cde0da8bea332ec81241b1d5f...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8a12fa3-90a8-4317-baaf-799014f95fb5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>20/09/2023 13:10:24</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>NFE 20237.pdf</td>\n",
       "      <td>11624359.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NF</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>1976ffe84e27b6bb1a5840018c4a47f6bac483b10f686a...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44eaa169-167f-4a45-b60d-7d4f760a0664</th>\n",
       "      <td>4.0</td>\n",
       "      <td>20/09/2023 13:10:24</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Ampla.pdf</td>\n",
       "      <td>11777624.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>default</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>ad2522b53349fffd748376bca7fe4b90fd59359971d992...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80fc007c-a947-470d-923e-dad0b1ed5557</th>\n",
       "      <td>5.0</td>\n",
       "      <td>20/09/2023 13:10:24</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>analise</td>\n",
       "      <td>scan_analise</td>\n",
       "      <td>PREPROCESS_EXTRACT</td>\n",
       "      <td>Analise</td>\n",
       "      <td>Supernova.pdf</td>\n",
       "      <td>11777624.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>default</td>\n",
       "      <td>prov_nota_fiscal</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>c26954dad71d508d4b5315f69dc4c6291a1c5bf70548e7...</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      seq            date_time     batch  \\\n",
       "document_unique_id                                                         \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f  1.0  20/09/2023 13:10:24  Batch_21   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9  2.0  20/09/2023 13:10:24  Batch_21   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5  3.0  20/09/2023 13:10:24  Batch_21   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664  4.0  20/09/2023 13:10:24  Batch_21   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557  5.0  20/09/2023 13:10:24  Batch_21   \n",
       "\n",
       "                                     fase_processo nome_atividade  \\\n",
       "document_unique_id                                                  \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f       analise   scan_analise   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9       analise   scan_analise   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5       analise   scan_analise   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664       analise   scan_analise   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557       analise   scan_analise   \n",
       "\n",
       "                                        status_documento acao_executada  \\\n",
       "document_unique_id                                                        \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f  PREPROCESS_EXTRACT        Analise   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9  PREPROCESS_EXTRACT        Analise   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5  PREPROCESS_EXTRACT        Analise   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664  PREPROCESS_EXTRACT        Analise   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557  PREPROCESS_EXTRACT        Analise   \n",
       "\n",
       "                                                                 original_file_name  \\\n",
       "document_unique_id                                                                    \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f  NF CRJ PRIMEIRA QUINZENA DE JULHO DE 2023.pdf   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9    Nota Fiscal Eletrônica Quallit 24072023.pdf   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5                                  NFE 20237.pdf   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664                                      Ampla.pdf   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557                                  Supernova.pdf   \n",
       "\n",
       "                                       directory  one_page  pages  \\\n",
       "document_unique_id                                                  \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f  11756286.0       1.0    1.0   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9  11766341.0       1.0    1.0   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5  11624359.0       1.0    1.0   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664  11777624.0       1.0    1.0   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557  11777624.0       1.0    1.0   \n",
       "\n",
       "                                     palavra_chave      document_tag  \\\n",
       "document_unique_id                                                     \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f            NF  prov_nota_fiscal   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9          nota  prov_nota_fiscal   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5            NF  prov_nota_fiscal   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664       default  prov_nota_fiscal   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557       default  prov_nota_fiscal   \n",
       "\n",
       "                                     action_item  level  \\\n",
       "document_unique_id                                        \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f     PROCESS    3.0   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9     PROCESS    3.0   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5     PROCESS    3.0   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664     PROCESS    3.0   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557     PROCESS    3.0   \n",
       "\n",
       "                                                 parent_document_unique_id  \\\n",
       "document_unique_id                                                           \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "\n",
       "                                                                              file_hash  \\\n",
       "document_unique_id                                                                        \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f  b3acfffea4847108d0064ecbd62a73359961f88741a037...   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9  ae37292a66dff093838f3cde0da8bea332ec81241b1d5f...   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5  1976ffe84e27b6bb1a5840018c4a47f6bac483b10f686a...   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664  ad2522b53349fffd748376bca7fe4b90fd59359971d992...   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557  c26954dad71d508d4b5315f69dc4c6291a1c5bf70548e7...   \n",
       "\n",
       "                                                                              file_path  \\\n",
       "document_unique_id                                                                        \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f  pipeline_extracao_documentos/2_documentos_para...   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9  pipeline_extracao_documentos/2_documentos_para...   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5  pipeline_extracao_documentos/2_documentos_para...   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664  pipeline_extracao_documentos/2_documentos_para...   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557  pipeline_extracao_documentos/2_documentos_para...   \n",
       "\n",
       "                                      informations  \n",
       "document_unique_id                                  \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f           NaN  \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9           NaN  \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5           NaN  \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664           NaN  \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557           NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 Apagar os arquivos PDF:Zone\n",
    "# apagar_zone(documentos_extracao_path)\n",
    "\n",
    "batch_name = \"Batch_21\" #Excepcionalmente\n",
    "# fake_parent_document_unique_id = generate_unique_id()\n",
    "\n",
    "nome_formado_json = batch_name +\".json\"\n",
    "\n",
    "\n",
    "# 0. XXX SE deseja importar o DF df_analise_pipe\n",
    "\n",
    "df_root_pipe_path = \"processamentos/df_root_analise4.xlsx\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria df_documento_recebido\n",
    "df_root_pipe = pd.read_excel(df_root_pipe_path)\n",
    "\n",
    "# Ajusta o indice\n",
    "df_root_pipe.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "\n",
    "df_root_pipe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b>2.4</b> ExecuÇao do Pipeline de Extracao </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta e o principio da melhor funcao do mundo\n",
    "def extracao_pipeline(qualquer_df, fase, atividade, status, debug=False, prestador=True, tomador=True, servicos=True, total=True, cnae=True, valores_impostos=True, complementares=True, outras_informacoes=True, observacoes=True):\n",
    "    \n",
    "    doc_info = {}\n",
    "    resumo = {}\n",
    "    row_teste_info = []\n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    func_fase = fase\n",
    "    func_atividade = atividade\n",
    "    func_status = status\n",
    "    lista_dicts = []\n",
    "    conf_processo = {}\n",
    "    lista_conferencia = []\n",
    "   \n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        dados_iniciais = {}\n",
    "        row_info = row.to_dict()\n",
    "        message_erro = []\n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        map_document_unique_id = idx\n",
    "        map_seq = row['seq']\n",
    "        map_batch_name = row['batch']\n",
    "        map_fase_processo = row['fase_processo']\n",
    "        map_nome_atividade = row['nome_atividade']\n",
    "        map_status_documento = row['status_documento']\n",
    "        map_original_file_name = row['original_file_name']\n",
    "        map_directory = row['directory']\n",
    "        map_one_page = row['one_page']\n",
    "        map_palavra_chave = row['palavra_chave']\n",
    "        map_document_tag = row['document_tag']\n",
    "        map_action_item = row['action_item']\n",
    "        map_level = row['level']\n",
    "        file_path = row['file_path']\n",
    "        row_info['document_unique_id'] = map_document_unique_id\n",
    "    \n",
    "        # XXX Nivel 1 - Definindo que documentos serao tratados    \n",
    "        if map_status_documento == 'PREPROCESS_EXTRACT':\n",
    "            \n",
    "            action_item_row_info = 'CONTINUE_PROCESS'\n",
    "            row_info['action_item'] = action_item_row_info\n",
    "            information_row_info = 'iniciado processamento'\n",
    "            row_info['informations'] = information_row_info\n",
    "            \n",
    "            \n",
    "            # 0. DADOS GERAIS DOCUMENTO\n",
    "            section = \"0. DADOS INICIAIS\"\n",
    "            try:\n",
    "                valores = {}\n",
    "                # 0.1. Busco prefeitura, de/para e modelo - se nao achar seta status documento para NO_PROCESS\n",
    "                valores = processar_dados_iniciais(idx, row, row_info, section, map_directory, map_original_file_name, file_path, debug)\n",
    "            except Exception as e:\n",
    "                msg = (f'Erro ao processar_dados_iniciais: {e}')\n",
    "            finally:\n",
    "                row_info.update(valores)\n",
    "            \n",
    "            #map_status_documento_row_info = row_info.get('status_documento')\n",
    "            action_item_row_info = row_info.get('action_item')\n",
    "            \n",
    "            # XXX Nivel 2 - Definindo que os documentos legiveis serao tratados\n",
    "            if action_item_row_info == 'CONTINUE_PROCESS':\n",
    "                \n",
    "                prefeitura_map = row_info.get('prefeitura')\n",
    "                pdf_pesquisavel_map = row_info.get('pdf_pesquisavel')\n",
    "                de_para_map = row_info.get('de_para_pm')\n",
    "                model_map = row_info.get('model')\n",
    "                \n",
    "                \n",
    "                if not pdf_pesquisavel_map:\n",
    "                    # NOVO PROCESSO DE TRATAMENTO DE IMAGEM - Convertendo a imagem para numpy array\n",
    "                    if debug:\n",
    "                        print(\"irei gerar a imagem_np\")\n",
    "                    imagem_gray, image_resized_name = convert_resize_gray(map_original_file_name, file_path, image_resized_path)\n",
    "                    imagem_gray_rgb = imagem_gray.convert(\"RGB\")\n",
    "                    imagem_gray_np = np.array(imagem_gray_rgb)\n",
    "                    row_info['image_np'] = imagem_gray_np\n",
    "                \n",
    "                # 1. CABECALHO\n",
    "                # try:\n",
    "                section = \"1. CABECALHO\"\n",
    "                valores = {}\n",
    "                #valores_P = {}\n",
    "                f_0 = 1\n",
    "                f_1 = 1\n",
    "                mapping_method = \"frame_&_sframe_field\" # significa que as coordenadas estao em frames e os valores dos campos nos sframe_fields\n",
    "                context_mapping = \"data_cabecalho\"\n",
    "                def_replace = True \n",
    "                \n",
    "                if pdf_pesquisavel_map:\n",
    "                    valores = extrai_cabecalho_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                    row_info.update(valores) \n",
    "                else:\n",
    "                    valores = processar_cabecalho_R_PDF(idx, row, row_info, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, map_original_file_name, file_path, debug)   \n",
    "                    row_info.update(valores)\n",
    "         \n",
    "                #status_documento_row_info = row_info.get('status_documento')\n",
    "                action_item_row_info = row_info.get('action_item')\n",
    "                information_row_info = row_info.get('informations')   \n",
    "                \n",
    "                \n",
    "                # XXX Nivel 3 - Definindo que os documentos legiveis serao tratados realmente\n",
    "                if action_item_row_info == 'BREAK_PROCESS':\n",
    "                    #msg = (f'Processo inicial: {map_batch_name} | {map_original_file_name} | diretorio: {map_directory} - information_row_info: {information_row_info}')\n",
    "                    if debug:\n",
    "                        print(f'\\nINFELIZMENTE - seq: {map_seq} doc: {map_original_file_name} dir: {map_directory} - NAO SERA PROCESSADO  | inf: {information_row_info} \\n\\n')\n",
    "               \n",
    "                    #row_info['informations'] = msg\n",
    "                    # logging.error(msg)\n",
    "                    lista_dicts.append(row_info)\n",
    "                    continue \n",
    "                \n",
    "                    \n",
    "                elif action_item_row_info == 'CONTINUE_PROCESS':\n",
    "                    if debug:\n",
    "                        print(f'\\nEBA, BORA CONTINUAR - seq: {map_seq} - proxima section: | PDF Pesquisavel: {pdf_pesquisavel_map} doc: {map_original_file_name} dir: {map_directory} | action_item: {action_item_row_info} | inf: {information_row_info} \\n\\n')\n",
    "                        print()\n",
    "                        print(valores)\n",
    "                    \n",
    "                    information_row_info = 'Cabecalho processado'\n",
    "                    row_info['informations'] = information_row_info\n",
    "                    \n",
    "                    \n",
    "                    guarda_texto_doc = {}\n",
    "                    guarda_texto_doc, linhas = cria_guarda_doc_ref_R_PDF(idx, row, de_para_map, model_map, map_original_file_name, file_path, image_resized_path, debug)\n",
    "            \n",
    "                    # 2. PRESTADOR DE SERVIÇO\n",
    "                    if prestador == True:\n",
    "                        section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        valores = {}\n",
    "                        erros_prestador = {}\n",
    "                        data_tomador = {}\n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valores = extrai_prestador_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                        else:\n",
    "                            valores = extrai_prestador_R_PDF(idx, row, row_info, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                        \n",
    "                        if not isinstance(valores, dict):\n",
    "                            msg_erro = (f\"\\nErro na linha {idx}: 'valores' não é um dicionário. Tipo: {type(valores)}, Valor: {valores}\")\n",
    "                        else:\n",
    "                            row_info.update(valores)\n",
    "                            \n",
    "                        # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                        # if debug:\n",
    "                        #     print(msg)\n",
    "                        # logging.info(msg)\n",
    "                    \n",
    "                    # 3. TOMADOR DE SERVIÇO\n",
    "                    if tomador == True:\n",
    "                        section = \"3. TOMADOR DE SERVIÇO\"\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        \n",
    "                        valores = {}\n",
    "                        erros = []\n",
    "                        data_tomador = {}\n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        \n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valores = extrai_tomador_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                        else:   \n",
    "                            valores = extrai_tomador_R_PDF(idx, row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                            \n",
    "                        if not isinstance(valores, dict):\n",
    "                            print(f\"\\nErro na linha {idx}: 'valores' não é um dicionário. Tipo: {type(valores)}, Valor: {valores}\")\n",
    "                        else:\n",
    "                            row_info.update(valores)\n",
    "                        \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)\n",
    "                    \n",
    "                    # 4. DESCRIMINACAO DOS SERVIÇOS\n",
    "                    if servicos == True:\n",
    "                        if debug:\n",
    "                            print(f'processando servicos para: {map_original_file_name}')\n",
    "                        section = \"4. DESCRIMINACAO DOS SERVIÇOS\"\n",
    "                        valores = {}\n",
    "                        nf_data_servico = {} \n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        \n",
    "                        if pdf_pesquisavel_map:\n",
    "                            nf_data_servico = processar_servicos_pdf_pesquisavel(row, pdf_pesquisavel_map, model_map, map_original_file_name, file_path, debug)\n",
    "                        else:\n",
    "                            label = \"discriminacao_servicos\"\n",
    "                            tipo = \"field_box\"\n",
    "                            def_replace = True\n",
    "                            \n",
    "                            # ItSs  working\n",
    "                            texto_extraido = extracao_documento_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_map, model_map, def_replace, map_original_file_name, debug)\n",
    "                            row_info[label] = texto_extraido\n",
    "                            \n",
    "                        msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                        if debug:\n",
    "                            print(msg)\n",
    "                        logging.info(msg)     \n",
    "\n",
    "\n",
    "                        try:\n",
    "                            texto_extraido = nf_data_servico['discriminacao_servicos'] \n",
    "                            row_info['discriminacao_servicos'] = texto_extraido \n",
    "                        except Exception as e:\n",
    "                            msg = (f\"doc: {map_original_file_name} | {e}\")\n",
    "                            discrimanacao_servico = \"Descricao nao encontrada\"\n",
    "                            row_info['discriminacao_servicos'] = texto_extraido\n",
    "\n",
    "                    \n",
    "                    # 5. VALOR TOTAL\n",
    "                    if total == True:\n",
    "                        section = \"5. VALOR TOTAL\"\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        #valores = {}\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valor_total_documento = processar_valor_total_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, model_map, map_original_file_name, file_path, debug)\n",
    "                            if valor_total_documento:\n",
    "                                if debug:\n",
    "                                    print(f'\\nvalor_total_documento: {valor_total_documento} | doc: {map_original_file_name}\\n')\n",
    "                                row_info['valor_total_nota'] = valor_total_documento\n",
    "                        else:\n",
    "                            label = \"valor_total_nota\"\n",
    "                            tipo = \"field_box\"\n",
    "                            def_replace = True\n",
    "                            texto_extraido = extracao_documento_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_map, model_map, def_replace, map_original_file_name, debug)\n",
    "                            if texto_extraido: \n",
    "                                valor_total_match = re.search(r'R\\$ ([\\d,.]+)', texto_extraido)\n",
    "                                if valor_total_match:\n",
    "                                    valor_total_sem_formatacao = valor_total_match.group(1).replace('.', '').replace(',', '.')\n",
    "                                    try:\n",
    "                                        # valores['secao'] = section\n",
    "                                        valor_total_documento = float(valor_total_sem_formatacao)\n",
    "                                    except Exception as e:\n",
    "                                        # valores['secao'] = section\n",
    "                                        valor_total_documento = 0.0\n",
    "                                        msg = (f'Processo inicial: {batch_name} | {map_original_file_name:>25} | diretorio: {map_directory} | {e}')\n",
    "                                        #logging.error(f\" {batch_name} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")\n",
    "                        \n",
    "                                    if valor_total_documento:\n",
    "                                        if debug:\n",
    "                                            print(f'\\nvalor_total_documento: {valor_total_documento} | doc: {map_original_file_name}\\n')\n",
    "                                        row_info['valor_total_nota'] = valor_total_documento\n",
    "         \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)\n",
    "                    \n",
    "                    # 6. CNAE e Item da Lista de Serviços \n",
    "                    if cnae == True:\n",
    "                        section = \"6. CNAE e Item da Lista de Serviços\"\n",
    "                        data_box_valores = {}\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        f_0_cnae = 0.95\n",
    "                        f_1_cnae = 1.15\n",
    "                        f_0_it = 0.95     #0.95\n",
    "                        f_1_it = 1.15    # 1\n",
    "                        \n",
    "                        mapping_method = \"frame_&_sframe_field\" # significa que as coordenadas estao em frames e os valores dos campos nos sframe_fields\n",
    "                        context_mapping = \"data_cnae\"\n",
    "                        def_replace = True\n",
    "                        \n",
    "                        if pdf_pesquisavel_map:\n",
    "                            data_box_valores = extracao_documento_CNAE_ITEM_PDF_P(idx, row, row_info, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, def_replace, map_original_file_name, file_path, debug)\n",
    "                        else:\n",
    "                            data_box_valores = extracao_documento_CNAE_ITEM_R_PDF(idx, row, row_info, guarda_texto_doc, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, def_replace, map_original_file_name, file_path, debug)\n",
    "                            \n",
    "                        if data_box_valores:\n",
    "                            row_info.update(data_box_valores)    \n",
    "\n",
    "                    \n",
    "                    # 7. VALORES E IMPOSTOS\n",
    "                    if valores_impostos == True:\n",
    "                        section = \"7. VALORES E IMPOSTOS\"\n",
    "                        # if debug:\n",
    "                        print(f'processando {section} para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                        valores = {}\n",
    "                        nf_data_valores = {}\n",
    "                        lista_impostos = []\n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valores = extrai_valores_impostos_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                            row_info.update(valores)\n",
    "                        \n",
    "                        else:\n",
    "                            tipo = \"field_box\"\n",
    "                            father_value = \"5_frame_valores_impostos\"\n",
    "                            valores = extracao_impostos_R_PDF(section, tipo, father_value, de_para_map, model_map, map_original_file_name, file_path)\n",
    "                            #print(valores)\n",
    "                            row_info.update(valores)\n",
    "                        \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg) \n",
    "                    \n",
    "                    # 8. DADOS COMPLEMENTARES\n",
    "                    if complementares == True:\n",
    "                        section = '8. DADOS COMPLEMENTARES'\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        nf_data_dados_complementares = {}\n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            nf_data_valores = extrai_dados_complementares_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path, debug)\n",
    "                        else:\n",
    "                            label = \"dados_complementares\"\n",
    "                            tipo = \"field_box\"\n",
    "                            def_replace = False\n",
    "                            # ItSs  working\n",
    "                            texto_extraido = extracao_complementar_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_map, model_map, def_replace, map_original_file_name, debug)\n",
    "                            row_info[label] = texto_extraido  \n",
    "\n",
    "                    \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)     \n",
    "                    \n",
    "                    # 9. OUTRAS INFORMAÇOES / CRITICAS\n",
    "                    if outras_informacoes == True:\n",
    "                        section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')\n",
    "                        tipo = \"field_box\"\n",
    "                        father_value = \"5_frame_inf_criticas\"\n",
    "                        valores = {} \n",
    "                        nf_data_outras_informacoes = {}\n",
    "                        f_0 = 1\n",
    "                        f_1 = 1\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valores = extrai_outras_informacoes_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "                            if valores:\n",
    "                                row_info.update(valores)\n",
    "                        else:\n",
    "                            section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "                            tipo = \"field_box\"\n",
    "                            father_value = \"5_frame_inf_criticas\"\n",
    "                            valores = extracao_inforacoes_criticas_R_PDF(section, tipo, father_value, de_para_map, model_map, map_original_file_name, file_path)\n",
    "                            if valores:\n",
    "                                row_info.update(valores)\n",
    "                            \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)\n",
    "                    # logging.info(msg)          \n",
    "                            \n",
    "                    \n",
    "                    # 10. OBSERVACOES\n",
    "                    if observacoes == True:  \n",
    "                        section = \"10. OBSERVACOES\"\n",
    "                        if debug:\n",
    "                            print(f'processando {section} para: {map_original_file_name}')  \n",
    "                        data_observacao = {}\n",
    "                        valores = {}\n",
    "                        f_0 = 0.9\n",
    "                        f_1 = 1.1\n",
    "                        if pdf_pesquisavel_map:\n",
    "                            valores = extrai_outras_informacoes_PDF_P(row, pdf_pesquisavel_map, de_para_map, model_map, f_0, f_1, map_original_file_name, file_path)\n",
    "                            if valores:\n",
    "                                row_info.update(valores)\n",
    "                        else:\n",
    "                            section = '10. OBSERVACOES'\n",
    "                            tipo = \"field_box\"\n",
    "                            father_value = \"6_section_inf_complementares_criticas\" \n",
    "                            \n",
    "                            label = \"observacao\"\n",
    "                            tipo = \"field_box\"\n",
    "                            def_replace = True\n",
    "                            valores = extracao_observacoees_R_PDF(idx, row, guarda_texto_doc, section, tipo, label, de_para_map, model_map, def_replace, map_original_file_name, debug)\n",
    "                            if valores:\n",
    "                                row_info.update(valores)\n",
    "                    \n",
    "                    # msg = (f'secao: {section:>15} processada para: {map_original_file_name} - diretorio: {map_directory}')\n",
    "                    # if debug:\n",
    "                    #     print(msg)           \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                lista_dicts.append(row_info)\n",
    "                \n",
    "                \n",
    "            elif action_item_row_info == 'BREAK_PROCESS':\n",
    "                \n",
    "                msg = (f'Documento sem qualidade para pesquisa inicial: {map_batch_name} | {map_original_file_name} | diretorio: {map_directory}')\n",
    "                row_info['informations'] = msg  \n",
    "                \n",
    "            \n",
    "                lista_dicts.append(row_info)\n",
    "                continue\n",
    "                         \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        elif map_status_documento == 'NO_PROCESS':\n",
    "            msg = (f'Documento nao sera tratado neste escopo: {map_batch_name} | {map_original_file_name} | diretorio: {map_directory}')\n",
    "            row_info['action_item'] = \"NO_PROCESS\"    \n",
    "            row_info['informations'] = msg \n",
    "            lista_dicts.append(row_info)\n",
    "            continue\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        #lista_dicts.append(row_info)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    logging.info(f'processamento finalizado para: {batch_name}') \n",
    "    \n",
    "    print(f'processamento de {i} documentos')\n",
    "    \n",
    "    novo_df = pd.DataFrame(lista_dicts)\n",
    "    \n",
    "    return novo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 500.0 600.0 700.0\n",
      "processando 7. VALORES E IMPOSTOS para: NF CRJ PRIMEIRA QUINZENA DE JULHO DE 2023.pdf - diretorio: 11756286.0\n",
      "0.0 500.0 600.0 700.0\n",
      "processando 7. VALORES E IMPOSTOS para: Nota Fiscal Eletrônica Quallit 24072023.pdf - diretorio: 11766341.0\n",
      "0.0 500.0 600.0 700.0\n",
      "processando 7. VALORES E IMPOSTOS para: NFE 20237.pdf - diretorio: 11624359.0\n",
      "processando 7. VALORES E IMPOSTOS para: Ampla.pdf - diretorio: 11777624.0\n",
      "processando 7. VALORES E IMPOSTOS para: Supernova.pdf - diretorio: 11777624.0\n",
      "processando 7. VALORES E IMPOSTOS para: Blue Lord.pdf - diretorio: 11777624.0\n",
      "0.0 500.0 600.0 700.0\n",
      "processando 7. VALORES E IMPOSTOS para: NF 2023158.pdf - diretorio: 11285853.0\n",
      "0.0 500.0 600.0 700.0\n",
      "processando 7. VALORES E IMPOSTOS para: NF 2023157.pdf - diretorio: 11285853.0\n",
      "processando 7. VALORES E IMPOSTOS para: nota_07_2023.pdf - diretorio: 11778425.0\n",
      "0.0 500.0 600.0 700.0\n",
      "processando 7. VALORES E IMPOSTOS para: NFS-e 22.pdf - diretorio: 11778003.0\n",
      "0.0 500.0 600.0 700.0\n",
      "processando 7. VALORES E IMPOSTOS para: NFSe-e 23.pdf - diretorio: 11778003.0\n",
      "0.0 500.0 600.0 700.0\n",
      "processando 7. VALORES E IMPOSTOS para: NF 202315- SJDI 35 JUL 23.pdf - diretorio: 11777556.0\n",
      "0.0 500.0 600.0 700.0\n",
      "processando 7. VALORES E IMPOSTOS para: Heidelberg 21 07 2023 NOTA FISCAL.pdf - diretorio: 11779531.0\n",
      "processamento de 37 documentos\n"
     ]
    }
   ],
   "source": [
    "# XXX Processando o pipeline\n",
    "ver = get_template_version('MAGE')\n",
    "# analisar_pdf_pesquisavel\n",
    "fase = 'analise'\n",
    "atividade = 'PREPROCESS' \n",
    "status = 'PREPROCESS_EXTRACT'\n",
    "raw_document_list = []\n",
    "dados_prest = {}\n",
    "\n",
    "lista_dicts = []\n",
    "logging.info(f'Execuçao do pipeline para {batch_name} | df_root_pipe: {df_root_pipe_path} fase: {fase} atividade: {atividade} status: {status}  template: {ver}')\n",
    "\n",
    "\n",
    "# XXX 1.Processar todas as secoes do documento\n",
    "df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=True, tomador=True, servicos=True, total=True, cnae=True, valores_impostos=True, complementares=True, outras_informacoes=True, observacoes=True)\n",
    "\n",
    "# 2. Processar valor Total\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=True, cnae=False, valores_impostos=False, complementares=False, outras_informacoes=False, observacoes=False)\n",
    "\n",
    "# 3. Processar CNAE\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=False, cnae=True, valores_impostos=False, complementares=False, outras_informacoes=False, observacoes=False)\n",
    "\n",
    "# 4. Processar Impostos\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=False, cnae=False, valores_impostos=, complementares=False, outras_informacoes=False, observacoes=False)\n",
    "\n",
    "# 5. complementar e observaçoes\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=False, cnae=False, valores_impostos=False, complementares=True, outras_informacoes=True, observacoes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando DF para analises\n",
    "df.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "ordem_status = ['PREPROCESS_EXTRACT', 'NO_PROCESS', 'root_analise']\n",
    "ordem_action_item = ['CONTINUE_PROCESS', 'BREAK_PROCESS', 'NO_PROCESS']\n",
    "\n",
    "\n",
    "df['status_documento'] = pd.Categorical(df['status_documento'], categories=ordem_status, ordered=True)\n",
    "df['action_item'] = pd.Categorical(df['action_item'], categories=ordem_action_item, ordered=True)\n",
    "\n",
    "df.sort_values(by=['status_documento', 'action_item', 'seq'], ascending=[True, True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o subset para analise\n",
    "df_conf = df[['seq', 'original_file_name', 'directory', 'status_documento', 'model', 'secao', 'action_item', 'numero_nota_fiscal',  'competencia', 'dt_hr_emissao', 'codigo_verificacao', 'conf_cod', 'informations', 'cnae', 'item_lista_servicos']]\n",
    "df_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>level</th>\n",
       "      <th>parent_document_unique_id</th>\n",
       "      <th>file_hash</th>\n",
       "      <th>batch</th>\n",
       "      <th>directory</th>\n",
       "      <th>original_file_name</th>\n",
       "      <th>pages</th>\n",
       "      <th>one_page</th>\n",
       "      <th>file_path</th>\n",
       "      <th>...</th>\n",
       "      <th>outras_retencoes</th>\n",
       "      <th>valor_liquido</th>\n",
       "      <th>exigibilidade_iss</th>\n",
       "      <th>regime_tributacao</th>\n",
       "      <th>simples_nacional</th>\n",
       "      <th>issqn_retido</th>\n",
       "      <th>local_pretacao_servico</th>\n",
       "      <th>local_incidencia</th>\n",
       "      <th>dados_complementares</th>\n",
       "      <th>observacao</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>923d536b-2f47-4994-8c77-538eab1c3c5f</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>b3acfffea4847108d0064ecbd62a73359961f88741a037...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11756286.0</td>\n",
       "      <td>NF CRJ PRIMEIRA QUINZENA DE JULHO DE 2023.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7599.57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DADOS COMPLEM ENTARES OUTRAS INFORMAÇÕES / CRI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28303a7b-07ee-49f3-b7d6-7bc0f60baaa9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>ae37292a66dff093838f3cde0da8bea332ec81241b1d5f...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11766341.0</td>\n",
       "      <td>Nota Fiscal Eletrônica Quallit 24072023.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://nota.pmspa.rj.gov.br 1/1 DADOS COMPLEM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8a12fa3-90a8-4317-baaf-799014f95fb5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>1976ffe84e27b6bb1a5840018c4a47f6bac483b10f686a...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11624359.0</td>\n",
       "      <td>NFE 20237.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37693.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DADOS COMPLEMENTARES ITAU, Agencia : 6101, Con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44eaa169-167f-4a45-b60d-7d4f760a0664</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>ad2522b53349fffd748376bca7fe4b90fd59359971d992...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11777624.0</td>\n",
       "      <td>Ampla.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2750.00</td>\n",
       "      <td>None</td>\n",
       "      <td>Microempresário Individual (MEI)</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Sim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>São Pedro da Aldeia - RJ</td>\n",
       "      <td>{'dados_complementares': ''}</td>\n",
       "      <td>Valor Aproximado dos Tributos Federais R$ 369,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80fc007c-a947-470d-923e-dad0b1ed5557</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>c26954dad71d508d4b5315f69dc4c6291a1c5bf70548e7...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11777624.0</td>\n",
       "      <td>Supernova.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>None</td>\n",
       "      <td>Microempresário Individual (MEI)</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Sim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>São Pedro da Aldeia - RJ</td>\n",
       "      <td>{'dados_complementares': ''}</td>\n",
       "      <td>Valor Aproximado dos Tributos Federais R$ 672,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82fa0c74-b18d-49dd-bafc-3b05ec33edc5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>63b6603be7b1df359e67b96e9e128336b46fa3fbcfdadc...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11777624.0</td>\n",
       "      <td>Blue Lord.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>None</td>\n",
       "      <td>Microempresário Individual (MEI)</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Sim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>São Pedro da Aldeia - RJ</td>\n",
       "      <td>{'dados_complementares': ''}</td>\n",
       "      <td>Valor Aproximado dos Tributos Federais R$ 134,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6ffe74e6-64f5-421b-b1cc-f2022c3d8928</th>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>abcfcce04cabd261b30730f04cbb98e030f2a765bb31aa...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11285853.0</td>\n",
       "      <td>NF 2023158.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://nota.pmspa.rj.gov.br 1/1 DADOS COMPLEM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ab93e7cb-15de-48ac-a35f-32891b339bbd</th>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>c8c5ae8ccca5e9befb6445e21701dc99266746e877c630...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11285853.0</td>\n",
       "      <td>NF 2023157.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://nota.pmspa.rj.gov.br 1/1 DADOS COMPLEM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b639d369-35ac-4271-85fc-f2aed845d19f</th>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>33bc08587f5c227ab3e035011ed9b20340f99599e65ff7...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11778425.0</td>\n",
       "      <td>nota_07_2023.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5070.00</td>\n",
       "      <td>Exigível</td>\n",
       "      <td>Microempresário Individual (MEI)</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Sim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>São Pedro da Aldeia - RJ</td>\n",
       "      <td>{'dados_complementares': None}</td>\n",
       "      <td>Valor Aproximado dos Tributos Federais R$ 681,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2dd60abe-6265-4ef0-8c88-fe57951cba63</th>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>0039c12e1e972817f623e70ce05f54cb037a6a7e55cc3a...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11778003.0</td>\n",
       "      <td>NFS-e 22.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8523.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://nota.pmspa.rj.gov.br/?serv=20 1/1 DADO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b762be55-5956-4d97-8c01-a2e7f878e724</th>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>940b1ffaf961f72198f07d10909177a62706acf88aa333...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11778003.0</td>\n",
       "      <td>NFSe-e 23.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3774.43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://nota.pmspa.rj.gov.br/?serv=20 1/1 DADO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0c9b23e9-698c-4936-b4a2-21f9b3c9ed67</th>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>c0fd28f51a6eb88508566d880c95d8c99490067af70311...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11777556.0</td>\n",
       "      <td>NF 202315- SJDI 35 JUL 23.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8510.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://nota.pmspa.rj.gov.br 1/1 DADOS COMPLEM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fcc49b4b-4461-446b-8374-d4fd5bc439e5</th>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>4073b25641024505cd6a5b7052266f1c494acfe517fcb1...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11779531.0</td>\n",
       "      <td>Heidelberg 21 07 2023 NOTA FISCAL.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://nota.pmspa.rj.gov.br 1/1 R$ 0,00 R$ 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da9300be-91d8-4a3f-a164-bb3d5f2125ca</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>59274230e7fb7897397bb426ac31d953e2a14def1fd25e...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11359989.0</td>\n",
       "      <td>nf 59.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863a5345-9cd6-42ad-9697-e5941712a984</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>fb05f10a57aaffd529244e962ea6d2dfba001045869b54...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11359989.0</td>\n",
       "      <td>nf 63.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95650ce4-1347-450c-b2d8-45667dcd0284</th>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>5acdb785ad22a4d4fbd2e5b99de2e86300a34e460d4b27...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11359989.0</td>\n",
       "      <td>nf 60.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3653c4ea-9747-4170-aacc-68cb9036ec6a</th>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>d41c76becd4476eb626b63787adb9aaabfbbf08f182612...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11359989.0</td>\n",
       "      <td>nf 58.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01291d98-6bfd-40e1-ba6a-de70e8f6e8d9</th>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>58f15df43aaaf86ae15e9b4c2869dc236814001385440c...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11359989.0</td>\n",
       "      <td>nf 65.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d2c18a2c-60e7-4cd2-ae74-8381337dcedc</th>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>07edad2d23ed32fbe526111b0dd0c47de7bd9c387a0ea7...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11359989.0</td>\n",
       "      <td>nf 62.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23ecde91-2cf7-4dd9-afcc-c45baaa3de83</th>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>5514dfbdf73d25069675e75f61c32381e5035a28876635...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11359989.0</td>\n",
       "      <td>nf 61.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7492da8b-c6bc-46e3-a743-bd7456d8f6cd</th>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>92b195edf2b2f1b8f08bf22260c231a372c93499f9faba...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11359989.0</td>\n",
       "      <td>nf 64.pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467c66c5-a3ae-441c-bb3f-83eab70429e6</th>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>608adb56abc1d2229cf2930aeca0a2f4a04290474b1c71...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11674905.0</td>\n",
       "      <td>5CBB9967-367A-42EE-BCD8-A25F161906E3.PDF</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fb06ec19-8cde-4713-ae89-37b374948deb</th>\n",
       "      <td>22.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>f976c128-1f41-4551-bffd-fac687c1c8b2</td>\n",
       "      <td>c0c55e14c885f423b5cc49a32202e90e1e58bfec85a48e...</td>\n",
       "      <td>Batch_21</td>\n",
       "      <td>11779053.0</td>\n",
       "      <td>resposta.PDF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pipeline_extracao_documentos/2_documentos_para...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       seq  level  \\\n",
       "document_unique_id                                  \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f   1.0    3.0   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9   2.0    3.0   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5   3.0    3.0   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664   4.0    3.0   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557   5.0    3.0   \n",
       "82fa0c74-b18d-49dd-bafc-3b05ec33edc5   6.0    3.0   \n",
       "6ffe74e6-64f5-421b-b1cc-f2022c3d8928   7.0    3.0   \n",
       "ab93e7cb-15de-48ac-a35f-32891b339bbd   8.0    3.0   \n",
       "b639d369-35ac-4271-85fc-f2aed845d19f   9.0    3.0   \n",
       "2dd60abe-6265-4ef0-8c88-fe57951cba63  18.0    3.0   \n",
       "b762be55-5956-4d97-8c01-a2e7f878e724  19.0    3.0   \n",
       "0c9b23e9-698c-4936-b4a2-21f9b3c9ed67  21.0    3.0   \n",
       "fcc49b4b-4461-446b-8374-d4fd5bc439e5  23.0    3.0   \n",
       "da9300be-91d8-4a3f-a164-bb3d5f2125ca  10.0    3.0   \n",
       "863a5345-9cd6-42ad-9697-e5941712a984  11.0    3.0   \n",
       "95650ce4-1347-450c-b2d8-45667dcd0284  12.0    3.0   \n",
       "3653c4ea-9747-4170-aacc-68cb9036ec6a  13.0    3.0   \n",
       "01291d98-6bfd-40e1-ba6a-de70e8f6e8d9  14.0    3.0   \n",
       "d2c18a2c-60e7-4cd2-ae74-8381337dcedc  15.0    3.0   \n",
       "23ecde91-2cf7-4dd9-afcc-c45baaa3de83  16.0    3.0   \n",
       "7492da8b-c6bc-46e3-a743-bd7456d8f6cd  17.0    3.0   \n",
       "467c66c5-a3ae-441c-bb3f-83eab70429e6  20.0    3.0   \n",
       "fb06ec19-8cde-4713-ae89-37b374948deb  22.0    3.0   \n",
       "\n",
       "                                                 parent_document_unique_id  \\\n",
       "document_unique_id                                                           \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "82fa0c74-b18d-49dd-bafc-3b05ec33edc5  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "6ffe74e6-64f5-421b-b1cc-f2022c3d8928  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "ab93e7cb-15de-48ac-a35f-32891b339bbd  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "b639d369-35ac-4271-85fc-f2aed845d19f  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "2dd60abe-6265-4ef0-8c88-fe57951cba63  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "b762be55-5956-4d97-8c01-a2e7f878e724  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "0c9b23e9-698c-4936-b4a2-21f9b3c9ed67  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "fcc49b4b-4461-446b-8374-d4fd5bc439e5  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "da9300be-91d8-4a3f-a164-bb3d5f2125ca  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "863a5345-9cd6-42ad-9697-e5941712a984  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "95650ce4-1347-450c-b2d8-45667dcd0284  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "3653c4ea-9747-4170-aacc-68cb9036ec6a  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "01291d98-6bfd-40e1-ba6a-de70e8f6e8d9  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "d2c18a2c-60e7-4cd2-ae74-8381337dcedc  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "23ecde91-2cf7-4dd9-afcc-c45baaa3de83  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "7492da8b-c6bc-46e3-a743-bd7456d8f6cd  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "467c66c5-a3ae-441c-bb3f-83eab70429e6  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "fb06ec19-8cde-4713-ae89-37b374948deb  f976c128-1f41-4551-bffd-fac687c1c8b2   \n",
       "\n",
       "                                                                              file_hash  \\\n",
       "document_unique_id                                                                        \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f  b3acfffea4847108d0064ecbd62a73359961f88741a037...   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9  ae37292a66dff093838f3cde0da8bea332ec81241b1d5f...   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5  1976ffe84e27b6bb1a5840018c4a47f6bac483b10f686a...   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664  ad2522b53349fffd748376bca7fe4b90fd59359971d992...   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557  c26954dad71d508d4b5315f69dc4c6291a1c5bf70548e7...   \n",
       "82fa0c74-b18d-49dd-bafc-3b05ec33edc5  63b6603be7b1df359e67b96e9e128336b46fa3fbcfdadc...   \n",
       "6ffe74e6-64f5-421b-b1cc-f2022c3d8928  abcfcce04cabd261b30730f04cbb98e030f2a765bb31aa...   \n",
       "ab93e7cb-15de-48ac-a35f-32891b339bbd  c8c5ae8ccca5e9befb6445e21701dc99266746e877c630...   \n",
       "b639d369-35ac-4271-85fc-f2aed845d19f  33bc08587f5c227ab3e035011ed9b20340f99599e65ff7...   \n",
       "2dd60abe-6265-4ef0-8c88-fe57951cba63  0039c12e1e972817f623e70ce05f54cb037a6a7e55cc3a...   \n",
       "b762be55-5956-4d97-8c01-a2e7f878e724  940b1ffaf961f72198f07d10909177a62706acf88aa333...   \n",
       "0c9b23e9-698c-4936-b4a2-21f9b3c9ed67  c0fd28f51a6eb88508566d880c95d8c99490067af70311...   \n",
       "fcc49b4b-4461-446b-8374-d4fd5bc439e5  4073b25641024505cd6a5b7052266f1c494acfe517fcb1...   \n",
       "da9300be-91d8-4a3f-a164-bb3d5f2125ca  59274230e7fb7897397bb426ac31d953e2a14def1fd25e...   \n",
       "863a5345-9cd6-42ad-9697-e5941712a984  fb05f10a57aaffd529244e962ea6d2dfba001045869b54...   \n",
       "95650ce4-1347-450c-b2d8-45667dcd0284  5acdb785ad22a4d4fbd2e5b99de2e86300a34e460d4b27...   \n",
       "3653c4ea-9747-4170-aacc-68cb9036ec6a  d41c76becd4476eb626b63787adb9aaabfbbf08f182612...   \n",
       "01291d98-6bfd-40e1-ba6a-de70e8f6e8d9  58f15df43aaaf86ae15e9b4c2869dc236814001385440c...   \n",
       "d2c18a2c-60e7-4cd2-ae74-8381337dcedc  07edad2d23ed32fbe526111b0dd0c47de7bd9c387a0ea7...   \n",
       "23ecde91-2cf7-4dd9-afcc-c45baaa3de83  5514dfbdf73d25069675e75f61c32381e5035a28876635...   \n",
       "7492da8b-c6bc-46e3-a743-bd7456d8f6cd  92b195edf2b2f1b8f08bf22260c231a372c93499f9faba...   \n",
       "467c66c5-a3ae-441c-bb3f-83eab70429e6  608adb56abc1d2229cf2930aeca0a2f4a04290474b1c71...   \n",
       "fb06ec19-8cde-4713-ae89-37b374948deb  c0c55e14c885f423b5cc49a32202e90e1e58bfec85a48e...   \n",
       "\n",
       "                                         batch   directory  \\\n",
       "document_unique_id                                           \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f  Batch_21  11756286.0   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9  Batch_21  11766341.0   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5  Batch_21  11624359.0   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664  Batch_21  11777624.0   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557  Batch_21  11777624.0   \n",
       "82fa0c74-b18d-49dd-bafc-3b05ec33edc5  Batch_21  11777624.0   \n",
       "6ffe74e6-64f5-421b-b1cc-f2022c3d8928  Batch_21  11285853.0   \n",
       "ab93e7cb-15de-48ac-a35f-32891b339bbd  Batch_21  11285853.0   \n",
       "b639d369-35ac-4271-85fc-f2aed845d19f  Batch_21  11778425.0   \n",
       "2dd60abe-6265-4ef0-8c88-fe57951cba63  Batch_21  11778003.0   \n",
       "b762be55-5956-4d97-8c01-a2e7f878e724  Batch_21  11778003.0   \n",
       "0c9b23e9-698c-4936-b4a2-21f9b3c9ed67  Batch_21  11777556.0   \n",
       "fcc49b4b-4461-446b-8374-d4fd5bc439e5  Batch_21  11779531.0   \n",
       "da9300be-91d8-4a3f-a164-bb3d5f2125ca  Batch_21  11359989.0   \n",
       "863a5345-9cd6-42ad-9697-e5941712a984  Batch_21  11359989.0   \n",
       "95650ce4-1347-450c-b2d8-45667dcd0284  Batch_21  11359989.0   \n",
       "3653c4ea-9747-4170-aacc-68cb9036ec6a  Batch_21  11359989.0   \n",
       "01291d98-6bfd-40e1-ba6a-de70e8f6e8d9  Batch_21  11359989.0   \n",
       "d2c18a2c-60e7-4cd2-ae74-8381337dcedc  Batch_21  11359989.0   \n",
       "23ecde91-2cf7-4dd9-afcc-c45baaa3de83  Batch_21  11359989.0   \n",
       "7492da8b-c6bc-46e3-a743-bd7456d8f6cd  Batch_21  11359989.0   \n",
       "467c66c5-a3ae-441c-bb3f-83eab70429e6  Batch_21  11674905.0   \n",
       "fb06ec19-8cde-4713-ae89-37b374948deb  Batch_21  11779053.0   \n",
       "\n",
       "                                                                 original_file_name  \\\n",
       "document_unique_id                                                                    \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f  NF CRJ PRIMEIRA QUINZENA DE JULHO DE 2023.pdf   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9    Nota Fiscal Eletrônica Quallit 24072023.pdf   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5                                  NFE 20237.pdf   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664                                      Ampla.pdf   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557                                  Supernova.pdf   \n",
       "82fa0c74-b18d-49dd-bafc-3b05ec33edc5                                  Blue Lord.pdf   \n",
       "6ffe74e6-64f5-421b-b1cc-f2022c3d8928                                 NF 2023158.pdf   \n",
       "ab93e7cb-15de-48ac-a35f-32891b339bbd                                 NF 2023157.pdf   \n",
       "b639d369-35ac-4271-85fc-f2aed845d19f                               nota_07_2023.pdf   \n",
       "2dd60abe-6265-4ef0-8c88-fe57951cba63                                   NFS-e 22.pdf   \n",
       "b762be55-5956-4d97-8c01-a2e7f878e724                                  NFSe-e 23.pdf   \n",
       "0c9b23e9-698c-4936-b4a2-21f9b3c9ed67                  NF 202315- SJDI 35 JUL 23.pdf   \n",
       "fcc49b4b-4461-446b-8374-d4fd5bc439e5          Heidelberg 21 07 2023 NOTA FISCAL.pdf   \n",
       "da9300be-91d8-4a3f-a164-bb3d5f2125ca                                      nf 59.pdf   \n",
       "863a5345-9cd6-42ad-9697-e5941712a984                                      nf 63.pdf   \n",
       "95650ce4-1347-450c-b2d8-45667dcd0284                                      nf 60.pdf   \n",
       "3653c4ea-9747-4170-aacc-68cb9036ec6a                                      nf 58.pdf   \n",
       "01291d98-6bfd-40e1-ba6a-de70e8f6e8d9                                      nf 65.pdf   \n",
       "d2c18a2c-60e7-4cd2-ae74-8381337dcedc                                      nf 62.pdf   \n",
       "23ecde91-2cf7-4dd9-afcc-c45baaa3de83                                      nf 61.pdf   \n",
       "7492da8b-c6bc-46e3-a743-bd7456d8f6cd                                      nf 64.pdf   \n",
       "467c66c5-a3ae-441c-bb3f-83eab70429e6       5CBB9967-367A-42EE-BCD8-A25F161906E3.PDF   \n",
       "fb06ec19-8cde-4713-ae89-37b374948deb                                   resposta.PDF   \n",
       "\n",
       "                                      pages  one_page  \\\n",
       "document_unique_id                                      \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f    1.0       1.0   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9    1.0       1.0   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5    1.0       1.0   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664    1.0       1.0   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557    1.0       1.0   \n",
       "82fa0c74-b18d-49dd-bafc-3b05ec33edc5    1.0       1.0   \n",
       "6ffe74e6-64f5-421b-b1cc-f2022c3d8928    1.0       1.0   \n",
       "ab93e7cb-15de-48ac-a35f-32891b339bbd    1.0       1.0   \n",
       "b639d369-35ac-4271-85fc-f2aed845d19f    1.0       1.0   \n",
       "2dd60abe-6265-4ef0-8c88-fe57951cba63    1.0       1.0   \n",
       "b762be55-5956-4d97-8c01-a2e7f878e724    1.0       1.0   \n",
       "0c9b23e9-698c-4936-b4a2-21f9b3c9ed67    1.0       1.0   \n",
       "fcc49b4b-4461-446b-8374-d4fd5bc439e5    1.0       1.0   \n",
       "da9300be-91d8-4a3f-a164-bb3d5f2125ca    1.0       1.0   \n",
       "863a5345-9cd6-42ad-9697-e5941712a984    1.0       1.0   \n",
       "95650ce4-1347-450c-b2d8-45667dcd0284    1.0       1.0   \n",
       "3653c4ea-9747-4170-aacc-68cb9036ec6a    1.0       1.0   \n",
       "01291d98-6bfd-40e1-ba6a-de70e8f6e8d9    1.0       1.0   \n",
       "d2c18a2c-60e7-4cd2-ae74-8381337dcedc    1.0       1.0   \n",
       "23ecde91-2cf7-4dd9-afcc-c45baaa3de83    1.0       1.0   \n",
       "7492da8b-c6bc-46e3-a743-bd7456d8f6cd    1.0       1.0   \n",
       "467c66c5-a3ae-441c-bb3f-83eab70429e6    7.0       0.0   \n",
       "fb06ec19-8cde-4713-ae89-37b374948deb    1.0       1.0   \n",
       "\n",
       "                                                                              file_path  \\\n",
       "document_unique_id                                                                        \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f  pipeline_extracao_documentos/2_documentos_para...   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9  pipeline_extracao_documentos/2_documentos_para...   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5  pipeline_extracao_documentos/2_documentos_para...   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664  pipeline_extracao_documentos/2_documentos_para...   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557  pipeline_extracao_documentos/2_documentos_para...   \n",
       "82fa0c74-b18d-49dd-bafc-3b05ec33edc5  pipeline_extracao_documentos/2_documentos_para...   \n",
       "6ffe74e6-64f5-421b-b1cc-f2022c3d8928  pipeline_extracao_documentos/2_documentos_para...   \n",
       "ab93e7cb-15de-48ac-a35f-32891b339bbd  pipeline_extracao_documentos/2_documentos_para...   \n",
       "b639d369-35ac-4271-85fc-f2aed845d19f  pipeline_extracao_documentos/2_documentos_para...   \n",
       "2dd60abe-6265-4ef0-8c88-fe57951cba63  pipeline_extracao_documentos/2_documentos_para...   \n",
       "b762be55-5956-4d97-8c01-a2e7f878e724  pipeline_extracao_documentos/2_documentos_para...   \n",
       "0c9b23e9-698c-4936-b4a2-21f9b3c9ed67  pipeline_extracao_documentos/2_documentos_para...   \n",
       "fcc49b4b-4461-446b-8374-d4fd5bc439e5  pipeline_extracao_documentos/2_documentos_para...   \n",
       "da9300be-91d8-4a3f-a164-bb3d5f2125ca  pipeline_extracao_documentos/2_documentos_para...   \n",
       "863a5345-9cd6-42ad-9697-e5941712a984  pipeline_extracao_documentos/2_documentos_para...   \n",
       "95650ce4-1347-450c-b2d8-45667dcd0284  pipeline_extracao_documentos/2_documentos_para...   \n",
       "3653c4ea-9747-4170-aacc-68cb9036ec6a  pipeline_extracao_documentos/2_documentos_para...   \n",
       "01291d98-6bfd-40e1-ba6a-de70e8f6e8d9  pipeline_extracao_documentos/2_documentos_para...   \n",
       "d2c18a2c-60e7-4cd2-ae74-8381337dcedc  pipeline_extracao_documentos/2_documentos_para...   \n",
       "23ecde91-2cf7-4dd9-afcc-c45baaa3de83  pipeline_extracao_documentos/2_documentos_para...   \n",
       "7492da8b-c6bc-46e3-a743-bd7456d8f6cd  pipeline_extracao_documentos/2_documentos_para...   \n",
       "467c66c5-a3ae-441c-bb3f-83eab70429e6  pipeline_extracao_documentos/2_documentos_para...   \n",
       "fb06ec19-8cde-4713-ae89-37b374948deb  pipeline_extracao_documentos/2_documentos_para...   \n",
       "\n",
       "                                      ... outras_retencoes valor_liquido  \\\n",
       "document_unique_id                    ...                                  \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f  ...              0.0       7599.57   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9  ...              0.0        171.00   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5  ...              0.0      37693.37   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664  ...              0.0       2750.00   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557  ...              0.0       5000.00   \n",
       "82fa0c74-b18d-49dd-bafc-3b05ec33edc5  ...              0.0       1000.00   \n",
       "6ffe74e6-64f5-421b-b1cc-f2022c3d8928  ...              0.0         37.40   \n",
       "ab93e7cb-15de-48ac-a35f-32891b339bbd  ...              0.0         22.75   \n",
       "b639d369-35ac-4271-85fc-f2aed845d19f  ...              0.0       5070.00   \n",
       "2dd60abe-6265-4ef0-8c88-fe57951cba63  ...              0.0       8523.65   \n",
       "b762be55-5956-4d97-8c01-a2e7f878e724  ...              0.0       3774.43   \n",
       "0c9b23e9-698c-4936-b4a2-21f9b3c9ed67  ...              0.0       8510.64   \n",
       "fcc49b4b-4461-446b-8374-d4fd5bc439e5  ...              NaN           NaN   \n",
       "da9300be-91d8-4a3f-a164-bb3d5f2125ca  ...              NaN           NaN   \n",
       "863a5345-9cd6-42ad-9697-e5941712a984  ...              NaN           NaN   \n",
       "95650ce4-1347-450c-b2d8-45667dcd0284  ...              NaN           NaN   \n",
       "3653c4ea-9747-4170-aacc-68cb9036ec6a  ...              NaN           NaN   \n",
       "01291d98-6bfd-40e1-ba6a-de70e8f6e8d9  ...              NaN           NaN   \n",
       "d2c18a2c-60e7-4cd2-ae74-8381337dcedc  ...              NaN           NaN   \n",
       "23ecde91-2cf7-4dd9-afcc-c45baaa3de83  ...              NaN           NaN   \n",
       "7492da8b-c6bc-46e3-a743-bd7456d8f6cd  ...              NaN           NaN   \n",
       "467c66c5-a3ae-441c-bb3f-83eab70429e6  ...              NaN           NaN   \n",
       "fb06ec19-8cde-4713-ae89-37b374948deb  ...              NaN           NaN   \n",
       "\n",
       "                                     exigibilidade_iss  \\\n",
       "document_unique_id                                       \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f               NaN   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9               NaN   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5               NaN   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664              None   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557              None   \n",
       "82fa0c74-b18d-49dd-bafc-3b05ec33edc5              None   \n",
       "6ffe74e6-64f5-421b-b1cc-f2022c3d8928               NaN   \n",
       "ab93e7cb-15de-48ac-a35f-32891b339bbd               NaN   \n",
       "b639d369-35ac-4271-85fc-f2aed845d19f          Exigível   \n",
       "2dd60abe-6265-4ef0-8c88-fe57951cba63               NaN   \n",
       "b762be55-5956-4d97-8c01-a2e7f878e724               NaN   \n",
       "0c9b23e9-698c-4936-b4a2-21f9b3c9ed67               NaN   \n",
       "fcc49b4b-4461-446b-8374-d4fd5bc439e5               NaN   \n",
       "da9300be-91d8-4a3f-a164-bb3d5f2125ca               NaN   \n",
       "863a5345-9cd6-42ad-9697-e5941712a984               NaN   \n",
       "95650ce4-1347-450c-b2d8-45667dcd0284               NaN   \n",
       "3653c4ea-9747-4170-aacc-68cb9036ec6a               NaN   \n",
       "01291d98-6bfd-40e1-ba6a-de70e8f6e8d9               NaN   \n",
       "d2c18a2c-60e7-4cd2-ae74-8381337dcedc               NaN   \n",
       "23ecde91-2cf7-4dd9-afcc-c45baaa3de83               NaN   \n",
       "7492da8b-c6bc-46e3-a743-bd7456d8f6cd               NaN   \n",
       "467c66c5-a3ae-441c-bb3f-83eab70429e6               NaN   \n",
       "fb06ec19-8cde-4713-ae89-37b374948deb               NaN   \n",
       "\n",
       "                                                     regime_tributacao  \\\n",
       "document_unique_id                                                       \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f                               NaN   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9                               NaN   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5                               NaN   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664  Microempresário Individual (MEI)   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557  Microempresário Individual (MEI)   \n",
       "82fa0c74-b18d-49dd-bafc-3b05ec33edc5  Microempresário Individual (MEI)   \n",
       "6ffe74e6-64f5-421b-b1cc-f2022c3d8928                               NaN   \n",
       "ab93e7cb-15de-48ac-a35f-32891b339bbd                               NaN   \n",
       "b639d369-35ac-4271-85fc-f2aed845d19f  Microempresário Individual (MEI)   \n",
       "2dd60abe-6265-4ef0-8c88-fe57951cba63                               NaN   \n",
       "b762be55-5956-4d97-8c01-a2e7f878e724                               NaN   \n",
       "0c9b23e9-698c-4936-b4a2-21f9b3c9ed67                               NaN   \n",
       "fcc49b4b-4461-446b-8374-d4fd5bc439e5                               NaN   \n",
       "da9300be-91d8-4a3f-a164-bb3d5f2125ca                               NaN   \n",
       "863a5345-9cd6-42ad-9697-e5941712a984                               NaN   \n",
       "95650ce4-1347-450c-b2d8-45667dcd0284                               NaN   \n",
       "3653c4ea-9747-4170-aacc-68cb9036ec6a                               NaN   \n",
       "01291d98-6bfd-40e1-ba6a-de70e8f6e8d9                               NaN   \n",
       "d2c18a2c-60e7-4cd2-ae74-8381337dcedc                               NaN   \n",
       "23ecde91-2cf7-4dd9-afcc-c45baaa3de83                               NaN   \n",
       "7492da8b-c6bc-46e3-a743-bd7456d8f6cd                               NaN   \n",
       "467c66c5-a3ae-441c-bb3f-83eab70429e6                               NaN   \n",
       "fb06ec19-8cde-4713-ae89-37b374948deb                               NaN   \n",
       "\n",
       "                                     simples_nacional issqn_retido  \\\n",
       "document_unique_id                                                   \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f              NaN          NaN   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9              NaN          NaN   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5              NaN          NaN   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664              Sim          Sim   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557              Sim          Sim   \n",
       "82fa0c74-b18d-49dd-bafc-3b05ec33edc5              Sim          Sim   \n",
       "6ffe74e6-64f5-421b-b1cc-f2022c3d8928              NaN          NaN   \n",
       "ab93e7cb-15de-48ac-a35f-32891b339bbd              NaN          NaN   \n",
       "b639d369-35ac-4271-85fc-f2aed845d19f              Sim          Sim   \n",
       "2dd60abe-6265-4ef0-8c88-fe57951cba63              NaN          NaN   \n",
       "b762be55-5956-4d97-8c01-a2e7f878e724              NaN          NaN   \n",
       "0c9b23e9-698c-4936-b4a2-21f9b3c9ed67              NaN          NaN   \n",
       "fcc49b4b-4461-446b-8374-d4fd5bc439e5              NaN          NaN   \n",
       "da9300be-91d8-4a3f-a164-bb3d5f2125ca              NaN          NaN   \n",
       "863a5345-9cd6-42ad-9697-e5941712a984              NaN          NaN   \n",
       "95650ce4-1347-450c-b2d8-45667dcd0284              NaN          NaN   \n",
       "3653c4ea-9747-4170-aacc-68cb9036ec6a              NaN          NaN   \n",
       "01291d98-6bfd-40e1-ba6a-de70e8f6e8d9              NaN          NaN   \n",
       "d2c18a2c-60e7-4cd2-ae74-8381337dcedc              NaN          NaN   \n",
       "23ecde91-2cf7-4dd9-afcc-c45baaa3de83              NaN          NaN   \n",
       "7492da8b-c6bc-46e3-a743-bd7456d8f6cd              NaN          NaN   \n",
       "467c66c5-a3ae-441c-bb3f-83eab70429e6              NaN          NaN   \n",
       "fb06ec19-8cde-4713-ae89-37b374948deb              NaN          NaN   \n",
       "\n",
       "                                     local_pretacao_servico  \\\n",
       "document_unique_id                                            \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f                    NaN   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9                    NaN   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5                    NaN   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664                    NaN   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557                    NaN   \n",
       "82fa0c74-b18d-49dd-bafc-3b05ec33edc5                    NaN   \n",
       "6ffe74e6-64f5-421b-b1cc-f2022c3d8928                    NaN   \n",
       "ab93e7cb-15de-48ac-a35f-32891b339bbd                    NaN   \n",
       "b639d369-35ac-4271-85fc-f2aed845d19f                    NaN   \n",
       "2dd60abe-6265-4ef0-8c88-fe57951cba63                    NaN   \n",
       "b762be55-5956-4d97-8c01-a2e7f878e724                    NaN   \n",
       "0c9b23e9-698c-4936-b4a2-21f9b3c9ed67                    NaN   \n",
       "fcc49b4b-4461-446b-8374-d4fd5bc439e5                    NaN   \n",
       "da9300be-91d8-4a3f-a164-bb3d5f2125ca                    NaN   \n",
       "863a5345-9cd6-42ad-9697-e5941712a984                    NaN   \n",
       "95650ce4-1347-450c-b2d8-45667dcd0284                    NaN   \n",
       "3653c4ea-9747-4170-aacc-68cb9036ec6a                    NaN   \n",
       "01291d98-6bfd-40e1-ba6a-de70e8f6e8d9                    NaN   \n",
       "d2c18a2c-60e7-4cd2-ae74-8381337dcedc                    NaN   \n",
       "23ecde91-2cf7-4dd9-afcc-c45baaa3de83                    NaN   \n",
       "7492da8b-c6bc-46e3-a743-bd7456d8f6cd                    NaN   \n",
       "467c66c5-a3ae-441c-bb3f-83eab70429e6                    NaN   \n",
       "fb06ec19-8cde-4713-ae89-37b374948deb                    NaN   \n",
       "\n",
       "                                              local_incidencia  \\\n",
       "document_unique_id                                               \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f                       NaN   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9                       NaN   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5                       NaN   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664  São Pedro da Aldeia - RJ   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557  São Pedro da Aldeia - RJ   \n",
       "82fa0c74-b18d-49dd-bafc-3b05ec33edc5  São Pedro da Aldeia - RJ   \n",
       "6ffe74e6-64f5-421b-b1cc-f2022c3d8928                       NaN   \n",
       "ab93e7cb-15de-48ac-a35f-32891b339bbd                       NaN   \n",
       "b639d369-35ac-4271-85fc-f2aed845d19f  São Pedro da Aldeia - RJ   \n",
       "2dd60abe-6265-4ef0-8c88-fe57951cba63                       NaN   \n",
       "b762be55-5956-4d97-8c01-a2e7f878e724                       NaN   \n",
       "0c9b23e9-698c-4936-b4a2-21f9b3c9ed67                       NaN   \n",
       "fcc49b4b-4461-446b-8374-d4fd5bc439e5                       NaN   \n",
       "da9300be-91d8-4a3f-a164-bb3d5f2125ca                       NaN   \n",
       "863a5345-9cd6-42ad-9697-e5941712a984                       NaN   \n",
       "95650ce4-1347-450c-b2d8-45667dcd0284                       NaN   \n",
       "3653c4ea-9747-4170-aacc-68cb9036ec6a                       NaN   \n",
       "01291d98-6bfd-40e1-ba6a-de70e8f6e8d9                       NaN   \n",
       "d2c18a2c-60e7-4cd2-ae74-8381337dcedc                       NaN   \n",
       "23ecde91-2cf7-4dd9-afcc-c45baaa3de83                       NaN   \n",
       "7492da8b-c6bc-46e3-a743-bd7456d8f6cd                       NaN   \n",
       "467c66c5-a3ae-441c-bb3f-83eab70429e6                       NaN   \n",
       "fb06ec19-8cde-4713-ae89-37b374948deb                       NaN   \n",
       "\n",
       "                                                dados_complementares  \\\n",
       "document_unique_id                                                     \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f                             NaN   \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9                             NaN   \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5                             NaN   \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664    {'dados_complementares': ''}   \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557    {'dados_complementares': ''}   \n",
       "82fa0c74-b18d-49dd-bafc-3b05ec33edc5    {'dados_complementares': ''}   \n",
       "6ffe74e6-64f5-421b-b1cc-f2022c3d8928                             NaN   \n",
       "ab93e7cb-15de-48ac-a35f-32891b339bbd                             NaN   \n",
       "b639d369-35ac-4271-85fc-f2aed845d19f  {'dados_complementares': None}   \n",
       "2dd60abe-6265-4ef0-8c88-fe57951cba63                             NaN   \n",
       "b762be55-5956-4d97-8c01-a2e7f878e724                             NaN   \n",
       "0c9b23e9-698c-4936-b4a2-21f9b3c9ed67                             NaN   \n",
       "fcc49b4b-4461-446b-8374-d4fd5bc439e5                             NaN   \n",
       "da9300be-91d8-4a3f-a164-bb3d5f2125ca                             NaN   \n",
       "863a5345-9cd6-42ad-9697-e5941712a984                             NaN   \n",
       "95650ce4-1347-450c-b2d8-45667dcd0284                             NaN   \n",
       "3653c4ea-9747-4170-aacc-68cb9036ec6a                             NaN   \n",
       "01291d98-6bfd-40e1-ba6a-de70e8f6e8d9                             NaN   \n",
       "d2c18a2c-60e7-4cd2-ae74-8381337dcedc                             NaN   \n",
       "23ecde91-2cf7-4dd9-afcc-c45baaa3de83                             NaN   \n",
       "7492da8b-c6bc-46e3-a743-bd7456d8f6cd                             NaN   \n",
       "467c66c5-a3ae-441c-bb3f-83eab70429e6                             NaN   \n",
       "fb06ec19-8cde-4713-ae89-37b374948deb                             NaN   \n",
       "\n",
       "                                                                             observacao  \n",
       "document_unique_id                                                                       \n",
       "923d536b-2f47-4994-8c77-538eab1c3c5f  DADOS COMPLEM ENTARES OUTRAS INFORMAÇÕES / CRI...  \n",
       "28303a7b-07ee-49f3-b7d6-7bc0f60baaa9  https://nota.pmspa.rj.gov.br 1/1 DADOS COMPLEM...  \n",
       "e8a12fa3-90a8-4317-baaf-799014f95fb5  DADOS COMPLEMENTARES ITAU, Agencia : 6101, Con...  \n",
       "44eaa169-167f-4a45-b60d-7d4f760a0664  Valor Aproximado dos Tributos Federais R$ 369,...  \n",
       "80fc007c-a947-470d-923e-dad0b1ed5557  Valor Aproximado dos Tributos Federais R$ 672,...  \n",
       "82fa0c74-b18d-49dd-bafc-3b05ec33edc5  Valor Aproximado dos Tributos Federais R$ 134,...  \n",
       "6ffe74e6-64f5-421b-b1cc-f2022c3d8928  https://nota.pmspa.rj.gov.br 1/1 DADOS COMPLEM...  \n",
       "ab93e7cb-15de-48ac-a35f-32891b339bbd  https://nota.pmspa.rj.gov.br 1/1 DADOS COMPLEM...  \n",
       "b639d369-35ac-4271-85fc-f2aed845d19f  Valor Aproximado dos Tributos Federais R$ 681,...  \n",
       "2dd60abe-6265-4ef0-8c88-fe57951cba63  https://nota.pmspa.rj.gov.br/?serv=20 1/1 DADO...  \n",
       "b762be55-5956-4d97-8c01-a2e7f878e724  https://nota.pmspa.rj.gov.br/?serv=20 1/1 DADO...  \n",
       "0c9b23e9-698c-4936-b4a2-21f9b3c9ed67  https://nota.pmspa.rj.gov.br 1/1 DADOS COMPLEM...  \n",
       "fcc49b4b-4461-446b-8374-d4fd5bc439e5  https://nota.pmspa.rj.gov.br 1/1 R$ 0,00 R$ 0,...  \n",
       "da9300be-91d8-4a3f-a164-bb3d5f2125ca                                                NaN  \n",
       "863a5345-9cd6-42ad-9697-e5941712a984                                                NaN  \n",
       "95650ce4-1347-450c-b2d8-45667dcd0284                                                NaN  \n",
       "3653c4ea-9747-4170-aacc-68cb9036ec6a                                                NaN  \n",
       "01291d98-6bfd-40e1-ba6a-de70e8f6e8d9                                                NaN  \n",
       "d2c18a2c-60e7-4cd2-ae74-8381337dcedc                                                NaN  \n",
       "23ecde91-2cf7-4dd9-afcc-c45baaa3de83                                                NaN  \n",
       "7492da8b-c6bc-46e3-a743-bd7456d8f6cd                                                NaN  \n",
       "467c66c5-a3ae-441c-bb3f-83eab70429e6                                                NaN  \n",
       "fb06ec19-8cde-4713-ae89-37b374948deb                                                NaN  \n",
       "\n",
       "[23 rows x 72 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. XXX Criando o df_conf_export para analise excel\n",
    "df_conf_export = df[['seq', 'level', 'parent_document_unique_id', 'file_hash', 'batch', 'directory', 'original_file_name', 'pages', 'one_page', 'file_path', 'palavra_chave', 'document_tag', 'action_item', 'fase_processo', 'nome_atividade', 'status_documento', 'prefeitura', 'de_para_pm', 'model', 'secao', 'pdf_pesquisavel', 'numero_nota_fiscal', 'competencia', 'dt_hr_emissao', 'codigo_verificacao', 'conf_cod', 'informations',  'p_cpf_cnpj_com_mascara', 'p_cpf_cnpj_sem_mascara','p_telefone', 'p_inscricao_estadual', 'p_inscricao_municipal', 'p_razao_social', 'p_nome_fantasia','p_endereco','p_email', 'cnae', 'item_lista_servicos','t_cpf_cnpj_com_mascara', 't_cpf_cnpj_sem_mascara','t_telefone', 't_inscricao_municipal', 't_razao_social','t_endereco','t_email', 'discriminacao_servicos', 'valor_total_nota', 'cnae', 'item_lista_servicos', 'valor_servicos', 'valor_deducao', 'desc_incond', 'base_calculo', 'aliquota', 'valor_iss', 'valor_iss_retido', 'desc_cond', 'valor_pis', 'valor_cofins', 'valor_ir', 'valor_inss', 'valor_csll', 'outras_retencoes', 'valor_liquido', 'exigibilidade_iss', 'regime_tributacao', 'simples_nacional', 'issqn_retido', 'local_pretacao_servico', 'local_incidencia', 'dados_complementares', 'observacao' ]]\n",
    "df_conf_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. XXX Exportando o df_conf_export para analise excel\n",
    "df_conf_export.to_excel('processamentos/df_conf_export_batch_22.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>2.3.1</b> Conferencia do Processamento </mark> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte2 - dados prestador - OK\n",
    "df_conf2 = df[['seq', 'p_cpf_cnpj_com_mascara', 'p_cpf_cnpj_sem_mascara','p_telefone', 'p_inscricao_estadual', 'p_inscricao_municipal', 'p_razao_social', 'p_nome_fantasia','p_endereco','p_email']] #\n",
    "df_conf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste a largura máxima das colunas para um valor específico (por exemplo, 100 caracteres)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "\n",
    "# Criando o subset para analise\n",
    "df_conf = df[['seq', 'original_file_name', 'file_path']]\n",
    "df_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['action_item'] = pd.Categorical(df['action_item'], categories=ordem_action_item, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste a largura da coluna 'sua_coluna' para 50 caracteres\n",
    "df_conf['file_path'].astype(str).str.ljust(100), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte3 - Dados Tomador - OK\n",
    "df_conf3 = df[['t_cpf_cnpj_com_mascara', 't_cpf_cnpj_sem_mascara','t_telefone', 't_inscricao_municipal', 't_razao_social','t_endereco','t_email', 'discriminacao_servicos', 'pdf_pesquisavel']] #\n",
    "df_conf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 4- Outros itens\n",
    "df_conf4 = df[['seq', 'original_file_name', 'pdf_pesquisavel', 'discriminacao_servicos', 'valor_total_nota', 'cnae', 'item_lista_servicos']] #\n",
    "df_conf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'prefeitura', 'pdf_pesquisavel', 'image_np'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('teste2.xlsx')\n",
    "# IMPORTANTE, saber o tipo da coluna\n",
    "print(df['status_documento'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf0 = df[['seq', 'original_file_name','directory', 'file_hash', 'file_path', 'model', 'pdf_pesquisavel','informations', 'image_np']]\n",
    "df_conf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte1 - dados do cabeçalho - OK\n",
    "df_conf1 = df[['numero_nota_fiscal', 'competencia', 'dt_hr_emissao', 'codigo_verificacao', 'prefeitura', 'pdf_pesquisavel']]\n",
    "df_conf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 4- Outros itens\n",
    "df_conf4 = df[['original_file_name', 'discriminacao_servicos', 'valor_total_nota', 'cnae', 'item_lista_servicos']] #\n",
    "df_conf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 5- Imposto\n",
    "df_conf5 = df[['original_file_name', 'directory', 'pdf_pesquisavel', 'valor_servicos', 'valor_deducao', 'desc_incond', 'base_calculo', 'aliquota', 'valor_iss', 'valor_iss_retido', 'desc_cond', 'valor_pis', 'valor_cofins', 'valor_ir', 'valor_inss', 'valor_csll', 'outras_retencoes', 'valor_liquido', 'de_para_pm', 'batch', 'model','directory','file_path']] #\n",
    "df_conf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 6- Informacoes criticas\n",
    "df_conf2 = df[['original_file_name', 'pdf_pesquisavel', 'exigibilidade_iss', 'regime_tributacao', 'simples_nacional', 'issqn_retido', 'local_pretacao_servico', 'local_incidencia', 'dados_complementares', 'observacao']] #\n",
    "df_conf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF conferencia: \n",
    "df_conferencia = df[['numero_nota_fiscal', 'codigo_verificacao', 'competencia', 'dt_hr_emissao', 'prefeitura', 'p_cpf_cnpj_com_mascara', 'p_cpf_cnpj_sem_mascara','p_telefone', 'p_inscricao_estadual', 'p_inscricao_municipal', 'p_razao_social', 'p_nome_fantasia','p_endereco','p_email', 't_cpf_cnpj_com_mascara', 't_cpf_cnpj_sem_mascara','t_telefone', 't_inscricao_municipal', 't_razao_social','t_endereco','t_email', 'discriminacao_servicos', 'valor_total_nota', 'cnae', 'item_lista_servicos','valor_servicos', 'valor_deducao', 'desc_incond', 'base_calculo', 'aliquota', 'valor_iss', 'valor_iss_retido', 'desc_cond', 'valor_pis', 'valor_cofins', 'valor_ir', 'valor_inss', 'valor_csll', 'outras_retencoes', 'valor_liquido', 'exigibilidade_iss', 'regime_tributacao', 'simples_nacional', 'issqn_retido', 'local_pretacao_servico', 'local_incidencia', 'dados_complementares', 'observacao','de_para_pm', 'batch', 'model','directory','file_path']]\n",
    "df_conferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Volto novamente o indice do DF\n",
    "df_conferencia.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. XXX Salvo em Excel (pode ser feito durante fases)\n",
    "df_conferencia.to_excel(\"processamento_mage_1.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. XXX SE deseja importar o DF df_analise_pipe\n",
    "\n",
    "df_conf_path = \"processamento_mage_1.xlsx\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria df_documento_recebido\n",
    "df_conferencia = pd.read_excel(df_conf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processamento = pd.concat([df_conf, df_conf2, df_conf3, df_conf4,], ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Criando uma nova coluna no DF\n",
    "df_conferencia.insert(loc=50, column='original_file_name', value=df_conferencia['file_path'].apply(lambda x: os.path.basename(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. XXX Salvo em Excel (pode ser feito durante fases)\n",
    "df_root_pipe.to_excel(\"df_analise_pipe_b21.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste a largura máxima das colunas para um valor específico (por exemplo, 100 caracteres)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste a largura da coluna 'sua_coluna' para 50 caracteres\n",
    "df_conf['cabecalho'] = df_conf['cabecalho'].astype(str).str.ljust(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>2.3.2</b> ExportaÇao do Json </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = os.path.join(json_path, nome_formado_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_para_pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo_zip = \"fwdnotasfaltantesnosistemadeemissoim20734_106187.zip\"\n",
    "titulo = (f'Processamento {batch_name} - {de_para_pm} - arq:{arquivo_zip}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para armazenar os dicionários\n",
    "dados_json = {}\n",
    "\n",
    "# Iterar sobre cada linha no DataFrame\n",
    "for index, row in df_conferencia.iterrows():\n",
    "    # dados_df e o dicionario para armazenar os dados da nota fiscal atual\n",
    "    #diretorio = str(row['directory'])\n",
    "    dados_nf = {\n",
    "               \"dados_NF_PDF\": {\n",
    "                                \"data_cabecalho\": {\n",
    "                                    \"secao\": \"1 - CABECALHO\",\n",
    "                                    \"nome_prefeitura\": row['prefeitura'],\n",
    "                                    \"numero_nota_fiscal\": row['numero_nota_fiscal'],\n",
    "                                    \"competencia\": row['competencia'],\n",
    "                                    \"dt_hr_emissoa\": row['dt_hr_emissao'],\n",
    "                                    \"codigo_verificacao\": row['codigo_verificacao']\n",
    "                                },\n",
    "                                \"data_prestador\": {\n",
    "                                    \"secao\": \"2. PRESTADOR DE SERVIÇO\",\n",
    "                                    \"cpf_cnpj_com_mascara\": row['p_cpf_cnpj_com_mascara'],\n",
    "                                    \"cpf_cnpj_sem_mascara\": row['p_cpf_cnpj_sem_mascara'],\n",
    "                                    \"inscricao_municipal\": row['p_inscricao_municipal'],\n",
    "                                    \"inscricao_estadual\": row['p_inscricao_estadual'],\n",
    "                                    \"telefone\": row['p_telefone'],\n",
    "                                    \"razao_social\": row['p_razao_social'],\n",
    "                                    \"nome_fantasia\": row['p_nome_fantasia'],\n",
    "                                    \"endereco\": row['p_endereco'],\n",
    "                                    \"email\": row['p_email']\n",
    "                                },\n",
    "                                \"data_tomador\": {\n",
    "                                    \"secao\": \"3. TOMADOR DE SERVIÇO\",\n",
    "                                    \"cpf_cnpj_com_mascara\": row['t_cpf_cnpj_com_mascara'],\n",
    "                                    \"cpf_cnpj_sem_mascara\": row['t_cpf_cnpj_sem_mascara'],\n",
    "                                    \"rg\": row['t_rg'],\n",
    "                                    \"inscricao_municipal\": row['t_inscricao_municipal'],\n",
    "                                    \"inscricao_estadual\": row['t_inscricao_estadual'],\n",
    "                                    \"telefone\": row['t_telefone'],\n",
    "                                    \"razao_social\": row['t_razao_social'],\n",
    "                                    \"endereco\": row['t_endereco'],\n",
    "                                    \"email\": row['t_email']\n",
    "                                },\n",
    "                                \"data_servico\": {\n",
    "                                    \"secao\": \"4. DESCRIMINACAO DOS SERVIÇOS\",\n",
    "                                    \"discriminacao_servicos\": row['discriminacao_servicos']\n",
    "                                },\n",
    "                                \"data_valor_total\": {\n",
    "                                    \"secao\": \"5. VALOR TOTAL\",\n",
    "                                    \"valor_total_nota\": row['valor_total_nota']\n",
    "                                },\n",
    "                                \"data_CNAE\": {\n",
    "                                    \"secao\": \"6. CNAE e Item da Lista de Serviços\",\n",
    "                                    \"cnae\": row['cnae'],\n",
    "                                    \"item_lista_servicos\": row['item_lista_servicos']\n",
    "                                },\n",
    "                                \"data_valores\": {\n",
    "                                    \"secao\": \"7. VALORES E IMPOSTOS\",\n",
    "                                    \"valor_servicos\": row['valor_servicos'],\n",
    "                                    \"valor_deducao\": row['valor_deducao'],\n",
    "                                    \"desc_incond\" : row['desc_incond'],\n",
    "                                    \"base_calculo\": row['base_calculo'],\n",
    "                                    \"aliquota\": row['aliquota'],\n",
    "                                    \"valor_iss\": row['valor_iss'],\n",
    "                                    \"valor_iss_retido\": row['valor_iss_retido'],\n",
    "                                    \"desc_cond\": row['desc_cond'],\n",
    "                                    \"valor_pis\": row['valor_pis'],\n",
    "                                    \"valor_cofins\": row['valor_cofins'],\n",
    "                                    \"valor_ir\": row['valor_ir'],\n",
    "                                    \"valor_inss\": row['valor_inss'],\n",
    "                                    \"valor_csll\": row['valor_csll'],\n",
    "                                    \"outras_retencoes\": row['outras_retencoes'],\n",
    "                                    \"valor_liquido\": row['valor_liquido']\n",
    "                                },\n",
    "                                \"data_dados_complementares\": {\n",
    "                                    \"secao\": \"8. DADOS COMPLEMENTARES\",\n",
    "                                    \"dados_complementares\": row['dados_complementares']\n",
    "                                },\n",
    "                                \"data_outras_informacoes\": {\n",
    "                                    \"secao\": \"9. OUTRAS INFORMAÇOES / CRITICAS\",\n",
    "                                    \"exigibilidade_iss\": row['exigibilidade_iss'],\n",
    "                                    \"regime_tributacao\": row['regime_tributacao'],\n",
    "                                    \"simples_nacional\": row['simples_nacional'],\n",
    "                                    \"issqn_retido\": row['issqn_retido'],\n",
    "                                    \"local_prestacao_servico\": row['local_pretacao_servico'],\n",
    "                                    \"local_incidencia\": row['local_incidencia']\n",
    "                                },\n",
    "                                \"data_observacao\": {\n",
    "                                    \"secao\": \"10. OBSERVACOES\",\n",
    "                                    \"observacao\": row['observacao']\n",
    "                                },\n",
    "                            },\n",
    "                            \"batch\": row['batch'],    \n",
    "                            \"diretorio\": str(row['directory']),\n",
    "                            \"nome_arquivo\": row['original_file_name'],\n",
    "                            \"pdf_pesquisavel\": row['pdf_pesquisavel'],\n",
    "                            \"modelo\": row['model'],   \n",
    "                            \"document_unique_id\": index,\n",
    "                    }        \n",
    "                \n",
    "    \n",
    "    numero_nota_fiscal = str(row['numero_nota_fiscal'])\n",
    "    dados_json[numero_nota_fiscal] = dados_nf\n",
    "\n",
    "# Salvando em formato JSON\n",
    "json_file_path = os.path.join(json_path, nome_formado_json)\n",
    "with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dados_json, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "print(f\"As informações foram salvas em {json_file_path}\")  \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>2.3.3. 1</b> Desenho do Template Mathplotlib </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para testes, NAO JOGAR FORA\n",
    "\n",
    "file_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_21/SAO PEDRO DA ALDEIA_PDF_31282023_2257/11756286/NF CRJ PRIMEIRA QUINZENA DE JULHO DE 2023.pdf\"\n",
    "original_file_name = os.path.basename(file_path)\n",
    "\n",
    "model = \"SPA\"\n",
    "pdf_pesquisavel_map = False\n",
    "mapping_method = \"frame_&_sframe_field\" # significa que as coordenadas estao em frames e os valores dos campos nos sframe_fields\n",
    "context_mapping = \"data_cabecalho\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pipeline_extracao_documentos/6_geral_administacao/temp_docs/images/processadas/ampla.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(file_path).convert(\"RGB\")\n",
    "# Converta a imagem para um array NumPy\n",
    "image_np = np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Determine as colunas de coordenadas a serem usadas\n",
    "    x0_col, y0_col, x1_col, y1_col = ('x0_p', 'y0_p', 'x1_p', 'y1_p') if is_searchable else ('x0', 'y0', 'x1', 'y1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mapping = {\n",
    "    \"red\": (1, 0, 0),\n",
    "    \"purple\": (0.5, 0, 0.5),\n",
    "    \"orange\": (1, 0.647, 0),\n",
    "    \"green\": (0, 0.5, 0.196),\n",
    "    \"blue\": (0, 0, 1),\n",
    "    \"yellow\": (1, 1, 0),\n",
    "}\n",
    "\n",
    "\n",
    "def draw_boxes(image_np, df, modelo, draw_types=None):\n",
    "    plt.figure(figsize=(25, 25))\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Filtrar baseado no modelo e nos tipos de \"boxes\" a serem desenhados\n",
    "    filtered_df = df[df['model'] == modelo]\n",
    "    if draw_types:\n",
    "        filtered_df = filtered_df[filtered_df['type'].isin(draw_types)]\n",
    "    \n",
    "    for index, row in filtered_df.iterrows():\n",
    "        x0, y0, x1, y1 = row['x0'], row['y0'], row['x1'], row['y1']\n",
    "        \n",
    "        color = color_mapping.get(row['color'], 'black')\n",
    "        \n",
    "        # Adicionando o retângulo\n",
    "        plt.gca().add_patch(Rectangle((x0, y0), x1-x0, y1-y0, linewidth=1, edgecolor=color, facecolor='none'))\n",
    "        \n",
    "        # Adicionando o rótulo, se existir\n",
    "        label = str(row['label']) if pd.notnull(row['label']) else None\n",
    "        if label:\n",
    "            plt.text(x0 + 10, y0 - 15, label, color=color, fontsize=10)\n",
    "            plt.text(x0 + 20, y0 + 55,(x0, y0, x1, y1), color='black', fontsize=7)\n",
    "\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_model_path = \"pipeline_extracao_documentos/6_geral_administacao/modelos/frames_nf_v11.xlsx\"\n",
    "\n",
    "\n",
    "#Le a planilha e cria do DF\n",
    "frames_nf_v4_df = pd.read_excel(nf_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_boxes(image_np, frames_nf_v4_df, 'SPA', draw_types=['boundaries', 'field_box'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_boxes(image_np, frames_nf_v4_df, 'SPA', draw_types=['boundaries', 'frame', 'field_box'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de apresentaçao da imagem\n",
    "x0 = 0\n",
    "y0 = 0\n",
    "plt.figure(figsize=(25, 25))\n",
    "plt.imshow(imagem_gray_np)\n",
    "plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "plt.text(x0 + 1, y0 + 10,original_file_name, color='black', fontsize=20)\n",
    "#plt.text(original_file_name, color='black', fontsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mapping_method == \"frame_&_sframe_field\":\n",
    "    tipo_4_coordinates = \"frame\"\n",
    "    tipo_4_filter = \"sframe_field\"\n",
    "coordinates = get_coordinates_filter_by_context(pdf_pesquisavel_map, model, context_mapping, tipo_4_coordinates)\n",
    "x0, y0, x1, y1 = coordinates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrar_df(df, **kwargs):\n",
    "    query = \" & \".join(f\"{key} == @kwargs['{key}']\" for key in kwargs)\n",
    "    result = df.query(query)\n",
    "    return result\n",
    "\n",
    "# 11. Pesquiso Unique_ID por file\n",
    "def get_document_id_by_file(batch, file):\n",
    "    \n",
    "    result = filtrar_df(df_id_relations, Batch=batch, File=file)\n",
    "    document_unique_id = result['Unique_ID'].values[0]\n",
    "    \n",
    "    return document_unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_file_name = \"Doria Marinho 0297 Raquel.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = filtrar_df(df_conf0, original_file_name=original_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de apresentaçao da imagem\n",
    "x0 = 0\n",
    "y0 = 0\n",
    "plt.figure(figsize=(25, 25))\n",
    "plt.imshow(result['image_np'].values[0])\n",
    "plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "plt.text(x0 + 1, y0 + 10,original_file_name, color='black', fontsize=20)\n",
    "#plt.text(original_file_name, color='black', fontsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_np = result['image_np'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = int(x0)\n",
    "y0 = int(y0)\n",
    "x1 = int(x1)    \n",
    "y1 = int(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_image_np = imagem_gray_np[y0:y1, x0:x1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de apresentaçao da imagem\n",
    "x0 = 0\n",
    "y0 = 0\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(cropped_image_np)\n",
    "plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "#plt.text(x0 + 1, y0 + 10,original_file_name, color='black', fontsize=20)\n",
    "#plt.text(original_file_name, color='black', fontsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = frames_nf_v4_df[frames_nf_v4_df['type'] == 'field_box']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_info = {}\n",
    "i = 1\n",
    "for idx, row in boxes.iterrows():\n",
    "    x0 = int(row['x0'])\n",
    "    y0 = int(row['y0'])\n",
    "    x1 = int(row['x1'])\n",
    "    y1 = int(row['y1'])\n",
    "    cropped_image_np = imagem_gray_np[y0:y1, x0:x1]\n",
    "    \n",
    "    boxes_info[f'box_{i}'] = {\n",
    "        'coordinates': (x0, y0, x1, y1),\n",
    "        'image': cropped_image_np,\n",
    "        # ... qualquer outra informação que você deseja armazenar\n",
    "    }\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_info['box_1']['coordinates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_info['box_1']['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for box in boxes_info:\n",
    "    x0, y0, x1, y1 = boxes_info[box]['coordinates']\n",
    "    image = boxes_info[box]['image']\n",
    "    # plt.figure(figsize=(25, 25))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8\n",
    "\n",
    "x0 = 0\n",
    "y0 = 0\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(boxes_info[f'box_{i}']['image'])\n",
    "plt.text(x0 , y0,boxes_info[f'box_{i}']['coordinates'], color='green', fontsize=7)\n",
    "plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize um dicionário vazio para armazenar as informações dos \"boxes\"\n",
    "boxes_info = {}\n",
    "\n",
    "# Suponha que você está em um loop onde está processando vários \"boxes\"\n",
    "for i, box in enumerate(boxes):\n",
    "    # Obtenha as coordenadas do \"box\"\n",
    "    x0, y0, x1, y1 = coordinates[0] \n",
    "    # Corte a área do \"box\" da imagem original\n",
    "    cropped_image = imagem_gray_np[y0:y1, x0:x1]\n",
    "    \n",
    "    # Armazene as informações do \"box\" no dicionário\n",
    "    boxes_info[f'box_{i}'] = {\n",
    "        'coordinates': (x0, y0, x1, y1),\n",
    "        'image': cropped_image,\n",
    "        # ... qualquer outra informação que você deseja armazenar\n",
    "    }\n",
    "\n",
    "# Agora, `boxes_info` contém informações detalhadas sobre cada \"box\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Variáveis para armazenar as coordenadas do retângulo\n",
    "startX, startY, endX, endY = -1, -1, -1, -1\n",
    "drawing = False\n",
    "\n",
    "def draw_rectangle(event, x, y, flags, param):\n",
    "    global startX, startY, endX, endY, drawing\n",
    "\n",
    "    # Se o botão esquerdo do mouse for pressionado, comece a desenhar o retângulo\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        startX, startY = x, y\n",
    "\n",
    "    # Se o botão esquerdo do mouse for solto, finalize o retângulo\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        endX, endY = x, y\n",
    "        cv2.rectangle(img, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "\n",
    "# Carregue sua imagem como um array NumPy\n",
    "img = np.copy(image_np) # Substitua 'image_np' pelo seu array NumPy da imagem\n",
    "\n",
    "# Crie uma janela e atribua a função de callback\n",
    "cv2.namedWindow(\"Image\")\n",
    "cv2.setMouseCallback(\"Image\", draw_rectangle)\n",
    "\n",
    "while True:\n",
    "    # Exibe a imagem\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    \n",
    "    # Se a tecla 'q' for pressionada, saia do loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Fecha todas as janelas OpenCV\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><mark> <b>2.3.3.0</b> Ajustando medidas</mark></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> IMPORTANTE: </mark> processo de ajustar coordenadas no frames_nf_v4_df e depois salvalas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pipeline_extracao_documentos/0_arquivos_teste_pipeline/pdf_pesquisavel/Mage/NF 6235.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_20/fwdnotasfaltantesnosistemadeemissoim20734_106187/Doria Marinho 0301 Ultrascan.pdf\"\n",
    "original_file_name = os.path.basename(file_path)\n",
    "\n",
    "section = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "tipo = \"field_box\"\n",
    "father_value = \"5_frame_inf_criticas\"\n",
    "model_map = \"MAGE\"\n",
    "de_para_pm = \"PM_MAGE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Foi copiada - somente para referencia\n",
    "def convert_resize_gray(original_file_name, file_path, image_resized_path):\n",
    "\n",
    "    name_image = conv_filename_no_ext(original_file_name)\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(name_image)}.jpg')\n",
    "    pages = convert_from_path(file_path, 500, poppler_path=poppler_path)\n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((4134, 5846))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "    imagem_gray = resized_pages[0].convert('L')\n",
    "    imagem_gray.save(image_resized_name, 'JPEG')\n",
    "\n",
    "    return  imagem_gray, image_resized_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota de mage que nao bateu as coordenadas\n",
    "file_path = \"pipeline_extracao_documentos/0_arquivos_teste_pipeline/pdf_pesquisavel/Mage_fora_padrao/1128 desmonte em 01-08-23.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processo para acertar as coordenadas de frames_nf_v4_df\n",
    "\n",
    "novo_x0 = 700\n",
    "novo_y0 = 3895\n",
    "novo_x1 = 3900\n",
    "novo_y1 = 4500\n",
    "\n",
    "frames_nf_v4_df.at[indice_original, 'x0'] = novo_x0\n",
    "frames_nf_v4_df.at[indice_original, 'y0'] = novo_y0\n",
    "frames_nf_v4_df.at[indice_original, 'x1'] = novo_x1\n",
    "frames_nf_v4_df.at[indice_original, 'y1'] = novo_y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para pesquisar as colunas que quero\n",
    "frames_nf_v4_df[['model', 'section_json', 'label', 'reference', 'x0', 'y0', 'x1', 'y1']].head(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_nf_v4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path para leitura do arquivo excel do DF frames_nf_v4_df_path  (ATUALIZAR DOCUMENTO)\n",
    "frames_nf_v4_df_path = \"pipeline_extracao_documentos/6_geral_administacao/modelos/frames_nf_v10.xlsx\"\n",
    "\n",
    "#Le a planilha e cria do DF\n",
    "frames_nf_v4_df = pd.read_excel(frames_nf_v4_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path para leitura do arquivo excel do DF frames_nf_v4_df_path  (ATUALIZAR DOCUMENTO)\n",
    "frames_nf_v4_df_path = \"pipeline_extracao_documentos/6_geral_administacao/modelos/frames_nf_v10.xlsx\"\n",
    "\n",
    "\n",
    "# Salvando o DF para excel\n",
    "frames_nf_v4_df.to_excel(frames_nf_v4_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path para salvar o arquivo excel do DF frames_nf_v4_df_path  (ATUALIZAR DOCUMENTO)\n",
    "frames_nf_v4_df_path = \"pipeline_extracao_documentos/6_geral_administacao/modelos/frames_nf_v10.xlsx\"\n",
    "\n",
    "# Salvando o DF para excel\n",
    "frames_nf_v4_df.to_excel(frames_nf_v4_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listando 59 linhas do dataframe\n",
    "frames_nf_v4_df.head(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar o índice da linha no DataFrame original\n",
    "indice_original = row_frame.index[0]\n",
    "\n",
    "# Atualizando o valor diretamente no DataFrame original\n",
    "novo_valor_x0 = ...  # Substitua com o novo valor\n",
    "frames_nf_v4_df.at[indice_original, 'x0'] = novo_valor_x0\n",
    "\n",
    "# Verificando se o valor foi atualizado no DataFrame original\n",
    "print(frames_nf_v4_df.at[indice_original, 'x0'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"imagem_gray.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'document'].iloc[0]\n",
    "boundaries_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'boundaries']\n",
    "sections_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'section']\n",
    "frames_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'frame']\n",
    "sframe_fields_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'sframe_field']\n",
    "field_boxes_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'field_box']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_nf_v4_df[frames_nf_v4_df['type'] == 'document'].iloc[0]\n",
    "frames_nf_v4_df[frames_nf_v4_df['type'] == 'boundaries']\n",
    "frames_nf_v4_df[frames_nf_v4_df['type'] == 'section']\n",
    "frames_nf_v4_df[frames_nf_v4_df['type'] == 'frame']\n",
    "frames_nf_v4_df[frames_nf_v4_df['type'] == 'field_box']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_boxes_info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"MAGE\"\n",
    "modelo = \"MAGE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"imagem_gray.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_unique_id = result['Unique_ID'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['coluna_cnae'] = df['coluna_cnae'].apply(lambda x: x.strip() if isinstance(x, str) else x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tables-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
