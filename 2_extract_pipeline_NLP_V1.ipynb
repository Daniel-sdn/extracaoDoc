{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <mark> <b> > 2.0 </b> Pipeline de Extracao de dados de documentos - NLP </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2_extract_pipeline_NLP_V0.ipynb</b>    |     Atual notebook com as funçoes para processamento de documentos com soluçao NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules e config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import platform\n",
    "import subprocess\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from urllib import response\n",
    "\n",
    "from outlook_msg import Message\n",
    "import extract_msg\n",
    "import zipfile\n",
    "from pyunpack import Archive\n",
    "import py7zr\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import PyPDF2\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "import locale\n",
    "import time, copy\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import cv2\n",
    "import fitz  # Módulo PyMuPDF\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import Image, ImageDraw\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar\n",
    "import matplotlib.pyplot as plt\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Token\n",
    "from spacy.language import Language\n",
    "from difflib import SequenceMatcher # verificar\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "# Modulos da solucao\n",
    "# import modules.extrai_pdf_pesquisavel as Extc\n",
    "import modules.cronometro as cron\n",
    "import modules.nova_extracao_pdf_pesquisavel as novaextra \n",
    "import modules.trata_model as tmod\n",
    "import modules.trata_pdf as tpdf\n",
    "import modules.utils as utl\n",
    "\n",
    "ner = nlp.remove_pipe('ner')\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b>1.0</b> Configuraçoes, funcoes e patterns NLP  </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipo de documento em uso\n",
    "tipo_doc_work = 'nfs_e'\n",
    "\n",
    "# 1. XXX Path para planilha de processamento de batches\n",
    "conf_export_plan_path = 'processamentos/processamento_batches/df_conf_export_batch.xlsx'\n",
    "\n",
    "\n",
    "\n",
    "# 2. XXX  Tratando nome de carga do df_processamento\n",
    "map_analise_path = \"processamentos/mapeamento_analise\"\n",
    "\n",
    "# 3. XXX  prefixo de nome do arquivo de exportaçao\n",
    "df_root_pipe_file = \"df_root_\"\n",
    "\n",
    "# 4. XXX Tipos de documentos para extracao\n",
    "tipo_documento_path = \"config/tipo_documentos\"\n",
    "\n",
    "# 5. XXX Path para tipo de documento patterns\n",
    "tipo_documento_patterns_path = \"config/tipo_documentos/patterns\"\n",
    "\n",
    "# 6. XXX Nome do caminha para dict Tipo de documento\n",
    "config_tipo_doc_path = \"config/tipo_documentos/tipo_documento.json\"\n",
    "\n",
    "# Paths de trabalho para Raster_PDF\n",
    "raster_process_pdf_path = 'processamentos/temp/pdf'\n",
    "raster_process_txt_path = 'processamentos/temp/txt'\n",
    "\n",
    "\n",
    "# 6. IMPORTANTE - MUDOU - Path para gestao de imagens resized\n",
    "image_resized_path = \"processamentos/temp/images/processadas\"\n",
    "\n",
    "\n",
    "#### Config - E-mail\n",
    "# 1. Caminho do arquivo uma mensagem especifica\n",
    "msg_dir_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/11_emails'\n",
    "\n",
    "# 2. Path para arquivos atachados compactados\n",
    "msg_attachment_zip = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/13_attachments'\n",
    "\n",
    "\n",
    "#### Config - messages\n",
    "# 3. Caminho do arquivo uma mensagem especifica\n",
    "msg_outros_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/12_messages'\n",
    "\n",
    "# 4. Path para arquivos recebidos manualmente\n",
    "arquivos_recebidos_path = 'pipeline_extracao_documentos/1_emails_documentos_recebidos/14_documentos_recebidos'\n",
    "\n",
    "\n",
    "####Config Processamento Pipeline\n",
    "\n",
    "# 5. Path para documentos para extracao\n",
    "documentos_extracao_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento\"\n",
    "\n",
    "\n",
    "\n",
    "# 7. path para arquivos json\n",
    "json_path = \"processamentos/jsons\"\n",
    "\n",
    "# 7. Path para DFs e CSVs exportados\n",
    "export_path = \"pipeline_extracao_documentos/6_geral_administacao/exports\"\n",
    "\n",
    "# 8. Path para lixeira\n",
    "root_garbage_path = \"pipeline_extracao_documentos/0_lixeira\"\n",
    "\n",
    "\n",
    "#### paths de objetos para criacao/gestao (dicionarios/datasets)\n",
    "cnae_dict_path = \"pipeline_extracao_documentos/6_geral_administacao/datasets/CNAE_X_ITEM_SERVICO_PREFEITURAS.xlsx\"\n",
    "\n",
    "\n",
    "# 12. poppler path\n",
    "poppler_path = \"/home/dani-boy/miniconda3/envs/tables-detr/bin\"\n",
    "\n",
    "# 13. path para config Tesseract\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "#Modelo atual\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/dani-boy/miniconda3/envs/tables-detr/share/tessdata/\" --user-patterns \"novo_modelo/modelos/user-patterns2.txt\" --dpi 600 --oem 3 --psm 6'\n",
    "\n",
    "# definindo localizadcao para pt_BR\n",
    "locale.setlocale(locale.LC_TIME, \"pt_BR.utf8\")\n",
    "\n",
    "# logging.basicConfig(\n",
    "#     filename='config/log_ocorrencias.log',\n",
    "#     level=logging.INFO, \n",
    "#     format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "#     datefmt='%d/%m/%Y %H:%M:%S'\n",
    "# )\n",
    "\n",
    "# logging.info(\"kernel reiniciado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>1.0</b> Funcoes legadas e NLP </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================================================================#\n",
    "#                                                                                                   #\n",
    "#                                       FUNCOES LEGADAS                                             #\n",
    "#                                                                                                   #\n",
    "#===================================================================================================#\n",
    "\n",
    "# XXX IMPORTANTE - ESTA E A FUNCAO PARA SER UTILIZADA: POIS CONVERTE PARA CINZA E RESIZE: (4134, 5846)\n",
    "def convert_resize_gray(original_file_name, file_path, image_resized_path):\n",
    "\n",
    "    name_image = utl.conv_filename_no_ext(original_file_name)\n",
    "    image_resized_name = os.path.join(f'{image_resized_path}/{str(name_image)}.jpg')\n",
    "    pages = convert_from_path(file_path, 500, poppler_path=poppler_path)\n",
    "    # 4. Verifica se ha mais que uma pagina\n",
    "    if len(pages) > 1:\n",
    "        raise ValueError(\"Erro, documento com mais de uma página\")\n",
    "    else:\n",
    "        # 5. Iterar pelas páginas e redimensionar\n",
    "        resized_pages = []\n",
    "        for page in pages:\n",
    "            resized_page = page.resize((4134, 5846))\n",
    "            resized_pages.append(resized_page)\n",
    "            \n",
    "    imagem_gray = resized_pages[0].convert('L')\n",
    "    imagem_gray.save(image_resized_name, 'JPEG')\n",
    "\n",
    "    return  imagem_gray, image_resized_name\n",
    "\n",
    "# XXX Pequenos mas poderosos\n",
    "def extract_text_PIL(image, coordinates):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    image_croped = image.crop((x0, y0, x1, y1))\n",
    "    texto_extraido = pytesseract.image_to_string(image_croped, lang='por', config='--psm 6')\n",
    "    return texto_extraido \n",
    "\n",
    "\n",
    "# 5. XXX Ajusta textoYYY\n",
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited\n",
    "\n",
    "\n",
    "##### Frame functions\n",
    "\n",
    "# funçao importante para buscar coordenadas do frame em funçao do contexto\n",
    "def get_coordinates_filter_by_context(pdf_pesquisavel_map, model_map, context_mapping, tipo):\n",
    "    \n",
    "    row_frame = utl.filtrar_df(frames_nf_v4_df, model=model_map, context_mapping=context_mapping, type=tipo)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [((row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p']) if pdf_pesquisavel_map else (row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1']))]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates\n",
    "\n",
    "\n",
    "\n",
    "# XXXpara buscar melhor as coordendas dos FRAMES\n",
    "def get_coordinates_filter(pdf_pesquisavel_map, model, tipo, label, section):\n",
    "    \n",
    "    row_frame = utl.filtrar_df(frames_nf_v4_df, model=model, type=tipo, label=label, section_json=section)\n",
    "    \n",
    "    # Verificando se row_frame não está vazio\n",
    "    if not row_frame.empty:\n",
    "        # Acessando a primeira linha do DataFrame filtrado e depois acessando as colunas\n",
    "        coodinates = [((row_frame.iloc[0]['x0_p'], row_frame.iloc[0]['y0_p'], row_frame.iloc[0]['x1_p'], row_frame.iloc[0]['y1_p']) if pdf_pesquisavel_map else (row_frame.iloc[0]['x0'], row_frame.iloc[0]['y0'], row_frame.iloc[0]['x1'], row_frame.iloc[0]['y1']))]\n",
    "    else:\n",
    "        # Retornando uma tupla de valores NaN se o DataFrame filtrado estiver vazio\n",
    "        coodinates = [(float('nan'), float('nan'), float('nan'), float('nan'))]\n",
    "    \n",
    "    return coodinates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 0.A Dados iniciais - PDF PESQUISAVEL\t\n",
    "def pesquisa_prefeitura_pdf_pesquisavel(idx, row, row_info, map_directory, original_file_name, file_path, debug):    \n",
    "    \n",
    "    \n",
    "   # Carregar o arquivo PDF\n",
    "    pdf_document = fitz.open(file_path)\n",
    "\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Definir retângulo de interesse\n",
    "    x0 = 0\n",
    "    y0 = 4\n",
    "    x1 = 600\n",
    "    y1 = 200  # Ajuste este valor para delimitar a região vertical\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    \n",
    "    if debug:\n",
    "        print(f'\\ndentro da funçao: pesquisa_prefeitura_pdf_pesquisavel: doc.:{original_file_name} | diretorio: {map_directory}  text: \\n\\n{text}\\n\\n')\n",
    "    \n",
    "    if text:\n",
    "       page_number = 0\n",
    "       #print(page_number)\n",
    "    else:\n",
    "       page_number = 1\n",
    "       #print(page_number)\n",
    "    \n",
    "    pdf_document.close()\n",
    "   \n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 0. INFOMACOES INICIAIS - RASTER PDF\n",
    "def processar_dados_iniciais(idx, row, row_info, section, map_directory, original_file_name, file_path, debug):\n",
    "    \n",
    "    # lista_texto_extraido = []\n",
    "\n",
    "    nf_dados_doc = {}\n",
    "    nf_dados_doc['secao'] = section\n",
    "    pdf_pesquisavel = None\n",
    "    extracted_txt = pesquisa_prefeitura_pdf_pesquisavel(idx, row, row_info, map_directory, original_file_name, file_path, debug)\n",
    "    if debug:\n",
    "        print(f'\\n1. funcao: processar_dados_iniciais: doc.:{original_file_name} | diretorio: {map_directory} apos funcao: pesquisa_prefeitura_pdf_pesquisavel: extracted_txt:\\n{extracted_txt}\\n\\n')\n",
    "    \n",
    "    if extracted_txt:\n",
    "        pdf_pesquisavel = True\n",
    "        print(f'extracted_txt: {extracted_txt} - portanto pdf_pesquisavel: {pdf_pesquisavel} ')\n",
    "        \n",
    "    else:\n",
    "        pdf_pesquisavel = False\n",
    "        print(f'extracted_txt: {extracted_txt} - portanto pdf_pesquisavel: {pdf_pesquisavel} ') \n",
    "       \n",
    "       \n",
    "        # WTF\n",
    "        x0 = 220\n",
    "        y0 = 0\n",
    "        x1= 3858\n",
    "        y1 = 1572\n",
    "        \n",
    "        # usando novo processo que gera o arquivo \"on the fly\" imagem_gray (converte PDF para imagem de tamanho grande (4134, 5846) - torna-a cinza e a salva)\n",
    "        imagem_gray, image_resized_name = convert_resize_gray(original_file_name, file_path, image_resized_path)\n",
    "        extracted_txt = extract_text_PIL(imagem_gray, (x0, y0, x1, y1))\n",
    "        #print(f'extracted_txt: {extracted_txt}')\n",
    "        if debug:\n",
    "            print(f'\\n2. funcao: processar_dados_iniciaisdoc.:{original_file_name} | diretorio: {map_directory}  apos : extract_text_PIL: extracted_txt:\\n{extracted_txt}\\n\\n')\n",
    "    \n",
    "    nf_dados_doc['file_name'] = original_file_name    \n",
    "    nf_dados_doc['pdf_pesquisavel'] = pdf_pesquisavel \n",
    "    value = {}   \n",
    "    texto_tratado = texto_extraido(extracted_txt)\n",
    "    value = define_dados_iniciais(idx, row, row_info, texto_tratado, debug)\n",
    "    if debug:\n",
    "        print(f'\\n3. funcao: processar_dados_iniciais doc.:{original_file_name} | diretorio: {map_directory} | apos funcao: define_dados_iniciais() value \\n{value}\\n\\n')\n",
    "    if value:\n",
    "        nf_dados_doc.update(value)\n",
    "   \n",
    "\n",
    "\n",
    "    return nf_dados_doc\n",
    "\n",
    "\n",
    "\n",
    "##### Extractions fucntions\n",
    "\n",
    "\n",
    "# 1.B CABECALHO XXX Funcoes de extracao -cabecalho Raster\n",
    "def processar_cabecalho_R_PDF(idx, row, row_info, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "    \n",
    "    data_box_valores = {}\n",
    "    data_box_conferencia = {}\n",
    "    data_box_valores['secao'] = section\n",
    "    \n",
    "    batch_name_row_info = row_info.get('batch')\n",
    "    #status_documento_row_info = row_info.get('status_documento')\n",
    "    information_row_info = row_info.get('informations')\n",
    "    action_item_row_info = row_info.get('action_item')\n",
    "    \n",
    "    # Busco a imagem np do documento\n",
    "    image_np_row_info = row_info.get('image_np')\n",
    "    \n",
    "    data_box_valores['action_item'] = action_item_row_info\n",
    "    data_box_valores['informations'] = information_row_info\n",
    "    data_box_valores['processo'] = context_mapping\n",
    "    data_box_valores['conf_cod'] = 0\n",
    "\n",
    "\n",
    "                     \n",
    "    \n",
    "    # busco coordenadas para o contexto\n",
    "    if mapping_method == \"frame_&_sframe_field\":\n",
    "        tipo_4_coordinates = \"frame\"\n",
    "        tipo_4_filter = \"sframe_field\"\n",
    "    \n",
    "    #print(f'\\n2. Dentro func: section: {section} mapping_method: {mapping_method} | context_mapping: {context_mapping} | model_map: {model_map} | original_file_name: {original_file_name}\\n')\n",
    "   \n",
    "    # 2. usando a funcao de extracao de coordenadas por contexto    \n",
    "    coordinates = get_coordinates_filter_by_context(pdf_pesquisavel_map, model_map, context_mapping, tipo_4_coordinates)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    #print(f'x0: {x0} | y0: {y0} | x1: {x1} | y1: {y1}')\n",
    "    x0 = int(x0)\n",
    "    y0 = int(y0)\n",
    "    x1 = int(x1)\n",
    "    y1 = int(y1) \n",
    "    # 3. Cropo a imagem - novo modelo\n",
    "    cropped_image_np = image_np_row_info[y0:y1, x0:x1] # ajustar nos demais\n",
    "    data_box_conferencia[f'box_{context_mapping}'] = cropped_image_np\n",
    "    data_box_conferencia[f'coordinates_{context_mapping}'] = coordinates\n",
    "    # 4. Converto para PIL\n",
    "    cropped_image_pil = Image.fromarray(cropped_image_np)\n",
    "    # 6. Executo OCR\n",
    "    texto_extraido = pytesseract.image_to_string(cropped_image_pil, lang='por')\n",
    "    # 7. Trato o texto extraido = text_splited\n",
    "    text_splited = texto_extraido_cabecalho(texto_extraido)\n",
    "    if debug:\n",
    "        print()\n",
    "        plt.imshow(cropped_image_np)\n",
    "        plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "        plt.show()\n",
    "        print(f'\\ncoordinates {coordinates} - \\ntexto_extraido:\\n{text_splited}\\n')\n",
    "        \n",
    "    # 8. Efetuo o filtro para a iteracao\n",
    "    filtered_frame_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['context_mapping'] == context_mapping) & (frames_nf_v4_df['type'] == tipo_4_filter)]\n",
    "    \n",
    "    # 9. iter sobre o filtro\n",
    "    for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "        try:\n",
    "            section = row_frame['section_json']\n",
    "            label = row_frame['label']\n",
    "            reference = row_frame['reference']\n",
    "            string_pesquisa = row_frame['marcador_inicio']  \n",
    "            keyword_list = ['Número da Nota:', 'Competência:', 'Data e Hora da Emissão:', 'Código Verificação:']\n",
    "            texto = pesquisa_keyword(string_pesquisa, text_splited, keyword_list)\n",
    "            data_box_valores[label] = texto\n",
    "            if debug:\n",
    "               print(f'\\nidx: {index_frame:> 3} | label: {label} |  string_pesquisa:{string_pesquisa} | dentro do try do raster PDF cabecalho - texto: \\n{texto}\\n\\n')\n",
    "        except Exception as e:\n",
    "            msg = (f\"{e}\")\n",
    "            data_box_conferencia[label] = msg\n",
    "    \n",
    "\n",
    "    # Verificações após o loop\n",
    "    for key, value in data_box_valores.items():\n",
    "        if key == 'numero_nota_fiscal' and value is None:\n",
    "            action_item_row_info = 'BREAK_PROCESS'\n",
    "            information_row_info = 'Número da Nota não encontrado'\n",
    "            #logging.error(f\" {batch_name} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")  # Ou registre o erro de outra forma que preferir\n",
    "        \n",
    "        elif key == 'codigo_verificacao' and value != None:\n",
    "            codigo_verificacao_nf = value\n",
    "            tam_codigo_verificacao = len(codigo_verificacao_nf)\n",
    "            data_box_valores['conf_cod'] = tam_codigo_verificacao\n",
    "            \n",
    "        \n",
    "        elif key != 'numero_nota_fiscal' and value is None:\n",
    "            logging.error(f\" {batch_name_row_info} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")  # Ou registre o erro de outra forma que preferir\n",
    "\n",
    "            \n",
    "      # if value is None:\n",
    "        #     logging.error(f\" {batch_name} |  doc: {original_file_name:>25} | setion:{section:20} | item: {key:>20} | erro na extracaçao | file_path: {file_path:>40} \")  # Ou registre o erro de outra forma que preferir\n",
    "\n",
    "    data_box_valores['action_item'] = action_item_row_info\n",
    "    data_box_valores['informations'] = information_row_info\n",
    "\n",
    "    \n",
    "    return data_box_valores\n",
    "\n",
    "# 1.A CABECALHO - PDF PESQUISAVEL  \n",
    "def extrai_cabecalho_PDF_P(idx, row, row_info, section, pdf_pesquisavel_map, de_para_pm, model_map, f_0, f_1, original_file_name, file_path, debug):\n",
    "    \n",
    "    nf_data_cabecalho = {}\n",
    "    lista_erros = []\n",
    "    label = \"1_frame_dados_nf\"\n",
    "    \n",
    "    batch_name_row_info = row_info.get('batch')\n",
    "    information_row_info = row_info.get('informations')\n",
    "    action_item_row_info = row_info.get('action_item')\n",
    "    \n",
    "    nf_data_cabecalho['secao'] = section\n",
    "    nf_data_cabecalho['action_item'] = action_item_row_info\n",
    "    nf_data_cabecalho['informations'] = information_row_info\n",
    "    nf_data_cabecalho['processo'] = 'mapeamento regex - PDF pesquisavel'\n",
    "    \n",
    "    if debug:\n",
    "        print(f'\\n\\n2. dentro da funçao extrai_cabecalho_PDF: batch_name: {batch_name_row_info}\\n\\n')\n",
    "    \n",
    "    pdf_document = fitz.open(file_path)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]    \n",
    "    tipo = \"frame\"\n",
    "\n",
    "    coordinates = get_coordinates_filter(pdf_pesquisavel_map=pdf_pesquisavel_map, model=model_map, tipo=tipo, label=label, section=section)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    y0 = y0 * f_0\n",
    "    y1 = y1 * f_1\n",
    "    \n",
    "    text = page.get_text(\"text\", clip=(x0, y0, x1, y1))\n",
    "    if debug:\n",
    "        print(f'\\n3. x0: {x0}, y0: {y0}, x1: {x1}, y1: {y1} f_0: {f_0} f_1: {f_1} | text: \\n{text} \\n\\n')\n",
    "\n",
    "    try:\n",
    "        numero_nota_match = re.search(r'Número da Nota:\\s+(\\d+)', text)\n",
    "        if numero_nota_match:\n",
    "            numero_nf = numero_nota_match.group(1)\n",
    "            nf_data_cabecalho['numero_nota_fiscal'] = numero_nf\n",
    "            #nf_data_cabecalho['informations'] = 'documento com numero de nota fiscal'\n",
    "            if debug:\n",
    "                print(f'\\nnr_nro_nf: {nr_nro_nf} - doc: {original_file_name}\\n')\n",
    "        else:\n",
    "            msg = (f\"Número da Nota não encontrado\")\n",
    "            nf_data_cabecalho['numero_nota_fiscal'] = None\n",
    "            information_row_info = 'Número da Nota não encontrado'\n",
    "            nf_data_cabecalho['informations'] = information_row_info\n",
    "            action_item_row_info = 'BREAK_PROCESS'\n",
    "            nf_data_cabecalho['action_item'] = action_item_row_info\n",
    "    except Exception as e:\n",
    "        msg = (f\"doc: {original_file_name} | numero NF nao encontrado {e}\")\n",
    "        nf_data_cabecalho['numero_nota_fiscal'] = None\n",
    "        information_row_info = 'Número da Nota não encontrado'\n",
    "        nf_data_cabecalho['informations'] = information_row_info\n",
    "        action_item_row_info = 'BREAK_PROCESS'\n",
    "        nf_data_cabecalho['action_item'] = action_item_row_info\n",
    "\n",
    "    # Extrair Competência\n",
    "    competencia_match = re.search(r'Competência:\\s+(.+)', text)\n",
    "    if competencia_match:\n",
    "        nf_data_cabecalho['competencia'] = competencia_match.group(1)\n",
    "\n",
    "    # Extrair Data e Hora de Emissão\n",
    "    data_emissao_match = re.search(r'Data e Hora da Emissão:\\s+(.+)', text)\n",
    "    if data_emissao_match:\n",
    "        nf_data_cabecalho['dt_hr_emissao'] = data_emissao_match.group(1)\n",
    "        \n",
    "    # Extrair codigo Verificacao\n",
    "    codigo_verificacao_match = re.search(r'Código Verificação:\\s+(.+)', text)\n",
    "    if codigo_verificacao_match:\n",
    "        codigo_verificacao_nf = codigo_verificacao_match.group(1)\n",
    "        nf_data_cabecalho['codigo_verificacao'] =  codigo_verificacao_nf\n",
    "        tam_codigo_verificacao = len(codigo_verificacao_nf)\n",
    "        nf_data_cabecalho['conf_cod'] = tam_codigo_verificacao\n",
    "        \n",
    "    \n",
    "    \n",
    "    pdf_document.close()\n",
    "    \n",
    "    return nf_data_cabecalho\n",
    "\n",
    "#===================================================================================================#\n",
    "#                                                                                                   #\n",
    "#                                         PRIMEIRAS FUNCOES NLP                                     #\n",
    "#                                                                                                   #\n",
    "#===================================================================================================#\n",
    "\n",
    "\n",
    "def show_ent_new(text, patterns):\n",
    "    #nlp = spacy.blank(\"pt\")\n",
    "    #ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    ruler.add_patterns(patterns)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tokens = []\n",
    "    ents = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        span = doc.char_span(ent.start_char, ent.end_char, label=ent.label_)\n",
    "        ents.append(span)\n",
    "        \n",
    "    for token in doc:\n",
    "        start = token.idx\n",
    "        end = start + len(token)\n",
    "        tokens.append((token.text, start, end))\n",
    "        \n",
    "    return doc, tokens, ents\n",
    "\n",
    "\n",
    "\n",
    "# chunk.text, chunk.start, chunk.end, chunk.root.head.lemma_, chunk.root.dep_, chunk.doc\n",
    "def load_json(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Funcoes para salvar e carregar entity ruler patterns\n",
    "def write_patterns_to_file(patterns, colors, filename):\n",
    "    data = {\"patterns\": patterns, \"colors\": colors}\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, ensure_ascii=True, indent=2)\n",
    "        \n",
    "\n",
    "# Funcao para carregar as cores e patterns do Entity ruler        \n",
    "def load_patterns_and_colors(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        patterns = data[\"patterns\"]\n",
    "        colors = data[\"colors\"]\n",
    "    return patterns, colors \n",
    "\n",
    "\n",
    "# Salva do tipo de documento em arquivo\n",
    "def save_tipo_doc_to_file(dic, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(dic, f) \n",
    "        \n",
    "# carrega o tipo de documento do arquivo        \n",
    "def load_dict_from_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Salva o dict doc_content em arquivo\n",
    "def save_doc_content_to_file(doc_content):\n",
    "    file_doc_content_path = os.path.join(map_analise_path, 'doc_content_' + batch_name + \".json\")\n",
    "    with open(file_doc_content_path, 'w') as f:\n",
    "        json.dump(doc_content, f)\n",
    "        \n",
    "# Carrega o dict doc_content em arquivo\n",
    "def load_doc_content_from_file():\n",
    "    file_doc_content_path = os.path.join(map_analise_path, 'doc_content_' + batch_name + \".json\")\n",
    "    with open(file_doc_content_path, 'r') as f:\n",
    "        return json.load(f) \n",
    "\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "\n",
    "def run_ocrmypdf(input_file, output_file):\n",
    "    command = [\n",
    "        'ocrmypdf',\n",
    "        '--language', 'por',\n",
    "        '--deskew',\n",
    "        input_file,\n",
    "        output_file\n",
    "    ]\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"OCRmyPDF completed successfully. Output saved to {output_file}.\")\n",
    "    else:\n",
    "        print(f\"OCRmyPDF failed with error: {result.stderr.decode('utf-8')}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# Função para definir o atributo \"is_cnpj\"\n",
    "@Language.component(\"set_cnpj_attribute\")\n",
    "def set_cnpj_attribute(doc):\n",
    "    for i, token in enumerate(doc):\n",
    "        if i < len(doc) - 1:\n",
    "            next_token = doc[i + 1]\n",
    "            if token.shape_ == \"dd.ddd.ddd/\" and next_token.shape_ == \"dddd-dd\":\n",
    "                token._.is_cnpj = True\n",
    "                next_token._.is_cnpj = True\n",
    "            else:\n",
    "                token._.is_cnpj = False\n",
    "    return doc        \n",
    "\n",
    "\n",
    "# Registro do atributo 'is_cnpj'\n",
    "Token.set_extension('is_cnpj', force=True, default=False)\n",
    "\n",
    "\n",
    "# Função para aplicar o matcher\n",
    "@Language.component(\"apply_cnpj_matcher\")\n",
    "def apply_cnpj_matcher(doc):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        for token in span:\n",
    "            token._.is_cnpj = True\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>1.1</b> Carregando o dict de tipo de documento e frames_nf_v4_df </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Buscar proximo Batch caso nao esteja rodando email\n",
    "batch_name = utl.busca_proximo_batch(conf_export_plan_path)\n",
    "\n",
    "\n",
    "\n",
    "# 0. XXX IMPORTANTE, carregar o dict de tipo de documento \n",
    "tipo_documento_dict = load_dict_from_file(config_tipo_doc_path)\n",
    "\n",
    "\n",
    "\n",
    "matcher_pattern_path = tipo_documento_dict.get(tipo_doc_work, {}).get('matcher_pattern_path', 'valor_padrao')\n",
    "entity_ruler_pattern_path = tipo_documento_dict.get(tipo_doc_work, {}).get('entity_ruler_pattern_path', 'valor_padrao')\n",
    "\n",
    "\n",
    "\n",
    "# 2. XXX IMPORTANTE - Modelo a ser utilizado para carregar o dict doc_content\n",
    "doc_content = {}\n",
    "doc_content = load_doc_content_from_file()\n",
    "\n",
    "\n",
    "\n",
    "# 3. XXX Definiçao do path para salvar o arquivo\n",
    "file_path_root_pipe = os.path.join(map_analise_path, df_root_pipe_file + batch_name + \".xlsx\")\n",
    "\n",
    "\n",
    "# 4. XXX Ler a planilha e cria df_documento_recebido\n",
    "df_root_pipe = pd.read_excel(file_path_root_pipe)\n",
    "\n",
    "\n",
    "# 5. XXX  Ajustar o indice\n",
    "df_root_pipe.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "\n",
    "def define_dados_iniciais(idx, row, row_info, texto_tratado, debug):\n",
    "    \n",
    "    dados_iniciais_nf = {}\n",
    "    #status_documento_row_info = row_info.get('status_documento')\n",
    "    action_item_row_info = row_info.get('action_item')\n",
    "    information_row_info = row_info.get('informations')\n",
    "    \n",
    "    dados_iniciais_nf['action_item'] = action_item_row_info\n",
    "    dados_iniciais_nf['informations'] = information_row_info\n",
    "    \n",
    "    print(f'\\nDentro da func define_dados_iniciais:  -action_item_row_info: {action_item_row_info}')\n",
    "   \n",
    "\n",
    "\n",
    "    prefeitura_encontrada = None\n",
    "    de_para_encontrado = None\n",
    "\n",
    "    # 7. ZZZ Dicionário para mapear Prefeitura com sua sigla\n",
    "    de_para_prefeitura = {\n",
    "        \"PREFEITURA DA CIDADE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA DA CIDADE DE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA MUNICIPAL DE MAGE\": \"PM_MAGE\",\n",
    "        \"PREFEITURA MUNICIPAL DE SAO PEDRO DA ALDEIA\": \"PM_SPA\",\n",
    "        \"MUNICIPAL DE SAO PEDRO DA ALDEIA\": \"PM_SPA\",\n",
    "        \"PREFEITURA MUNICIPAL DE SAO PEDRO DA\\nALDEIA\": \"PM_SPA\",\n",
    "        \"PREFEITURA MUNICIPAL DE SAO PEDRO DA\": \"PM_SPA\",\n",
    "        \"PREFEITURA MUNICIPAL DE MESQUITA\": \"PM_MESQUITA\",\n",
    "        \"PREFEITURA MUNICIPAL DE DE MESQUITA\": \"PM_MESQUITA\",\n",
    "        # ... adicione \n",
    "    }\n",
    "    \n",
    "\n",
    "    templates = {\n",
    "        (\"PM_MAGE\", None): \"MAGE\",\n",
    "        (\"PM_SPA\", None): \"SPA\",\n",
    "        (\"PM_MESQUITA\", None): \"MESQUITA\",\n",
    "        (\"Pague agora com o seu Pix\", None): \"NAO_PROCESSAR\",\n",
    "        # ... adicione outras combinações aqui\n",
    "    }\n",
    "\n",
    "    cnpj_encontrado = None\n",
    "    # Verifique cada linha do texto\n",
    "    for linha in texto_tratado:\n",
    "        for pref in de_para_prefeitura.keys():\n",
    "            if pref in linha:\n",
    "                #print(linha)\n",
    "                prefeitura_encontrada = pref\n",
    "                dados_iniciais_nf['prefeitura'] = prefeitura_encontrada\n",
    "                if debug:\n",
    "                    print(f'\\n4.funcao: define_dados_iniciais(texto_tratado) - dentro do loop for de pesquisa prefeitura - prefeitura_encontrada: \\n{prefeitura_encontrada}\\n\\n')\n",
    "    # Saímos do loop, agora vamos verificar qual template usar\n",
    "    if prefeitura_encontrada:\n",
    "        de_para_pm = de_para_prefeitura.get(prefeitura_encontrada)\n",
    "        dados_iniciais_nf['de_para_pm'] = de_para_pm\n",
    "        if debug:\n",
    "            print(f'\\n5.funcao: define_dados_iniciais(texto_tratado) - if prefeitura_encontrada - de_para_pm \\n{de_para_pm}\\n\\n')\n",
    "        if not de_para_pm:\n",
    "            de_para_pm = de_para_prefeitura.get(prefeitura_encontrada, \"NAO_PROCESSAR\")\n",
    "            dados_iniciais_nf['de_para_pm'] = de_para_pm\n",
    "            #print(de_para_pm)\n",
    "    else:\n",
    "        de_para_pm = \"NAO_PROCESSAR\"\n",
    "        action_item_row_info = 'BREAK_PROCESS'\n",
    "        information_row_info = 'Nao identificado dados iniciais para o documento'\n",
    "        \n",
    "     \n",
    "        \n",
    "    # Verifique cada linha do texto\n",
    "    for linha in texto_tratado:\n",
    "        for de_para, cnpj in templates.keys():\n",
    "            if cnpj and cnpj in linha:\n",
    "                cnpj_encontrado = cnpj\n",
    "                dados_iniciais_nf['cnpj_encontrado'] = cnpj_encontrado\n",
    "                \n",
    "                \n",
    "    # Saímos do loop, agora vamos verificar qual template usar\n",
    "    if de_para_pm:\n",
    "        template_usar = templates.get((de_para_pm, cnpj_encontrado))\n",
    "        logging.info(f'usara template {template_usar} para: {cnpj_encontrado}')\n",
    "        # print(template_usar)\n",
    "        dados_iniciais_nf['model'] = template_usar\n",
    "        if not template_usar:\n",
    "            template_usar = templates.get((de_para_pm, None), \"TEMPLATE_NAO_ENCONTRADO\")\n",
    "            dados_iniciais_nf['model'] = 'NAO_ENC.' \n",
    "            action_item_row_info = 'BREAK_PROCESS'\n",
    "            information_row_info = 'model nao encontrado'\n",
    "    else:\n",
    "        template_usar = \"TEMPLATE_NAO_ENCONTRADO\"\n",
    "        dados_iniciais_nf['model'] = 'NAO_ENC.'\n",
    "        action_item_row_info = 'BREAK_PROCESS'\n",
    "        information_row_info = 'model nao encontrado'\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    #Confirmando se template existe em frames    \n",
    "    try:        \n",
    "        f_type = 'frame'\n",
    "        #template_usar = 'SAO_PEDRO_SUPERMIX'\n",
    "        result = filtrar_df(frames_nf_v4_df, type=f_type, de_para_pm=de_para_pm, model=template_usar)\n",
    "        model = result['model'].values[0]\n",
    "        if model:\n",
    "            template_oficial = model\n",
    "            if model == template_usar:\n",
    "                dados_iniciais_nf['model'] = template_oficial\n",
    "            else:    \n",
    "                template_usar = \"necessario cadastrar\"\n",
    "                dados_iniciais_nf['model'] = \"CADASTRAR\"\n",
    "                \n",
    "            dados_iniciais_nf['model'] = template_usar\n",
    "        else:\n",
    "            template_usar = \"necessario cadastrar\"\n",
    "            dados_iniciais_nf['model'] = \"CADASTRAR\"\n",
    "\n",
    "                \n",
    "    except Exception as e:\n",
    "       error_msg = (f\"Erro busca do template: {e}\") \n",
    "    \n",
    "    dados_iniciais_nf['action_item'] = action_item_row_info \n",
    "    dados_iniciais_nf['informations'] = information_row_info         \n",
    "        \n",
    "    return dados_iniciais_nf  \n",
    "\n",
    "\n",
    "\n",
    "nf_model_path = \"config/modelos/frames_nf_v11.xlsx\"\n",
    "\n",
    "#Le a planilha e cria do DF\n",
    "frames_nf_v4_df = pd.read_excel(nf_model_path)\n",
    "\n",
    "\n",
    "# Cria dicionários para armazenar diferentes tipos de elementos do modelo\n",
    "document_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'document'].iloc[0]\n",
    "boundaries_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'boundaries']\n",
    "sections_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'section']\n",
    "frames_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'frame']\n",
    "sframe_fields_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'sframe_field']\n",
    "field_boxes_info = frames_nf_v4_df[frames_nf_v4_df['type'] == 'field_box']\n",
    "\n",
    "ver = tmod.get_template_version(frames_nf_v4_df, 'MAGE')\n",
    "\n",
    "frames_nf_v4_df.head(5)\n",
    "\n",
    "print(f'frames_nf_v4_df: {ver}')\n",
    "print()\n",
    "\n",
    "\n",
    "# Unica funcao que devera ser movida para junto das demais funcoes de dicionario\n",
    "def define_rotulo_acao(nome_arquivo):\n",
    "    \n",
    "    for palavra_chave, rotulo in mapeamento_palavras_chave.items():\n",
    "        if palavra_chave.lower() in nome_arquivo.lower():\n",
    "            break\n",
    "    else:\n",
    "        rotulo = 'prov_nota_fiscal' #\"sem_rotulo\"\n",
    "        palavra_chave = 'default'\n",
    "        acao_sugerida = sugestoes_acao.get(rotulo, 'None')\n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "        # palavra_chave = 'None' #\"sem_palavra_chave\"\n",
    "        # acao_sugerida = 'None' #\"sem_acao_sugerida\"\n",
    "        \n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "        #print(f'nome_arquivo: {nome_arquivo} | rotulo: {rotulo}')\n",
    "    if rotulo != 'None': #\"sem_rotulo\"\n",
    "        acao_sugerida = sugestoes_acao.get(rotulo, 'None') # \"Ação não definida\"\n",
    "        return palavra_chave, rotulo, acao_sugerida\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Dicionário para mapear palavras-chave a rótulos\n",
    "mapeamento_palavras_chave = {\n",
    "    \"relatorio\": \"prov_relatorio\",\n",
    "    \"listagem\": \"prov_listagem\",\n",
    "    \"NF\": \"prov_nota_fiscal\",\n",
    "    \"nf\": \"prov_nota_fiscal\",\n",
    "    \"relatorio\": \"prov_listagem\",\n",
    "    \"sintetico\": \"prov_listagem\",\n",
    "    \"livro\": \"prov_livro_registro\",\n",
    "    \"sintético\": \"prov_listagem\",\n",
    "    \"nota\": \"prov_nota_fiscal\",\n",
    "    \"zip\": \"doc_zip\",\n",
    "    \"rar\": \"doc_rar\",\n",
    "    \"valores\": \"prov_dinheiro\",\n",
    "}\n",
    "\n",
    "# Dicionário mapeando rótulos a ações sugeridas\n",
    "sugestoes_acao = {\n",
    "    \"prov_relatorio\": \"NO_PROCESS\",\n",
    "    \"prov_listagem\": \"NO_PROCESS\",\n",
    "    \"prov_nota_fiscal\": \"PROCESS\",\n",
    "    \"sem_rotulo\": \"MANUAL_REV\",\n",
    "    \"prov_livro_registro\": \"NO_PROCESS\",\n",
    "    \"doc_nao_pdf\": \"verificar\",\n",
    "    \"nao_pdf\": \"NO_PROCESS\",\n",
    "    \"doc_zip\": \"NO_PROCESS\",\n",
    "    \"pdf_mul_paginas\": \"SPLIT\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# 2.Testando\n",
    "nome_arquivo = 'batatinha_quando_nasce.pdf' # 'pre-processamento'\n",
    "#palavra_chave, rotulo, acao_sugerida = define_rotulo_acao(nome_arquivo, debug)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_root_pipe.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>1.2</b> Carregando Entity Ruler Patterns </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity ruler patterns\n",
    "colors = {\n",
    "            \"secretaria\": \"linear-gradient(90deg, #2ADB5E, #1FA346)\", # Verde Degrade\n",
    "            \"tipo_documento\": \"linear-gradient(90deg, #09D6FF, #08A0D1)\", #Azul medio degrade\n",
    "            \"nome_prefeitura\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\", # Roxo claro para lilaz - degrade bem bacana\n",
    "            \"nome_section\": \"linear-gradient(90deg, #FFA9FB, #BF7FBC)\", #  lilaz - Degrade\n",
    "            \"nome_section\": \"#FFEA7F\", # Laranja claro\n",
    "            \"INSC_MUNICIPAL\": \"#CCA10C\", # Terracota\n",
    "            \"CPF_CNPJ\": \"#CCA10C\", # Terracota\n",
    "            \"campos\": \"#AB9BFC\", # Roxo claro \n",
    "            \"SAFRA\": \"#7AECEC\", # Azul bem claro\n",
    "            \"encerrador\": \"#EE8AF8\" # Rosa medio\n",
    "        }          \n",
    "\n",
    "patternsPrefeitura = [\n",
    "                        {\"label\": \"nome_prefeitura\", \"pattern\": [{\"LOWER\": \"prefeitura\"}, {\"LOWER\": \"municipal\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"mesquita\"}], \"id\": \"PM_MESQUITA\"},\n",
    "                        {\"label\": \"nome_prefeitura\", \"pattern\": [{\"LOWER\": \"prefeitura\"}, {\"LOWER\": \"municipal\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"mage\"}], \"id\": \"PM_MAGE\"},\n",
    "                        {\"label\": \"nome_prefeitura\", \"pattern\": [{\"LOWER\": \"prefeitura\"}, {\"LOWER\": \"municipal\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"sao\"}, {\"LOWER\": \"pedro\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"aldeia\"}], \"id\": \"PM_SPA\"},\n",
    "                        {\"label\": \"nome_prefeitura\", \"pattern\": [{\"LOWER\": \"prefeitura\"}, {\"LOWER\": \"municipal\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"sao\"}, {\"LOWER\": \"pedro\"}, {\"LOWER\": \"da\"}, {\"LOWER\": \"aldeia\"}], \"id\": \"PM_SPA\"}\n",
    "\n",
    "                        ]\n",
    "\n",
    "\n",
    "patternsSection = [     \n",
    "                        {\"label\": \"nome_section\", \"pattern\": [{\"LOWER\": \"número\"}, {\"LOWER\": \"da\"}, {\"LOWER\": \"nota\"}, {\"ORTH\": \":\"}], \"id\": \"1. CABECALHO\"},\n",
    "                        {\"label\": \"nome_section\", \"pattern\": [{\"LOWER\": \"prestador\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"serviços\"}], \"id\": \"2. PRESTADOR DE SERVIÇO\"},\n",
    "                        {\"label\": \"nome_section\", \"pattern\": [{\"LOWER\": \"prestador\"}], \"id\": \"2. PRESTADOR DE SERVIÇO\"},\n",
    "                        {\"label\": \"nome_section\", \"pattern\": [{\"LOWER\": \"tomador\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"serviços\"}], \"id\": \"3. TOMADOR DE SERVIÇO\"},\n",
    "                        {\"label\": \"nome_section\", \"pattern\": [{\"LOWER\": \"tomador\"}], \"id\": \"3. TOMADOR DE SERVIÇO\"},\n",
    "                        {\"label\": \"nome_section\", \"pattern\": [{\"LOWER\": \"discriminação\"}, {\"LOWER\": \"dos\"}, {\"LOWER\": \"serviços\"}], \"id\": \"4. DESCRIMINACAO DOS SERVIÇOS\"},\n",
    "                        {\"label\": \"nome_section\", \"pattern\": [{\"LOWER\": \"valor\"}, {\"LOWER\": \"total\"}, {\"LOWER\": \"da\"}, {\"LOWER\": \"nota\"},{\"ORTH\": \":\"}], \"id\": \"5. VALOR TOTAL\"},\n",
    "                        {\"label\": \"nome_section\", \"pattern\": [{\"LOWER\": \"cnae\"}], \"id\": \"6. CNAE e Item da Lista de Serviços\"},\n",
    "                        {\"label\": \"nome_section\", \"pattern\": [{\"LOWER\": \"valor\"}, {\"LOWER\": \"serviços\"}], \"id\": \"7. VALORES E IMPOSTOS\"},\n",
    "                        {\"label\": \"nome_section\", \"pattern\": [{\"LOWER\": \"dados\"}, {\"LOWER\": \"complementares\"}], \"id\": \"8. DADOS COMPLEMENTARES\"},\n",
    "                        {\"label\": \"nome_section\", \"pattern\": [{\"LOWER\": \"outras\"}, {\"LOWER\": \"informações\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"criticas\"}], \"id\": \"9. OUTRAS INFORMAÇOES / CRITICAS\"},\n",
    "                        {\"label\": \"nome_section\", \"pattern\": [{\"LOWER\": \"observação\"},{\"ORTH\": \":\"}], \"id\": \"10. OBSERVACOES\"}\n",
    "\n",
    "                        ]\n",
    "\n",
    "patternsSecretarias = [{\"label\": \"secretaria\", \"pattern\": [{\"LOWER\": \"secretaria\"}, {\"LOWER\": \"municipal\"}, {\"LOWER\": \"da\"}, {\"LOWER\": \"fazenda\"},], \"id\": \"SECRETARIA\"}] \n",
    "\n",
    "\n",
    "patternsTipoDocumento = [\n",
    "                        {\"label\": \"tipo_documento\", \"pattern\": [{\"LOWER\": \"nota\"}, {\"LOWER\": \"fiscal\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"serviços\"}, {\"LOWER\": \"eletrônica\"}, {\"LOWER\": \"-\"}, {\"LOWER\": \"nfs-e\"}], \"id\": \"NFS-e\"}\n",
    "                        ]\n",
    "\n",
    "\n",
    "patternsCampos = [\n",
    "                        {\"label\": \"campos\", \"pattern\": [{\"LOWER\": \"telefone\"},{\"ORTH\": \":\"}], \"id\": \"telefone\"},\n",
    "                        {\"label\": \"campos\", \"pattern\": [{\"LOWER\": \"nome\"},{\"ORTH\": \"/\"},{\"LOWER\": \"razão\"},{\"LOWER\": \"social\"},{\"ORTH\": \":\"}], \"id\": \"razao_social\"},\n",
    "                        {\"label\": \"campos\", \"pattern\": [{\"LOWER\": \"endereço\"},{\"ORTH\": \":\"}], \"id\": \"endereco\"},\n",
    "                        {\"label\": \"campos\", \"pattern\": [{\"LOWER\": \"e\"},{\"ORTH\": \"-\"},{\"LOWER\": \"mail\"},{\"ORTH\": \":\"}], \"id\": \"email\"},\n",
    "                        {\"label\": \"campos\", \"pattern\": [{\"LOWER\": \"e-mail\"}], \"id\": \"e_mail\"},\n",
    "                        {\"label\": \"campos\", \"pattern\": [{\"LOWER\": \"inscrição\"},{\"LOWER\": \"municipal\"},{\"ORTH\": \":\"}], \"id\": \"inscricao_municipal\"},\n",
    "                        {\"label\": \"campos\", \"pattern\": [{\"LOWER\": \"inscrição\"},{\"LOWER\": \"estadual\"},{\"ORTH\": \":\"}], \"id\": \"inscricao_estadual\"},\n",
    "                        {\"label\": \"campos\", \"pattern\": [{\"LOWER\": \"rg\"},{\"ORTH\": \":\"}], \"id\": \"rg\"},\n",
    "                        {\"label\": \"campos\", \"pattern\": [{\"LOWER\": \"nome\"},{\"LOWER\": \"de\"},{\"LOWER\": \"fantasia\"},{\"ORTH\": \":\"}], \"id\": \"nome_fantasia\"},\n",
    "                        {\"label\": \"campos\", \"pattern\": [{\"LOWER\": \"cpf\"},{\"ORTH\": \"/\"},{\"LOWER\": \"cnpj\"},{\"ORTH\": \":\"}], \"id\": \"cpf_cnpj_com_mascara\"},\n",
    "                        {\"label\": \"campos\", \"pattern\": [{\"LOWER\": \"exigibilidade\"},{\"LOWER\": \"iss\"}], \"id\": \"exigibilidade_iss\"},\n",
    "                        {\"label\": \"campos\", \"pattern\": [{\"LOWER\": \"regime\"},{\"LOWER\": \"tributação\"}], \"id\": \"regime_tributacao\"},\n",
    "                        {\"label\": \"campos\", \"pattern\": [{\"LOWER\": \"simples\"},{\"LOWER\": \"nacional\"}, {\"IS_TITLE\": \"False\"}], \"id\": \"simples_nacional\"},\n",
    "                        {\"label\": \"campos\", \"pattern\": [{\"LOWER\": \"issqn\"},{\"LOWER\": \"retido\"}], \"id\": \"issqn_retido\"},\n",
    "                        {\"label\": \"campos\", \"pattern\": [{\"LOWER\": \"local\"},{\"ORTH\": \".\"},{\"LOWER\": \"prestação\"},{\"LOWER\": \"serviço\"}], \"id\": \"local_pretacao_servico\"},\n",
    "                        {\"label\": \"campos\", \"pattern\": [{\"LOWER\": \"local\"},{\"LOWER\": \"incidência\"}], \"id\": \"local_incidencia\"},\n",
    "                        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# patternsValores = [{\"label\": \"INSC_ESTADUAL\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"^\\d{8}$\"}}], \"id\": \"inscricao_estadual\"},\n",
    "#                    {\"label\": \"INSC_MUNICIPAL\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"^\\d{7}$\"}}], \"id\": \"inscricao_municipal\"},\n",
    "#                    {\"label\": \"CPF_CNPJ\", \"pattern\": [{\"ORTH\": {\"REGEX\": \"^\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2}$\"}}], \"id\": \"cpf_cnpj_com_mascara\"}\n",
    "#                    ]\n",
    "\n",
    "\n",
    "\n",
    "patternsEncerradores = [{\"label\": \"encerrador\", \"pattern\": [{\"LOWER\": \"https\"},{\"ORTH\": \":\"}], \"id\": \"encerrador\"},\n",
    "                        {\"label\": \"encerrador\", \"pattern\": [{\"LOWER\": \"https://nfs-e.mage.rj.gov.br\"}], \"id\": \"encerrador\"}\n",
    "                        ] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "patternsCnpj = [\n",
    "    {\n",
    "        \"label\": \"CNPJ\",\n",
    "        \"pattern\": [\n",
    "            {\"ORTH\": {\"REGEX\": \"^\\d{2}\\.\\d{3}\\.\\d{3}/$\"}},\n",
    "            {\"ORTH\": {\"REGEX\": \"^\\d{4}-\\d{2}$\"}}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "patterns = patternsPrefeitura + patternsSection + patternsSecretarias + patternsTipoDocumento + patternsEncerradores + patternsCampos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # XXX Processo para salvar entity ruler pattern para json\n",
    "\n",
    "# nome_entityruler_pattern_json = tipo_doc_work + \"_entity_ruler_pattern.json\"\n",
    "# entityruler_file_path = os.path.join(tipo_documento_patterns_path, nome_entityruler_pattern_json)\n",
    "\n",
    "# write_patterns_to_file(patterns=patterns, colors=colors, filename=entityruler_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # XXX Processo de load de  patterns do Entity Ruler\n",
    "\n",
    "# entity_ruler_pattern_path = tipo_documento_dict.get(tipo_doc_work, {}).get('entity_ruler_pattern_path', 'valor_padrao')\n",
    "\n",
    "# patterns, colors = load_patterns_and_colors(entity_ruler_pattern_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>1.3</b> Tratando os Matchers patterns </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # XXX Processo para salvar matcher pattern para json\n",
    "# nome_matches_pattern_json = tipo_doc_work + \"_matcher_pattern.json\"\n",
    "# path_matches_pattern_json = os.path.join(tipo_documento_patterns_path, nome_matches_pattern_json)\n",
    "# with open(path_matches_pattern_json, \"w\") as f:\n",
    "#     json.dump(matcher_pattern_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # XXX Rotina para carregar e atribuir ao matcher os patterns do disco\n",
    "# matcher_pattern_path = tipo_documento_dict.get(tipo_doc_work, {}).get('matcher_pattern_path', 'valor_padrao')\n",
    "\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# # XXX Carregar matcher patterns do disco\n",
    "# with open(matcher_pattern_path, \"r\") as f:\n",
    "#     loaded_patterns = json.load(f)\n",
    "    \n",
    "# # Adicionar ao Matcher\n",
    "# for label, pattern in loaded_patterns.items():\n",
    "#     matcher.add(label, [pattern])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Matcher Patterns\n",
    "#======================================== 1. CABECALHO\n",
    "# 1. Número da Nota:\n",
    "numero_nota_pattern = [\n",
    "    {\"LOWER\": \"número\"},\n",
    "    {\"LOWER\": \"da\"},\n",
    "    {\"LOWER\": \"nota\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_DIGIT\": True}\n",
    "]\n",
    "matcher.add(\"numero_nota_fiscal\", [numero_nota_pattern])\n",
    "\n",
    "\n",
    "# 2. Competência:\n",
    "competencia_pattern = [\n",
    "    {\"LOWER\": \"competência\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"?\"},\n",
    "    {\"ORTH\": {\"REGEX\": \"^[A-Z][a-z]+/[0-9]{4}$\"}}   \n",
    "]    \n",
    "matcher.add(\"competencia\", [competencia_pattern])\n",
    "\n",
    "# 3. Data e Hora de Emissão:\n",
    "data_hora_emissao_pattern = [\n",
    "    {\"LOWER\": \"data\"},\n",
    "    {\"LOWER\": \"e\"},\n",
    "    {\"LOWER\": \"hora\"},\n",
    "    {\"LOWER\": \"da\"},\n",
    "    {\"LOWER\": \"emissão\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"SHAPE\": \"dd/dd/dddd\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"SHAPE\": \"dd:dd:dd\"}\n",
    "]\n",
    "matcher.add(\"dt_hr_emissao\", [data_hora_emissao_pattern])\n",
    "\n",
    "# 4. Código de Verificação:\n",
    "codigo_verificacao_pattern = [\n",
    "    {\"LOWER\": \"código\"},\n",
    "    {\"LOWER\": \"verificação\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_ASCII\": True, \"LENGTH\": 9}\n",
    "]\n",
    "matcher.add(\"codigo_verificacao\", [codigo_verificacao_pattern])\n",
    "\n",
    "\n",
    "#========================================  5. VALOR TOTAL\n",
    "valor_total_nota_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"total\"},\n",
    "    {\"LOWER\": \"da\", \"OP\": \"?\"},\n",
    "    {\"LOWER\": \"nota\", \"OP\": \"?\"},\n",
    "    {\"TEXT\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"valor_total_nota\", [valor_total_nota_pattern])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#======================================== 7. VALORES E IMPOSTOS\n",
    "\n",
    "# 1. VALOR_SERVICOS\n",
    "valor_servicos_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"serviços\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"valor_servicos\", [valor_servicos_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 2. VALOR DEDUÇÃO:\n",
    "valor_deducao_1_pattern = [\n",
    "    {\"LOWER\": \"dedução\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"valor_deducao\", [valor_deducao_1_pattern])\n",
    "\n",
    "# 2.1 VALOR DEDUÇÃO:\n",
    "valor_deducao_2_pattern = [\n",
    "    {\"LOWER\": \"dedução\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"Number=Sing\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"valor_deducao\", [valor_deducao_2_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 2.2 DESC. INCOND1:\n",
    "valor_deducao_3_pattern= [\n",
    "    {\"LOWER\": \"dedução\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"}  \n",
    "]\n",
    "matcher.add(\"valor_deducao\", [valor_deducao_3_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 3. DESC. INCOND: - PDF_Pesquisavel   #DESC. INCOND:\n",
    "desc_incond_pattern = [\n",
    "    {\"LOWER\": \"desc\"},\n",
    "    {\"LOWER\": \"incond\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"IS_SPACE\": True},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}  \n",
    "]\n",
    "matcher.add(\"desc_incond\", [desc_incond_pattern])\n",
    "\n",
    "# 3.1 DESC. INCOND1:\n",
    "desc_incond_1_pattern = [\n",
    "    {\"LOWER\": \"base\"},\n",
    "    {\"LOWER\": \"de\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"}  \n",
    "]\n",
    "matcher.add(\"desc_incond\", [desc_incond_1_pattern])\n",
    "\n",
    "\n",
    "# 3.2 DESC. INCOND: - PDF_Pesquisavel   #DESC. INCOND:\n",
    "desc_incond_2_pattern = [\n",
    "    {\"LOWER\": \"incond\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"LOWER\": \"base\"},\n",
    "    {\"LOWER\": \"de\"},\n",
    "    {\"IS_SPACE\": True},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}  \n",
    "]\n",
    "matcher.add(\"desc_incond\", [desc_incond_2_pattern])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4.1 BASE DE CÁLCULO:  PDF_P\n",
    "base_calculo_1_pattern = [\n",
    "    {\"LOWER\": \"base\"},\n",
    "    {\"LOWER\": \"de\"},\n",
    "    {\"LOWER\": \"cálculo\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"base_calculo\", [base_calculo_1_pattern])\n",
    "\n",
    "\n",
    "# 4.2 BASE DE CÁLCULO:  RASTER_PDF\n",
    "base_calculo_2_pattern = [\n",
    "    {\"LOWER\": \"calculo\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\": \"?\"},  # para lidar com possíveis quebras de linha\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \",\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"base_calculo\", [base_calculo_2_pattern])\n",
    "\n",
    "\n",
    "# 5. Aliquota\n",
    "aliquota_pattern = [\n",
    "            {\"LOWER\": \"alíquota\"},\n",
    "            {\"ORTH\": \":\"},\n",
    "            {\"SHAPE\": \"d\", \"OP\": \"?\"},\n",
    "            {\"ORTH\": \"%\"}\n",
    "        ]\n",
    "matcher.add(\"aliquota\", [aliquota_pattern])\n",
    "\n",
    "aliquota_1_pattern =  [\n",
    "            {\"LOWER\": \"alíquota\"},\n",
    "            {\"ORTH\": \":\"},\n",
    "            {\"SHAPE\": \"d,dd\", \"OP\": \"?\"},\n",
    "            {\"ORTH\": \"%\"}\n",
    "        ]\n",
    "matcher.add(\"aliquota\", [aliquota_1_pattern])\n",
    "\n",
    "\n",
    "aliquota_2_pattern = [\n",
    "            {\"LOWER\": \"valor\"},\n",
    "            {\"LOWER\": \"iss\"},\n",
    "            {\"ORTH\": \":\"},\n",
    "            {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "            {\"IS_DIGIT\": True, \"OP\": \"+\"},\n",
    "            {\"ORTH\": \"\", \"OP\": \"?\"},\n",
    "            {\"IS_DIGIT\": True, \"OP\": \"*\"},\n",
    "            {\"ORTH\": \"%\"}\n",
    "        ]\n",
    "matcher.add(\"aliquota\", [aliquota_2_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 6. VALOR ISS:\n",
    "valor_iss_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"iss\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"valor_iss\", [valor_iss_pattern])\n",
    "\n",
    "\n",
    "# 7. VALOR ISS RETIDO:\n",
    "valor_iss_retido_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"iss\"},\n",
    "    {\"LOWER\": \"retido\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"valor_iss_retido\", [valor_iss_retido_pattern])\n",
    "\n",
    "\n",
    "# 8. DESC. COND:\n",
    "desc_cond_pattern = [\n",
    "    {\"LOWER\": \"desc\"},\n",
    "    {\"ORTH\": \".\"},\n",
    "    {\"LOWER\": \"cond\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"desc_cond\", [desc_cond_pattern])\n",
    "\n",
    "\n",
    "# 9. VALOR PIS:\n",
    "valor_pis_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"pis\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"valor_pis\", [valor_pis_pattern])\n",
    "\n",
    "\n",
    "# 10. VALOR COFINS:\n",
    "valor_cofins_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"cofins\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"valor_cofins\", [valor_cofins_pattern])\n",
    "\n",
    "\n",
    "# 11. VALOR IR:\n",
    "valor_ir_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"ir\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"valor_ir\", [valor_ir_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 12. VALOR INSS:\n",
    "valor_inss_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"inss\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"valor_inss\", [valor_inss_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 13. VALOR CSLL:\n",
    "valor_csll_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"csll\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"valor_csll\", [valor_csll_pattern])\n",
    "\n",
    "\n",
    "\n",
    "# 14. OUTRAS RETENÇÕES:\n",
    "outras_retencoes_pattern = [\n",
    "    {\"LOWER\": \"outras\"},\n",
    "    {\"LOWER\": \"retenções\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"outras_retencoes\", [outras_retencoes_pattern])\n",
    "\n",
    "\n",
    "# 15. VALOR LÍQUIDO:\n",
    "valor_liquido_pattern = [\n",
    "    {\"LOWER\": \"valor\"},\n",
    "    {\"LOWER\": \"líquido\"},\n",
    "    {\"ORTH\": \":\"},\n",
    "    {\"SHAPE\": \"X$\"},\n",
    "    {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "    {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "    {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "]\n",
    "matcher.add(\"valor_liquido\", [valor_liquido_pattern])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Criar um dicionário para matchers patterns - nfs-e\n",
    "# Criar nova rotina de salvamento de dicionário de matchers patterns\n",
    "\n",
    "\n",
    "matcher_pattern_dict = [\n",
    "                            {\n",
    "                                \"label\": \"endereco_site\",\n",
    "                                \"pattern\": [\n",
    "                                        {\"SHAPE\": \"xxxx://xxx.xxxx.xx.xxx.xx\"}\n",
    "                                ]         \n",
    "                            },\n",
    "                            {\n",
    "                                \"label\": \"OUTRO_LABEL\",\n",
    "                                \"pattern\": [\n",
    "                                        {\"LOWER\": \"valor\"},\n",
    "                                        {\"LOWER\": \"líquido\"},\n",
    "                                        {\"ORTH\": \":\"},\n",
    "                                        {\"SHAPE\": \"X$\"},\n",
    "                                        {\"MORPH\": \"NumType=Card\", \"OP\": \"+\"},\n",
    "                                        {\"LOWER\": \".\", \"OP\": \"?\"},\n",
    "                                        {\"IS_DIGIT\": True, \"OP\": \"*\"}\n",
    "                                ]\n",
    "                            }\n",
    "                        \n",
    "                        ]\n",
    "\n",
    "# Salvando com indentações\n",
    "with open(\"dados_formatados.json\", \"w\") as f:\n",
    "    json.dump(matcher_pattern_dict, f, indent=4)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Carregar o JSON dos padrões\n",
    "with open(\"dados_formatados.json\", \"r\") as f:\n",
    "    loaded_patterns = json.load(f)\n",
    "\n",
    "# Adicionar os padrões ao Matcher\n",
    "for item in loaded_patterns:\n",
    "    label = item[\"label\"]\n",
    "    pattern = item[\"pattern\"]\n",
    "    matcher.add(label, [pattern])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>1.4</b> Testes de templates e  patterns  </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Carregagando o sample documento do tipo de documento em execucao\n",
    "sample_text = tipo_documento_dict.get(tipo_doc_work, {}).get('sample_content', 'valor_padrao')\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>1.4.1</b> Entity Ruler Patterns  </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Aplicar a funçao show_ent_new para exibir o resultado\n",
    "doc, tokens, ents = show_ent_new(sample_text, patterns=patterns)\n",
    "\n",
    "displacy.render(doc, style=\"ent\", options={\"colors\": colors})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>1.4.2</b> Matcher Patterns  </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sample_text)\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "spans = []\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    span.label_ = nlp.vocab.strings[match_id]  # Adicionar o label ao span\n",
    "    spans.append(span)\n",
    "    \n",
    "# Configuração de cores\n",
    "colors = {\n",
    "            \"numero_nota_fiscal\": \"orange\",\n",
    "            \"competencia\": \"lightblue\",\n",
    "            \"dt_hr_emissao\": \"lightblue\",\n",
    "            \"codigo_verificacao\": \"silver\",\n",
    "            \"valor_total_nota\": \"#EE8AF8\",  \n",
    "            \"ALIQUOTA2\": \"turquoise\",\n",
    "            \"VALOR_TOTAL\":\"linear-gradient(90deg, #2ADB5E, #1FA346)\",\n",
    "            \"VALOR_SERVICOS\":\"linear-gradient(90deg, #09D6FF, #08A0D1)\",\n",
    "            \"VALOR_DEDUCAO\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\",\n",
    "            \"VALOR_DEDUCAO2\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\",\n",
    "            \"VALOR_CSLL\": \"linear-gradient(90deg, #FFA9FB, #BF7FBC)\",\n",
    "            \"VALOR_CSLL2\": \"linear-gradient(90deg, #FFA9FB, #BF7FBC)\",\n",
    "            \"VALOR_INSS\": \"lightblue\",\n",
    "            \"VALOR_INSS2\": \"lightblue\",\n",
    "            \"BASE_CALCULO\": \"lightblue\",\n",
    "            \"VALOR_CSLL\": \"#FFEA7F\", # Laranja claro\n",
    "            \"INSC_MUNICIPAL\": \"#CCA10C\", # Terracota\n",
    "            \"CPF_CNPJ\": \"#CCA10C\", # Terracota\n",
    "            \"campos\": \"#AB9BFC\", # Roxo claro \n",
    "            \"OUTRAS_RETENCOES\": \"#7AECEC\", # Azul bem claro\n",
    "            \"VALOR_LIQUIDO\": \"#EE8AF8\", # Rosa medio\n",
    "            \"VALOR_CSLL\": \"silver\",\n",
    "            \"VALOR_IR\": \"orange\",\n",
    "            \"VALOR_COFINS\": \"lime\",\n",
    "            \"VALOR_PIS\":\"cyan\",\n",
    "            \"DESC_COND\": \"#AB9BFC\",\n",
    "            \"VALOR_ISS_RETIDO\": \"mint\",\n",
    "            \"VALOR_ISS\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\",\n",
    "            \"BASE_DE_CALCULO\": \"salmon\",\n",
    "            \"DESC_INCOND\": \"#AB9BFC\",\n",
    "            \"DESC_INCOND1\": \"pink\",\n",
    "            \"VALOR_DEDUCAO\": \"white\",\n",
    "            \"endereco_site\": \"orange\",\n",
    "          }\n",
    "\n",
    "# Renderizar\n",
    "displacy.render([{'text': doc.text, 'ents': [{'start': span.start_char, 'end': span.end_char, 'label': span.label_} for span in spans]}], style='ent', manual=True, options={\"colors\": colors})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relaçao de valores encontrados - Matcher Patterns\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Obter a string de identificação\n",
    "    span = doc[start:end]  # Obter o trecho correspondente\n",
    "    print(f\"{string_id:>30} | {span.text:>50} | {span.start_char:>5}  {span.end_char:>5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amostra = \"https://nfe.mesquita.rj.gov.br\"\n",
    "doc1 = nlp(amostra)\n",
    "\n",
    "\n",
    "# Analisys\n",
    "syntatic = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"T_texto\",\"T_shape\", \"T_is_alpha\", \"T_is_digit\", \"T_is_title\", \"T_is_punct\", \"T_is_sent_start\", \"T_is_right_punct\", \"T_is_stop\", \"T_is_quote\", \"T_is_currency\", \"T_morph\"])\n",
    "i = 0\n",
    "for token in doc1:\n",
    "    syntatic.loc[i,\"id\"] = token.i\n",
    "    syntatic.loc[i,\"T_texto\"] = token.text\n",
    "    syntatic.loc[i,\"T_shape\"] = token.shape_\n",
    "    syntatic.loc[i,\"T_is_alpha\"] = token.is_alpha\n",
    "    syntatic.loc[i,\"T_is_digit\"] = token.is_digit\n",
    "    syntatic.loc[i,\"T_is_title\"] = token.is_title\n",
    "    syntatic.loc[i,\"T_is_punct\"] = token.is_punct\n",
    "    syntatic.loc[i,\"T_is_sent_start\"] = token.is_sent_start\n",
    "    syntatic.loc[i,\"T_is_right_punct\"] = token.is_right_punct\n",
    "    syntatic.loc[i,\"T_is_stop\"] = token.is_stop\n",
    "    syntatic.loc[i,\"T_is_quote\"] = token.is_quote\n",
    "    syntatic.loc[i,\"T_is_currency\"] = token.is_currency\n",
    "    syntatic.loc[i,\"T_morph\"] = token.morph\n",
    "    i = i+1\n",
    "\n",
    "syntatic.head(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization for tokens \n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"Texto\",\"Lemma\", \"Tag\", \"Tag_explainned\", \"token_POS\", \"POS_explainned\", \"dep\", \"T. Head\", \"dep explained\"])\n",
    "i = 0\n",
    "for token in doc1:\n",
    "    lemmatization.loc[i,\"id\"] = token.i\n",
    "    lemmatization.loc[i,\"Texto\"] = token.text\n",
    "    lemmatization.loc[i,\"Lemma\"] = token.lemma_\n",
    "    lemmatization.loc[i,\"Tag\"] = token.tag_\n",
    "    lemmatization.loc[i,\"Tag_explainned\"] = spacy.explain(token.tag_)\n",
    "    lemmatization.loc[i,\"token_POS\"] = token.pos_\n",
    "    lemmatization.loc[i,\"POS_explainned\"] = spacy.explain(token.pos_)\n",
    "    lemmatization.loc[i,\"dep\"] = token.dep_\n",
    "    lemmatization.loc[i,\"T. Head\"] = token.head.text\n",
    "    lemmatization.loc[i,\"dep explained\"] = token.morph\n",
    "    \n",
    "    i = i+1\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc1, style=\"dep\", options={\"colors\": colors})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <b>1.3</b> Criando novos patterns </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"set_cnpj_attribute\") # Adicione esta etapa se você quiser definir o atributo manualmente\n",
    "\n",
    "nlp.add_pipe(\"apply_cnpj_matcher\")  # Adicione esta etapa para aplicar o matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark> <b>2.0</b> Mapeamento e Extracao  </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>2.1</b> Funcoes mapeamento e extracao NLP  </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao que ajusta muito bem formataçao de numeros - moeda e %\n",
    "def format_number2(number_str):\n",
    "    number_str = number_str.replace('R$', '').replace('.', '').replace(',', '.')\n",
    "    if '%' in number_str:\n",
    "        number_str = number_str.replace('%', '')\n",
    "        return float(number_str)  # multiplica por 100 para fields %\n",
    "    return float(number_str)\n",
    "\n",
    "\n",
    "\n",
    "# 0.B XXX Funçao de criacao do PDF Pesquisavel\n",
    "def run_ocrmypdf(input_file, output_file):\n",
    "    command = [\n",
    "        'ocrmypdf',\n",
    "        '--language', 'por',\n",
    "        '--deskew',\n",
    "        '--force-ocr',\n",
    "        input_file,\n",
    "        output_file\n",
    "    ]\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"OCRmyPDF completed successfully. Output saved to {output_file}.\")\n",
    "    else:\n",
    "        print(f\"OCRmyPDF failed with error: {result.stderr.decode('utf-8')}\")\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "        \n",
    "        \n",
    "# 1.A XXX Extracao de texto de todo o documento - PDF PESQUISAVEL\t\n",
    "def extracao_texto_PDF_P(i, document_unique_id, nome_arquivo, file_path, debug):    \n",
    "    \n",
    "   # Carregar o arquivo PDF\n",
    "    pdf_document = fitz.open(file_path)\n",
    "\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text_P = page.get_text(\"text\")\n",
    "    \n",
    "    pdf_document.close()\n",
    "    \n",
    "    texto_PDF_P = re.sub('\\s+', ' ', text_P).strip()  \n",
    "    if debug:\n",
    "        print(f'\\nFUNC analise_texto_PDF: doc.:{nome_arquivo}   texto_PDF_P: \\n\\n{texto_PDF}\\n\\n')\n",
    "\n",
    "    return texto_PDF_P  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1.B XXX Extracao de texto de todo o documento - RASTER PDF\n",
    "def extracao_texto_Raster_P(i, document_unique_id, nome_arquivo, file_path, debug):    \n",
    "    \n",
    "    output_file = None\n",
    "    txt_document_file = None\n",
    "    \n",
    "    input_file = file_path\n",
    "    \n",
    "    #output_file = \"/home/dani-boy/extractNF/processamentos/temp/documento.pdf\"\n",
    "    output_file = os.path.join(raster_process_pdf_path, document_unique_id + '.pdf')\n",
    "\n",
    "    # 1. XXX Executar o comando OCRmyPDF\n",
    "    run_ocrmypdf(input_file, output_file)\n",
    "\n",
    "    txt_document_file = os.path.join(raster_process_txt_path, document_unique_id + '.txt')\n",
    "\n",
    "    txt_dir = os.path.dirname(txt_document_file)\n",
    "    if not os.path.exists(txt_dir):\n",
    "        os.makedirs(txt_dir)\n",
    "\n",
    "    # Execute o comando\n",
    "    subprocess.run([\"pdftotext\", output_file, txt_document_file])\n",
    "\n",
    "    # 3. XXX Ler o arquivo TXT\n",
    "    with open(txt_document_file, 'r', encoding='utf-8') as arquivo:\n",
    "        texto_OCR_R = arquivo.read()\n",
    "    texto_Raster_P = re.sub('\\s+', ' ', texto_OCR_R).strip()\n",
    "    \n",
    "    os.remove(output_file)\n",
    "    os.remove(txt_document_file)\n",
    "    \n",
    "    return texto_Raster_P  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. XXX Analise de silimaridade doc1 x doc2\n",
    "def analisa_similiaridade_doc(doc1, doc2):\n",
    "    # TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([doc1, doc2])\n",
    "\n",
    "    # Calculando Similaridade de Cosseno\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])\n",
    "\n",
    "    print(f\"Score de Similaridade: {similarity_scores[0][0]}\")\n",
    "    \n",
    "    return similarity_scores[0][0]    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 0.A Extracao de texto de todo o documento - PDF PESQUISAVEL\t\n",
    "def extrai_texto_PDF_P(idx, row, row_info, section, map_directory, original_file_name, file_path, debug):    \n",
    "    \n",
    "   # Carregar o arquivo PDF\n",
    "    pdf_document = fitz.open(file_path)\n",
    "\n",
    "    # Página do PDF  ATENCAO  (UNICA PAGINA)\n",
    "    page_number = 0  # Defina o número da página que deseja analisar\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Extrair texto dentro do retângulo\n",
    "    text_P = page.get_text(\"text\")\n",
    "    \n",
    "    pdf_document.close()\n",
    "    \n",
    "    texto_PDF_P = text_P.replace('\\n', ' ') \n",
    "    if debug:\n",
    "        print(f'\\nFUNC extrai_texto_PDF_P: doc.:{original_file_name} | diretorio: {map_directory}  texto_PDF_P: \\n\\n{texto_PDF_P}\\n\\n')\n",
    "\n",
    "    return texto_PDF_P\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def processar_cabecalho_PDF_P(idx, row, row_info, section, matches, doc, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "    \n",
    "    data_box_valores = {}\n",
    "    if mapping_method == \"frame_&_sframe_field\":\n",
    "        tipo_4_coordinates = \"frame\"\n",
    "        tipo_4_filter = \"sframe_field\"\n",
    "    \n",
    "    # 8. Efetuo o filtro para a iteracao\n",
    "    filtered_frame_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['context_mapping'] == context_mapping) & (frames_nf_v4_df['type'] == tipo_4_filter)]\n",
    "\n",
    "    # 9. iter sobre o filtro\n",
    "    for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "        section = row_frame['section_json']\n",
    "        label = row_frame['label']\n",
    "        reference = row_frame['reference']\n",
    "        string_pesquisa = row_frame['marcador_inicio']  \n",
    "        \n",
    "        raw_value = next((doc[start:end].text for match_id, start, end in matches if nlp.vocab.strings[match_id] == label), None)\n",
    "        \n",
    "        most_similar_reference = max([reference], key=lambda x: similar(x, raw_value))\n",
    "        ##print(f'\\nmost_similar_reference: {most_similar_reference}\\n')\n",
    "        final_value = raw_value.split(\":\", 1)[-1].strip()\n",
    "        data_box_valores[label] = final_value\n",
    "\n",
    "    return data_box_valores \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extrac_cabecalho_R_PDF(idx, row, row_info, doc_content, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, map_original_file_name, file_path, debug):   \n",
    "     \n",
    "    data_box_valores = {}\n",
    "    data_box_valores['secao'] = section\n",
    "\n",
    "    # Busco a imagem np do documento\n",
    "    # image_np_row_info = row_info.get('image_np')\n",
    "    image_np_row_info = doc_content.get(idx, {}).get('image_np', 'valor_padrao')\n",
    "\n",
    "    # busco coordenadas para o contexto\n",
    "    if mapping_method == \"frame_&_sframe_field\":\n",
    "        tipo_4_coordinates = \"frame\"\n",
    "        tipo_4_filter = \"sframe_field\"\n",
    "\n",
    "    # 2. usando a funcao de extracao de coordenadas por contexto    \n",
    "    coordinates = get_coordinates_filter_by_context(pdf_pesquisavel_map, model_map, context_mapping, tipo_4_coordinates)\n",
    "    x0, y0, x1, y1 = coordinates[0]\n",
    "    x0 = int(x0)\n",
    "    y0 = int(y0)\n",
    "    x1 = int(x1)\n",
    "    y1 = int(y1) \n",
    "    # 3. Cropo a imagem - novo modelo\n",
    "    cropped_image_np = image_np_row_info[y0:y1, x0:x1] # ajustar nos demais\n",
    "    # 4. Converto para PIL\n",
    "    cropped_image_pil = Image.fromarray(cropped_image_np)\n",
    "    # 6. Executo OCR\n",
    "    texto_extraido = pytesseract.image_to_string(cropped_image_pil, lang='por')\n",
    "    # 7. Trato o texto extraido = text_splited\n",
    "    texto_cabechalho_PDF_Raster = re.sub('\\s+', ' ', texto_extraido).strip() \n",
    "\n",
    "    if debug:\n",
    "        plt.imshow(cropped_image_np)\n",
    "        plt.axis('off')  # Desativa os eixos para uma visualização mais limpa\n",
    "        plt.show()\n",
    "        print(f'texto_cabechalho_PDF_Raster: {texto_cabechalho_PDF_Raster}\\n')  \n",
    "    \n",
    "    return texto_cabechalho_PDF_Raster \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ajusta_texto_Raster_P(idx, row, document_unique_id, original_text, texto_cabechalho_PDF_Raster):\n",
    "\n",
    "    # Passa pelo mapeamento de doc ents para ajustar o texto\n",
    "    doc, tokens, ents = show_ent_new(original_text, patterns=patterns)\n",
    "\n",
    "    nome_prefeitura_start_char = [ent.start_char for ent in doc.ents if ent.label_ == 'nome_prefeitura'][0]\n",
    "    nome_prefeitura_end_char = [ent.end_char for ent in doc.ents if ent.label_ == 'nome_prefeitura'][0]\n",
    "    secretaria_start_char = [ent.start_char for ent in doc.ents if ent.label_ == 'secretaria'][0]\n",
    "    secretaria_end_char = [ent.end_char for ent in doc.ents if ent.label_ == 'secretaria'][0]\n",
    "    tipo_documento_start_char = [ent.start_char for ent in doc.ents if ent.label_ == 'tipo_documento'][0]\n",
    "    tipo_documento_end_char = [ent.end_char for ent in doc.ents if ent.label_ == 'tipo_documento'][0]\n",
    "    prestador_start_char = [ent.start_char for ent in doc.ents if ent.id_ == '2. PRESTADOR DE SERVIÇO'][0]\n",
    "\n",
    "    bloco_prefeitura = original_text[nome_prefeitura_start_char:nome_prefeitura_end_char]\n",
    "    bloco_secretaria = original_text[secretaria_start_char:secretaria_end_char]\n",
    "    bloco_tipo_documento = original_text[tipo_documento_start_char:tipo_documento_end_char]\n",
    "    bloco_inicio_prestador = original_text[prestador_start_char:]\n",
    "\n",
    "    recomposed_text = bloco_prefeitura + ' | ' + bloco_secretaria + ' | ' + bloco_tipo_documento  + ' | ' + texto_cabechalho_PDF_Raster + ' | ' + bloco_inicio_prestador\n",
    "    \n",
    "    return recomposed_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mapeia_cabecalho(idx, row, row_info, doc_content, doc, matches, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    if mapping_method == \"frame_&_sframe_field\":\n",
    "        tipo_4_coordinates = \"frame\"\n",
    "        tipo_4_filter = \"sframe_field\"\n",
    "\n",
    "    # 8. Efetuo o filtro para a iteracao\n",
    "    filtered_frame_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['context_mapping'] == context_mapping) & (frames_nf_v4_df['type'] == tipo_4_filter)]\n",
    "\n",
    "    # 9. iter sobre o filtro\n",
    "    i = 1\n",
    "    for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "        section = row_frame['section_json']\n",
    "        label = row_frame['label']\n",
    "        reference = row_frame['reference']\n",
    "        string_pesquisa = row_frame['marcador_inicio'] \n",
    "        # if debug:\n",
    "        #print(f'\\n1. label: {label} | reference: {reference} | string_pesquisa{string_pesquisa}\\n') \n",
    "        \n",
    "        raw_value = next((doc[start:end].text for match_id, start, end in matches if nlp.vocab.strings[match_id] == label), None)\n",
    "        #print(f'\\n2. reference: {reference} | raw_value: {raw_value}\\n')\n",
    "        \n",
    "        most_similar_reference = max([reference], key=lambda x: similar(x, raw_value))\n",
    "        #print(f'\\n3. most_similar_reference: {most_similar_reference} \\n')\n",
    "        \n",
    "        if debug:\n",
    "            print(f'\\nmost_similar_reference: {most_similar_reference}\\n')\n",
    "        final_value = raw_value.split(\":\", 1)[-1].strip()\n",
    "        data_box_valores[label] = final_value\n",
    "        #print(f'i: {i} | file_name: {original_file_name}  | label: {label} | reference: {reference} | raw_value: {raw_value} | most_similar_reference: {most_similar_reference}  \\n\\n')\n",
    "        i += 1\n",
    "    \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "\n",
    "def mapeia_prestador(idx, row, row_info, doc_content, doc, matches, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    \n",
    "    texto = doc_content.get(idx, {}).get('content', 'valor_padrao')\n",
    "\n",
    "    # 2. PRESTADOR DE SERVIÇO\n",
    "    prestador_end_char = [ent.end_char for ent in doc.ents if ent.id_ == '2. PRESTADOR DE SERVIÇO'][0]\n",
    "    tomador_start_char = [ent.start_char for ent in doc.ents if ent.id_ == '3. TOMADOR DE SERVIÇO'][0]\n",
    "    text = texto[prestador_end_char:tomador_start_char]\n",
    "\n",
    "    # 1. CNPJ com e sem mascara- Prestador\n",
    "    match = re.search(r'CPF/CNPJ:\\s+(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "    if match:\n",
    "        p_cpf_cnpj_com_mascara = match.group(1).strip()\n",
    "        data_box_valores['p_cpf_cnpj_com_mascara'] = p_cpf_cnpj_com_mascara\n",
    "        #print(f\"p_cpf_cnpj_com_mascara: {p_cpf_cnpj_com_mascara}\")\n",
    "        p_cpf_cnpj_sem_mascara = re.sub(r'\\D', '', match.group(1))\n",
    "        data_box_valores['p_cpf_cnpj_sem_mascara'] = p_cpf_cnpj_sem_mascara\n",
    "        #print(f\"p_cpf_cnpj_sem_mascara: {p_cpf_cnpj_sem_mascara}\")\n",
    "        \n",
    "    # 2. Telefone Prestador\n",
    "    match = re.search(r\"Telefone:(.*?)Inscrição\", text)\n",
    "    if match:\n",
    "        p_telefone = match.group(1).strip()\n",
    "        data_box_valores['p_telefone'] = p_telefone\n",
    "        #print(f\"p_telefone: {p_telefone}\")\n",
    "        \n",
    "    # doc = nlp(p_telefone)  \n",
    "\n",
    "    # 3. Inscrição Municipal - Prestador\n",
    "    match = re.search(r\"Inscrição Municipal:(.*?)Telefone:\", text)\n",
    "    if match:\n",
    "        p_inscricao_municipal = match.group(1).strip()\n",
    "        data_box_valores['p_inscricao_municipal'] = p_inscricao_municipal\n",
    "        #print(f\"p_inscricao_municipal: {p_inscricao_municipal}\")\n",
    "        \n",
    "    # doc = nlp(p_inscricao_municipal)    \n",
    "\n",
    "\n",
    "    # 4. Para o campo Inscrição Estadual PRESTADOR - Neste caso ele acho a correspondencia mas nao extraiu o valor pois nao ha valor\n",
    "    match = re.search(r\"Inscrição Estadual:(.*?)Nome/Razão Social:\", text)\n",
    "    if match:\n",
    "        p_inscricao_estadual = match.group(1).strip()\n",
    "        data_box_valores['p_inscricao_estadual'] = p_inscricao_estadual \n",
    "    else:\n",
    "        p_inscricao_estadual = None\n",
    "        data_box_valores['p_inscricao_estadual'] = p_inscricao_estadual \n",
    "        \n",
    "    #print(f\"p_inscricao_estadual: {p_inscricao_estadual}\")   \n",
    "\n",
    "    # 5. Razao Social Prestador\n",
    "    match = re.search(r\"Nome/Razão Social:(.*?)Nome de Fantasia:\", text)\n",
    "    if match:\n",
    "        razao_social_prestador = match.group(1).strip()\n",
    "        data_box_valores['razao_social_prestador'] = razao_social_prestador\n",
    "        #print(f\"razao_social: {razao_social_prestador}\")\n",
    "        \n",
    "\n",
    "    # 6. Nome de Fantasia - Prestador\n",
    "    match = re.search(r\"Nome de Fantasia:(.*?)Endereço:\", text)\n",
    "    if match:\n",
    "        p_nome_fantasia = match.group(1).strip()\n",
    "        data_box_valores['p_nome_fantasia'] = p_nome_fantasia\n",
    "        #print(f\"p_nome_fantasia: {p_nome_fantasia}\")\n",
    "        \n",
    "\n",
    "    # 7. Endereço Prestador\n",
    "    match = re.search(r\"Endereço:(.*?)E-mail:\", text)\n",
    "    if match:\n",
    "        endereco_prestador = match.group(1).strip()\n",
    "        data_box_valores['endereco_prestador'] = endereco_prestador \n",
    "        #print(f\"endereco_prestador: {endereco_prestador}\")\n",
    "        \n",
    "\n",
    "    # 8. email: Prestador  - Neste caso ele acho a correspondencia mas nao extraiu o valor pois nao ha valor\n",
    "    match = re.search(r'E-mail:\\s+(.+)', text)\n",
    "    if match:\n",
    "        p_email = match.group(1).strip()\n",
    "        data_box_valores['p_email'] = p_email\n",
    "    else:\n",
    "        p_email = None\n",
    "        data_box_valores['p_email'] = p_email\n",
    "        \n",
    "\n",
    "    return data_box_valores \n",
    "\n",
    "\n",
    "\n",
    "def mapeia_tomador(idx, row, row_info, doc_content, doc, matches, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    \n",
    "    texto = doc_content.get(idx, {}).get('content', 'valor_padrao')\n",
    "    \n",
    "    # 3. TOMADOR DE SERVIÇO\n",
    "    tomador_end_char = [ent.end_char for ent in doc.ents if ent.id_ == '3. TOMADOR DE SERVIÇO'][0]\n",
    "    servicos_star_char = [ent.start_char for ent in doc.ents if ent.id_ == '4. DESCRIMINACAO DOS SERVIÇOS'][0]\n",
    "    text = texto[tomador_end_char:servicos_star_char]\n",
    "\n",
    "    # 1. CNPJ Tomador\n",
    "    match = re.search(r'CPF/CNPJ:\\s+(\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2})', text)\n",
    "    if match:\n",
    "        t_cpf_cnpj_com_mascara = match.group(1).strip()\n",
    "        data_box_valores['t_cpf_cnpj_com_mascara'] = t_cpf_cnpj_com_mascara\n",
    "        #print(f\"t_cpf_cnpj_com_mascara: {t_cpf_cnpj_com_mascara}\")\n",
    "        t_cpf_cnpj_sem_mascara = re.sub(r'\\D', '', match.group(1))\n",
    "        data_box_valores['t_cpf_cnpj_sem_mascara'] = t_cpf_cnpj_sem_mascara\n",
    "        #print(f\"t_cpf_cnpj_sem_mascara: {t_cpf_cnpj_sem_mascara}\")\n",
    "    \n",
    "    # 2. Inscrição Municipal TOMADOR - Neste caso ele acho a correspondencia mas nao extraiu o valor pois nao ha valor\n",
    "    match = re.search(r\"INSC:MUNICIPAL:(.*?)RG:\", text)\n",
    "    if match:\n",
    "        t_inscricao_municipal = match.group(1).strip()\n",
    "        data_box_valores['t_inscricao_municipal'] = t_inscricao_municipal\n",
    "    else:\n",
    "        t_inscricao_municipal = None\n",
    "        data_box_valores['t_inscricao_municipal'] = t_inscricao_municipal\n",
    "\n",
    "\n",
    "    #2.1 Inscrição Municipal TOMADOR - variacao de texto\n",
    "    match = re.search(r\"Inscrição Municipal:(.*?)RG:\", text)\n",
    "    if match:\n",
    "        t_inscricao_municipal = match.group(1).strip()\n",
    "        data_box_valores['t_inscricao_municipal'] = t_inscricao_municipal\n",
    "    else:\n",
    "        t_inscricao_municipal = None\n",
    "        data_box_valores['t_inscricao_municipal'] = t_inscricao_municipal\n",
    "\n",
    "\n",
    "    # 3.  Telefone TOMADOR - Neste caso ele acho a correspondencia mas nao extraiu o valor pois nao ha valor\n",
    "    match = re.search(r\"Telefone:(.*?)Inscrição Estadual:\", text)\n",
    "    if match:\n",
    "        t_telefone = match.group(1).strip()\n",
    "        data_box_valores['t_telefone'] = t_telefone\n",
    "    else:\n",
    "        t_telefone = None\n",
    "        data_box_valores['t_telefone'] = t_telefone\n",
    "        \n",
    "    # 4. RG TOMADOR - Neste caso ele acho a correspondencia mas nao extraiu o valor pois nao ha valor\n",
    "    match = re.search(r\"RG:(.*?)Telefone:\", text)\n",
    "    if match:\n",
    "        t_RG = match.group(1).strip()\n",
    "        data_box_valores['t_RG'] = t_RG\n",
    "    else:\n",
    "        t_RG = None\n",
    "        data_box_valores['t_RG'] = t_RG\n",
    "\n",
    "    # 5. Inscrição Estadual TOMADOR - Neste caso ele acho a correspondencia mas nao extraiu o valor pois nao ha valor\n",
    "    match = re.search(r\"Inscrição Estadual:(.*?)Nome/Razão Social:\", text)\n",
    "    if match:\n",
    "        t_inscricao_estadual = match.group(1).strip()\n",
    "        data_box_valores['t_inscricao_estadual'] = t_inscricao_estadual\n",
    "    else:\n",
    "        t_inscricao_estadual = None\n",
    "        data_box_valores['t_inscricao_estadual'] = t_inscricao_estadual\n",
    "\n",
    "    # 6. Nome/Razão Social TOMADOR - Neste caso ele acho a correspondencia mas nao extraiu o valor pois nao ha valor\n",
    "    match = re.search(r\"Nome/Razão Social:(.*?)Endereço:\", text)\n",
    "    if match:\n",
    "        t_nome_razao_social = match.group(1).strip()\n",
    "        data_box_valores['t_nome_razao_social'] = t_nome_razao_social\n",
    "    else:\n",
    "        t_nome_razao_social = None\n",
    "        data_box_valores['t_nome_razao_social'] = t_nome_razao_social\n",
    "    \n",
    "    # 7. Endereço: TOMADOR - Neste caso ele acho a correspondencia mas nao extraiu o valor pois nao ha valor\n",
    "    match = re.search(r\"Endereço:(.*?)E-mail:\", text)\n",
    "    if match:\n",
    "        t_endereco = match.group(1).strip()\n",
    "        data_box_valores['t_endereco'] = t_endereco\n",
    "    else:\n",
    "        t_endereco = None\n",
    "        data_box_valores['t_endereco'] = t_endereco\n",
    "        \n",
    "    # 8. email: TOMADOR - Neste caso ele acho a correspondencia mas nao extraiu o valor pois nao ha valor\n",
    "    match = re.search(r'E-mail:\\s+(.+)', text)\n",
    "    if match:\n",
    "        t_email = match.group(1).strip()\n",
    "        data_box_valores['t_email'] = t_email\n",
    "    else:\n",
    "        t_email = None\n",
    "        data_box_valores['t_email'] = t_email\n",
    "        \n",
    "\n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "\n",
    "def mapeia_campos_valores(idx, row, row_info, doc_content, doc, matches, section, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "    \n",
    "    # for match_id, start, end in matches:\n",
    "    #     string_id = nlp.vocab.strings[match_id]  # Obter a string de identificação\n",
    "    #     span = doc[start:end]  # Obter o trecho correspondente\n",
    "    #     #print(f\"{string_id}: {span.text}\")\n",
    "        \n",
    "    data_box_valores = {}\n",
    "    context_mapping = 'context_matcher'\n",
    "    tipo_4_filter = \"field_box\"\n",
    "    #text = doc_content.get(idx, {}).get('content', 'valor_padrao')\n",
    "    #print(texto)\n",
    "\n",
    "\n",
    "    # 8. Efetuo o filtro para a iteracao\n",
    "    filtered_frame_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['context_mapping'] == context_mapping) & (frames_nf_v4_df['type'] == tipo_4_filter)]\n",
    "\n",
    "    # 9. iter sobre o filtro\n",
    "    i = 1\n",
    "    for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "        section = row_frame['section_json']\n",
    "        label = row_frame['label']\n",
    "        reference = row_frame['reference']\n",
    "        string_pesquisa = row_frame['marcador_inicio'] \n",
    "        # if debug:\n",
    "        #print(f'\\n1. label: {label} | reference: {reference}\\n') \n",
    "        \n",
    "        raw_value = next((doc[start:end].text for match_id, start, end in matches if nlp.vocab.strings[match_id] == label), None)\n",
    "        #print(f'\\n2. reference: {reference} | raw_value: {raw_value}\\n')\n",
    "        try:\n",
    "            most_similar_reference = max([reference], key=lambda x: similar(x, raw_value))\n",
    "            \n",
    "            final_value = raw_value.split(\":\", 1)[-1].strip()\n",
    "            final_value = format_number2(final_value)\n",
    "            data_box_valores[label] = final_value\n",
    "            #print(f'i: {i} | file_name: {original_file_name}  | label: {label} | reference: {reference} | raw_value: {raw_value} | most_similar_reference: {most_similar_reference}  final_value: {final_value}\\n')\n",
    "        except Exception as e:\n",
    "            #print(f'Erro: reference: {reference} raw_value: {raw_value} | {e}')\n",
    "            most_similar_reference = None    \n",
    "        #print(f'\\n3. most_similar_reference: {most_similar_reference} \\n')\n",
    "        \n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "def mapeia_cnae_item(idx, row, row_info, doc_content, doc, matches, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    \n",
    "    texto = doc_content.get(idx, {}).get('content', 'valor_padrao')\n",
    "\n",
    "    # 6. CNAE e Item da Lista de Serviços\n",
    "    valor_cnae_start_char = [ent.start_char for ent in doc.ents if ent.id_ == '6. CNAE e Item da Lista de Serviços'][0]\n",
    "    valores_impostos_start_char = [ent.start_char for ent in doc.ents if ent.id_ == '7. VALORES E IMPOSTOS'][0]\n",
    "    text = texto[valor_cnae_start_char:valores_impostos_start_char]\n",
    "\n",
    "    match = re.search(r\"CNAE - (.*?)Item da Lista de Serviços\", text)\n",
    "    if match:\n",
    "        cnae_value = match.group(1).strip()\n",
    "        data_box_valores['cnae'] = cnae_value\n",
    "        #print(f\"cnae_value:{cnae_value}\")\n",
    "        #data_box_valores['t_telefone'] = t_telefone\n",
    "    else:\n",
    "        cnae_value = None\n",
    "        data_box_valores['cnae'] = cnae_value\n",
    "        #print(f\"cnae_value: {cnae_value}\")\n",
    "        #data_box_valores['t_telefone'] = t_telefone\n",
    "        \n",
    "    match = re.search(r'Item da Lista de Serviços -\\s+(.+)', text)\n",
    "    if match:\n",
    "        item_value = match.group(1).strip()\n",
    "        data_box_valores['item_lista_servicos'] = item_value\n",
    "        #print(f\"item_value:{item_value}\")\n",
    "        #data_box_valores['t_telefone'] = t_telefone\n",
    "    else:\n",
    "        item_value = None\n",
    "        data_box_valores['item_lista_servicos'] = item_value\n",
    "        #print(f\"item_value: {item_value}\")\n",
    "        #data_box_valores['t_telefone'] = t_telefone  \n",
    "        \n",
    "    return data_box_valores\n",
    "\n",
    "\n",
    "\n",
    "def mapeia_informacoes_criticas(idx, row, row_info, doc_content, doc, matches, section, mapping_method, context_mapping, pdf_pesquisavel_map, model_map, original_file_name, file_path, debug):\n",
    "\n",
    "    data_box_valores = {}\n",
    "    \n",
    "    texto = doc_content.get(idx, {}).get('content', 'valor_padrao')\n",
    "    \n",
    "    # 9. OUTRAS INFORMAÇOES / CRITICAS\n",
    "    outras_informacoes_end_char = [ent.end_char for ent in doc.ents if ent.id_ == '9. OUTRAS INFORMAÇOES / CRITICAS'][0]\n",
    "    observacoes_start_char = [ent.start_char for ent in doc.ents if ent.id_ == '10. OBSERVACOES'][0]\n",
    "    text = texto[outras_informacoes_end_char:observacoes_start_char]\n",
    "\n",
    "\n",
    "    match = re.search(r\" EXIGIBILIDADE ISS(.*?) REGIME TRIBUTAÇÃO\", text)\n",
    "    if match:\n",
    "        exigibilidade_iss_value = match.group(1).strip()\n",
    "        data_box_valores['exigibilidade_iss'] = exigibilidade_iss_value\n",
    "\n",
    "    else:\n",
    "        exigibilidade_iss_value = None\n",
    "        data_box_valores['exigibilidade_iss'] = exigibilidade_iss_value\n",
    "        \n",
    "    match = re.search(r\"REGIME TRIBUTAÇÃO(.*?) SIMPLES NACIONAL\", text)\n",
    "    if match:\n",
    "        regime_tributacao_value = match.group(1).strip()\n",
    "        data_box_valores['regime_tributacao'] = regime_tributacao_value\n",
    "\n",
    "    else:\n",
    "        regime_tributacao_value = None\n",
    "        data_box_valores['regime_tributacao'] = regime_tributacao_value\n",
    "\n",
    "        \n",
    "    match = re.search(r\"SIMPLES NACIONAL(.*?) ISSQN RETIDO\", text)\n",
    "    if match:\n",
    "        simples_nacional_value = match.group(1).strip()\n",
    "        data_box_valores['simples_nacional'] = simples_nacional_value\n",
    "\n",
    "    else:\n",
    "        simples_nacional_value = None\n",
    "        data_box_valores['simples_nacional'] = simples_nacional_value\n",
    "        \n",
    "    match = re.search(r\"ISSQN RETIDO(.*?) LOCAL. PRESTAÇÃO SERVIÇO\", text)\n",
    "    if match:\n",
    "        local_prestacao_servico_value = match.group(1).strip()\n",
    "        data_box_valores['issqn_retido'] = local_prestacao_servico_value \n",
    "\n",
    "    else:\n",
    "        local_prestacao_servico_value = None\n",
    "        data_box_valores['issqn_retido'] = local_prestacao_servico_value   \n",
    "        \n",
    "    match = re.search(r\"LOCAL. PRESTAÇÃO SERVIÇO(.*?) LOCAL INCIDÊNCIA\", text)\n",
    "    if match:\n",
    "        local_prestacao_servico_value = match.group(1).strip()\n",
    "        data_box_valores['local_prestacao_servico'] = local_prestacao_servico_value\n",
    "\n",
    "    else:\n",
    "        local_prestacao_servico_value = None\n",
    "        data_box_valores['local_prestacao_servico'] = local_prestacao_servico_value  \n",
    "        \n",
    "    match = re.search(r'LOCAL INCIDÊNCIA\\s+(.+)', text)\n",
    "    if match:\n",
    "        local_incidencia_value = match.group(1).strip()\n",
    "        data_box_valores['local_incidencia'] = local_incidencia_value\n",
    "    else:\n",
    "        local_incidencia_value = None\n",
    "        data_box_valores['local_incidencia'] = local_incidencia_value   \n",
    "        \n",
    "\n",
    "    return data_box_valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>2.2</b> ExecuÇao do Pipeline de Extracao </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta e o principio da melhor funcao do mundo\n",
    "def extracao_pipeline(qualquer_df, doc_content, fase, atividade, status, debug=False, prestador=True, tomador=True, servicos=True, total=True, cnae=True, valores_impostos=True, complementares=True, outras_informacoes=True, observacoes=True):\n",
    "    \n",
    "    doc_info = {}\n",
    "    resumo = {}\n",
    "    row_teste_info = []\n",
    "    time_now = cron.timenow_pt_BR()\n",
    "    func_fase = fase\n",
    "    func_atividade = atividade\n",
    "    func_status = status\n",
    "    lista_dicts = []\n",
    "    conf_processo = {}\n",
    "    lista_conferencia = []\n",
    "   \n",
    "    i = 1\n",
    "    for idx, row in qualquer_df.iterrows():\n",
    "        dados_iniciais = {}\n",
    "        row_info = row.to_dict()\n",
    "        message_erro = []\n",
    "        # 1. Mapeamento de informacoes do DF\n",
    "        map_document_unique_id = idx\n",
    "        map_seq = row['seq']\n",
    "        map_batch_name = row['batch']\n",
    "        map_fase_processo = row['fase_processo']\n",
    "        map_nome_atividade = row['nome_atividade']\n",
    "        map_status_documento = row['status_documento']\n",
    "        map_original_file_name = row['original_file_name']\n",
    "        map_directory = row['directory']\n",
    "        map_one_page = row['one_page']\n",
    "        map_palavra_chave = row['palavra_chave']\n",
    "        map_document_tag = row['document_tag']\n",
    "        map_action_item = row['action_item']\n",
    "        map_pdf_pesquisavel = row['pdf_pesquisavel']\n",
    "        map_level = row['level']\n",
    "        file_path = row['file_path']\n",
    "        row_info['document_unique_id'] = map_document_unique_id\n",
    "    \n",
    "        # XXX Nivel 1 - Definindo que documentos serao tratados   \n",
    "        if map_status_documento == 'PREPROCESS_EXTRACT':\n",
    "            print(f'seq: {i:>3} | {batch_name} | idx: {idx} | pdf pesq.: {map_pdf_pesquisavel} | status_documento: {map_status_documento} | processando doc: {map_original_file_name} ')\n",
    "            # 1. Buscando o texto do documento pelo doc_content\n",
    "            texto_dict = doc_content.get(map_document_unique_id, {}).get('content', 'valor_padrao')\n",
    "            \n",
    "            # 2. XXX IMPORTANTE - Efetuo a busca de entidades e efetuo a tokenizaÇao do documento\n",
    "            doc, tokens, ents = show_ent_new(texto_dict, patterns=patterns)\n",
    "            matches = matcher(doc)\n",
    "                    \n",
    "            prefeitura_map = [ent.orth_ for ent in doc.ents if ent.label_ == \"nome_prefeitura\"][0]\n",
    "            de_para_pm = [ent.id_ for ent in doc.ents if ent.label_ == \"nome_prefeitura\"][0]\n",
    "            secretaria_map = [ent.orth_ for ent in doc.ents if ent.label_ == \"secretaria\"][0]\n",
    "            tipo_documento_map = [ent.orth_ for ent in doc.ents if ent.label_ == \"tipo_documento\"][0] \n",
    "         \n",
    "            f_type = 'document'\n",
    "            result = utl.filtrar_df(frames_nf_v4_df, type=f_type, de_para_pm=de_para_pm)\n",
    "            model_map = result['model'].values[0]\n",
    "            if debug:\n",
    "                print(f'\\nresult: {result}\\n')\n",
    "            \n",
    "            section = \"1. CABECALHO\"\n",
    "            valores = {}\n",
    "            mapping_method = \"frame_&_sframe_field\" # significa que as coordenadas estao em frames e os valores dos campos nos sframe_fields\n",
    "            context_mapping = \"data_cabecalho\"\n",
    "            def_replace = True\n",
    "            \n",
    "            if map_pdf_pesquisavel == False:\n",
    "                imagem_gray, image_resized_name = convert_resize_gray(map_original_file_name, file_path, image_resized_path)\n",
    "                imagem_gray_rgb = imagem_gray.convert(\"RGB\")\n",
    "                imagem_gray_np = np.array(imagem_gray_rgb)\n",
    "                doc_content[map_document_unique_id]['image_np'] = imagem_gray_np\n",
    "\n",
    "                # vou tratar o texto do cabecalho\n",
    "                texto_cabecalho_PDF_Raster = extrac_cabecalho_R_PDF(idx, row, row_info, doc_content, section, mapping_method, context_mapping, map_pdf_pesquisavel, model_map, map_original_file_name, file_path, debug)\n",
    "                #print(f'\\n1. texto_cabechalho_PDF_Raster: {texto_cabechalho_PDF_Raster}\\n')\n",
    "                # Ajusto o texto todo\n",
    "                if texto_cabecalho_PDF_Raster:\n",
    "                    texto_recomposto = ajusta_texto_Raster_P(idx, row, map_document_unique_id, texto_dict, texto_cabecalho_PDF_Raster)\n",
    "                    if debug:\n",
    "                        print(f'\\n2. texto_dict: {texto_recomposto}\\n')\n",
    "                    # Atualizao do doc_content\n",
    "                    doc_content[map_document_unique_id]['content'] = texto_recomposto\n",
    "                    texto_dict = texto_recomposto    \n",
    "            \n",
    "            #matcher = Matcher(nlp.vocab)\n",
    "            matches = matcher(doc)\n",
    "            \n",
    "            # XXX Rotina para carregar e atribuir ao matcher os patterns do disco\n",
    "            matcher_pattern_path = tipo_documento_dict.get(tipo_doc_work, {}).get('matcher_pattern_path', 'valor_padrao')\n",
    "            # XXX Carregar matcher patterns do disco\n",
    "            with open(matcher_pattern_path, \"r\") as f:\n",
    "                loaded_patterns = json.load(f)\n",
    "                \n",
    "            # Adicionar ao Matcher\n",
    "            for label, pattern in loaded_patterns.items():\n",
    "                matcher.add(label, [pattern])  \n",
    "\n",
    "            doc, tokens, ents = show_ent_new(texto_dict, patterns=patterns)\n",
    "            matches = matcher(doc)\n",
    "            \n",
    "            # 3. XXX Mapeamento dados do cabecalho\n",
    "            valores = mapeia_cabecalho(idx, row, row_info, doc_content, doc, matches, section, mapping_method, context_mapping, map_pdf_pesquisavel, model_map, map_original_file_name, file_path, debug)\n",
    "            \n",
    "            if debug:\n",
    "                print(f'\\nvalores: {valores}\\n')\n",
    "            \n",
    "            row_info['model'] = model_map\n",
    "            row_info['de_para_pm'] = de_para_pm\n",
    "            row_info['tipo_nota_fiscal'] = tipo_documento_map\n",
    "            row_info['secretaria'] = secretaria_map\n",
    "            row_info['prefeitura'] = prefeitura_map\n",
    "            if debug:\n",
    "                print(f'1. row_info: {row_info}\\n')\n",
    "            \n",
    "            row_info.update(valores)\n",
    "            information_row_info = \"Este e apenas um comeco - mas bem comeco mesmo\"\n",
    "            action_item_row_info = 'CONTINUE_PROCESS'\n",
    "\n",
    "            # 4. XXX Mapeamento dados do prestador\n",
    "            valores = {}\n",
    "            valores = mapeia_prestador(idx, row, row_info, doc_content, doc, matches, section, mapping_method, context_mapping, map_pdf_pesquisavel, model_map, map_original_file_name, file_path, debug)\n",
    "            if valores:\n",
    "                row_info.update(valores)\n",
    "                \n",
    "            # 5. XXX Mapeamento dados do tomador    \n",
    "            valores = {}    \n",
    "            valores = mapeia_tomador(idx, row, row_info, doc_content, doc, matches, section, mapping_method, context_mapping, map_pdf_pesquisavel, model_map, map_original_file_name, file_path, debug)\n",
    "            if valores:\n",
    "                row_info.update(valores)\n",
    "            \n",
    "            # 4. DESCRIMINACAO DOS SERVIÇOS\n",
    "            texto = doc_content.get(idx, {}).get('content', 'valor_padrao')\n",
    "            servicos_end_char = [ent.end_char for ent in doc.ents if ent.id_ == '4. DESCRIMINACAO DOS SERVIÇOS'][0]\n",
    "            valor_total_start_char = [ent.start_char for ent in doc.ents if ent.id_ == '5. VALOR TOTAL'][0]\n",
    "            descr_servicos = texto[servicos_end_char:valor_total_start_char]\n",
    "            row_info['discriminacao_servicos'] = descr_servicos     \n",
    "\n",
    "            matches = matcher(doc)\n",
    "            \n",
    "            # XXX Rotina para carregar e atribuir ao matcher os patterns do disco\n",
    "            matcher_pattern_path = tipo_documento_dict.get(tipo_doc_work, {}).get('matcher_pattern_path', 'valor_padrao')\n",
    "\n",
    "            # XXX Carregar matcher patterns do disco\n",
    "            with open(matcher_pattern_path, \"r\") as f:\n",
    "                loaded_patterns = json.load(f)\n",
    "                \n",
    "            # Adicionar ao Matcher\n",
    "            for label, pattern in loaded_patterns.items():\n",
    "                matcher.add(label, [pattern])  \n",
    "\n",
    "            doc, tokens, ents = show_ent_new(texto_dict, patterns=patterns)\n",
    "            matches = matcher(doc)\n",
    "            \n",
    "            # 6. Mapeia cnae e Itens\n",
    "            valores = {}    \n",
    "            valores = mapeia_cnae_item(idx, row, row_info, doc_content, doc, matches, section, mapping_method, context_mapping, map_pdf_pesquisavel, model_map, map_original_file_name, file_path, debug)\n",
    "            if valores:\n",
    "                row_info.update(valores)\n",
    "            \n",
    "            \n",
    "            # 7. Mapeia valores\n",
    "            valores = {}                     \n",
    "            valores = mapeia_campos_valores(idx, row, row_info, doc_content, doc, matches, section, map_pdf_pesquisavel, model_map, map_original_file_name, file_path, debug)\n",
    "            if valores:\n",
    "                row_info.update(valores)\n",
    "            \n",
    "            \n",
    "            # 8. DADOS COMPLEMENTARES\n",
    "            dados_complementares_end_char = [ent.end_char for ent in doc.ents if ent.id_ == '8. DADOS COMPLEMENTARES'][0]\n",
    "            outras_informacoes_start_char = [ent.start_char for ent in doc.ents if ent.id_ == '9. OUTRAS INFORMAÇOES / CRITICAS'][0]\n",
    "            dados_complementares = texto[dados_complementares_end_char:outras_informacoes_start_char]\n",
    "            row_info['dados_complementares'] = dados_complementares\n",
    "            \n",
    "           # 9. Mapeia Informaçoes criticas\n",
    "            valores = {}    \n",
    "            valores = mapeia_informacoes_criticas(idx, row, row_info, doc_content, doc, matches, section, mapping_method, context_mapping, map_pdf_pesquisavel, model_map, map_original_file_name, file_path, debug)\n",
    "            if valores:\n",
    "                row_info.update(valores)     \n",
    "                \n",
    "            # 9. OUTRAS INFORMAÇOES / CRITICAS\n",
    "            outras_informacoes_end_char = [ent.end_char for ent in doc.ents if ent.id_ == '9. OUTRAS INFORMAÇOES / CRITICAS'][0]\n",
    "            observacoes_start_char = [ent.start_char for ent in doc.ents if ent.id_ == '10. OBSERVACOES'][0]\n",
    "            observacao = texto[outras_informacoes_end_char:observacoes_start_char]\n",
    "            row_info['observacao'] = observacao\n",
    "            \n",
    "            \n",
    "            lista_dicts.append(row_info)\n",
    "            i += 1\n",
    "            # continue\n",
    "\n",
    "        \n",
    "        elif map_status_documento == 'NO_PROCESS' or map_status_documento == 'root_analise':\n",
    "            msg = (f'Documento nao sera tratado neste escopo: {map_batch_name} | {map_original_file_name} | diretorio: {map_directory}')\n",
    "            row_info['action_item'] = \"NO_PROCESS\"    \n",
    "            row_info['informations'] = msg \n",
    "            lista_dicts.append(row_info)\n",
    "            #print(f'\\nprocessando: batch: {batch_name} | seq: {i} | file_name: {map_original_file_name:>40} | idx: {idx} - Nao sera processado')\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "    #logging.info(f'processamento finalizado para: {batch_name}') \n",
    "    print(f'\\n\\nprocessamento de {i - 1} documentos')\n",
    "    novo_df = pd.DataFrame(lista_dicts)\n",
    "  \n",
    "    return novo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analisar_pdf_pesquisavel\n",
    "fase = 'analise'\n",
    "atividade = 'PREPROCESS' \n",
    "status = 'PREPROCESS_EXTRACT'\n",
    "lista_dicts = []\n",
    "#logging.info(f'Execuçao do pipeline para {batch_name} | df_root_pipe: {file_path_root_pipe} fase: {fase} atividade: {atividade} status: {status}  template: {ver}')\n",
    "# 1. Processar somente dados iniciais e cabeçalho\n",
    "df_result_pipe = extracao_pipeline(df_root_pipe, doc_content, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=False, cnae=False, valores_impostos=False, complementares=False, outras_informacoes=False, observacoes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. XXX Volto novamente o indice do DF IMPORTANTE\n",
    "df_result_pipe.set_index('document_unique_id', inplace=True)\n",
    "\n",
    "ordem_status = ['PREPROCESS_EXTRACT', 'NO_PROCESS', 'root_analise']\n",
    "ordem_action_item = ['CONTINUE_PROCESS', 'BREAK_PROCESS', 'NO_PROCESS']\n",
    "\n",
    "df_result_pipe['status_documento'] = pd.Categorical(df['status_documento'], categories=ordem_status, ordered=True)\n",
    "df_result_pipe['action_item'] = pd.Categorical(df['action_item'], categories=ordem_action_item, ordered=True)\n",
    "\n",
    "df_result_pipe.sort_values(by=['status_documento', 'action_item', 'seq'], ascending=[True, True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_pipe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>2.3</b> Conferencia Extracao </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dados basicos do documento\n",
    "df_conf0 = df_result_pipe[['batch', 'sigla_tipo', 'fase_processo', 'nome_atividade', 'nome_atividade', 'acao_executada', 'original_file_name', 'directory', 'pdf_pesquisavel', 'score', 'pages', 'document_tag', 'action_item']]\n",
    "df_conf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Conferencia 1\n",
    "df_conf1 = df_result_pipe[['original_file_name', 'directory', 'pdf_pesquisavel','model','tipo_nota_fiscal','secretaria','prefeitura', 'numero_nota_fiscal', 'competencia', 'dt_hr_emissao','codigo_verificacao']]\n",
    "df_conf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Conferencia 2\n",
    "df_conf2 = df_result_pipe[['original_file_name', 'directory', 'pdf_pesquisavel','p_cpf_cnpj_com_mascara', 'p_cpf_cnpj_sem_mascara', 'p_telefone', 'p_inscricao_municipal', 'p_inscricao_estadual', 'razao_social_prestador', 'p_nome_fantasia', 'endereco_prestador', 'p_email']]\n",
    "df_conf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Conferencia 3\n",
    "df_conf3 = df_result_pipe[['original_file_name', 'directory', 'pdf_pesquisavel','t_cpf_cnpj_com_mascara', 't_cpf_cnpj_sem_mascara', 't_inscricao_municipal', 't_telefone', 't_RG', 't_inscricao_estadual', 't_nome_razao_social', 't_endereco', 't_email']]\n",
    "df_conf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Conferencia 4\n",
    "df_conf4 = df_result_pipe[['original_file_name', 'directory', 'pdf_pesquisavel','discriminacao_servicos', 'valor_total_nota', 'cnae','item_lista_servicos']]\n",
    "df_conf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Conferencia 5\n",
    "df_conf5 = df_result_pipe[['original_file_name', 'directory', 'pdf_pesquisavel','valor_servicos', 'valor_deducao', 'base_calculo','aliquota', 'valor_iss', 'valor_iss_retido', 'desc_cond', 'valor_pis', 'valor_cofins', 'valor_ir', 'valor_inss', 'valor_csll','outras_retencoes', 'valor_liquido']]\n",
    "df_conf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Conferencia 6\n",
    "df_conf6 = df_result_pipe[['original_file_name', 'directory', 'pdf_pesquisavel','dados_complementares','exigibilidade_iss', 'regime_tributacao', 'simples_nacional', 'issqn_retido', 'local_prestacao_servico', 'local_incidencia', 'observacao']]\n",
    "df_conf6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Definiçao do path para salvar o arquivo\n",
    "file_path_df_result_pipe = os.path.join(map_analise_path, 'df_result_pipe_' + batch_name + \".xlsx\")\n",
    "\n",
    "# 2. XXX Salvando o arquivo de df: df_result_pipe\n",
    "df_result_pipe.to_excel(file_path_df_result_pipe, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> <b>2.4</b> Exportacao json </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 XXX. Leio a planilha de conferencia avalidada\n",
    "file_path_df_result_pipe = os.path.join(map_analise_path, 'df_result_pipe_' + batch_name + \".xlsx\")\n",
    "\n",
    "\n",
    "#Le a planilha e cria do DF\n",
    "df_validated_pipe = pd.read_excel(file_path_df_result_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Crio subset somente dos documentos para exportar\n",
    "df_conf_validada = utl.filtrar_df(df_validated_pipe, action_item=\"EXPORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Ajusto o Index\n",
    "df_conf_validada.set_index('document_unique_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Dados para formataÇao do Json\n",
    "de_para_pm = df_conf_validada['de_para_pm'].values[0]\n",
    "municipio = df_conf_validada['municipio'].values[0]\n",
    "arquivo_zip = df_conf_validada['parent_file'].values[0]\n",
    "data_processamento = cron.timenow_pt_BR()\n",
    "nome_formado_json = batch_name +\".json\"\n",
    "titulo = (f'Processamento {batch_name} - {de_para_pm} - data:{data_processamento}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Crio o JSON \n",
    "dados_json = {}\n",
    "\n",
    "# Iterar sobre cada linha no DataFrame\n",
    "for index, row in df_conf_validada.iterrows():\n",
    "    # dados_df e o dicionario para armazenar os dados da nota fiscal atual\n",
    "    #diretorio = str(row['directory'])\n",
    "    dados_nf = {\n",
    "            \"dados_NF_PDF\": {\n",
    "                                \"data_cabecalho\": {\n",
    "                                    \"secao\": \"1 - CABECALHO\",\n",
    "                                    \"nome_prefeitura\": row['prefeitura'],\n",
    "                                    \"numero_nota_fiscal\": row['numero_nota_fiscal'],\n",
    "                                    \"competencia\": row['competencia'],\n",
    "                                    \"dt_hr_emissoa\": row['dt_hr_emissao'],\n",
    "                                    \"codigo_verificacao\": row['codigo_verificacao']\n",
    "                                },\n",
    "                                \"data_prestador\": {\n",
    "                                    \"secao\": \"2. PRESTADOR DE SERVIÇO\",\n",
    "                                    \"cpf_cnpj_com_mascara\": row['p_cpf_cnpj_com_mascara'],\n",
    "                                    \"cpf_cnpj_sem_mascara\": row['p_cpf_cnpj_sem_mascara'],\n",
    "                                    \"inscricao_municipal\": row['p_inscricao_municipal'],\n",
    "                                    \"inscricao_estadual\": row['p_inscricao_estadual'],\n",
    "                                    \"telefone\": row['p_telefone'],\n",
    "                                    \"razao_social\": row['razao_social_prestador'],\n",
    "                                    \"nome_fantasia\": row['p_nome_fantasia'],\n",
    "                                    \"endereco\": row['endereco_prestador'],\n",
    "                                    \"email\": row['p_email']\n",
    "                                },\n",
    "                                \"data_tomador\": {\n",
    "                                    \"secao\": \"3. TOMADOR DE SERVIÇO\",\n",
    "                                    \"cpf_cnpj_com_mascara\": row['t_cpf_cnpj_com_mascara'],\n",
    "                                    \"cpf_cnpj_sem_mascara\": row['t_cpf_cnpj_sem_mascara'],\n",
    "                                    \"rg\": row['t_RG'],\n",
    "                                    \"inscricao_municipal\": row['t_inscricao_municipal'],\n",
    "                                    \"inscricao_estadual\": row['t_inscricao_estadual'],\n",
    "                                    \"telefone\": row['t_telefone'],\n",
    "                                    \"razao_social\": row['t_nome_razao_social'],\n",
    "                                    \"endereco\": row['t_endereco'],\n",
    "                                    \"email\": row['t_email']\n",
    "                                },\n",
    "                                \"data_servico\": {\n",
    "                                    \"secao\": \"4. DESCRIMINACAO DOS SERVIÇOS\",\n",
    "                                    \"discriminacao_servicos\": row['discriminacao_servicos']\n",
    "                                },\n",
    "                                \"data_valor_total\": {\n",
    "                                    \"secao\": \"5. VALOR TOTAL\",\n",
    "                                    \"valor_total_nota\": row['valor_total_nota']\n",
    "                                },\n",
    "                                \"data_CNAE\": {\n",
    "                                    \"secao\": \"6. CNAE e Item da Lista de Serviços\",\n",
    "                                    \"cnae\": row['cnae'],\n",
    "                                    \"item_lista_servicos\": row['item_lista_servicos']\n",
    "                                },\n",
    "                                \"data_valores\": {\n",
    "                                    \"secao\": \"7. VALORES E IMPOSTOS\",\n",
    "                                    \"valor_servicos\": row['valor_servicos'],\n",
    "                                    \"valor_deducao\": row['valor_deducao'],\n",
    "                                    \"desc_incond\" : row['desc_incond'],\n",
    "                                    \"base_calculo\": row['base_calculo'],\n",
    "                                    \"aliquota\": row['aliquota'],\n",
    "                                    \"valor_iss\": row['valor_iss'],\n",
    "                                    \"valor_iss_retido\": row['valor_iss_retido'],\n",
    "                                    \"desc_cond\": row['desc_cond'],\n",
    "                                    \"valor_pis\": row['valor_pis'],\n",
    "                                    \"valor_cofins\": row['valor_cofins'],\n",
    "                                    \"valor_ir\": row['valor_ir'],\n",
    "                                    \"valor_inss\": row['valor_inss'],\n",
    "                                    \"valor_csll\": row['valor_csll'],\n",
    "                                    \"outras_retencoes\": row['outras_retencoes'],\n",
    "                                    \"valor_liquido\": row['valor_liquido']\n",
    "                                },\n",
    "                                \"data_dados_complementares\": {\n",
    "                                    \"secao\": \"8. DADOS COMPLEMENTARES\",\n",
    "                                    \"dados_complementares\": row['dados_complementares']\n",
    "                                },\n",
    "                                \"data_outras_informacoes\": {\n",
    "                                    \"secao\": \"9. OUTRAS INFORMAÇOES / CRITICAS\",\n",
    "                                    \"exigibilidade_iss\": row['exigibilidade_iss'],\n",
    "                                    \"regime_tributacao\": row['regime_tributacao'],\n",
    "                                    \"simples_nacional\": row['simples_nacional'],\n",
    "                                    \"issqn_retido\": row['issqn_retido'],\n",
    "                                    \"local_prestacao_servico\": row['local_prestacao_servico'],\n",
    "                                    \"local_incidencia\": row['local_incidencia']\n",
    "                                },\n",
    "                                \"data_observacao\": {\n",
    "                                    \"secao\": \"10. OBSERVACOES\",\n",
    "                                    \"observacao\": row['observacao']\n",
    "                                },\n",
    "                            },\n",
    "                            \"batch\": row['batch'],    \n",
    "                            \"diretorio\": str(row['directory']),\n",
    "                            \"nome_arquivo\": row['original_file_name'],\n",
    "                            \"pdf_pesquisavel\": row['pdf_pesquisavel'],\n",
    "                            \"modelo\": row['model'],   \n",
    "                            \"document_unique_id\": index,\n",
    "                            \"parent_file\": row['parent_file'],\n",
    "                    }        \n",
    "            \n",
    "    \n",
    "    numero_nota_fiscal = str(row['numero_nota_fiscal'])\n",
    "    dados_json['titulo'] = titulo\n",
    "    dados_json['batch'] = batch_name\n",
    "    dados_json['municipio'] = municipio\n",
    "    # dados_json['data_processamento'] = cron.timenow_pt_BR()\n",
    "    dados_json[numero_nota_fiscal] = dados_nf\n",
    "\n",
    "# Salvando em formato JSON\n",
    "json_file_path = os.path.join(json_path, nome_formado_json)\n",
    "with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dados_json, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "print(f\"As informações foram salvas em {json_file_path}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XXX Definiçao do path para salvar o arquivo\n",
    "file_path_df_conf_export = os.path.join(map_analise_path, 'df_conf_export_' + batch_name + \".xlsx\")\n",
    "\n",
    "# 2. XXX Salvando o arquivo de df: df_result_pipe\n",
    "df_validated_pipe.to_excel(file_path_df_conf_export, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf_validada['de_para_pm'].values[0]\n",
    "df_conf_validada['municipio'].values[0]\n",
    "df_conf_validada['parent_file'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['coluna_cnae'] = df['coluna_cnae'].apply(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf_avalida_batch_21['numero_nota_fiscal'] = df_conf_avalida_batch_21['numero_nota_fiscal'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustes Matchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX 1.Processar todas as secoes do documento\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=True, tomador=True, servicos=True, total=True, cnae=True, valores_impostos=True, complementares=True, outras_informacoes=True, observacoes=True)\n",
    "\n",
    "\n",
    "\n",
    "# 5. Processar valor Total\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=True, cnae=False, valores_impostos=False, complementares=False, outras_informacoes=False, observacoes=False)\n",
    "\n",
    "# 6. Processar CNAE\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=False, cnae=True, valores_impostos=False, complementares=False, outras_informacoes=False, observacoes=False)\n",
    "\n",
    "# 7. Processar Impostos\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=False, cnae=False, valores_impostos=, complementares=False, outras_informacoes=False, observacoes=False)\n",
    "\n",
    "# 8. complementar e observaçoes\n",
    "#df = extracao_pipeline(df_root_pipe, fase, atividade, status, debug=False, prestador=False, tomador=False, servicos=False, total=False, cnae=False, valores_impostos=False, complementares=True, outras_informacoes=True, observacoes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)\n",
    "\n",
    "# Exibir os resultados\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Obter a string de identificação\n",
    "    span = doc[start:end]  # Obter o trecho correspondente\n",
    "    print(f\"{string_id:>30} | {span.text:>50} | {span.start_char:>5}  {span.end_char:>5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_value = next((doc[start:end].text for match_id, start, end in matches if nlp.vocab.strings[match_id] == label), None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_reference = max([reference], key=lambda x: similar(x, raw_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root_pipe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_unique_id = \"ab2457b7-ea5c-4191-acf0-bc8edc04879e\"\n",
    "file_path = \"pipeline_extracao_documentos/2_documentos_para_extracao/21_aguardando_processamento/Batch_23/MESQUITA_PDF_31282023_2258/teste/1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. DESCRIMINACAO DOS SERVIÇOS\n",
    "servicos_end_char = [ent.end_char for ent in doc.ents if ent.id_ == '4. DESCRIMINACAO DOS SERVIÇOS'][0]\n",
    "valor_total_start_char = [ent.start_char for ent in doc.ents if ent.id_ == '5. VALOR TOTAL'][0]\n",
    "bloco_discriminacao_servico = texto[servicos_end_char:valor_total_start_char]\n",
    "bloco_discriminacao_servico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_unique_id = 'ab2457b7-ea5c-4191-acf0-bc8edc04879e'\n",
    "texto = doc_content.get(document_unique_id, {}).get('content', 'valor_padrao')\n",
    "doc = nlp(texto)\n",
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. OUTRAS INFORMAÇOES / CRITICAS\n",
    "outras_informacoes_end_char = [ent.end_char for ent in doc.ents if ent.id_ == '9. OUTRAS INFORMAÇOES / CRITICAS'][0]\n",
    "observacoes_start_char = [ent.start_char for ent in doc.ents if ent.id_ == '10. OBSERVACOES'][0]\n",
    "text = texto[outras_informacoes_end_char:observacoes_start_char]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. OUTRAS INFORMAÇOES / CRITICAS\n",
    "def extract_fields_outras_info(text):\n",
    "    nf_data_outras_informacoes = {}\n",
    "    #nf_data_outras_informacoes['secao'] = \"9. OUTRAS INFORMAÇOES / CRITICAS\"\n",
    "    \n",
    "    # Extrair EXIGIBILIDADE ISS:\n",
    "    exigibilidade_iss_match = re.search(r'EXIGIBILIDADE ISS\\s+(.+)', text)\n",
    "    if exigibilidade_iss_match:\n",
    "        exigibilidade_iss_value = exigibilidade_iss_match.group(1).strip()\n",
    "        nf_data_outras_informacoes['exigibilidade_iss'] = exigibilidade_iss_value\n",
    "        \n",
    "    # Extrair REGIME TRIBUTAÇÃO:\n",
    "    regime_tributacao_match = re.search(r'REGIME TRIBUTAÇÃO\\s+(.+)', text)\n",
    "    if regime_tributacao_match:\n",
    "        regime_tributacao_value = regime_tributacao_match.group(1).strip()\n",
    "        nf_data_outras_informacoes['regime_tributacao'] = regime_tributacao_value\n",
    "    \n",
    "    # Extrair SIMPLES NACIONAL:\n",
    "    simples_nacional_match = re.search(r'SIMPLES NACIONAL\\s+(.+)', text)\n",
    "    if simples_nacional_match:\n",
    "        simples_nacional_value = simples_nacional_match.group(1).strip()\n",
    "        nf_data_outras_informacoes['simples_nacional'] = simples_nacional_value\n",
    "        \n",
    "        \n",
    "    # Extrair ISSQN RETIDO:\n",
    "    local_prestacao_servico_match = re.search(r'ISSQN RETIDO\\s+(.+)', text)\n",
    "    if local_prestacao_servico_match:\n",
    "        local_prestacao_servico_value = local_prestacao_servico_match.group(1).strip()\n",
    "        nf_data_outras_informacoes['issqn_retido'] = local_prestacao_servico_value        \n",
    "        \n",
    "    \n",
    "    # Extrair LOCAL PRESTAÇÃO SERVIÇO:\n",
    "    local_prestacao_servico_match = re.search(r'LOCAL\\. PRESTAÇÃO\\s+SERVIÇO\\s+(.+)', text)\n",
    "    if local_prestacao_servico_match:\n",
    "        local_prestacao_servico_value = local_prestacao_servico_match.group(1).strip()\n",
    "        nf_data_outras_informacoes['local_prestacao_servico'] = local_prestacao_servico_value\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Extrair LOCAL INCIDÊNCIA:\n",
    "    local_incidencia_match = re.search(r'LOCAL INCIDÊNCIA\\s+(.+)', text)\n",
    "    if local_incidencia_match:\n",
    "        local_incidencia_value = local_incidencia_match.group(1).strip()\n",
    "        nf_data_outras_informacoes['local_incidencia'] = local_incidencia_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outras_ref = ['EXIGIBILIDADE ISS', 'REGIME TRIBUTAÇÃO', 'SIMPLES NACIONAL', 'ISSQN RETIDO', 'LOCAL. PRESTAÇÃO SERVIÇO', 'LOCAL INCIDÊNCIA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novo_texto = \"EXIGIBILIDADE ISS\\nExigivel\\nREGIME TRIBUTAÇÃO\\nSociedade Limitada\\nSIMPLES NACIONAL\\nSim (2,01% )\\nISSQN RETIDO\\nNão\\nLOCAL. PRESTAÇÃO SERVIÇO\\nMagé - RJ\\nLOCAL INCIDÊNCIA\\nMagé - RJ\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_outras_informacoes = {}\n",
    "data_outras_informacoes = novaextra.extract_fields_outras_info(novo_texto)\n",
    "data_outras_informacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_outras_informacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. OBSERVACOES\n",
    "observacoes_end_char = [ent.end_char for ent in doc.ents if ent.id_ == '10. OBSERVACOES'][0]\n",
    "encerrador_start_char = [ent.start_char for ent in doc.ents if ent.label_ == 'encerrador'][0]\n",
    "bloco_observacoes = texto[observacoes_end_char:encerrador_start_char]\n",
    "bloco_observacoes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisys\n",
    "syntatic = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"T_texto\",\"T_shape\", \"T_is_alpha\", \"T_is_digit\", \"T_is_title\", \"T_is_punct\", \"T_is_sent_start\", \"T_is_right_punct\", \"T_is_stop\", \"T_is_quote\", \"T_is_currency\", \"T_morph\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    syntatic.loc[i,\"id\"] = token.i\n",
    "    syntatic.loc[i,\"T_texto\"] = token.text\n",
    "    syntatic.loc[i,\"T_shape\"] = token.shape_\n",
    "    syntatic.loc[i,\"T_is_alpha\"] = token.is_alpha\n",
    "    syntatic.loc[i,\"T_is_digit\"] = token.is_digit\n",
    "    syntatic.loc[i,\"T_is_title\"] = token.is_title\n",
    "    syntatic.loc[i,\"T_is_punct\"] = token.is_punct\n",
    "    syntatic.loc[i,\"T_is_sent_start\"] = token.is_sent_start\n",
    "    syntatic.loc[i,\"T_is_right_punct\"] = token.is_right_punct\n",
    "    syntatic.loc[i,\"T_is_stop\"] = token.is_stop\n",
    "    syntatic.loc[i,\"T_is_quote\"] = token.is_quote\n",
    "    syntatic.loc[i,\"T_is_currency\"] = token.is_currency\n",
    "    syntatic.loc[i,\"T_morph\"] = token.morph\n",
    "    i = i+1\n",
    "\n",
    "syntatic.head(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posições de início e fim\n",
    "start_pos = 881\n",
    "end_pos = 1520\n",
    "\n",
    "# Extraindo a sub-string usando slicing\n",
    "extracted_text = texto[start_pos:end_pos]\n",
    "\n",
    "print(f\"Texto extraído: {extracted_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bloco_discriminacao_servico = original_text[servicos_end_char:valor_total_start_char]\n",
    "bloco_discriminacao_servico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "servicos_end = [ent.end for ent in doc.ents if ent.id_ == '4. DESCRIMINACAO DOS SERVIÇOS'][0]\n",
    "servicos_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_total_start = [ent.start for ent in doc.ents if ent.id_ == '5. VALOR TOTAL'][0]\n",
    "valor_total_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bloco_discriminacao_servico = texto[servicos_end:valor_total_start]\n",
    "bloco_discriminacao_servico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte do mapeamento do cabeÇalho para Raster_PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_unique_id = 'ab2457b7-ea5c-4191-acf0-bc8edc04879e'\n",
    "texto_R_PDF = doc_content.get(document_unique_id, {}).get('content', 'valor_padrao')\n",
    "texto_R_PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_R_PDF = Novo_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc, tokens, ents = show_ent_new(texto_R_PDF, patterns=patterns)\n",
    "\n",
    "displacy.render(doc, style=\"ent\", options={\"colors\": colors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = {}\n",
    "section = \"2. PRESTADOR DE SERVIÇO\"\n",
    "mapping_method = \"frame_&_sframe_field\" # significa que as coordenadas estao em frames e os valores dos campos nos sframe_fields\n",
    "context_mapping = \"data_prestador\"\n",
    "model_map = \"MESQUITA\"\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscando a razao social - Prestador\n",
    "label = \"p_razao_social\"\n",
    "end_chars_razao_prestado = [token.idx + len(token) for token in doc if token.ent_id_ == label][4]\n",
    "end_chars_razao_prestado = end_chars_razao_prestado + 1\n",
    "label = \"p_nome_fantasia\"\n",
    "star_chars_fantasia = [token.idx for token in doc if token.ent_id_ == label][0]\n",
    "star_chars_fantasia = star_chars_fantasia - 1\n",
    "razao_social_texto = texto_R_PDF[end_chars_razao_prestado:star_chars_fantasia]\n",
    "razao_social_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ini_char = min(end_chars)\n",
    "end_char = max(end_chars)\n",
    "\n",
    "fatiado = doc.text[ini_char:end_char]\n",
    "\n",
    "print(f'ini_char: {ini_char} | end_char: {end_char} | fatiado: {fatiado} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_box_valores = {}\n",
    "if mapping_method == \"frame_&_sframe_field\":\n",
    "    tipo_4_coordinates = \"frame\"\n",
    "    tipo_4_filter = \"sframe_field\"\n",
    "\n",
    "# 8. Efetuo o filtro para a iteracao\n",
    "filtered_frame_nf_v4_df = frames_nf_v4_df[(frames_nf_v4_df['model'] == model_map) & (frames_nf_v4_df['context_mapping'] == context_mapping) & (frames_nf_v4_df['type'] == tipo_4_filter)]\n",
    "\n",
    "# 9. iter sobre o filtro\n",
    "for index_frame, row_frame in filtered_frame_nf_v4_df.iterrows():\n",
    "    section = row_frame['section_json']\n",
    "    label = row_frame['label']\n",
    "    reference = row_frame['reference']\n",
    "    string_pesquisa = row_frame['marcador_inicio'] \n",
    "    # last_token_end = 0\n",
    "    if label == \"p_inscricao_municipal\":\n",
    "        last_token_end = 48\n",
    "        new_token_start = 160\n",
    "        print(f'label: {label}\\n')\n",
    "        \n",
    "        end_chars = [token.idx + len(token) for token in doc if token.ent_id_ == label]\n",
    "        ini_char = min(end_chars)\n",
    "        end_char = max(end_chars)\n",
    "        \n",
    "        fatiado = doc.text[ini_char:end_char]\n",
    "        \n",
    "        print(f'ini_char: {ini_char} | end_char: {end_char} | fatiado: {fatiado} \\n')\n",
    "        \n",
    "        syntatic = pd.DataFrame(data=[], \\\n",
    "        columns=[\"id\", \"T_texto\", \"T_idx\", \"T_start_char\", \"T_end_char\", \"T_shape\", \"T_ent_id_\", \"T_ent_type_\"])\n",
    "        \n",
    "        i = 0\n",
    "        for token in doc[last_token_end:new_token_start]:\n",
    "            \n",
    "            start_char = token.idx\n",
    "            end_char = start_char + len(token)\n",
    "            syntatic.loc[i,\"id\"] = token.i\n",
    "            syntatic.loc[i,\"T_texto\"] = token.text\n",
    "            syntatic.loc[i,\"T_idx\"] = token.idx\n",
    "            syntatic.loc[i,\"T_start_char\"] = start_char \n",
    "            syntatic.loc[i,\"T_end_char\"] = end_char\n",
    "            syntatic.loc[i,\"T_shape\"] = token.shape_\n",
    "            syntatic.loc[i,\"T_ent_id_\"] = token.ent_id_\n",
    "            syntatic.loc[i,\"T_ent_type_\"] = token.ent_type_\n",
    "            token_end_char = end_char\n",
    "\n",
    "\n",
    "            i = i+1\n",
    "        last_token_end = ent.end_char    \n",
    "\n",
    "syntatic.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = \"1. CABECALHO\"\n",
    "valores = {}\n",
    "mapping_method = \"frame_&_sframe_field\" # significa que as coordenadas estao em frames e os valores dos campos nos sframe_fields\n",
    "context_mapping = \"data_cabecalho\"\n",
    "def_replace = True\n",
    "model_map = \"MAGE\"\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'numero_nota'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_value = next((doc[start:end].text for match_id, start, end in matches if nlp.vocab.strings[match_id] == label), None)\n",
    "raw_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recomposicao do documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_box_valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_cabechalho_PDF_Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_c = nlp(texto_cabechalho_PDF_Raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_unique_id = '27df9e70-b5fb-45b7-8f59-0c04ed9728e2'\n",
    "texto_R_PDF = doc_content.get(document_unique_id, {}).get('content', 'valor_padrao')\n",
    "texto_R_PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_unique_id = 'ab2457b7-ea5c-4191-acf0-bc8edc04879e'\n",
    "texto_PDF_P = doc_content.get(document_unique_id, {}).get('content', 'valor_padrao')\n",
    "texto_PDF_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc, tokens, ents = show_ent_new(texto_PDF_P, patterns=patterns)\n",
    "\n",
    "displacy.render(doc, style=\"ent\", options={\"colors\": colors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc_c)\n",
    "\n",
    "# Exibir os resultados\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Obter a string de identificação\n",
    "    span = doc[start:end]  # Obter o trecho correspondente\n",
    "    print(f\"{string_id}: {span.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.start:>5} | {ent.text:>50} | {ent.label_:>25} | {ent.id_:>35}  |   {ent.end:>4}   ||   {ent.start_char:>6} | {ent.end_char:>6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seu texto OCR completo\n",
    "original_text = texto_R_PDF\n",
    "original_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc, tokens, ents = show_ent_new(original_text, patterns=patterns)\n",
    "\n",
    "displacy.render(doc, style=\"ent\", options={\"colors\": colors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc, tokens, ents = show_ent_new(recomposed_text, patterns=patterns)\n",
    "\n",
    "displacy.render(doc, style=\"ent\", options={\"colors\": colors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.start:>5} | {ent.text:>50} | {ent.label_:>25} | {ent.id_:>35}  |   {ent.end:>4}   ||   {ent.start_char:>6} | {ent.end_char:>6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texto_amostra = \"NFS-e Número da Nota: 20234 Competência: Julho/2023\"\n",
    "texto_amostra = \"'outras informações / criticas'\"\n",
    "\n",
    "doc = nlp(texto_amostra)\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Exibir os resultados\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Obter a string de identificação\n",
    "    span = doc[start:end]  # Obter o trecho correspondente\n",
    "    print(f\"{string_id}: {span.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisys\n",
    "syntatic = pd.DataFrame(data=[], \\\n",
    "  columns=[\"id\", \"T_texto\",\"T_shape\", \"T_is_alpha\", \"T_is_digit\", \"T_is_title\", \"T_is_punct\", \"T_is_sent_start\", \"T_is_right_punct\", \"T_is_stop\", \"T_is_quote\", \"T_is_currency\", \"T_morph\"])\n",
    "i = 0\n",
    "for token in doc:\n",
    "    syntatic.loc[i,\"id\"] = token.i\n",
    "    syntatic.loc[i,\"T_texto\"] = token.text\n",
    "    syntatic.loc[i,\"T_shape\"] = token.shape_\n",
    "    syntatic.loc[i,\"T_is_alpha\"] = token.is_alpha\n",
    "    syntatic.loc[i,\"T_is_digit\"] = token.is_digit\n",
    "    syntatic.loc[i,\"T_is_title\"] = token.is_title\n",
    "    syntatic.loc[i,\"T_is_punct\"] = token.is_punct\n",
    "    syntatic.loc[i,\"T_is_sent_start\"] = token.is_sent_start\n",
    "    syntatic.loc[i,\"T_is_right_punct\"] = token.is_right_punct\n",
    "    syntatic.loc[i,\"T_is_stop\"] = token.is_stop\n",
    "    syntatic.loc[i,\"T_is_quote\"] = token.is_quote\n",
    "    syntatic.loc[i,\"T_is_currency\"] = token.is_currency\n",
    "    syntatic.loc[i,\"T_morph\"] = token.morph\n",
    "    i = i+1\n",
    "\n",
    "syntatic.head(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = re.sub('\\s+', ' ', texto_OCR_R).strip()\n",
    "\n",
    "clean_text = text.replace(': ', ':').replace(', ', ',')\n",
    "\n",
    "clean_text = re.sub('\\s+', ' ', text.replace(': ', ':').replace(', ', ',')).strip()\n",
    "\n",
    "text_splited = texto.split('\\n')\n",
    "text_splited = [x for x in text_splited if x.strip()]\n",
    "text_splited = [s.replace(\";\", \"\").strip() for s in text_splited]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tables-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
